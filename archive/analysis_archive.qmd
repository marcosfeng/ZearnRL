---
title: "analysis_archive"
format: pdf
---

# The Kernel Function

The kernel function is a mathematical tool used to measure the similarity between different states or actions [@ormoneit2002; @domingues; @liu]. In the context of the Q-learning model with states and a kernel, the kernel function is used to compute a weighted average of the rewards obtained in similar state-action pairs in the past. This allows the model to generalize from past experience and to make more informed decisions about future actions.

We anticipate that the reward associated with a given action may be delayed, reflecting the time it takes for the effects of an action to manifest. For instance, teachers might spend the first week of each month planning activities on the platform, with the rewards of these actions becoming apparent in subsequent weeks. To capture this temporal aspect of teacher behavior, we employ a kernel function that incorporates a measure of similarity between the current and past state-action pairs, modulated by a discount factor that accounts for the delay in reward. The kernel function is defined as the Jaccard similarity as follows:

$$
K(t, t') = 0.5 \cdot \text{I}(a_t = a_{t'}) \cdot \left[ 1 + \text{I}(s_t = s_{t'})\right]
$$

where:

-   $K(t, t')$ is the kernel function,
-   $t$ and $t'$ are the current and past time steps (weeks), respectively,
-   $\text{I}(\cdot)$ is the indicator function, which equals 1 if the condition inside the parentheses is true and 0 otherwise,
-   $a_t$ and $a_{t'}$ are the choices made at the current and past time steps, respectively,
-   $s_t$ and $s_{t'}$ are the states at the current and past time steps, respectively.

## Kernel-Based Reinforcement

We use the kernel function to compute a weighted average of past rewards (kernel reward), which updates the Q-value for the current state-action pair. The kernel reward is:

$$
R_K = \frac{\sum_{t' = 1}^{T} K(t, t') \cdot \gamma^{(t - t')}  \text{Badges}_{t'}}{\sum_{t' = 1}^{T} K(t, t')}
$$

where:

-   $R_K$ is the kernel reward,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_{t'}$ is the number of badges earned at the past time step $t'$,
-   $T$ is the number of lags in the kernel.

Subsequently, the Q-value for the current state-action pair is updated:

$$
Q(s, a) = Q(s, a) + \alpha \left( R_K - \text{cost}(a) - Q(s, a) \right)
$$




## Autoencoder

```{python train autoencoder}
#| eval: false
import os # To fix: https://github.com/tensorflow/tensorflow/issues/59779
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from keras.models import Model
from keras.layers import Input, Dense
from kerastuner.tuners import Hyperband
from kerastuner.engine.hyperparameters import HyperParameters
from keras.constraints import NonNeg
from keras.callbacks import EarlyStopping
from keras.regularizers import l1, l2

# Create training and testing data frames
train_df = dfpca_py[dfpca_py['set'] == 'train']
test_df = dfpca_py[dfpca_py['set'] == 'test']

# Predictors
X_train = train_df.drop([
  'Badges.per.Active.User', 'Classroom.ID', 'MDR.School.ID',
  'set', 'week'
  ], axis=1)
X_test = test_df.drop([
  'Badges.per.Active.User', 'Classroom.ID', 'MDR.School.ID',
  'set', 'week'
  ], axis=1)

X_train = pd.DataFrame(X_train, columns=X_cols)
X_test = pd.DataFrame(X_test, columns=X_cols)

# Get the target variable
Y = dfpca_py[['Badges.per.Active.User']]
Y_train = train_df[['Badges.per.Active.User']]
Y_test = test_df[['Badges.per.Active.User']]

# Define the number of components and features
n_features = X_scaled.shape[1]
n_labels = 1  # For regression, we usually have just one output node

# Determine loss weights according to the data structure:
decoding_weight = Y.std() / (Y.std() + X_scaled.std(numeric_only=True).mean())
prediction_weight = 1 - decoding_weight
def build_model(hp):
  # Define the input layer
  input_data = Input(shape=(n_features,))
  
  # Define the encoding layer(s)
  n_layers = hp.Int('n_layers', min_value=1, max_value=4, step=1)
  n_units = [
    hp.Choice('units_' + str(i), values=[8, 16, 32, 64, 128, 256, 512])
    for i in range(n_layers)
  ]
  encoded = input_data
  for i in range(n_layers):
      encoded = Dense(
        units=n_units[i],
        kernel_regularizer=l2(0.001),
        activation='relu')(encoded)
        
  # Generate the latent vector
  latent_dim = hp.Int('encoding_units', min_value=2, max_value=n_comp, step=1)
  l1_value = hp.Float('l1_value', min_value=0.0001, max_value=0.001, default=0.0005, step=0.0001)
  latent = Dense(
    units=latent_dim,
    activation='linear',
    # activity_regularizer= L1(l1=l1_value),
    kernel_constraint=NonNeg(),
    name='latent')(encoded)      

  # Decoder
  decoded = latent
  for i in range(n_layers):
      decoded = Dense(
        units=n_units[n_layers - i - 1],
        activation='relu')(decoded)
  decoded = Dense(n_features, activation='sigmoid', name='decoded')(decoded)
  
  # Define the label output layer
  label_output = Dense(n_labels, activation='linear', name='label_output')(latent)

  # Define the autoencoder model
  autoencoder = Model(inputs=input_data, outputs=[decoded, label_output])
  # Compile the model
  autoencoder.compile(optimizer='adadelta',
                  loss={
                    'decoded': 'mean_squared_error',
                    'label_output': 'mean_squared_error'
                  },
                  loss_weights={
                    'decoded': decoding_weight,
                    'label_output': prediction_weight
                  })
                  
  return autoencoder

# Set up hyperparameter tuner
tuner = Hyperband(build_model,
                  objective='val_loss',
                  max_epochs=20,
                  directory='autoencoder_tuning',
                  project_name='autoencoder_3rd_tuning')

# Perform hyperparameter search
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)
tuner.search(x=X_train,
            y=[X_train, Y_train],
            epochs=50,
            validation_data=(X_test, [X_test, Y_test]),
            callbacks=[early_stopping_callback])
# tuner.results_summary()
# Get the optimal hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=2)[1]
model = tuner.hypermodel.build(best_hyperparameters)
history = model.fit(x=X_train, 
                    y=[X_train, Y_train],
                    epochs=2_000,
                    validation_data=(X_test, [X_test, Y_test]),
                    callbacks=[early_stopping_callback])
best_model = model


# Function to get encoded representation and components
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

# Get encoded representation and components
get_encoded_representation_and_components(best_model, X_scaled)

# save the model
model.save('./autoencoder_tuning/final_model.h5')

```

```{python load autoencoder}
#| eval: false
from tensorflow.keras.models import load_model
from keras.models import Model
from keras.layers import Input

# Function to get encoded representation and components
loaded_model = load_model('./autoencoder_tuning/final_model.h5')
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

n_features = X_scaled.shape[1]
# Get encoded representation and components
get_encoded_representation_and_components(loaded_model, X_scaled)
```


# State-Based Q-Learning Model

In this model, the state $s$ represents the current situation or context in which the teacher decides. The state could include factors such as the students' current performance levels. The state-dependent model is particularly relevant in teaching, where the effectiveness of a given strategy may depend on the specific circumstances of the class. Here, the Q-value for a given state $s$ and action $a$ is:

$$ Q(s, a) = Q(s, a) + \alpha \left( \gamma (\text{Badges}_t - \text{cost}(a)) - Q(s, a) \right) $$

where:

-   $\alpha$ is the learning rate,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_t$ is the immediate reward received after taking action $a$ in state $s$,
-   $\text{cost}(a)$ is the cost associated with action $a$,
-   $Q(s, a)$ is the current estimate of the Q-value for action $a$ in state $s$.

Like the state-free model, the softmax function determines the probability of choosing a particular action.

$$
\begin{align*}
\text{Action}_t =& \ \sum_{i=1}^{L} \left( \beta_{i} \text{Reward}_{t-i} + \gamma_i \text{Action}_{t-i} + \sum_{j=i}^{L} \delta_{ij} (\text{Reward}_{t-i} \times \text{Action}_{t-j}) \right) \\
& + \phi \text{State}_t + \psi (\text{State}_t \times \text{Action}_{t-1}) + \mu_{\text{Teacher}} + \lambda_{\text{Week}} + \epsilon_t
\end{align*}
$$

where $\text{Action}_t$ denotes the binary outcome at time $t$, $\text{Reward}_{t-i}$ and $\text{Action}_{t-i}$ represent the reward and action variables lagged by $i$ periods, and $L$ is the maximum lag considered. $\mu_{\text{Teacher}}$ and $\lambda_{\text{Week}}$ represent fixed effects for teachers and weeks, respectively.

# Regression Analysis of Teacher Characteristics

```{r}
#| label: tbl-school-characteristics
#| eval: false

library(fixest)

school_df <- do.call(rbind, lapply(filtered_results, function(item) {
  # Merge with badges data
  temp <- item$coef %>%
    right_join(badges_df, by = "Teacher.User.ID") %>%
    mutate(auc_logit = log(auc_out/(1-auc_out))) %>%
    select(Teacher.User.ID, MDR.School.ID, auc_logit, logLik_ind,
           school.account, charter.school, income, poverty) %>%
    mutate(state = ifelse(is.null(item$State), "None", item$State),
           reward = item$Reward,
           action = item$Method)
})) %>%
  mutate(across(c(state, reward, action), factor))

reg_school <- feols(auc_logit ~ i(poverty, "40-75% (Mid-High)") +
                      school.account + charter.school |
                      state^reward^action,
                 data = school_df)

Xpower <- sapply(levels(school_df$income), 
               function(x)ifelse(school_df$income==x, 1, 0))
colnames(Xpower) <- c(levels(school_df$income))
Xpower <- Xpower %*% contr.poly(length(levels(school_df$income)))[,1:3]
colnames(Xpower) <- c("income.L", "income.Q", "income.C")
school_df <- cbind(school_df, Xpower)
reg_school <- feols(auc_logit ~ income.L + income.Q + income.C + 
                      school.account + charter.school |
                      state^reward^action,
                 data = school_df)

school_df <- school_df %>%
  mutate(low_pov = if_else(poverty == "0-40% (Low)", 1, 0),
         mid_pov = if_else(poverty == "40-75% (Mid-High)", 1, 0),
         high_pov = if_else(poverty == "75-100% (High)", 1, 0))

reg_school <- feols(auc_logit ~ i(low_pov, state:reward:action) +
                      i(mid_pov, state:reward:action) +
                      school.account + charter.school |
                      state^reward^action,
                 data = school_df)

iplot(reg_school)

```

```{r re-estimation}
#| eval: false

create_RL_formula <- function(action, reward, state = NULL, lag = 1,
                              group = "Teacher.User.ID") {
  re_terms <- c()
  
  for (i in 1:lag) {
    re_terms <- c(re_terms, paste0(reward, "_", i), paste0(action, "_", i))
    # Add the interaction term for Reward_i * action_i
    re_terms <- c(re_terms, paste0(reward, "_", i, ":", action, "_", i))
    if (i != lag) {
      for (j in (i + 1):lag) {
        # Add the interaction for Reward_i * action_j when i < lag
        re_terms <- c(re_terms, paste0(reward, "_", i, ":", action, "_", j))
      }
    }
  }
  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(re_terms, collapse = " + "), " + ",
                             "(", paste(re_terms, collapse = " + "), "|",
                             group, ") + ", "week_lag")
    return(as.formula(formula_string))
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(re_terms, collapse = " + "), " + ",
                             "(", paste(re_terms, collapse = " + "), " + ",
                             state, " + ", paste0(state, ":", action, "_1"), "|",
                             group, ") + ", "week_lag")
  }
  
  return(as.formula(formula_string))
}


# Estimation
params <- params %>%
  filter(st == "Frobenius.NNDSVD_student3",
         grepl("1|2|4", rwd),
         act != "Minutes.on.Zearn...Total")

cl <- makeCluster(min(detectCores(), nrow(params)))
registerDoParallel(cl)
top_results <- foreach(i = 1:(nrow(params)),
                   .multicombine = TRUE,
                   .noexport = c("formula", "model"),
                   .export = ls(),
                   .packages = "lme4") %dopar% {
  act <- as.character(params$act[i])
  lag <- params$lag[i]
  rwd <- as.character(params$rwd[i])
  st  <- as.character(params$st[i])
  formula <- create_RL_formula(act, rwd, st, lag,
                               group = "Teacher.User.ID")

  # Return the results as a list
  list(Action = act,
       Reward = rwd,
       State = st,
       Lag = lag,
       equation = as.character(formula),
       model <- tryCatch(
         glmer(formula, data = train_data, family = binomial(link = "logit"),
               control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e3))),
         error = function(e) { return(NULL) }
         )
       )
}
# Stop the cluster
stopCluster(cl)
rm(cl)

save(top_results, file = "me-top-results.RData")

```

```{r}
#| label: fig-RL-refined
#| fig-cap: ""
#| eval: false

library(lme4)

load("me-top-results.RData")

# Function to create density plot
create_density_plot <- function(data, cols, title) {
  quantiles <- do.call(rbind, lapply(cols, function(col) {
    quantile(data %>% filter(Term == col) %>% pull(Coefficient), c(0.05, 0.95))
  }))
  bounds <- range(quantiles, na.rm = TRUE)
  
  ggplot() +
    geom_density(data = data,
                 aes(x = Coefficient, fill = Term),
                 alpha = 0.6,
                 adjust = 2,
                 bounds = bounds) +
    theme_minimal() +
    theme(
      panel.grid = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank(),
      axis.title.x = element_blank(),
      legend.title = element_blank(),
      plot.title = element_blank(),
      legend.position = "none"
    )
}

# Generating plots for each model
plot_list <- lapply(results, function(object) {
  model <- object[[6]]
  
  if (!is.null(model)) {
    re <- ranef(model)$Teacher.User.ID
    re <- re[,-1] # Remove the first column (Intercept)
    
    # Separate state and reward effects
    state_cols <- grep(object$State, names(re), value = TRUE)
    reward_cols <- setdiff(names(re), state_cols)
    
    # Reshape the data to a long format for plotting
    re_long_state <- pivot_longer(re[,state_cols], 
                                  cols = state_cols, 
                                  names_to = "Term", 
                                  values_to = "Coefficient")
    re_long_reward <- pivot_longer(re[,reward_cols], 
                                   cols = reward_cols, 
                                   names_to = "Term", 
                                   values_to = "Coefficient")
    
    # Create loess density plots
    plot_state <- create_density_plot(re_long_state, state_cols, "State Effects")
    plot_reward <- create_density_plot(re_long_reward, reward_cols, "Reward Effects")
    
    return(list(StatePlot = plot_state, RewardPlot = plot_reward))
  }
})

names(plot_list) <- lapply(top_results, function(object) {
  name <- paste(object$Action, " ~ ",
                "R", object$Reward,
                " S", object$State, sep = "")
  name <- gsub("Frobenius.NNDSVD_", "", name)
  name <- gsub("teacher", "IC", name)
  name <- gsub("student", "", name)
})

```


# Pooled Correlations between Components and Student Outcomes

We meta-analyzed individual correlations within our dataset to reveal the pooled relationships between student outcomes (badges) and the components from the dimensionality reduction procedure. This approach, termed 'internal meta-analysis,' is advantageous when dealing with large datasets with hierarchical structures, as it allows for simultaneously considering multiple, potentially correlated outcomes.

To conduct this internal meta-analysis, we first transformed the correlations using Fisher's z-transformation, ensuring a normal distribution of the correlations. We then ran a random effects model with each unique combination of "Teacher" and "School." This multivariate meta-analysis offers the advantage of modeling multiple, potentially correlated outcomes, providing a comprehensive estimate of the correlations for each outcome, considering the hierarchical structure of the data. The outcome of this process is a robust understanding of the relationships between different outcomes and the Badges across diverse schools and teachers. This approach provides a more resilient analysis than simple correlations, as it accounts for the inherent variability and dependencies within the data. @fig-meta-analysis displays the results and illustrates the pooled correlations between the components and student outcomes.

```{r Meta-correlation prep}
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
# Importing results from Python
results_list <- py$results
gc(verbose = FALSE)
```

```{r}
#| cache: true
#| label: fig-meta-analysis
#| fig-cap: "Results of the correlation meta-analysis."
library(metafor)
library(ppcor)

n_comp <- 3
method <- "FrobeniusNNDSVD"
selected_cols <- c("Classroom.ID", "Teacher.User.ID",
                   "MDR.School.ID", "District.Rollup.ID",
                   "week", "date",
                   # Main loadings of Components:
                   "tch_min", "tch_min_1",
                   paste0("FrobeniusNNDSVD", seq_len(n_comp)),
                   paste0("FrobeniusNNDSVD", seq_len(n_comp), "_1"),
                   "RD.optional_problem_sets",
                   "Guided.Practice.Completed", "Tower.Completed",
                   # Student Variables
                   "Active.Users...Total", "Minutes.per.Active.User",
                   "Badges.per.Active.User", "Boosts.per.Tower.Completion",
                   "Tower.Alerts.per.Tower.Completion",
                   # Classroom and Teacher Variables
                   "teacher_number_classes", "Grade.Level",
                   "Students...Total", "n_weeks",
                   # School Variables
                   "poverty", "income", "charter.school",
                   "school.account", "zipcode")  # Column to select

df_corr <- df %>%
  ungroup() %>%
  arrange(Classroom.ID, week) %>%
  dplyr::select(all_of(selected_cols)) %>%
  rename_with(~paste0("Frobenius", seq_len(n_comp)),
              paste0("FrobeniusNNDSVD", seq_len(n_comp))) %>%
  rename_with(~paste0("Frobenius", seq_len(n_comp), "_1"),
              paste0("FrobeniusNNDSVD", seq_len(n_comp), "_1"))

# Define a safe version of pcor.test that returns NA when there's an error
safe_pcor <- possibly(~pcor.test(..1, ..2, ..3, method = "spearman")$estimate,
                      otherwise = NA)
df_corr <- df_corr %>%
  group_by(Classroom.ID,Teacher.User.ID,MDR.School.ID,District.Rollup.ID) %>%
  summarise(
    n = n(),
    Frobenius1 = safe_pcor(Frobenius1, Badges.per.Active.User, Frobenius1_1),
    Frobenius2 = safe_pcor(Frobenius2, Badges.per.Active.User, Frobenius2_1),
    Frobenius3 = safe_pcor(Frobenius3, Badges.per.Active.User, Frobenius3_1),
    Minutes    = safe_pcor(tch_min, Badges.per.Active.User, tch_min_1),
    n_weeks = mean(n_weeks),
    teacher_number_classes = mean(teacher_number_classes),
    poverty = first(poverty),
    income = first(income),
    school.account = mean(school.account)
  ) %>%
  filter(!is.na(Frobenius1) &
           !is.na(Frobenius2) &
           !is.na(Frobenius3))

df_corr_sub <- df_corr %>%
  as_tibble() %>%
  slice_sample(prop = 0.1) %>%
  mutate_at(vars(paste0("Frobenius", seq_len(n_comp)), "Minutes"),
            list(~atanh(.))) %>%  # Fisher's z transformation
  mutate_at(vars(paste0("Frobenius", seq_len(n_comp)), "Minutes"),
            list(se = ~sqrt(1/(n - 2 - 2)))) %>%  # standard error sqrt(1/N−2−g)
  gather(key = "outcome", value = "correlation",
         c(paste0("Frobenius", seq_len(n_comp)), "Minutes")) %>%
  gather(key = "outcome_se", value = "se",
         c(paste0("Frobenius", seq_len(n_comp), "_se"), "Minutes_se")) %>%
  filter(str_replace(outcome, "_se", "") == str_replace(outcome_se, "_se", "")) %>%
  dplyr::select(-"outcome_se") %>%
  filter(!is.na(se))

# Run multivariate meta-analysis
res <- rma.mv(yi = correlation,
              V = se^2,
              random = ~ 1 | Classroom.ID/Teacher.User.ID/MDR.School.ID/District.Rollup.ID,
              mods = ~ -1 + outcome,
              data = df_corr_sub)

# Add columns for back-transformed effect sizes and their standard errors
res_df <- data.frame(
  estimate = coef(res),
  outcome = str_replace(names(coef(res)), "outcome", ""),
  # se = sqrt(diag(vcov(res)))
  ci.lb = res$ci.lb,
  ci.ub = res$ci.ub
  ) %>%
  mutate(
    estimate_r = tanh(estimate),
    ci.lb = tanh(ci.lb),
    ci.ub = tanh(ci.ub)
    # se_r = se / (1 - estimate^2)  # delta method for SE
  )
# Reverse the Fisher's Z transformation
df_corr_sub <- df_corr_sub %>%
  mutate(correlation = tanh(correlation))
# Create the plot
ggplot(df_corr_sub, aes(x = correlation, y = after_stat(density))) +
  geom_histogram(binwidth = 0.1, fill = "lightblue") +
  geom_density(aes(weight = weights(res)),
               alpha = 0.5) +
  geom_vline(data = res_df,
             aes(xintercept = estimate_r,
                 color = outcome),
             linetype = "dashed") +
  geom_segment(data = res_df,
               aes(x = ci.lb,
                   y = 0,
                   xend = ci.ub,
                   yend = 0,
                   color = outcome),
               linewidth = 1.5) +
  geom_text(data = res_df,
            aes(x = estimate_r,
                y = 0.15,
                label = sub('0\\.', '.', round(estimate_r, 2))),
            color = "black",
            size = 3.5,
            check_overlap = TRUE) +
  facet_wrap(~outcome, scales = "free",  ncol = 2) +
  labs(x = "Correlation", y = "Density") +
  theme_light() +
  scale_color_discrete(name = "Pooled effects \n (with 95% C.I.)")
  theme(legend.position = c(0.85, 0.24),
        legend.direction = "vertical")  # Set legend position and direction

```

```{r meta-analysis summary}
#| eval: false
summary(res)
```


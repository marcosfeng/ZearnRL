---
title: "Unveiling the Adaptive Pedagogy: Reinforcement Learning Models Illuminate Teacher Decision-Making in an Online Math-Teaching Platform"
abstract: "In the rapidly evolving landscape of education, understanding the decision-making process of teachers is crucial. Based on a sample of over 2,000 classrooms from the online math-teaching platform Zearn, this study leverages reinforcement learning (RL) algorithms to model how teachers learn and adapt pedagogical choices. We conceptualize the teacher's role as a multi-armed bandit problem, where teachers balance their weekly effort against the number of lessons their students complete. This exploration-exploitation trade-off is dynamic, with teachers continually learning and adapting their strategies. Our findings reveal that teachers who favor exploration tend to be more adaptive and responsive to student needs, leading to a more personalized and effective learning experience. In contrast, those who lean towards exploitation often rely on tried-and-true methods, resulting in consistent but potentially less innovative teaching strategies. This work underscores the potential of RL in providing insights into human behavior in real-world settings, with significant implications for policy and practice in education."
keywords: "Reinforcement Learning, Pedagogical Decision-Making, Digital Education Platforms, Teacher Behavioral Dynamics, Empirical Field Data, Exploration-Exploitation Dilemma, Instructional Adaptation"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
format:
  elsevier-pdf:
    keep-tex: true
    include-in-header:
      - text: |
          \usepackage{dcolumn}
    journal:
      formatting: preprint
      model: 3p
      layout: onecolumn
      cite-style: number
      # graphical-abstract: "![](abstract.png)"
bibliography: zearnrefs.bib

execute:
  echo: false
  warning: false
  error: false
---

# Introduction

Predicting repeated behavior has been a long-standing goal of the behavioral sciences, including economics, psychology, and neuroscience. Much of human behavior results from stimulus-response associations, which are context-sensitive and not always consciously deliberated [@buyalskaya2023]. Reinforcement learning (RL) algorithms have emerged as a prominent way of quantifying these relationships, assigning a mathematical relationship between contextual cues (states), behavior (actions), and reward [@sutton2018]. These algorithms have found wide application in neuroscience and cognitive psychology, where they are used on data sets to model agents in specific environments. However, these disciplines generally do not work with the practical and applied data typically used in social psychology and economics [@buyalskaya2023].

This gap presents a novel opportunity to use methods from one set of disciplines on data traditionally used in another. In this paper, we aim to further this integration by applying RL algorithms to model the decision-making process of teachers in the math-teaching platform Zearn. Reinforcement learning (RL) provides a system of rewards and punishments where the agent (in this case, the teacher) learns to make optimal decisions by maximizing the rewards and minimizing the punishments. By assuming every teacher has an objective function to balance with their potential rewards, we model the sequential behavior of a teacher throughout a school year. For instance, the teacher chooses which pedagogical actions to employ, such as assigning homework, checking student progress, or reviewing content, in anticipation of enhancing student achievement. Applying the RL algorithm allows for flexibility in learning the best strategy given certain contextual information, revealing the intricate dynamics of the teaching-learning process. By modeling the tradeoff between teachers exploring unknown options and exploiting known information about the Zearn system, the RL algorithm provides a nuanced understanding of how teachers adapt their strategies in response to student performance and other contextual factors. This approach offers a flexible and robust model for our available data and opens new avenues for understanding and enhancing human behavior in field settings.

## The Zearn Platform

Zearn is a digital platform for mathematics education designed to facilitate the teaching and learning of mathematics, providing a rich environment for modeling the decision-making process of teachers. About 25% of elementary-school students and over 1 million middle-school students across the United States use Zearn [@post-weblog]. The platform's unique blend of hands-on teaching and immersive digital learning offers a fertile ground for understanding how teachers adapt their strategies to optimize student achievement.

Zearn's pedagogical approach includes interactive digital lessons using visual aids (see @fig-zearn-poster) and real-time student feedback. The platform's approach to mathematical concepts, such as fractions, is particularly noteworthy. Students go through a series of representations: concrete, pictorial, and abstract, each designed to scaffold (i.e., "breaking down" problems, see [@jumaat2014; @reiser2014]) their understanding and prepare them for subsequent levels. This approach aligns with the Common Core State Standards, enhancing relevance in the contemporary educational landscape.

![Example of Teaching with Visual Models on Zearn](images/zearn-poster.jpg){#fig-zearn-poster fig-align="center"}

The platform's structure facilitates a personalized learning experience for students (see SI for a screenshot of the student portal), allowing teachers to track student progress and make informed decisions (see @fig-class-report for a sample class report). This structure includes self-paced online lessons and small group instruction, a rotational model that allows students to learn new grade-level content in two ways: independently through engaging digital lessons and in small groups with their teacher and classmates. This dual approach enables students to learn at their own pace, fostering a sense of autonomy and self-directed learning.

![Sample Class Report](images/class-report.png){#fig-class-report fig-align="center"}

A key feature of Zearn is its badge system, which tracks student progress and motivates continued learning (see @fig-badges-screen). Students earn badges upon mastery of specific skills, providing a tangible representation of their achievement. This system motivates students and provides teachers with valuable data on student performance, informing their decision-making process [@knudsen2020]. Zearn also incorporates notifications, known as Tower Alerts, sent to teachers when a student struggles with a specific concept. This feature allows teachers to provide timely support and address learning gaps, enhancing the platform's capacity for personalized learning.

![Badge System for Student Achievement](images/badges.PNG){#fig-badges-screen fig-align="center"}

Another noteworthy aspect is the platform's professional development component, which is available for schools with a paid account (see SI for a sample training schedule). Teachers explore each unit or mission through word problems, fluencies, and small group lessons, conducting collaborative analysis of student work and problem-solving strategies. This professional development revolves around each mission's big mathematical idea, visual representations to scaffold learning, and strategies to address unfinished learning from prior grades and preparation for future learning [@morrison2019].

In order to model interesting behavioral patterns, we focus our analysis on the teachers who most likely take advantage of a wide range of resources on the platform. Thus, we select teachers who consistently use the platform and work in traditional school settings.

The platform's integrated approach to math teaching and learning, connecting a print- and software-based curriculum with a rotational classroom model, professional development, and classroom- and school-level reports on student learning, provides a comprehensive data set for analysis. The variables of interest in this study have been made available by the Zearn team and include (1) teacher effort (as measured by a gamut of actions), (2) lesson completion (badges), (3) student performance (tower alerts), and the (4) time teachers and students spend on the platform.

<!--Lit review here -->

## Research Questions

We propose the following research questions to explore the complex dynamics of teacher behavior, the role of reinforcement learning in modeling these behaviors, and the subsequent impact on student achievement. We hope to shed light on the potential of reinforcement learning for understanding human behavior in field data.

1\. Characterizing Teacher Behavior: Which reinforcement learning model most accurately characterizes the behavior of teachers in the context of Zearn Math? What insights about teacher behavior do these differences in model fit bring?

2\. Impact of RL Parameters on Teacher Behavior and Student Achievement: How do the parameters of reinforcement learning models vary with teacher behavior? What effects do these variations have on student achievement?

3\. Influence of Teacher Background, Training, and Experience: How do factors such as teacher background, training, and experience influence their adaptation to and implementation of the Zearn Math curriculum? How can these factors be effectively incorporated into the reinforcement learning model to provide a more comprehensive understanding of teacher behavior and its impact on student outcomes?

# Theory

## Education production function

Previous research has explored how teacher effort affects student achievement. In particular, economists have studied the "education production function," in which educational outcomes are a function of various inputs, including teacher effort, student effort, school resources, and family background [@hedges1994]. This function serves as a theoretical framework for understanding how different factors contribute to educational achievement and how interventions can be designed to improve outcomes.

One of the key inputs in the education production function is teacher effort, as teachers play a crucial role in shaping students' learning experiences and outcomes. Their effort, which encompasses their time, energy, dedication, and instructional strategies, can significantly influence students' academic achievement [@rivkin2005]. However, measuring teacher effort and its impact on student outcomes can be challenging due to the complex and multifaceted nature of teaching.

To address this challenge, researchers have employed various strategies to identify the effects of teacher effort on student achievement. One common approach is to manipulate the conditions under which teachers operate, thereby changing the levels of teacher effort. For instance, Duflo, Dupas, and Kremer (2011) [@duflo2011] conducted a randomized controlled trial in Kenya to examine the effects of tracking, a practice of grouping students based on their ability levels. Tracking has been a prominent tool in the sociology of education and assumes that teacher inputs will depend on the ability level of the students. This tailoring of teaching strategies and content potentially enhances the instructor's effectiveness.

However, traditional social science approaches to studying the education production function often lack the flexibility to account for changes in context and experience and individual-level differences. Reinforcement learning (RL) offers a promising alternative approach. RL models incorporate a flexibility term (i.e., a learning rate) that allows for changes in behavior with experience and an exploration versus exploitation term (e.g., inverse temperature) that captures individual differences in decision-making strategies. Furthermore, the RL framework inherently accounts for the process of learning about rewards, making it a flexible and dynamic tool for studying teacher behavior [@sutton2018].

## Reinforcement Learning to Capture Patterns in Repeated Behavior

In RL, an agent learns to make decisions over time to maximize a cumulative reward. At the heart of RL is the concept of a policy, which is a mapping from states to actions or, more commonly, a probability distribution over actions [@sutton2018]. The agent's goal is to learn an optimal policy, which maximizes the expected cumulative reward, even when the parameters of the environment are not known a priori.

The application of RL in the context of education and teaching is not new. One of the pioneers in the field of Markov decision processes, Ronald Howard, attempted to apply his mathematical framework to instruction theory as early as 1960 [@howard1960]. Later, in 1972, Richard Atkinson proposed a theory of instruction that encapsulates the key components of a Markov decision process, including states, actions, transition probabilities, reward functions, and a time horizon [@atkinson1972]. In Atkinson's framework, actions are instructional activities (e.g., assigning problem sets) that can change a given state (e.g., student learning level). These changes in states can yield rewards minus the associated cost of the action. For example, a teacher may be rewarded with an increase in the knowledge or skill of a student, but such reward must be balanced with its associated effort (e.g., labor cost). Atkinson and colleagues continued to test many parametrizations of this idea, contributing significantly to the development of RL theory in the context of education (see [@doroudi2019] for a full review).

More recently, RL models have been used in the psychology of habit to explain learning and reward association. One common approach in human studies is to apply the "multi-armed bandit" task. In this type of experiment, participants are presented with multiple actions, each with an unknown payoff. The subject's goal is to learn the best outcome through trial and error. In the beginning, the reward-action relationships are unknown, so the participant must explore or sample each action [@sutton2018]. This exploration-exploitation trade-off is a central theme in RL and has the potential to provide valuable insights into how students learn and make decisions over time.

### Why Reinforcement Learning?

Before presenting these models, we argue for the usefulness of RL in our setting. Unlike traditional economic models that map teacher effort to student outcomes through a time-fixed education production function, RL allows us to model teacher behavior as a flexible, evolving process that can change week by week. The RL framework is inspired by the way animals learn from their experiences, with an agent representing a decision-maker who performs actions in an environment and receives observations and rewards in return. In the RL paradigm, teachers are modeled as agents that interact with their environment (the classroom), making decisions (actions) based on their current understanding of the environment (state) and the feedback they receive (rewards). This interaction can be represented as:

$$
\text{Agent} \xrightarrow[\text{Actions}]{\text{Performs}} \text{Environment} \xrightarrow[\text{Observations, Rewards}]{\text{Provides}} \text{Agent}
$$

In the context of predicting repeated behavior, we can use RL algorithms to model the decision-making process of individuals or groups, such as teachers, by learning from the patterns in their actions and the resulting outcomes. This approach allows us to model how individual teachers learn the relationships between actions, states, and rewards, thereby creating a typification of instructors. The RL framework's flexibility and robustness make it adaptable to changes in the learning environment and individual student needs. Furthermore, it allows for incorporating a diverse range of state, action, and reward variables and can be tailored to different educational contexts and objectives. RL models can also capture the inherent uncertainty in the teaching environment, providing a framework for making optimal decisions under uncertainty. This feature is particularly relevant in the context of teaching, where the effectiveness of a teaching strategy may not be immediately apparent, and the optimal strategy may evolve as students' understanding develops.

With RL, it then becomes possible to identify how teachers differ in their learning and behavior and how these characteristics relate to student outcomes. For example, teacher flexibility may be optimal in a learning environment, but without the estimation of individual parameters, differentiating teachers would not be possible. Further, if a policy-maker can shift these individual-level parameters, they can affect student outcomes. These so-called "counterfactual analyses" can be powerful tools for creating innovative interventions or nudges to improve an outcome of interest. Finally, the RL model carries potential beyond the goals of this study. For instance, it can learn the associations between actions and rewards, allowing for the automation of certain instructional inputs; if a teacher often assigns an activity under a certain state, the model could automate this action, freeing some of the teacher's time.

### Previous Applied RL Cases

- Discuss existing models and where they fall short, justifying your approach.
- Introduce the concept of data-driven exploration in RL models.

### Q-Learning Model

The first class of models we apply to our data is the so-called Q-learning algorithm. Q-learning is a model-free reinforcement learning algorithm designed to learn a policy, which tells an agent what action to take under what circumstances [@watkins1992]. It does not require a model of the environment (hence the connotation "model-free"), and it can handle problems with stochastic transitions and rewards without requiring adaptations. This model is inspired by the so-called "multi-armed bandit" problem. In this paradigm, an agent has a finite number of choices, each associated with a given reward. The agent must simply learn to choose which action yields the highest reward. Learning in this setting occurs by adjusting expectations and minimizing "surprises" (i.e., prediction errors). Notice that this setting does not require us to define a given state: in our simplest model, Q-learning assumes that the best action in one week is the best action at any other week. Thus, the model only prescribes an action-reward relationship. The teacher here learns the value of logging in regardless of the history of their classroom or students.

In the context of Q-learning, the agent interacts with the environment to gain experience and uses this experience to update its knowledge about the quality of particular actions, given the state of the environment. This knowledge is represented in the Q-values, a prediction of the future reward expected after taking an action in the current state (which in our context can be compared to subjective value or a utility). The goal of Q-learning is to accurately learn these Q-values by updating them iteratively [@rummery].

The Q-learning model is based on the concept of a Q-function, which is a function of both state and action and represents the expected future reward for taking a particular action in a particular state. The Q-function is updated iteratively using the Bellman equation, which expresses the value of a state-action pair in terms of the immediate reward plus the discounted value of the best future state-action pair.

The Q-function, denoted as $Q(s, a)$, is defined for all state-action pairs $(s, a)$, where $s$ is the state and $a$ is the action. The Q-function represents the expected return or future reward for taking action $a$ in state $s$ following a certain policy $\pi$. The Q-function is updated iteratively using the Bellman equation as follows:

$$
Q(s, a) = Q(s, a) + \alpha \delta
$$ {#eq-q-learn}

where $\alpha$ is the learning rate, which determines how much the Q-value is updated based on $\delta$, the reward prediction error. The reward prediction error is the difference between the estimated Q-value and the observed reward plus the discounted future Q-value. This error is used to update the Q-value in the direction of the observed reward, as follows:

$$
\delta = r + \gamma \max_{a'} Q(s', a') - Q(s, a)
$$ {#eq-RPE}

where:

-   $r$ is the immediate reward received after taking action $a$ in state $s$,

-   $\gamma$ is the discount factor,

-   $s'$ is the new state after taking action $a$,

-   $a'$ is the action to be taken in the new state $s'$,

-   $s$ is the current state,

-   $a$ is the action taken,

-   $\max_{a'} Q(s', a')$ is the maximum reward that can be obtained in the next state $s'$,

-   $Q(s, a)$ is the current estimate of the Q-value for action $a$ in state $s$.

The Q-learning algorithm uses this update rule to learn the Q-function and, hence, the optimal policy. The agent starts with an initial Q-function (which can be arbitrary) and then updates the Q-values based on the experiences it gathers from interactions with the environment. The update rule is applied every time the agent transitions from a state $s$ to a state $s'$ by taking an action $a$ and receiving a reward $r$. The agent selects actions based on a policy derived from the Q-values. A common choice is the softmax action selection method, which is a way to balance exploration and exploitation. The softmax method chooses actions probabilistically based on their Q-values. The softmax function determines the probability of choosing a particular action and is defined as follows:

$$
P(a) = \frac{e^{Q(s, a)/\tau}}{\sum_{a'} e^{Q(s, a')/\tau}}
$$ {#eq-softmax}

where:

-   $P(a)$ is the probability of choosing action $a$,

-   $Q(s, a)$ is the Q-value of action $a$ in state $s$,

-   $\tau$ is a parameter known as the temperature, which controls the level of exploration,

-   the denominator is the sum over all possible actions $a'$ of the exponential of their Q-values divided by the temperature.

The temperature parameter $\tau$ controls the trade-off between exploration and exploitation. When $\tau$ is high, the agent explores more because the action probabilities are more uniform. When $\tau$ is low, the agent exploits more because the action with the highest Q-value is more likely to be chosen than the others. As the agent learns, it can be beneficial to start with a high temperature to encourage exploration and then gradually decrease it to favor the exploitation of the learned policy.

### State-Free vs. State-Based Models

So far, we have defined state-based models incorporating the environment into the Q-function. In state-free models, the Q-function, denoted as $Q(a)$, is solely a function of the action, $a$. The environmental state does not factor into the decision-making process. This simplifying assumption can be beneficial in scenarios where the environmental state exerts minimal influence on the outcome of the action. The agent learns a global policy that is independent of the specific state. This approach can be effective in environments with low state-action complexity or when the state is difficult to define or observe [@sutton2018]. The Q-value update function, therefore, simplifies to:

$$
Q(a) = Q(a) + \alpha \left[ r(a) - Q(a) \right]
$$ {#eq-state-free}

where:

-   $Q(a)$ is the current estimate of the Q-value for action $a$,

-   $\alpha$ is the learning rate,

-   $r(a)$ is the immediate reward received after taking action $a$.

In this equation, the term in the brackets, $r(a) - Q(a)$, is the reward prediction error. It represents the difference between the observed reward and the current estimate of the Q-value. The Q-value is updated in the direction of this error, scaled by the learning rate $\alpha$.

### The Kernel Function

The kernel function is a mathematical tool used to measure the similarity between different states or actions [@ormoneit2002; @domingues; @liu]. In the context of the Q-learning model with states and a kernel, the kernel function is used to compute a weighted average of the rewards obtained in similar state-action pairs in the past. This allows the model to generalize from past experience and to make more informed decisions about future actions.

### The Actor-Critic Model

The next model we attempt to fit divides the action selection and action evaluation tasks into two components: the "actor" and the "critic" [@sutton2018b]. This division theoretically allows for more efficient learning, as the critic guides the actor's learning process.

The "actor" in this model selects actions based on a policy function, denoted as $\pi(a|s)$, which maps states to actions, determining the probability of taking each action in each state. The actor aims to learn an optimal policy that maximizes the expected cumulative reward. In our setting, we set the policy as a softmax function over action preferences:

$$
\pi(a|s) = \frac{e^{h(a, s)}}{\sum_{a'} e^{h(a', s)}}
$$

where $h(a, s)$ is the preference for action $a$ in state $s$.

The "critic," on the other hand, evaluates the actions taken by the actor by learning a value function, denoted as $V(s)$. Given the actor's current policy, the critic estimates the expected cumulative reward from each state. The critic's feedback, in the form of the value function, guides the actor's learning. We update the value function based on the Temporal Difference (TD) error, a measure of the difference between the estimated and actual return:

$$
\delta = r + \gamma V(s') - V(s)
$$

where $r$ is the reward, $\gamma$ is the discount factor, $s'$ is the new state, and $s$ is the current state.

This separation of action selection and evaluation distinguishes the Actor-Critic model. In Q-learning, a single Q-function selects and evaluates actions. Conversely, in the Actor-Critic case, the actor updates its policy to increase the probability of actions that lead to higher-than-expected returns and decrease the probability of actions that lead to lower-than-expected returns.

# Applying RL Models to the Zearn Context

In this section, we propose an application of the Reinforcement Learning (RL) algorithms for modeling teacher decision-making within the Zearn platform.

Consider a typical teaching scenario: the state is the students' current progress in the class, and the actions are a range of pedagogical strategies, such as assigning additional practice, providing personalized feedback, or adjusting lesson plans. Some relevant reward variables include improved student performance, increased student engagement, or reduced learning gaps. We represent this interaction as:

$$
State (S) \xrightarrow[\text{Action (A)}]{\text{Teacher Decides}} New State (S') \xrightarrow[\text{Reward (R)}]{\text{Resulting Outcome}} \text{Feedback}
$$ {#eq-zearn-RL}

In the Zearn context, we define the decision process as follows:

1.  Agents are the teachers.

2.  Actions include the time spent on the platform and the choice to apply specific pedagogical strategies.

3.  The environment is the Zearn platform with its students.

4.  The state is the week-over-week change in Tower Alerts.

5.  The reward is a linear function of the number of badges earned.

Mathematically mapping the agent-environment interaction is flexible, with many models potentially satisfying our initial assumptions. We approach this problem as a competition of models, selecting a set of models applicable to our setting, fitting them to the data, and comparing their performances.

## A Data-Driven Approach to Feature Selection

<!-- - Introduce the data-driven approach, its necessity, and the research question. -->
<!-- - Briefly mention the RL models and their initial shortcomings. -->

## State-Free Q-Learning Model

We use the Q-learning model to predict the actions of teachers based on their past actions and the rewards they received.

The Q-value for a given state $s$ and action $a$ is updated based on the reward prediction error $\delta$:

$$
Q(a) = Q(a) + \alpha \left( \gamma (\text{Badges}_t - \text{cost}(a)) - Q(a) \right)
$$

where

-   $\alpha$ is the learning rate,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_t$ is the immediate reward received after taking action $a$ in state $s$,
-   $\text{cost}(a)$ is the cost associated with action $a$,
-   $Q(a)$ is the current estimate of the Q-value for action $a$.

The probability of choosing a particular action is determined by the softmax function and is defined as follows:

$$
P(a) = \frac{e^{Q(a)/\tau}}{\sum_{a'} e^{Q(a')/\tau}}
$$

where:

-   $P(a)$ is the probability of choosing action $a$,
-   $Q(a)$ is the Q-value of action $a$,
-   $\tau$ is a parameter known as the temperature, which controls the level of exploration.

Note that the reward is assumed to be a linear function of the number of badges earned, with the learning rate $\alpha$ as the multiplier and the cost as the constant subtracted from the reward. We interpret each of these parameters as follows:

-   Reward: A linear function of the number of badges earned, indicating that the more badges a teacher earns, the higher the reward. The learning rate determines how quickly the teacher updates their Q-values in response to new information, while the cost represents the perceived effort or inconvenience associated with the action.

-   Cost: The perceived effort or inconvenience associated with the action, such as the effort required to complete a particular task or the inconvenience of deviating from a preferred teaching method.

-   Learning rate ($\alpha$): The extent to which the newly acquired information will override the old information. A factor of 0 will make the agent not learn anything.

-   Discount rate ($\gamma$): The degree to which future rewards are discounted compared to immediate rewards. A high discount rate means that future rewards are considered almost as valuable as immediate rewards, which encourages long-term planning. A low discount rate means that immediate rewards are much more valuable than future rewards, which encourages short-term thinking.

-   Inverse temperature ($\tau$): The degree of randomness in the choice behavior. A high inverse temperature means that the agent is more likely to choose the action with the highest expected reward, while a low inverse temperature means that the agent is more likely to choose actions randomly. This parameter can be interpreted as a measure of the agent's confidence in its Q-values, reflecting the trade-off between exploration (trying out new actions) and exploitation (sticking to known beneficial actions).

## State-Based Q-Learning Model

In this model, the state $s$ represents the current situation or context in which the teacher decides. The state could include factors such as the students' current performance levels. The state-dependent model is particularly relevant in teaching, where the effectiveness of a given strategy may depend on the specific circumstances of the class. Here, the Q-value for a given state $s$ and action $a$ is:

$$ Q(s, a) = Q(s, a) + \alpha \left( \gamma (\text{Badges}_t - \text{cost}(a)) - Q(s, a) \right) $$

where:

-   $\alpha$ is the learning rate,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_t$ is the immediate reward received after taking action $a$ in state $s$,
-   $\text{cost}(a)$ is the cost associated with action $a$,
-   $Q(s, a)$ is the current estimate of the Q-value for action $a$ in state $s$.

Like the state-free model, the softmax function determines the probability of choosing a particular action.

## The Kernel Function

We anticipate that the reward associated with a given action may be delayed, reflecting the time it takes for the effects of an action to manifest. For instance, teachers might spend the first week of each month planning activities on the platform, with the rewards of these actions becoming apparent in subsequent weeks. To capture this temporal aspect of teacher behavior, we employ a kernel function that incorporates a measure of similarity between the current and past state-action pairs, modulated by a discount factor that accounts for the delay in reward. The kernel function is defined as the Jaccard similarity as follows:

$$
K(t, t') = 0.5 \cdot \text{I}(a_t = a_{t'}) \cdot \left[ 1 + \text{I}(s_t = s_{t'})\right]
$$

where:

-   $K(t, t')$ is the kernel function,
-   $t$ and $t'$ are the current and past time steps (weeks), respectively,
-   $\text{I}(\cdot)$ is the indicator function, which equals 1 if the condition inside the parentheses is true and 0 otherwise,
-   $a_t$ and $a_{t'}$ are the choices made at the current and past time steps, respectively,
-   $s_t$ and $s_{t'}$ are the states at the current and past time steps, respectively.

### Kernel-Based Reinforcement

We use the kernel function to compute a weighted average of past rewards (kernel reward), which updates the Q-value for the current state-action pair. The kernel reward is:

$$
R_K = \frac{\sum_{t' = 1}^{T} K(t, t') \cdot \gamma^{(t - t')}  \text{Badges}_{t'}}{\sum_{t' = 1}^{T} K(t, t')}
$$

where:

-   $R_K$ is the kernel reward,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_{t'}$ is the number of badges earned at the past time step $t'$,
-   $T$ is the number of lags in the kernel.

Subsequently, the Q-value for the current state-action pair is updated:

$$
Q(s, a) = Q(s, a) + \alpha \left( R_K - \text{cost}(a) - Q(s, a) \right)
$$

## Dynamic Analysis (Lau & Glimcher, 2005)

In order to determine the number of lags in the kernel, we apply the Dynamic Analysis method, as proposed by [@lau2005]. The Dynamic Analysis method is based on the premise that both past reinforcers and past choices significantly influence an individual's choices on each trial. It is a powerful tool for understanding choice behavior in the context of reinforcement learning. This method is particularly useful in tasks where reinforcement contingencies vary within a session, as it allows for the analysis of behavior at a granular, response-by-response level while also providing insights into behavior averaged over multiple trials. It also implies that the ratio of responses to different alternatives matches the ratio of reinforcements obtained from these alternatives. The method allows for the prediction of response rate using weighted sums of past reinforcers, a technique that has been applied to predict choice allocation in concurrent schedules on both short (response-by-response) and longer (within and between sessions) time scales.

Based on logistic regression, the model captures the linear combination of past reinforcers and choices on each trial, as defined by:

$$
\begin{aligned}\log \left(\frac{p_{R, i}}{p_{L, i}}\right)= & \sum_{j=1} \alpha_{ j}( r_{R, i-j}-r_{L, i-j}) \\& +\sum_{j=1} \beta_{j} (c_{R, i-j}-c_{L, i-j})+\gamma,\end{aligned}
$$

where $p$ is the probability of choice $c$ ($R$, right or $L$, left) at time $i$ as a function of the rewards, $r$, and choices for every time point $i-j$ in the history.

A key aspect of Dynamic Analysis is its focus on local variations in behavior and how these relate to local variations in reinforcer history and behavioral history. This local focus allows for the identification of patterns and trends that may be obscured when looking at behavior on a larger scale. For example, the method can reveal a tendency to alternate between choices in the short term while also showing a longer-term tendency to persist with the more frequently chosen alternative.

The Dynamic Analysis method also allows for the quantitative assessment of the relative effects of past reinforcers and choices. This is achieved by including covariates ($\alpha, \beta$) for past choices in the analysis, which can reveal patterns such as a fix-and-sample pattern of choice, where there is a tendency to persist on the rich alternative with brief visits to the lean alternative.

In terms of the weighting of past reinforcers and choices, the Dynamic Analysis method suggests that the effects of these factors decay over time. This is represented by a decay function, which can take the form of an exponential or hyperbolic function, depending on the specific characteristics of the task and the behavior of the individual.

### Dynamic Analysis in the Zearn Context

In the context of the Zearn dataset, we implement the Dynamic Analysis method to understand the temporal dependencies and complex interactions between teacher actions, student outcomes, and the learning environment. The method is applied to a panel data model, which allows for the inclusion of both time-invariant and time-varying variables and accounts for individual-specific effects. The key variables in the model are the rewards and choices. In this context, the rewards are represented by Badges per active student in a given week. The choices are represented by Teacher Minutes or the components from our dimensionality reduction. The model also includes lagged versions of these variables, which capture the effects of past rewards and choices on current choices. The lag period ranges from 1 week to 8 weeks, allowing for the examination of the effects of past rewards and choices over different time horizons.

The model can be represented by the following equation:

$$
y_{it} = \beta_0 + \sum_{j=1}^{L} \beta_{1j} y_{it-j} + \beta_2 x_{it} + \sum_{j=1}^{L} \beta_{3j} x_{it-j} + u_i + \epsilon_{it}
$$

where:

-   $y_{it}$ is the dependent variable (teacher minutes or method component) for individual $i$ at time $t$,
-   $y_{i t-j}$ is the dependent variable lagged by $j$ periods,
-   $x_{it}$ is the independent variable (badges per active user) for individual $i$ at time $t$,
-   $x_{i t-j}$ is the independent variable lagged by $j$ periods,
-   $u_i$ is the individual-specific effect,
-   $\epsilon_{it}$ is the error term, and
-   $L$ is the number of lag periods.

The coefficients $\beta_1j$ and $\beta_3j$ capture the effects of past choices and rewards, respectively, lagged by $j$ periods, on current choices. The sum over $j$ from $1$ to $L$ represents the inclusion of lagged variables for all lag periods from 1 week to 8 weeks.

The Dynamic Analysis method offers several advantages in the context of the Zearn dataset. First, it captures the temporal dependencies and complex interactions between teacher actions, student outcomes, and the learning environment. This allows for a more nuanced understanding of the factors that influence teacher behavior on the platform.

Second, the method allows for the identification of a kernel that best describes the temporal dynamics of the data. This kernel, represented by the lagged variables in the model, captures how far back rewards and past actions matter. This can provide valuable insights into the temporal scope of the effects of rewards and actions and can inform strategies for influencing teacher behavior on the platform.

## The Actor-Critic Model

In this model, we estimate several variables: the cost in badge-units for each component, the discount rate ($\gamma$), and step sizes ($\alpha_v$ and $\alpha_\pi$). These step sizes are similar to learning rates, as they determine the extent to which the model updates the weights for the value and policy functions, respectively.

Our Actor-Critic model maintains two sets of parameters: the weights $w_a$ that parameterize the value function (the critic) and the parameters $\theta_a$ that define the policy (the actor). The weights and policy are updated based on the prediction error $\delta$. The equations for updating the weights and policy are as follows:

$$
w_a = w_a + \alpha_{v} \cdot \delta \cdot S \\
\theta_a = \theta_a + \alpha_{\pi} \cdot \delta \cdot S
$$

where $w_a, \theta_a$ are the vectors of policy and value weights, respectively, for action $a$, and $S$ is a vector that characterizes the current state, defined as $S = \begin{bmatrix} 1 \\ \text{Tower Alerts}_t - \text{Tower Alerts}_{t-1} \end{bmatrix}$.

We define the parameterized policy as $\text{Prob}(a)=\text{Logit}^{-1}(\theta_a \cdot S)$, the values of each state as $v(S,a) = w_a \cdot S$, and prediction error $\delta$ as the difference between the actual outcome and the estimated value of the state-action pair:

$$
\delta = (\text{Badges}_{t} - \text{cost}(a)) - \left( \gamma v(S,a) - v(S',a) \right)
$$

## Hierarchical Models

Our dataset comprises classrooms spanning a period of around 40 weeks. Given the limited amount of data per classroom, individual-level estimation using maximum likelihood estimation would yield noisy results. Although group-level estimation provides reliable estimates, it overlooks individual differences, which are crucial to our analysis.

To address this, we employed a hierarchical Bayesian analysis, which allows for information pooling across individuals while accommodating individual differences. In this approach, individual-level parameters are functions of group-level hyperparameters, and this anchoring enhances our power by assuming commonalities among individuals [@ahn2017]. A notable feature of this estimation technique is that the extent of pooling is reflected in the hyperparameter variance. Strong pooling corresponds to low hierarchical variance, and vice versa for weak pooling.

# Data

Our data from the Zearn platform follows a time-series structure, spanning across an academic year, with the unit of analysis being the classroom-week. This level of granularity enables us to capture the temporal dynamics of teacher-student interactions and their subsequent influence on student achievement. In particular, we can model the decision-making process of teachers as they allocate their time and effort on the Zearn platform weekly and how these decisions translate into student outcomes, measured by the completion of lessons or "badges."

Geographically, the data encompasses a diverse range of schools in Louisiana. This geographical focus provides a fascinating context for our study, given the state's unique educational landscape and the widespread adoption of the Zearn platform across its schools. However, the insights derived from this study potentially have broader implications, extending beyond the confines of Louisiana, given the universal teaching and learning principles underpinning our analysis.

@fig-teachers-map depicts the geographical distribution of teachers using the Zearn platform across various parishes in Louisiana. This map visually represents the number of teachers per parish, revealing a heterogeneous distribution of Zearn usage across Louisiana, with certain parishes demonstrating a higher concentration of teachers. The map also highlights the top five cities with the highest number of teachers using Zearn.

```{r load packages}
library(tidyverse)
library(data.table)
library(ggrepel)
library(ggpubr)
library(RColorBrewer)
library(grid)
library(gridExtra)
library(scales)
library(gt)
library(gtsummary)
library(PerformanceAnalytics)
library(doParallel)
library(foreach)
library(reticulate)
use_condaenv(condaenv = "./py-zearn")

set.seed(832399554)
random_py <- import("random")
random_py$seed(832399554)
# https://www.random.org/integers/
# Timestamp: 2023-05-17 16:18:28 UTC
```

```{r data prep}

df <- read.csv(file = "Data/df_clean.csv")

# Convert columns to appropriate data types
dt <- as.data.table(df)
# Rename variable
dt[, `:=`(
  Usage.Week = as.Date(Usage.Week),
  week = week(Usage.Week),
  poverty = factor(poverty, ordered = TRUE, exclude = c("")),
  income = factor(income, ordered = TRUE, exclude = c("")),
  charter.school = ifelse(charter.school == "Yes",
                          1, ifelse(charter.school == "No",
                                    0, NA)),
  school.account = ifelse(school.account == "Yes",
                          1, ifelse(school.account == "No",
                                    0, NA)),
  # Log Transform
  Badges.per.Active.User = log(Badges.per.Active.User + 1),
  Tower.Alerts.per.Tower.Completion = log(Tower.Alerts.per.Tower.Completion + 1),
  tch_min = log(tch_min + 1)
)]

# Create new variables using data.table syntax
dt[, min_week := week(min(Usage.Week)),
   by = Teacher.User.ID]
dt[, `:=`(
  week = ifelse(week >= min_week, week - min_week + 1, week - min_week + 53),
  mean_act_st = mean(Active.Users...Total)
), by = .(Classroom.ID)]
dt[, Tsubj := max(week), by = .(Classroom.ID)]
dt[, `:=`(
  st_login = ifelse(Minutes.per.Active.User > 0, 1, 0),
  tch_login = ifelse(tch_min > 0, 1, 0)
), by = .(Classroom.ID, Teacher.User.ID, week)]
# Update the Grade.Level values and labels
dt <- dt[!(Grade.Level %in% c(-1, 11))] # Ignore -1 and 11
dt[, Grade.Level := factor(Grade.Level,
                           ordered = TRUE,
                           exclude = c(""))]
dt[, Grade.Level := factor(Grade.Level,
                           levels = c(0:8),
                           labels = c("Kindergarten", "1st", "2nd",
                                      "3rd", "4th", "5th",
                                      "6th", "7th", "8th"))]

# Remove duplicate classroom-week pairs
dt <- dt[order(-Active.Users...Total),
         .SD[1],
         by = .(Classroom.ID, week)]

df <- as.data.frame(dt) %>%
  ungroup()

```

```{r}
#| cache: true
#| label: fig-teachers-map
#| fig-cap: "Geographical distribution of teachers across various parishes in Louisiana, and the top 5 cities with the highest number of teachers."

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
unique_zipcodes <- unique(dt$zipcode) %>%
  as.list()
plan(strategy = "multisession", workers = availableCores())
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
dt <- merge(dt, address_geodata,
            by.x = "zipcode",
            by.y = "postalcode",
            all.x = TRUE)

# Aggregate the data to get the number of teachers in each county
dt_map <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID)
), by = .(county)]

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID),
  lat = mean(lat),
  long = mean(long)
), by = .(city, county)]
# Get the top 5 cities with the most teachers
top_cities <- top_cities[order(-num_teachers)][1:5,]

# Get the Louisiana county map data
df_map <- tigris::counties(cb = TRUE,
                           resolution = "20m",
                           class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    as.data.frame(dt_map),
    by = c("NAMELSAD" = "county")
  ) %>%
  sf::st_as_sf()

ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers", low = "white", high = "red", na.value = "gray90") +
  labs(
    title = "Number of Teachers by Parish in Louisiana"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city), size = 3, color = "black")
```

## Descriptive Collection and Preprocessing

Zearn provided administrative data for teachers and students at a granular level. Teacher data was time-stamped by the second and included the time spent on the platform and the specific actions taken. Note that, although extensive, the set of actions was not comprehensive. On the other hand, student data was aggregated at the classroom-week level due to data privacy considerations. This aggregation included our variables of interest: (1) teacher effort (weekly time spent on the Zearn platform, in minutes), (2) student achievement (student lesson completion or "badges"), and (3) level of student struggle (tower alerts). These variables form the basis of our reinforcement learning models, providing a comprehensive view of the dynamics of teacher-student interactions on the Zearn platform.

The dataset represents various aspects of Zearn schools including identifiers, usage data, and demographic information. It contains information for `r length(unique(df$Classroom.ID))` classrooms and `r length(unique(df$Teacher.User.ID))` teachers, with an average of `r mean(df$Students...Total, 1)` students per classroom.

The school summary statistics, as presented in @tbl-summary, provide a snapshot of the schools' characteristics. The table reveals the average number of teachers, students, and weeks of data per school. @tbl-proportions presents the proportions of schools across different poverty levels, income brackets, and other variables, such as the percentage of charter schools and schools with paid accounts. These proportions provide insights into the socio-economic context of the schools in the dataset, which is crucial for understanding the external factors that may influence teacher effort and student achievement.

```{r}
#| label: tbl-summary
#| tbl-cap: "Summary statistics for the schools"

df_summary <- df %>%
  group_by(MDR.School.ID) %>%
  summarise(
    Unique_Teacher_Count = n_distinct(Teacher.User.ID)
  ) %>%
  left_join(df %>%
              group_by(Classroom.ID, MDR.School.ID) %>%
              summarise(
                Students_Total = mean(Students...Total, na.rm = TRUE),
                Weeks_Total = n_distinct(Usage.Week)
                ) %>%
              group_by(MDR.School.ID) %>%
              summarize(
                Students_Total = sum(Students_Total),
                Weeks_Total = mean(Weeks_Total)
                ),
            by = "MDR.School.ID") %>%
  ungroup() %>%
  summarise(
    Mean_Teachers = mean(Unique_Teacher_Count),
    SD_Teachers = sd(Unique_Teacher_Count),
    Min_Teachers = min(Unique_Teacher_Count),
    Max_Teachers = max(Unique_Teacher_Count),
    Mean_Students_Total = mean(Students_Total),
    SD_Students_Total = sd(Students_Total, na.rm = TRUE),
    Min_Students_Total = min(Students_Total),
    Max_Students_Total = max(Students_Total),
    Mean_Weeks_Total = mean(Weeks_Total),
    SD_Weeks_Total = sd(Weeks_Total, na.rm = TRUE),
    Min_Weeks_Total = min(Weeks_Total),
    Max_Weeks_Total = max(Weeks_Total)
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Measure", "Variable"), sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)
  
df_proportions <- df %>%
  group_by(poverty) %>%
  summarise(n = n()) %>%
  mutate(Percentage = paste0(round(n / sum(n) * 100, digits = 2), "%")) %>%
  select(-"n") %>%
  na.omit() %>%
  rename(Variable = poverty) %>%
  mutate(Variable = as.character(Variable)) %>%
  bind_rows(df %>%
              group_by(income) %>%
              summarise(n = n()) %>%
              mutate(Percentage = paste0(round(n / sum(n) * 100, digits = 2), "%")) %>%
              select(-"n") %>%
              na.omit() %>%
              rename(Variable = income) %>%
              mutate(Variable = as.character(Variable))
            ) %>%
  bind_rows(df %>%
              ungroup() %>%
              summarise(
                Charter_Schools = mean(charter.school)*100,
                Schools_with_Paid_Account = mean(school.account)*100
                ) %>%
              mutate(
                Charter_Schools = paste0(round(Charter_Schools, digits = 2), "%"),
                Schools_with_Paid_Account = paste0(round(Schools_with_Paid_Account, digits = 2), "%")
              ) %>%
              transpose(keep.names = "Variable") %>%
              rename(Percentage = V1)) %>%
  add_row(Variable = "**Poverty Level**", Percentage = "", .before = 1) %>%
  add_row(Variable = "**Income**", Percentage = "", .before = 5) %>%
  add_row(Variable = "**Other**", Percentage = "", .before = 23)

# Summary statistics table
gt_summary <- df_summary %>%
  gt(rowname_col = "Variable") %>%
  cols_label(Mean = "Mean", SD = "Standard Deviation", Min = "Minimum", Max = "Maximum") %>%
  fmt_number(
    columns = c("Mean", "SD"),
    decimals = 2
  ) %>%
  fmt_number(
    columns = c("Min","Max"),
    decimals = 0
  )
gt_summary
```

```{r}
#| label: tbl-proportions
#| tbl-cap: "Proportions of different variables."

# Create the proportions table
gt_proportions <- df_proportions %>%
  gt(rowname_col = "Variable") %>%
  cols_label(Percentage = "Proportions") %>%
  fmt_markdown(columns = Variable)

# Print tables
gt_proportions

```

### Temporal Dynamics of Student Engagement

The total number of weeks of data per classroom, as shown in @fig-classroom-weeks, reveals that the distribution of the number of weeks is bimodal, with one mode representing classrooms with less than 3 to 4 months of data and the other mode representing classrooms with four months or more. The classrooms with less than 16 weeks of data are color-coded in red and are excluded from the analysis.

@fig-logins-week illustrates the total number of student logins over time, offering a dynamic view of student engagement with the Zearn platform. Notably, there are marked decreases in logins during Thanksgiving and Christmas, as indicated by the text annotations on the plot.

```{r}
#| label: fig-classroom-weeks
#| fig-cap: "Total number of weeks of data per classroom."

# Create the histogram
df %>%
  group_by(Classroom.ID) %>%
  summarize(Tsubj = max(Tsubj)) %>%
  mutate(Tsubj_category = if_else(Tsubj < 16, "less than 16", "16 or more")) %>%
  ggplot(aes(x = Tsubj, fill = Tsubj_category)) +
  geom_histogram(color = "black", breaks = seq(min(df$Tsubj), max(df$Tsubj) + 1, by = 2)) +
  geom_vline(xintercept = 15, color = "darkgray", linetype = "dashed", size = 0.8) +
  annotate("text", x = 9, y = 3500, label = "Excluded\nClassrooms", vjust = 1, color = "red") +
  labs(title = "Histogram of Total Number of Weeks",
       x = "Total Number of Weeks",
       y = "Frequency") +
  scale_fill_manual(values = c("less than 16" = "red", "16 or more" = "steelblue")) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(1, seq(5, max(df$Tsubj), by = 5)))

```

```{r}
#| label: fig-logins-week
#| fig-cap: "Total number of student logins over time."

# Calculate the sum of login values by Usage.Week and Teacher.User.ID
login_data <- df %>%
  group_by(Usage.Week, Teacher.User.ID) %>%
  summarize(tch_login = max(tch_login),
            st_login  = max(st_login)) %>%
  group_by(Usage.Week) %>%
  summarize(tch_logins = sum(tch_login),
            st_logins  = sum(st_login))
# Create bar plot
bar_plot <- ggplot() +
  geom_bar(data = login_data, aes(x = Usage.Week, y = st_logins), stat = "identity") +
  # geom_point(data = login_data, aes(x = Usage.Week, y = tch_logins), color = "blue") +
  labs(
    title = "Mean of Logins Across Teachers' Classrooms",
    x = "Week",
    y = "Total Logins"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12)) +
  scale_x_date(date_breaks = "3 week", date_labels = "%Y-%m-%d")

# Add labels for Christmas and Thanksgiving
bar_plot +
  geom_text(aes(x = as.Date("2019-12-25"), y = 1250, label = "Christmas"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "red") +
  geom_text(aes(x = as.Date("2019-11-25"), y = 1250, label = "Thanksgiving"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "darkorange")

```

## Variables of interest

Our Zearn data can be viewed as time-series (over the course of the school year, \~ 40 weeks). We propose to analyze the relationship between teacher effort and student achievement through temporal dynamics in the data. For the implementation of RL models, we need both an input (action) and an output (reward). For this purpose, we first use teacher weekly log-ins (a binary variable, 0 = no log-ins and 1 = at least 1 log-in) as a quantifiable proxy for teacher effort. We assume that teachers on Zearn are in control of this input variable, and choosing how much time to allocate to the platform is central to this decision process. Although these are not perfect proxies (e.g., they do not account for time spent creating assignments and performing in person instruction to students), it is one of the few directly quantifiable information about teacher behavior available in the data. Further, we use student lesson completion (badges completed) as a measure of student effort level, given that Zearn emphasizes the importance of lesson completion (instead of a grade, for example).

### Teacher Actions

Another way of capturing teacher behavior is by analyzing their specific actions when they use Zearn. The platform provides a wide range of actions that teachers can take, including downloading resources, engaging in different teaching methods or activities, and choosing how much time they spend online. We can use these actions to create a more nuanced picture of teacher behavior on the platform. provides a list of the actions available in our data.

+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Variable                                | Description                                                                               |
+=========================================+===========================================================================================+
| Assessments Answer Key Download         | Answer keys for assessments to facilitate grading.                                        |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Assessments Download                    | Assessments to demonstrate understanding of the material.                                 |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Course Guide Download                   | Overview of the content, objectives, and structure of Zearn's PD courses.                 |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Course Notes Download                   | Notes from Zearn's professional development (PD) courses.                                 |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Curriculum Map Download                 | Comprehensive overview of the learning objectives and content for each grade level.       |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Elementary Schedule Download            | Schedule of elementary-level activities in the Zearn curriculum.                          |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Fluency Completed                       | Tracks completion of a fluency activity.                                                  |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Grade Level Overview Download           | Overview of the curriculum for a specific grade level.                                    |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Guided Practice Completed               | Tracks completion of the guided practice portion of a Zearn lesson.                       |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Kindergarten Activity Completed         | Tracks completion of a specific activity in the Kindergarten curriculum.                  |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Kindergarten Mission Download           | A specific learning mission in the Kindergarten curriculum.                               |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Kindergarten Schedule Download          | Detailed schedules for Kindergarten to help teachers plan their instruction.              |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Mission Overview Download               | Overview of a specific learning mission in the Zearn curriculum.                          |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Number Gym Activity Completed           | Tracks completion of a Number Gym activity.                                               |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Optional Homework Download              | Optional homework assignments for extra practice.                                         |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Optional Problem Sets Download          | Optional sets of problems for extra practice.                                             |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Small Group Lesson Download             | Lessons designed for completion in small groups.                                          |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Student Notes and Exit Tickets Download | Notes taken during a lesson and "exit tickets," brief assessments at the end of a lesson. |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Teaching and Learning Approach Download | Resources related to Zearn's teaching and learning approach.                              |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Tower Completed                         | Tracks completion of a "Tower of Power" activity in a Zearn lesson.                       |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Tower Stage Failed                      | Tracks failure of a stage in a "Tower of Power" activity.                                 |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Tower Struggled                         | Tracks struggles with a "Tower of Power" activity.                                        |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Whole Group Fluency Download            | Activities for practicing fluency skills as a whole group.                                |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Whole Group Word Problems Download      | Word problem activities completed as a whole group.                                       |
+-----------------------------------------+-------------------------------------------------------------------------------------------+

: List of Teacher Actions

### State Variables

For this paper's second class of Reinforcement Learning models, we require variables that can determine the state space of each week. Experienced teachers suggest that they strongly focus on measures of the level of difficulty encountered during the lessons. The Zearn platform provides a measure of this kind, namely, "Tower Alerts." If a student is struggling in a given lesson, the platform automatically provides scaffolded remediation (i.e., breaking the problems step by step), and if they struggle multiple times in that same lesson, a "Tower Alert" is generated for their teacher. We use the previous week's average Tower Alerts to measure the level of difficulty faced by the students. Further, we include the previous week's average student time usage (in minutes) as a state variable. Effectively, we assume that the value of a given state (i.e., the week at hand) is a function of the previous week's "tower alerts" and "student minutes."

## Exclusion criteria

The raw data underwent rigorous preprocessing to ensure its suitability for analysis. This process included performing log transformations on our variables of interest (i.e., minutes, badges, and tower alerts) to normalize their distributions. Given the diverse user base of Zearn, we applied specific exclusion criteria to select teachers who most likely come from traditional schools and classrooms that use the platform consistently. We selected virtual classrooms with at least five active students weekly, filtering out parents or tutors who may use Zearn outside the classroom setting. We removed teachers with more than four classrooms and those who logged in for less than 16 weeks. We excluded classrooms in the 6th to 8th grades, as those are a small proportion of our dataset. This deletion ensures a focus on traditional school settings and consistent platform usage, minimizing bias from teachers and schools that have not used Zearn consistently.

```{r preprocess data}
#| include: false
dt[, n_weeks := .N,
   by = Classroom.ID]

dt <- dt[
  n_weeks > 15 & # At least 4 months cumulative activity
    Tsubj < 2*n_weeks & # At least activity twice a month on average
    teacher_number_classes < 5 &
    Students...Total > 5 &
    mean_act_st > 3 &
    !(Grade.Level %in% c("6th","7th","8th")) &
    !(month(Usage.Week) %in% c(6, 7, 8)) &
    !is.na(District.Rollup.ID),
]

cols_to_select <- names(dt)[sapply(dt, function(x) !is.numeric(x) ||
                                     (is.numeric(x) && is.finite(sd(x)) && sd(x) != 0))]
dt <- dt[, ..cols_to_select]

df <- as.data.frame(dt) %>%
  ungroup() %>%
  arrange(Classroom.ID, week)
# Clean environment
rm(list = setdiff(ls(), c("df","random_py")))
gc(verbose = FALSE)

```

@tbl-summary-statistics summarizes the refined dataset, providing a snapshot of the key variables of interest. Their means and standard deviations (SD) are computed for each grade level and overall (across all grades).

```{r}
#| label: tbl-summary-statistics
#| caption: "Means (SD) of student variables by grade level."

create_summary <- function(var_name, var_label, type = "continuous") {
  summary_stat <- df %>%
    select(Grade.Level, var_name) %>%
    tbl_summary(
      by = Grade.Level,
      missing = "no",
      type = list(var_name ~ type),
      statistic = var_name ~ ifelse(type == "continuous", "{mean} ({sd})", "{n} ({p})")
    ) %>%
    add_overall() %>%
    as_tibble() 
  summary_stat[1] <- var_label
  return(summary_stat)
}

summaries_list <- list(
  # create_summary("Sessions.per.Active.User", "Sessions per Student"),
  create_summary("Minutes.per.Active.User", "Minutes per Student"),
  create_summary("Badges.per.Active.User", "Badges per Student"),
  create_summary("Tower.Alerts.per.Tower.Completion", "Tower Alerts per Lesson Completion"),
  # create_summary("tch_login", "Teacher Login (0/1)", type = "dichotomous"),
  create_summary("tch_min", "Minutes per Teacher")
)

summary_table <- bind_rows(summaries_list) %>% 
  transpose(keep.names = "Characteristic", make.names = 1)
names(summary_table)[1] <- "Grade Level"

gt(summary_table)

```

## Visualizing Relationships Between Variables

We begin to unveil the intricate relationships among the variables under consideration through a comprehensive correlation analysis, as depicted in @fig-corr. This correlation matrix elucidates the magnitude and direction of associations among variables such as badges earned, minutes spent per student, tower alerts, the number of students, and teacher minutes. These interconnections inform the construction of our reinforcement learning models by suggesting the influence of teacher effort on student achievement. In this correlation matrix, each cell represents the Spearman correlation coefficient between a pair of variables. The color and size of the circles in each cell reflect the strength and direction of the correlation, with blue indicating positive correlations and red indicating negative correlations. The histograms along the diagonal provide a visual representation of the distribution of each variable.

```{r}
#| cache: true
#| label: fig-corr
#| fig-cap: "Correlation coefficients between variables after log transformation"
#| fig-format: png

df_corr <- df  %>%
  select(Badges.per.Active.User,
         Active.Users...Total,
         Minutes.per.Active.User,
         Tower.Alerts.per.Tower.Completion,
         tch_min) %>%
  rename("Badges" = Badges.per.Active.User,
         "Minutes per Student" = Minutes.per.Active.User,
         "Tower Alerts" = Tower.Alerts.per.Tower.Completion,
         "# of Students" = Active.Users...Total,
         "Teacher Minutes" = tch_min)

chart.Correlation(df_corr, histogram = TRUE, method = "spearman",
                  pch = 20, cex = 0.5, col = rgb(0, 0, 1, 0.5))

```

## Dimensionality Reduction

To capture the complex decision-making processes and trade-offs made by teachers, we employed a Principal Component Analysis (PCA), a Nonnegative Matrix Factorization (NMF), and an autoencoder (AE). We chose these methods as candidates to condense the multifaceted nature of our variables, including Teacher Minutes, Teacher Sessions, and the set of teacher actions.

While PCA is a widespread technique, it is only appropriate for some datasets or research questions. PCA assumes the data follows a Gaussian distribution, which may not always hold [@jolliffe2016]. Further, PCA is an unsupervised method that only considers the input data structure and not the associated target variables or labels. On the other hand, methods like NMF and autoencoders offer different advantages. NMF, for instance, imposes a non-negativity constraint, which makes its components easier to interpret in many contexts, especially when the data elements represent quantities that cannot be negative, such as counts or frequencies. NMF also tends to produce a more parts-based, sparse representation of the data, which can be more interpretable in some cases [@lee1999]. Autoencoders are a type of neural network that can model more complex, non-linear relationships in the data. They can also be trained in a supervised manner to learn a reduced-dimension representation specifically optimized for predicting a target variable, leading to improved performance in subsequent predictive modeling tasks [@goodfellow2016].

By trying different techniques, we can explore these different trade-offs and potentially discover a reduced-dimension representation better suited to our specific dataset and research question than PCA alone would provide. This approach aligns with the principle of methodological triangulation, which suggests that using multiple methods can help to overcome the limitations of any single method and provide a more robust and comprehensive understanding of the data.

The Nonnegative Matrix Factorization (NMF) operates as follows:

The original matrix is a detailed description of all the teachers' behaviors. Each row in the matrix represents a unique teacher, and each column represents a specific behavior or action the teacher might take. The entry in a specific row and column corresponds to the frequency of that behavior for that particular teacher.

After the NMF, we have two matrices:

1.  Basis Matrix (W): This matrix represents underlying behavior patterns. Each column is a "meta-behavior" or a group of behaviors occurring together.
2.  Mixture Matrix (H): This matrix shows the extent to which each "meta-behavior" is present in each teacher. Each entry in this matrix represents the contribution of a "meta-behavior" to a particular teacher's behaviors.

By looking at these matrices, we can identify underlying patterns of behaviors (from the basis matrix) and see how these patterns are mixed and matched in different teachers (from the mixture matrix).

We first scale the data such that all features are in the range \[0, 1\], then we create training (80%) and testing (20%) sets. We then perform a PCA and calculate the sum of squared residuals (a measure of the difference between the original data and the data reconstructed from the PCA) and silhouette scores (a measure of how similar an object is to its cluster compared to other clusters). The silhouette score ranges from -1 to 1, with a high value indicating that the object is well-matched to its cluster and poorly matched to neighboring clusters.

Next, we perform an NMF using two different loss functions (Frobenius norm and Kullback-Leibler divergence) and two different initialization methods (nonnegative double singular value decomposition (NNDSVD) and NNDSVD with zeros filled with the average of the input matrix (NNDSVDA)). We also calculate the sum of squared residuals and silhouette scores for each model.

Finally, we implement an autoencoder to perform dimensionality reduction. The autoencoder is a neural network designed to learn an encoded, compressed representation of the input data and then reconstruct the original data from this encoded representation. The architecture of the autoencoder is optimized using a hyperparameter search, which involves training multiple versions of the model with different hyperparameters and selecting the one that performs best on the validation data. These hyperparameters include (1) the number of layers in the autoencoder, (2) the number of units in each layer, (3) the regularization strength, and (4) the number of components in the encoded representation. The target score for the training period is an average of mean squared error losses for the input reconstruction and the target variable prediction (Badges per Student), weighted by the standard deviations of the input data and the target variable.

We performed these model estimations with the scikit-learn library [@pedregosa2011]. Subsequently, we compare the PCA, NMF, and AE results by plotting the sum of squared residuals and silhouette scores for each method and the number of components used. @fig-nmf-pca-comparison visually compares the performance of the different methods and allows us to choose the one that provides the best balance between reconstruction accuracy (as measured by the sum of squared residuals) and cluster separation (as measured by the silhouette score). As such, we select the NMF with three components as our preferred method.

```{r pca nmf data-prep}
#| include: false
# Choose which Teacher.User.IDs will be train vs test
df$set <- ifelse(df$Teacher.User.ID %in%
                 sample(unique(df$Teacher.User.ID),
                        size = floor(0.8 * length(unique(df$Teacher.User.ID)))),
                 "train", "test")

# Create base data.table for models (faster than data.frame)
df_comp <- as.data.table(df %>% 
                           select(Classroom.ID, week, Badges.per.Active.User,
                                  set, tch_min,
                                  RD.elementary_schedule:RD.pd_course_guide))
# Arrange
setorder(df_comp, Classroom.ID, week)

## Prep data for PCA
df_pca <- as.data.frame(df_comp) %>%
  arrange(Classroom.ID, week) %>%
  ungroup() %>%
  mutate(across(everything(), ~ifelse(is.na(.), 0, .))) %>%
  mutate(across(RD.elementary_schedule:RD.pd_course_guide, ~log1p(.)))
# Calculate standard deviations
std_devs <- apply(df_pca %>% select(-c("Classroom.ID", "week", "set")), 2, sd)
# Identify columns with defined standard deviations (not NaN or Inf)
invalid_cols <- names(std_devs[is.na(std_devs) | is.infinite(std_devs)])
df_pca <- df_pca %>% select(-all_of(invalid_cols))

# Clean environment
rm(list = setdiff(ls(), c("df", "df_pca", "df_comp", "random_py")))
gc(verbose = FALSE)

```

```{python load data}
#| include: false
import numpy as np
import pandas as pd
from sklearnex import patch_sklearn
patch_sklearn()
from sklearn.decomposition import PCA, NMF
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import silhouette_score

## Basic Variables
# Import data from R
dfpca_py = pd.DataFrame(r.df_pca)
dfpca_py.sort_values(['Classroom.ID', 'week'], inplace=True)

# Initialize scaler
scaler = MinMaxScaler()

# Drop unnecessary columns
X = dfpca_py.drop(['Badges.per.Active.User', 'set', 'Classroom.ID', 'week'], axis=1)
X_cols  = X.columns
# Scale the data
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X_cols)

# Dictionaries for export
components = {}
results = {}
residuals = {}
silhouette = {}

# Number of components for NMF
n_comp = min(X.shape) // 3
```

```{python pca-nmf}

################ PCA
for n in range(2, n_comp):
  pca = PCA(n_components=n)
  X_pca = pca.fit_transform(X_scaled)
  pca_comp = pca.components_
  X_hat = pca.inverse_transform(X_pca)
  labels = np.argmax(pca_comp, axis=0)

  results.setdefault("PCA", {})[n] = X_pca
  components.setdefault("PCA", {})[n] = pca_comp
  residuals.setdefault("PCA", {})[n] =((X_scaled - X_hat)**2).sum().sum()  # RSS
  silhouette.setdefault("PCA", {})[n] = silhouette_score(pca_comp.transpose(), labels)

################ Non-negative Matrix Factorization
# Function for NMF
def nmf_method(n, method, initial, X_scaled, solv = 'mu'):
    method_name = f"{method.title()} {initial.upper()}"
    if method == 'frobenius' and initial == 'nndsvd': solv = 'cd'
    if method != 'frobenius' and initial == 'nndsvd': return
    if method != 'frobenius': method_name = f"{method.title()}"
    
    nmf = NMF(
      n_components=n,
      init=initial,
      beta_loss=method,
      solver=solv,
      max_iter=4_000
    )
    X_nmf, nmf_comp = nmf.fit_transform(X_scaled), nmf.components_
    X_hat = nmf.inverse_transform(X_nmf)
    labels = np.argmax(nmf_comp, axis=0)
    
    results.setdefault(method_name, {})[n] = X_nmf
    components.setdefault(method_name, {})[n] = nmf_comp
    residuals.setdefault(method_name, {})[n] = ((X_scaled - X_hat)**2).sum().sum()  # RSS
    silhouette.setdefault(method_name, {})[n] = silhouette_score(nmf_comp.transpose(), labels)

# Call the function for NMF
for n in range(2, n_comp):
  for method in {'frobenius', 'kullback-leibler'}:
    for initial in {'nndsvd', 'nndsvda'}:
      try:
        nmf_method(n, method, initial, X_scaled)
      except:
        continue
```

```{python train autoencoder}
#| eval: false
import os # To fix: https://github.com/tensorflow/tensorflow/issues/59779
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from keras.models import Model
from keras.layers import Input, Dense
from kerastuner.tuners import Hyperband
from kerastuner.engine.hyperparameters import HyperParameters
from keras.constraints import NonNeg
from keras.callbacks import EarlyStopping
from keras.regularizers import L1

# Create training and testing data frames
train_df = dfpca_py[dfpca_py['set'] == 'train']
test_df = dfpca_py[dfpca_py['set'] == 'test']

# Predictors
X_train = train_df.drop(['Badges.per.Active.User', 'set', 'Classroom.ID', 'week'], axis=1)
X_test = test_df.drop(['Badges.per.Active.User', 'set', 'Classroom.ID', 'week'], axis=1)
# Scale the data
X_train = scaler.transform(X_train)
X_train = pd.DataFrame(X_train, columns=X_cols)
X_test = scaler.transform(X_test)
X_test = pd.DataFrame(X_test, columns=X_cols)

# Get the target variable
Y = dfpca_py[['Badges.per.Active.User']]
Y_train = train_df[['Badges.per.Active.User']]
Y_test = test_df[['Badges.per.Active.User']]
Y = scaler.fit_transform(Y)
Y_train = scaler.transform(Y_train)
Y_test = scaler.transform(Y_test)

# Define the number of components and features
n_features = X_scaled.shape[1]
n_labels = 1  # For regression, we usually have just one output node

# Determine loss weights according to the data structure:
decoding_weight = Y.std() / (Y.std() + X_scaled.std(numeric_only=True).mean())
prediction_weight = 1 - decoding_weight
def build_model(hp):
    # Define the input layer
    input_data = Input(shape=(n_features,))
    
    # Define the encoding layer(s)
    n_layers = hp.Int('n_layers', min_value=1, max_value=4, step=1)
    n_units = [
      hp.Choice('units_' + str(i), values=[8, 16, 32, 64, 128, 256, 512])
      for i in range(n_layers)
    ]
    encoded = input_data
    for i in range(n_layers):
        encoded = Dense(
          units=n_units[i],
          activation='relu')(encoded)
          
    # Generate the latent vector
    latent_dim = hp.Int('encoding_units', min_value=2, max_value=n_comp, step=1)
    l1_value = hp.Float('l1_value', min_value=0.0001, max_value=0.001, default=0.0005, step=0.0001)
    latent = Dense(
      units=latent_dim,
      activation='linear',
      activity_regularizer= L1(l1=l1_value),
      kernel_constraint=NonNeg(),
      name='latent')(encoded)      
    
    # Decoder
    decoded = latent
    for i in range(n_layers):
        decoded = Dense(
          units=n_units[n_layers - i - 1],
          activation='relu')(decoded)
    decoded = Dense(n_features, activation='sigmoid', name='decoded')(decoded)
    
    # Define the label output layer
    label_output = Dense(n_labels, activation='linear', name='label_output')(latent)
    
    # Define the autoencoder model
    autoencoder = Model(inputs=input_data, outputs=[decoded, label_output])
    # Compile the model
    autoencoder.compile(optimizer='adadelta',
                    loss={
                      'decoded': 'mean_squared_error',
                      'label_output': 'mean_squared_error'
                    },
                    loss_weights={
                      'decoded': decoding_weight,
                      'label_output': prediction_weight
                    })
    
    return autoencoder

# Set up hyperparameter tuner
tuner = Hyperband(build_model,
                  objective='val_loss',
                  max_epochs=20,
                  directory='autoencoder_tuning',
                  project_name='autoencoder_2nd_tuning')

# Perform hyperparameter search
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)
tuner.search(x=X_train,
            y=[X_train, Y_train],
            epochs=50,
            validation_data=(X_test, [X_test, Y_test]),
            callbacks=[early_stopping_callback])
# tuner.results_summary()
# Get the optimal hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=2)[1]
model = tuner.hypermodel.build(best_hyperparameters)
history = model.fit(x=X_train, 
                    y=[X_train, Y_train],
                    epochs=2_000,
                    validation_data=(X_test, [X_test, Y_test]),
                    callbacks=[early_stopping_callback])
best_model = model


# Function to get encoded representation and components
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

# Get encoded representation and components
get_encoded_representation_and_components(best_model, X_scaled)

# save the model
model.save('./autoencoder_tuning/final_model.h5')

```

```{python load autoencoder}
#| include: false
from tensorflow.keras.models import load_model
from keras.models import Model
from keras.layers import Input

# Function to get encoded representation and components
loaded_model = load_model('./autoencoder_tuning/final_model.h5')
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

n_features = X_scaled.shape[1]
# Get encoded representation and components
get_encoded_representation_and_components(loaded_model, X_scaled)
```

```{python clean environment}
#| include: false
# get the names of all variables in the global namespace
all_vars = list(globals().keys())

# list of variables to keep
keep_vars = ['residuals', 'silhouette', 'components', 'results', 'r']

# delete all variables that are not in keep_vars
for var in all_vars:
    if var not in keep_vars:
        del globals()[var]
del keep_vars
del all_vars
del var

# manually call the garbage collector
import gc
gc.collect()

```

```{r}
#| cache: true
#| label: fig-nmf-pca-comparison
#| fig-cap: "Comparison of residuals and silhouette scores for PCA, Frobenius, and Kullback-Leibler methods."
# Importing from Python
residuals_list <- py$residuals
silhouette_list <- py$silhouette

# Creating dataframes
df_residuals <- do.call(rbind, lapply(names(residuals_list), function(method) {
  do.call(rbind, lapply(names(residuals_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Residuals = residuals_list[[method]][[n]]
    )
  }))
}))
# Creating separate dataframes for 'Autoencoder' and the rest of the methods
df_residuals_autoencoder <- df_residuals[df_residuals$Method == "Autoencoder", ]
df_residuals_others <- df_residuals[df_residuals$Method != "Autoencoder", ]

df_silhouette <- do.call(rbind, lapply(names(silhouette_list), function(method) {
  do.call(rbind, lapply(names(silhouette_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Silhouette = silhouette_list[[method]][[n]]
    )
  }))
}))
# Add nuisance row for Autoencoder to unify legends
df_silhouette <- rbind(df_silhouette,
                       data.frame(Method = "Autoencoder",
                                  Components = 4,
                                  Silhouette = mean(df_silhouette$Silhouette)))

# Plotting residuals
p1 <- ggplot() +
  geom_line(data = df_residuals_others,
            aes(x = Components, y = Residuals, color = Method)) +
  geom_point(data = df_residuals_autoencoder,
             aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Sum of Square Residuals",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals$Components),
                                  max(df_residuals$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())
# Plotting silhouette scores
p2 <- ggplot(df_silhouette, aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette$Components),
                                  max(df_silhouette$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette$Silhouette) +
                                  2*sd(df_silhouette$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Combine the plots and place the legend at the bottom
comparison_plot <- ggarrange(p1, p2,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
comparison_plot

```

#### Interpreting Components

@fig-nmf-heatmap shows the loadings of the NMF components using a heatmap, depicting how each original feature contributes to each component. In other words, each component combines the original features to explain a substantial portion of the variance, potentially improving the efficiency and interpretability of the reinforcement learning models. Given the loadings, we interpret the components as follows:

1.  **Component 1 (Teacher Engagement)**: This component seems to be heavily influenced by the variable "Teacher Minutes," suggesting teacher engagement with the Zearn platform. The higher the value in this component, the more time teachers are spending on the platform, which could indicate a higher level of engagement with the curriculum and resources. High values in this component suggest that teachers are actively using the platform, spending time reviewing reports and possibly engaging with other features [@morrison2019].

2.  **Component 2 (Resource Utilization)**: This component has high weights for variables related to different resources available on the Zearn platform, such as "Optional Problem Sets," "Student Notes and Exit Tickets," and "Mission Overview". High values in this component suggest that teachers are downloading and possibly using a variety of resources in their teaching. This pattern is consistent with the findings from [@knudsen2020] that teachers reported learning from a variety of Zearn Math resources and that the curriculum materials and their implementation are important sources of learning.

3.  **Component 3 (Pedagogical Content Knowledge)**: This component has high weights for variables related to student activities, such as "Guided Practice Completed," "Tower Completed," and "Fluency Completed," suggesting that teachers may be engaged in acquiring subject-matter-specific pedagogy, learning to scaffold and to explain concepts in a variety of ways. This finding aligns with [@morrison2019], where teachers were most likely to report using Independent Digital Lessons, student notes and workbooks, small-group lessons, and paper Exit Tickets frequently or very frequently.

```{r}
#| cache: true
#| label: fig-nmf-heatmap
#| fig-cap: "Heatmap of Non-negative Matrix Factorization (NMF) components for Zearn data. Each row represents a variable, and each column represents a component. The color intensity indicates the weight of each variable in each component, with darker colors indicating higher weights. Component 1 represents Teacher Engagement, Component 2 represents Resource Utilization, and Component 3 represents Pedagogical Content Knowledge."

library(pheatmap)

components_list <- py$components
df_heatmap <- components_list[["Kullback-Leibler"]][["3"]] %>%
  t() %>% as.data.frame()

# Define a named vector to map old variable names to new ones
variable_names <- c(
  "tch_min" = "Teacher Minutes",
  "RD.optional_problem_sets" = "Optional Problem Sets Download",
  "RD.student_notes_and_exit_tickets" = "Student Notes and Exit Tickets Download",
  "RD.mission_overview" = "Mission Overview Download",
  "RD.pd_course_notes" = "Course Notes Download",
  "RD.elementary_schedule" = "Elementary Schedule Download",
  "RD.whole_group_fluency" = "Whole Group Fluency Download",
  "Guided.Practice.Completed" = "Guided Practice Completed",
  "RD.small_group_lessons" = "Small Group Lesson Download",
  "Tower.Completed" = "Tower Completed",
  "Fluency.Completed" = "Fluency Completed",
  "Number.Gym.Activity.Completed" = "Number Gym Activity Completed",
  "RD.grade_level_overview" = "Grade Level Overview Download",
  "Tower.Stage.Failed" = "Tower Stage Failed",
  "Kindergarten.Activity.Completed" = "Kindergarten Activity Completed",
  "Tower.Struggled" = "Tower Struggled",
  "RD.k_mission" = "Kindergarten Mission Download",
  "RD.whole_group_word_problems" = "Whole Group Word Problems Download",
  "RD.assessments" = "Assessments Download",
  "RD.teaching_and_learning_approach" = "Teaching and Learning Approach Download",
  "RD.optional_homework" = "Optional Homework Download",
  "RD.k_schedule" = "Kindergarten Schedule Download",
  "RD.curriculum_map" = "Curriculum Map Download",
  "RD.assessments_answer_key" = "Assessments Answer Key Download",
  "RD.pd_course_guide" = "Course Guide Download"
)
# Rename the rows of the dataframe
row.names(df_heatmap) <- variable_names[names(df_pca)[-c(1:4)]]

names(df_heatmap) <- paste0("Comp ", 1:3)
df_heatmap <- df_heatmap %>% arrange(-`Comp 1`)

color_scheme <- colorRampPalette(
  c("#F7F7F7",brewer.pal(n = 9, name = "YlOrRd"))
  )(100)
breaks <- seq(0, 1, by = 0.01)
minmax <- function(x) ((x - min(x))/(max(x) - min(x)))
pheatmap(df_heatmap %>%
           mutate_all(minmax),
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         angle_col = 0,
         # main = "Loadings of NMF components",
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE,
         show_rownames = TRUE)

```

```{r export dimension reduction}
#| eval: false
# Importing results from Python
results_list <- py$results

# List of method names
methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler", "Autoencoder")
names(results_list) <- methods
# Initialize df_components
df_components <- df

# Loop over each method, extract the results at 3 components and bind to df_components
for (method in methods) {
  result_3 <- results_list[[method]][["3"]]
  df_components <- df_components %>%
    bind_cols(result_3)
  
  # Adjust column names
  new_cols <- paste0(method, 1:3)
  names(df_components)[(ncol(df_components) - 2):ncol(df_components)] <- new_cols
}

# Write to csv
write.csv(df_components, "./Bayesian/df.csv")

```

```{r load dimension reduction}
#| include: false
df <- read.csv(file = "./Bayesian/df.csv")
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
gc(verbose = FALSE)
```

# Methods

## Dynamic Analysis

We performed the dynamic analysis using the `plm` package in R [@croissant2008], which estimates linear models for panel data. We estimated both within and random effects models and used the Hausman test to select the most appropriate model based on its p-value [@hausman1978]. We estimated each model with different numbers of lags (from 1 to 8) and different methods of matrix factorization (PCA, NMF, and Autoencoder). We divided the data into training (80%) and testing (20%) sets to compute the Bayesian Information Criterion (BIC) and the negative log-likelihood (NLL) for each model. We then selected the number of lags based on the model with the lowest BIC and NLL.

## Base Models

Our initial approach involved fitting non-hierarchical models for three binary action variables using a simple logistic regression model. The dependent variables were derived from the Non-negative Matrix Factorization (NMF) using the Frobenius Nonnegative Double Singular Value Decomposition (NNDSVD) method. For each of these variables, we generated a lagged version and incorporated it into the model alongside the 'Badges' and 'State' variables. The models were constructed using the `brms` package, specifying the Bernoulli family and employing the `cmdstanr` backend [@bates2015; @brkner2017; @gabry2022; @stanmod2022; @brkner2021; @croissant2008]. The formula for these models are represented as follows:

$$
a_t = a_{t-1} + Badges_t + s_t + \epsilon_t
$$

where $a_t$ represents the action variable at time $t$, $a_{t-1}$ is the lagged version of the action variable, $Badges_t$ is the reward variable, $s_t$ is the 'State' variable, and $_t$ is the error term.

### Hierarchical Base Models

Subsequently, we constructed hierarchical models for the same set of action variables. We assume Classroom, Teacher, and School are the three levels of the hierarchy. The hierarchical models were similar to their non-hierarchical counterparts but included random intercepts and random slopes for the predictors.

## Reinforcement Learning Model Fit

The reinforcement learning models were implemented using the Stan programming language, a probabilistic programming language designed for statistical inference [@stanmod2022; @gabry2022]. We trained the models on data that included the number of weeks, choices made, and outcomes (log badges) for each classroom. The model parameters, including cost, discount rate, learning rate, and inverse temperature, were estimated from the data. The models employed a Bernoulli logit model to compute action probabilities and updated the expected values of the actions based on prediction errors. The models also generated posterior predictions and computed the log-likelihood for each subject.

Stan employs the Hamiltonian Monte Carlo (HMC) algorithm, a state-of-the-art Markov Chain Monte Carlo (MCMC) method that is particularly well-suited for high-dimensional and complex posterior distributions [@betancourt2017]. We specified three independent MCMC chains to check for convergence of the MCMC algorithm by comparing them. Each chain had 2,500 warmup (burn-in) iterations and 2,500 sampling iterations. During the warmup phase, the HMC algorithm adapts its parameters to the shape of the posterior distribution. The samples drawn during the warmup phase were discarded, and the models ran until they achieved convergence, as assessed by the R-hat statistic, which compares the within-chain and between-chain variance of the MCMC samples; values close to 1 indicate that the chains have converged to the same distribution [@gelman1992].

### Hierarchical RL Method

We extended the base reinforcement learning models by incorporating a hierarchical structure to account for individual commonalities and enhance robustness. This hierarchical framework defines individual-level parameters as random effects from a group-level distribution. We used the parameter values found by the non-hierarchical models to generate weakly informed priors for the hyper-parameters (group-level parameters). This approach was necessary to ensure the rapid convergence of the Hamiltonian Monte Carlo algorithm. We specified the priors as follows:

+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Parameter           | Group-level Prior                                 | Individual-level Prior                                                            |
+=====================+===================================================+===================================================================================+
| Cost                | $\mu_{\text{cost}} \sim \mathcal{N}(0.5, 1)$\     | $\text{cost}_{i,j} \sim \mathcal{N}(\mu_{\text{cost}_j}, \sigma_{\text{cost}_j})$ |
|                     | $\sigma_{\text{cost}} \sim \text{Cauchy}(0, 2.5)$ |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Discount Rate       | $\mu_{\gamma} \sim \mathcal{N}(0.7, 1)$\          | $\gamma_i \sim \mathcal{N}(\mu_{\gamma}, \sigma_{\gamma})$                        |
|                     | $\sigma_{\gamma} \sim \text{Cauchy}(0, 2.5)$      |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Step Size           | $\mu_{\alpha} \sim \mathcal{N}(0.5, 1)$\          | $\alpha_i \sim \mathcal{N}(\mu_{\alpha}, \sigma_{\alpha})$                        |
|                     | $\sigma_{\alpha} \sim \text{Cauchy}(0, 2.5)$      |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Inverse Temperature | $\mu_{\tau} \sim \mathcal{N}(1, 1)$\              | $\tau_i \sim \mathcal{N}(\mu_{\tau}, \sigma_{\tau})$                              |
|                     | $\sigma_{\tau} \sim \text{Cauchy}(0, 2.5)$        |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+

where $\mu$ and $\sigma$ denote the group-level hyperparameters, and the subscript $i$ signifies the individual-level parameters.

## Feature Selection

- Explain how you created different combinations of variables for rewards and state variables.
- Elaborate on data-driven exploration, model estimation, and HBI.
- Discuss the code used for actor-critic models and variable combinations.

## Model Performance

To evaluate the performance of the Bayesian models, we used the Leave-One-Out Information Criterion (LOOIC), a robust measure of model quality. The LOOIC is a variant of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). However, unlike AIC and BIC, which use asymptotic approximations, LOOIC is a fully Bayesian criterion that provides a more accurate estimate of out-of-sample prediction error. We used the `loo` package in R to compute the LOOIC [@vehtari2023]. The package uses Pareto smoothed importance sampling (PSIS), a beneficial technique for models where standard cross-validation is computationally expensive or impractical [@vehtari2017].

## Heterogeneity

To investigate the heterogeneity across teachers, we first extracted the posterior samples from the hierarchical model that demonstrated the best performance. For each teacher, we calculated the mean of each parameter: the estimated cost for each action, the discount rate, the learning rate, and the inverse temperature. We then correlated these parameters with the classroom-level variables: the average number of students, average minutes per student, average number of badges per student, average number of tower alerts, the number of classes per teacher, the grade level, the total number of weeks, the proportion of students under the poverty level, the average income level in the school, and whether the school had a paid Zearn account. Subsequently, we fit a linear regression model to predict the average number of badges per active user, using the estimated parameters as predictors.

### Across Demographics

We extended our analysis to examine the relationship between the estimated parameters and demographic variables for each classroom to reveal whether learning and decision-making processes captured by our model vary across different demographic groups.

# Results

## Action Variable and Lag Selection

We first select the most suitable action variable by comparing the Bayesian Information Criterion (BIC) and Negative Log-Likelihood (NLL) across different combinations of variables and lags. We incorporate lagged variables into the models using the Dynamic Analysis approach proposed to account for temporal autocorrelation and potential delayed effects. As depicted in @fig-panel-bic, the model that best fits the data includes four lags of the Non-negative Matrix Factorization (NMF) with the Frobenius Non-negative Double Singular Value Decomposition (NNDSVD) method. Including these four lags suggests that the teachers' actions over the past four weeks, encapsulated by this component, significantly influence their choices in the current week.

```{r}
bic_plm <- function(object) {
  # object is "plm", "panelmodel"
  sp = summary(object)
  if (class(object)[1] == "plm") {
    u.hat <- residuals(sp) # extract residuals
    model_data <- cbind(as.vector(u.hat), attr(u.hat, "index"))
    names(model_data)[1] <- "resid"
    c = length(unique(model_data[, "Classroom.ID"])) # extract classroom dimensions
    t = length(unique(model_data[, "week"])) # extract time dimension
    np = length(sp$coefficients[, 1]) # number of parameters
    n.N = nrow(sp$model) # number of data
    s.sq  <- log((sum(u.hat ^ 2) / (n.N))) # log sum of squares
    
    # effect = c("individual", "time", "twoways", "nested"),
    # model = c("within", "random", "ht", "between", "pooling", "fd")
    
    if (sp$args$model == "within" & sp$args$effect == "individual") {
      np = np + c + 1 # update number of parameters
    }
    
    if (sp$args$model == "within" & sp$args$effect == "time") {
      np = np + t + 1 # update number of parameters
    }
    
    if (sp$args$model == "within" & sp$args$effect == "twoways") {
      np = np + c + t # update number of parameters
    }
    
    if (sp$args$model == "random" & sp$args$effect == "twoways") {
      np = np + length(sp$ercomp$sigma2) # update number of parameters
    }
    
    bic <- round(log(n.N) * np  +  n.N * (log(2 * pi) + s.sq  + 1), 1)
    names(bic) = "BIC"
    return(bic)
  }
}

compute_nloglik <- function(residuals) {
  n <- length(residuals)
  sigma2 <- sum(residuals^2) / n
  nll <- n/2 * ( log(2 * pi) + log(sigma2) + 1 ) # Negative log likelihood
  return(as.numeric(nll))
}

```

```{r panel model comparison}
#| cache: true
library(plm)

# Helper functions
get_lag_value <- function(datatable, col, lag_period, n_comp = NULL) {
  # Add a column for week_lag
  datatable[, week_lag := c(0, diff(week)), by = Classroom.ID]

  if (is.null(n_comp)) {
    # Update the lag column with shift function
    datatable[, (paste0(col, "_", lag_period)) :=
                shift(get(col), lag_period, fill = 0, type = "lag"),
              by = Classroom.ID]
  } else {
    for (comp in 1:n_comp) {
      # Update the lag column with shift function
      datatable <- datatable[, (paste0(col, comp, "_", lag_period)) :=
                              shift(get(paste0(col, comp)), lag_period, fill = 0, type = "lag"),
                            by = Classroom.ID]
    }
  }
  
  return(datatable)
}

create_formula <- function(method, lag, comp) {
  if (lag == 1) {
    return(as.formula(
    paste0(method, comp, " ~ ",
           paste0(method, comp, "_", 1), " + ",
           "Badges.per.Active.User",
           " + ", "week_lag")))
  }
  return(as.formula(
    paste0(method, comp, " ~ ",
           paste0(method, comp, "_", 1:lag, collapse = " + "), " + ",
           "Badges.per.Active.User + ",
           paste0("Badges.per.Active.User", "_", 1:(lag - 1), collapse = " + "),
           " + ", "week_lag")))
}

model_selection <- function(wi, re) {
  if (phtest(wi, re)$p.value < 0.05) return(wi) else return(re)
}

create_model <- function(formula, data) {
  # Models
  wi <- plm(formula, data = data, effect = "twoway", model = "within")
  re <- plm(formula, data = data, effect = "twoway", model = "random")
  
  return(model_selection(wi, re))
}

# Data and Variables
df <- setDT(df)
results_df_list <- list()
lags <- c(1:8)
n_comp = 3
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler", "Autoencoder")
columns <- c("tch_min", "Badges.per.Active.User", methods)

# Create lags
for (col in columns) {
  for (lag_period in 1:n_lags) {
    if (!col %in% methods) {
      df <- get_lag_value(df, col, lag_period)
      next
    }
    df <- get_lag_value(df, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- pdata.frame(df[set == "train"], index = c("Classroom.ID", "week", "Teacher.User.ID"))
test_data <- pdata.frame(df[set == "test"], index = c("Classroom.ID", "week", "Teacher.User.ID"))

# Estimation
cl <- makeCluster(detectCores())
registerDoParallel(cl)
params <- expand.grid(col = c("tch_min", methods),
                      lag = lags,
                      comp = 1:n_comp) %>%
  mutate(comp = as.character(comp),
         comp = case_when(col == "tch_min" ~ "",
                          .default = comp)) %>%
  unique()
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .export = ls(),
                   .packages = "plm") %dopar% {
  col <- as.character(params$col[i])
  lag <- params$lag[i]
  comp <- params$comp[i]
  formula <- create_formula(col, lag, comp)
  model <- create_model(formula, train_data)

  # Out of Sample Log Likelihood
  predictions <- predict(model, newdata = test_data, na.fill = TRUE)
  residuals <- test_data[,paste0(col, comp)] - predictions

  # Return the results as a list
  list(Method = col,
       Component = comp,
       Lag = lag,
       nloglik = compute_nloglik(residuals),
       bic = as.numeric(bic_plm(model)),
       coef = summary(model)$coefficients)
}
# Stop the cluster
stopCluster(cl)
```

```{r}
#| label: fig-panel-bic
#| fig-cap: ""
# Wrangling the results data frame for plotting
results_df <- map_dfr(results, ~as.data.frame(t(unlist(.))))
# Convert variables to appropriate data types
results_df <- results_df %>%
  mutate(Method = as.factor(Method),
         Component = as.numeric(Component),
         Lag = as.numeric(Lag),
         nloglik = as.numeric(nloglik),
         bic = as.numeric(bic)) %>%
  group_by(Method, Lag) %>%
  summarise(avg_bic = mean(bic, na.rm = TRUE),
            avg_nloglik = mean(nloglik, na.rm = TRUE),
            .groups = "drop")
# Filter methods for the plots
methods_for_plots <- c("FrobeniusNNDSVD", "KullbackLeibler", "FrobeniusNNDSVDA")
plot_data <- filter(results_df, Method %in% methods_for_plots)
# Shared aesthetics for the plots
aes <- ggplot2::aes(x = Lag, y = value, color = Method)

# Summarize methods not included in the plots and prepare tables
methods_for_summary <- setdiff(unique(results_df$Method), methods_for_plots)
summary_data <- filter(results_df, Method %in% methods_for_summary) %>%
  group_by(Method) %>%
  summarise(BIC = mean(avg_bic, na.rm = TRUE),
            NLL = mean(avg_nloglik, na.rm = TRUE),
            .groups = "drop")
# Rename method
levels(summary_data$Method)[levels(summary_data$Method)=="tch_min"] <- "Minutes"
# Convert to scientific notation
summary_data$BIC <- format(summary_data$BIC, scientific = TRUE, digits = 2)
summary_data$NLL <- format(summary_data$NLL, scientific = TRUE, digits = 2)

# Create tables and remove headers
bic_table <- summary_data %>% select(Method, BIC) %>%
  tableGrob(theme = ttheme_minimal(base_size = 8, 
                                   core=list(bg_params = list(fill = "white", col=NA),
                                             fg_params=list(fontface=3))),
             rows = NULL)
bic_table <- bic_table[-1, ]  # Remove header
bic_data <- filter(plot_data, !is.na(avg_bic)) %>% 
  mutate(value = avg_bic, metric = "BIC")
# BIC Plot
bic_plot <- ggplot(bic_data, aes) + 
  geom_line() + 
  labs(title = "BIC", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none") +
  annotation_custom(bic_table, xmin = max(bic_data$Lag) - 3.5,
                    xmax = max(bic_data$Lag) - 0.5,
                    ymin = 1.01*max(bic_data$value),
                    ymax = Inf)

nll_table <- summary_data %>% select(Method, NLL) %>%
  tableGrob(theme = ttheme_minimal(base_size = 8, 
                                   core=list(bg_params = list(fill = "white", col=NA),
                                             fg_params=list(fontface=3))), 
             rows = NULL)
nll_table <- nll_table[-1, ]  # Remove header
nll_data <- filter(plot_data, !is.na(avg_nloglik)) %>% 
  mutate(value = avg_nloglik, metric = "Negative Log-Likelihood")
nll_plot <- ggplot(nll_data, aes) + 
  geom_line() + 
  labs(title = "Out-of-sample NLL", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none") +
  annotation_custom(nll_table, xmin = max(nll_data$Lag) - 3.5,
                    xmax = max(nll_data$Lag) - 0.5,
                    ymin = 1.01*max(nll_data$value),
                    ymax = Inf)

# Combine the plots and add a legend
combined_plot <- ggarrange(bic_plot, nll_plot,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
combined_plot

```

The trade-off between model complexity and predictive accuracy justifies including four lags. Including more lags would increase the complexity of the model, potentially leading to overfitting and poorer predictive performance. Conversely, including fewer lags might result in a model that fails to capture critical temporal dependencies in the data. The selection of four lags represents an inflection point in the BIC and NLL, indicating an optimal balance between model complexity and predictive accuracy.

In @fig-lags, we present the estimated coefficients of the lagged variables as derived from the random effects models. The lines represent the regression coefficients of different variables and their standard errors. The grey line and shaded area correspond to the coefficients for the lagged Badges per Student variable. This graphical representation elucidates the diminishing influence of each variable as the lag increases.

Taking the Frobenius 1 component as an example, the coefficient for the first lag is approximately 0.30, accompanied by a standard error of 0.25. As the lag increases, there is a noticeable decrease in the coefficient, implying a waning influence of this component over time. In contrast, the Badges per Student variable demonstrates a different pattern. First, the magnitude of these coefficients is smaller and non-significant. The coefficient also fluctuates around zero as the lag increases, suggesting a relatively consistent influence over time.

```{r}
#| label: fig-lags
#| fig-cap: "The estimated coefficients of the lagged variables in the random effects models. The lines represent different variables, and the shaded areas indicate the standard errors of the coefficients. The grey line and shaded area represent the coefficients for the lagged Badges per Student."

# Extract coefficients from models
lag_period = 4
col = "FrobeniusNNDSVD"
select_model <- function(model) {
  model[["Method"]] == col & model[["Lag"]] == lag_period
}
model_coeffs <- Filter(select_model, results)

extract_coeffs <- function(model) {
  coef_df <- data.frame(coeff_value_estimate = model$coef[,1],
                        coeff_value_se = model$coef[,2])
  coef_df$Coefficient_Name <- rownames(model$coef)
  # Add other model properties
  coef_df$Method <- model$Method
  coef_df$Component <- model$Component
  return(coef_df)
}
df_estimates <- do.call(rbind, lapply(model_coeffs, extract_coeffs)) %>%
  filter(Coefficient_Name != "(Intercept)" &
           Coefficient_Name != "week_lag") %>%
  mutate(Lag = as.numeric(str_extract(Coefficient_Name, "\\d+$")),
         Coefficient_Name = case_when(
           grepl("FrobeniusNNDSVD1", Coefficient_Name) ~ "Frobenius 1",
           grepl("FrobeniusNNDSVD2", Coefficient_Name) ~ "Frobenius 2",
           grepl("FrobeniusNNDSVD3", Coefficient_Name) ~ "Frobenius 3",
           grepl("Badges.per.Active.User", Coefficient_Name) ~ "Badges",
           TRUE ~ "Other"),
         Lag = case_when(is.na(Lag) ~ 1,
                         Coefficient_Name == "Badges" ~ Lag + 1,
                         .default = Lag),
         Coefficient_Name = case_when(Coefficient_Name == "Badges" ~ 
                                 paste0(Coefficient_Name, " ", Component),
                               .default = Coefficient_Name))

plot_comp <- ggplot(df_estimates %>% filter(!grepl("Badges", Coefficient_Name)),
       aes(x = Lag,
           y = coeff_value_estimate,
           color = Component,
           group = Component)) +
  geom_line() +
  geom_ribbon(aes(ymin = coeff_value_estimate - coeff_value_se,
                  ymax = coeff_value_estimate + coeff_value_se),
              alpha = 0.1) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 0.2) +
  facet_wrap(~Coefficient_Name, scales = "free_y") +
  labs(x = "Lag", y = "Coefficient", linetype = "") +
  theme_light() +
  theme(legend.position = "none")
plot_badge <- ggplot(data = df_estimates %>% filter(grepl("Badges", Coefficient_Name)),
            aes(x = Lag,
                y = coeff_value_estimate,
                group = Component,
                linetype = "Badges")) +
  geom_line(color = "black") +
  geom_ribbon(aes(ymin = coeff_value_estimate - coeff_value_se,
                  ymax = coeff_value_estimate + coeff_value_se),
              alpha = 0.1,
              fill = "grey",
              color = "grey") +  # set color to gray
  geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 0.2) +
  facet_wrap(~Coefficient_Name, scales = "free_y") +
  labs(x = "Lag", y = "Coefficient", linetype = "") +
  theme_light() +
  theme(legend.position = "none")

grid.arrange(plot_comp, plot_badge)

```

## Optimal Model Selection

In our pursuit to identify the most parsimonious model that optimally fits the data, we conducted a comparative analysis of the out-of-sample negative likelihood (NLL) across four distinct reinforcement learning models: State-Based Q-Learning, Kernalized Q-Learning, State-Free Q-Learning, and Actor-Critic. We evaluated these models using three different methods for non-negative matrix factorization: Frobenius (NNDSVD), Frobenius (NNDSVDA), and Kullback-Leibler. Each cell in @fig-panel-bic represents the median negative log-likelihood of a model given a specific method, with lower values signifying a superior model fit.

Our analysis reveals that the Kernalized Q-Learning model, when evaluated using the Frobenius (NNDSVD) method, provides the most optimal fit to the data, as evidenced by its lowest negative log-likelihood value. As a result, we select this combination of model and method for our remaining analyses.

```{r}
#| label: tbl-choose-RL-model
#| tbl-cap: "The table presents a comparison of the median negative log-likelihood values for posterior distributions across four reinforcement learning models: State-Based Q-Learning, Kernalized Q-Learning, State-Free Q-Learning, and Actor-Critic. These models are evaluated based on three methods for non-negative matrix factorization: Frobenius (NNDSVD), Frobenius (NNDSVDA), and Kullback-Leibler. Each cell in the table represents the median negative log-likelihood of a model's posterior given a method, with lower values indicating better model fit. The best performing combination of model and method (i.e., the one with the lowest negative log-likelihood value) is highlighted in light green."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]

# Function to extract median of lp__ from a model
extract_lp_median <- function(model_file) {
  fit <- readRDS(model_file)
  lp_median <- fit$summary("lp__", "median")  # Get the median of lp__
  data.frame(Model = gsub("Bayesian/Results/||.RDS", "", model_file),
             NLL_Median = -1 * as.numeric(lp_median))
}

# Apply function to all models and combine into one dataframe
lp_df <- purrr::map_df(results_files, extract_lp_median) %>%
  na.omit()
lp_df$Method <- gsub(".*-", "", lp_df$Model)
lp_df$ModelType <- stringr::str_extract(lp_df$Model, ".*(?=-)")

# Define descriptive names
model_names <- c("Q-learning-states" = "State-Based Q-Learning",
                 "Q-learning-kernel" = "Kernalized Q-Learning",
                 "Q-learning" = "State-Free Q-Learning",
                 "Actor-Critic" = "Actor-Critic")

method_names <- c("FR" = "Frobenius (NNDSVD)",
                  "FRa" = "Frobenius (NNDSVDA)",
                  "KL" = "Kullback-Leibler")

# Apply the descriptive names
lp_df$ModelType <- model_names[lp_df$ModelType]
lp_df$Method <- method_names[lp_df$Method]

# Create the table with models as rows and NMF methods as columns
table_df <- lp_df %>%
  dplyr::select(-Model) %>%
  tidyr::pivot_wider(names_from = Method, values_from = NLL_Median) %>%
  arrange(desc(ModelType)) %>%
  na.omit()

# Remove redundant 'ModelType' label
colnames(table_df)[colnames(table_df) == "ModelType"] <- "Model"

# To highlight the best value in the table
table_df %>%
  gt::gt() %>%
  gt::tab_style(
    style = gt::cell_fill(color = "lightgreen"),
    locations = gt::cells_body(
      columns = names(table_df)[colSums(table_df == min(table_df[,-1])) >= 1],
      rows = rowSums(table_df == min(table_df[,-1])) >= 1
    )
  )

```

## Model Comparison: Reinforcement Learning and Logistic Regression

We compared our base models, which used logistic regression, and our top-performing reinforcement learning (RL) model, which employed kernelized Q-learning, to identify which best fit the data. @tbl-RL-logit-comp presents the Leave-One-Out Information Criterion (LOOIC) estimates for these models, with lower values indicative of superior model performance.

Our analysis revealed that the hierarchical Q-learning model outperformed the others, as evidenced by its lowest LOOIC value. This finding suggests that reinforcement learning provides a more accurate representation of teacher behavior on Zearn when individual parameters are fitted. The hierarchical logistic regression model followed closely, demonstrating competitive performance. However, the models that did not incorporate a hierarchical structure yielded higher LOOIC values, indicating a lesser fit to the data. These findings highlight the significant heterogeneity in the data and emphasize the value of reinforcement learning models, particularly those with a hierarchical structure, in accurately capturing the dynamics of teacher behavior on Zearn.

```{r Bayesian LOOIC prep}
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py", "get_lag_value")))
gc(verbose = FALSE)
```

```{r}
#| cache: true
#| label: tbl-RL-logit-comp
#| tbl-cap: "Model comparison using Leave-One-Out Information Criterion (LOOIC). LOOIC values, a measure of model quality, were calculated for each model type. Lower values indicate better model performance. Q-learning models were built using a kernel-based approach. Hierarchical models incorporate a hierarchical structure to account for classroom-level variations."
library(brms)
library(loo)

# Non-hierarchical models
## Q-learning model
post <- read_rds("Bayesian/Results/Q-learning-kernel-FR.RDS")
loo_qlearn <- post$loo()$estimates["looic", ]
## Logit
models_nh <- readRDS("Bayesian/Results/logit.RDS")
loo_nh <- sapply(models_nh, loo)
looic_nh <- sum(unlist(loo_nh["looic",]))

# Hierarchical models
## Q-learning
post_hierarchical <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
loo_qhierarchical <- post_hierarchical$loo()$estimates["looic", ]
## Logit
models_h <- readRDS("Bayesian/Results/logit-hierarchical.RDS")
loo_h <- sapply(models_h, loo)
looic_h <- sum(unlist(loo_h["looic",]))

# Collect LOOIC values
looic_values <- c(loo_qlearn[1],looic_nh, 
                  loo_qhierarchical[1], looic_h)
df_looic <- data.frame(Model = c("Q-learning", "Logistic regression", "Hierarchical Q-learning", "Hierarchical logistic regression"),
                       LOOIC = looic_values)

# Create gt table
df_looic %>%
  gt() %>%
  cols_label(
    Model = "Model",
    LOOIC = "LOOIC Value"
  )

# Create gt table
gt(df_looic)

```

## Feature Selection

<!-- - Present the 4 models that stood out during the first estimation. -->
<!-- - Discuss the HBI results and model comparison statistics. -->

## Model Fit

We compared the hierarchical model's predictions and the original choice data, representing a distinct action. We averaged the model predictions across teachers weekly and overlayed them with the average action from the choice data. We also calculated the standard error around these averages accounting for missing data, which provided a measure of uncertainty around these values.

@fig-model-fit shows the model fit for each action where the y-axis represents the probability of a=1, and the x-axis represents the time in weeks. The line plot includes the mean probabilities, highlighted by colored lines, and their respective standard errors, represented by shaded ribbons surrounding the lines. The results section of the data for 'Action 1' revealed that the model predictions initially remained relatively close to the mean probability of 0.5, demonstrating some variability but remaining within a reasonable range. The model's variability increased over time, denoted by the broadening standard error ribbons, peaking at week 36 with a significant increase in the mean predicted probability. This peak corresponded to a drastic decline in the choice data, implying a divergence between the model's predictions and the data towards the end of the observed period.

In the case of 'Action 2' and 'Action 3,' the model prediction started at the mean probability of 0.5 and demonstrated a declining trend over the weeks. Although the declining trend was present in both model fit and data, the model predicted a less drastic decline. The standard error for this action also increased over time, albeit not as dramatically as in the case of 'Action 1', indicating that the model's predictions became more uncertain as time progressed. Despite this variability, the model provided a reasonable fit for the choice data across the weeks for the first months of the study period.

```{r}
#| cache: true
#| label: fig-model-fit
#| fig-cap: "Model predictions for teacher actions compared with choice data over time. The y-axis denotes the probability of a particular action being taken (a=1), while the x-axis indicates time in weeks. The figure showcases three distinct actions (NMF components). The solid lines represent the weekly averaged model predictions for each action, while the shaded ribbons denote the respective standard errors."
stan_data <- read_rds("./Bayesian/stan_data.RDS")
choice_data <- stan_data$choice

prediction_hierarchical <- read.csv("./Bayesian/Results/prediction_hierarchical.csv") %>%
  filter(grepl("y_pred", variable)) %>%
  dplyr::select(variable, mean)

prediction_hierarchical <- prediction_hierarchical %>%
  mutate(variable = str_extract(variable, "\\[.*\\]"),
         variable = str_replace_all(variable, "\\[|\\]", "")) %>%
  separate(variable, into = c("dim1", "dim2", "dim3"), sep = ",", convert = TRUE)

prediction_hierarchical_3d <- array(dim = c(max(prediction_hierarchical$dim1),
                                            max(prediction_hierarchical$dim2),
                                            max(prediction_hierarchical$dim3)))

for (i in 1:nrow(prediction_hierarchical)) {
  dim1 <- prediction_hierarchical$dim1[i]
  dim2 <- prediction_hierarchical$dim2[i]
  dim3 <- prediction_hierarchical$dim3[i]
  prediction_hierarchical_3d[dim1, dim2, dim3] <- prediction_hierarchical$mean[i]
}

prediction_hierarchical_3d[prediction_hierarchical_3d == 0] <- NA
choice_data[is.na(prediction_hierarchical_3d)] <- NA

# Get the number of layers
num_layers <- dim(prediction_hierarchical_3d)[3]
# Custom function to calculate standard error based on the number of non-NA elements
calc_se <- function(x) sd(x, na.rm = TRUE) / sqrt(sum(!is.na(x)))

df_compare <- data.frame()

# Generate a plot for each layer
for (k in 1:num_layers) {
  y_pred_avg <- apply(prediction_hierarchical_3d[, , k], 2, mean, na.rm = TRUE)
  y_pred_se <- apply(prediction_hierarchical_3d[, , k], 2, calc_se)
  
  choice_data_avg <- apply(choice_data[, , k], 2, mean, na.rm = TRUE)
  choice_data_se <- apply(choice_data[, , k], 2, calc_se)

  weeks <- seq_len(length(y_pred_avg))

  df_pred <- data.frame(weeks = weeks,
                        probability = y_pred_avg,
                        type = rep("Model Fit", length(y_pred_avg)),
                        se = y_pred_se,
                        action = rep(paste("Action", k), length(y_pred_avg)))

  df_real <- data.frame(weeks = weeks,
                        probability = choice_data_avg,
                        type = rep("Real Data", length(choice_data_avg)),
                        se = choice_data_se,
                        action = rep(paste("Action", k), length(choice_data_avg)))

  df_compare <- rbind(df_compare, df_pred, df_real)
}

p <- ggplot(df_compare, aes(x = weeks, y = probability, color = type)) +
  geom_line() +
  geom_ribbon(data = df_compare, aes(ymin = probability - se, ymax = probability + se, fill = type), alpha = 0.1) +
  labs(x = "Week", y = "Probability of a=1") +
  facet_wrap(~action, ncol = 1) +
  scale_color_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  scale_fill_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  theme_bw()

print(p)

```

```{r}
#| eval: false

stan_data <- read_rds("./Bayesian/stan_data.RDS")
choice_data <- stan_data$choice

# prediction_hierarchical <- post_hierarchical$summary()
prediction_hierarchical <- read.csv("./Bayesian/Results/prediction_hierarchical.csv") %>%
  filter(grepl("y_pred", variable)) %>%
  dplyr::select(variable, mean)
prediction_hierarchical <- prediction_hierarchical %>%
  mutate(variable = str_extract(variable, "\\[.*\\]"),
         variable = str_replace_all(variable, "\\[|\\]", "")) %>%
  separate(variable, into = c("dim1", "dim2", "dim3"), sep = ",", convert = TRUE)
# Create an empty 3D array
prediction_hierarchical_3d <- array(dim = c(max(prediction_hierarchical$dim1),
                                            max(prediction_hierarchical$dim2),
                                            max(prediction_hierarchical$dim3)))
# Fill the 3D array with the 'mean' values
for (i in 1:nrow(prediction_hierarchical)) {
  dim1 <- prediction_hierarchical$dim1[i]
  dim2 <- prediction_hierarchical$dim2[i]
  dim3 <- prediction_hierarchical$dim3[i]
  prediction_hierarchical_3d[dim1, dim2, dim3] <- prediction_hierarchical$mean[i]
}
# Now prediction_hierarchical_3d is a 3D array like choice_data.

prediction_hierarchical_3d[prediction_hierarchical_3d == 0] <- NA
choice_data[is.na(prediction_hierarchical_3d)] <- NA

y_pred_avg <- apply(prediction_hierarchical_3d, c(2, 3), mean, na.rm = TRUE)
y_pred_se <- apply(prediction_hierarchical_3d, c(2, 3), sd, na.rm = TRUE) / sqrt(dim(prediction_hierarchical_3d)[1])

choice_data_avg <- apply(choice_data, c(2, 3), mean, na.rm = TRUE)

weeks <- seq_len(dim(y_pred_avg)[1])

# Create data frames
df_pred <- data.frame(weeks = rep(weeks, dim(y_pred_avg)[2]),
                      probability = c(y_pred_avg),
                      type = rep("Model Fit", length(y_pred_avg)),
                      se = c(y_pred_se))

df_real <- data.frame(weeks = rep(weeks, dim(choice_data_avg)[2]),
                      probability = c(choice_data_avg),
                      type = rep("Real Data", length(choice_data_avg)),
                      se = rep(NA, length(choice_data_avg)))
# Combine data frames
df_compare <- rbind(df_pred, df_real)

# Plot
ggplot(df_compare, aes(x = weeks, y = probability, color = type)) +
  geom_line() +
  geom_ribbon(data = df_pred, aes(ymin = probability - se, ymax = probability + se), alpha = 0.1) +
  labs(x = "Number of Weeks", y = "Probability of Y=1") +
  scale_color_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  theme_bw()


```

## Optimality

To understand the factors contributing to a teacher's ability to maximize lesson completion, we conducted an in-depth analysis of teachers' performance across various parameters from the previous hierarchical model. We focused specifically on the learning rate ("Alpha") and the inverse temperature ("Tau"). We examined their correlation with the average weekly badges earned per teacher, a proxy for lesson completion, and Tower Alerts, a measure of student struggle with the materials.

@tbl-optimality presents the coefficients of six different linear regression models. Each model predicts the average weekly badges earned per teacher (Models 1-3) or the average weekly Tower Alerts (Models 4-6) based on different combinations of the parameters and control variables. The control variables include the number of active students, the number of classes taught by the teacher, the grade level, the number of weeks, the poverty level, the income level, whether the school is a charter school, and whether the school has a paid account.

Alpha and Tau achieved statistical significance when adding all the control variables (columns 3 and 6, respectively). These results suggest that a higher learning rate may lead to fewer badges earned and more Tower Alerts. A higher inverse temperature is associated with a slight increase in badges earned.

```{r, results='asis'}
#| cache: true
#| label: tbl-optimality
#| tbl-cap: "The impact of different parameters and control variables on average weekly badges and Tower Alerts. Six linear regression models were used to examine the correlations between a teacher's RL parameters (Cost 1, Cost 2, Cost 3, Gamma, Alpha, Tau) and two measures of student engagement: average weekly Badges earned per student (Models 1-3) and average weekly Tower Alerts per student (Models 4-6). Models 2-3 and 5-6 also control for other variables including number of active students, number of classes taught, grade level, weeks, poverty level, income level, whether the school is a charter school, and whether the school has a paid account. Coefficients and standard errors are provided for each parameter in each model."

library(tidybayes)
library(stargazer)

hierarchical_model <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
classroom_data <- read.csv("Bayesian/df_subset.csv") %>%
  group_by(Teacher.User.ID) %>%
  summarise(n_active_students = mean(Active.Users...Total),
            n_students = mean(Students...Total),
            minutes_students = mean(Minutes.per.Active.User),
            badges = mean(Badges.per.Active.User),
            boosts = mean(Boosts.per.Tower.Completion),
            tower_alers = mean(Tower.Alerts.per.Tower.Completion),
            n_classes_by_teacher = median(teacher_number_classes),
            grade = first(Grade.Level),
            n_weeks = mean(n_weeks),
            poverty = first(poverty),
            income = first(income),
            charter_school = first(charter.school),
            school_account = first(school.account))

posterior_samples <- hierarchical_model$draws() %>%
  spread_draws(cost[207,3],
               gamma[207],
               alpha[207], 
               tau[207])

summary_cost <- posterior_samples %>%
  unnest_wider(cost, names_sep = "_")
list_of_dfs <- lapply(seq_along(summary_cost)[grepl("cost", names(summary_cost))], function(x){
  temp_df <- as.data.frame(summary_cost[[x]])
  names(temp_df) <- paste0("C", 1:ncol(temp_df))
  temp_df$teacher <- paste0("teacher_", (x - 3))
  return(temp_df)
})
cost_df <- do.call(rbind, list_of_dfs)
cost_df$teacher <- as.numeric(gsub("teacher_", "", cost_df$teacher)) 
summary_cost <- cost_df %>% 
  group_by(teacher) %>% 
  summarise(across(starts_with("C"), mean, .names = "mean_{.col}"))

summary_gamma_alpha_tau <- posterior_samples %>%
  unnest_wider(gamma, names_sep = "_") %>%
  unnest_wider(alpha, names_sep = "_") %>%
  unnest_wider(tau, names_sep = "_") %>%
  summarise(across(c(starts_with("gamma"),
                     starts_with("alpha"),
                     starts_with("tau")),
                   mean, .names = "{.col}")) %>%
  pivot_longer(cols = c(starts_with("gamma"),
                        starts_with("alpha"),
                        starts_with("tau")),
               names_to = "variable",
               values_to = "value") %>%
  separate(variable, into = c("type", "teacher"), sep = "_", convert = TRUE) %>%
  pivot_wider(names_from = type, values_from = value)

# merge the two dataframes
summary_all <- merge(summary_cost, summary_gamma_alpha_tau, by = "teacher")

classroom_data <- classroom_data %>%
  arrange(Teacher.User.ID) %>%
  bind_cols(summary_all) %>%
  dplyr::mutate(
    income = factor(income, ordered = TRUE), 
    grade = factor(grade, ordered = TRUE,
                   levels = c("Kindergarten",
                              "1st", "2nd", "3rd", "4th", "5th")),
    poverty = factor(poverty, ordered = TRUE)
  )

ordered_factor <- function(fact_var) {
  categories <- levels(fact_var)
  n_cat <- length(categories)
  cont <- matrix(data = 0, nrow = n_cat, ncol = (n_cat - 1))
  cont[col(cont) < row(cont)] <- 1
  rownames(cont) <- categories
  colnames(cont) <- paste(categories[2:n_cat],
                          categories[1:(n_cat - 1)],
                          sep = " vs. ")
  contrasts(fact_var) <- cont
  return(fact_var)
}
classroom_data <- classroom_data %>%
  dplyr::mutate(
    income = ordered_factor(income), 
    grade = ordered_factor(grade),
    poverty = ordered_factor(poverty)
  )

model1 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau,
          classroom_data)

model2 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks,
          classroom_data)

model3 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks +
            poverty +
            income +
            charter_school +
            school_account,
          classroom_data)

model4 <- lm(tower_alers ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau,
          classroom_data)

model5 <- lm(tower_alers ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks,
          classroom_data)

model6 <- lm(tower_alers ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks +
            poverty +
            income +
            charter_school +
            school_account,
          classroom_data)

# Create table with stargazer
stargazer(model1, model2, model3, model4, model5, model6,
          type = "latex",
          header=FALSE,
          align = TRUE,
          covariate.labels = c("Cost 1", "Cost 2", "Cost 3",
                               "Gamma", "Alpha", "Tau",
                               "Number of Active Students", "Number of Classes",
                               "Number of Weeks", "Charter School", "Paid School Account"),
          omit = c("grade", "poverty", "income"),
          dep.var.caption = "Dependent variables:",
          dep.var.labels = c("Badges", "Tower Alerts"),
          star.cutoffs = c(.05, .01, .001), 
          star.char = c("*", "**", "***"),
          add.lines = list(c("Control for Grade Level",
                             "", "Yes", "Yes", "", "Yes", "Yes"),
                           c("Control for Poverty Level",
                             "", "", "Yes", "", "", "Yes"),
                           c("Control for Income Level",
                             "", "", "Yes", "", "", "Yes")))


```

### Feature Selection Model Fit

- Discuss the best-performing model using students online as a reward, and alerts, badges, and minutes as state variables.

## Heterogeneity

Building on these results, we investigated how teachers' adaptation and implementation of the Zearn Math curriculum vary with various school-specific and demographic factors. The goal was to uncover potential disparities and leverage points that could inform intervention strategies and policy decisions to reduce educational inequities.

Our multi-pronged statistical approach matched the diverse nature of our classroom variables. We selected each model to best account for the distribution and characteristics of the dependent variable. We used an ordered logistic regression for ordinal classroom variables, such as Income Level and Poverty Level. Binary variables, which included Charter School and Paid School Account status, naturally lent themselves to logistic regression modeling. Finally, we used Poisson regression to model the variable representing the number of classes taught by each teacher.

@fig-heterogeneity presents a detailed summary of the associations between different factors and classroom variables. Among these, the interaction between "Cost 3" and "Poverty Level" is notable. Our data suggest that a higher estimated cost of applying Pedagogical Content Knowledge predicts a lower median income level in the school. The estimated Discount Rate is another variable significantly associated with a school's Poverty Level ($\beta = -0.39,\text{ } p<0.05$). Furthermore, the negative relationship between being a Charter School and the Learning Rate, with a coefficient of -0.67 (p\<0.001), suggests that teachers in charter schools are slower to adapt and modify their teaching methods in response to new information.

```{r}
#| cache: true
#| label: fig-heterogeneity
#| fig-cap: "Predicting classroom variables with RL parameters. The data comes from a comprehensive set of classroom records, including income level, poverty level, charter school status, whether a school account is paid, and the number of classes each teaches. These classroom variables (columns) are regressed on the estimated cost parameters for each action (Cost 1, Cost 2, Cost 3), the discount rate, the learning rate, and the inverse temperature. Each cell in the heatmap represents the estimate of a linear, Poisson, or logistic regression model, depending on the variable type. For ordinal classroom variables (Income Level and Poverty Level), ordered logistic regression (proportional odds model) is used, while for binary variables (Charter School, Paid School Account), logistic regression is applied. The number of classes by each teacher, being a count data, is modeled with Poisson regression. The coefficients are scaled estimates of the effect of each parameter on the respective classroom variable. The asterisks indicate the level of statistical significance based on p-values (*p < 0.1; **p < 0.01; ***p < 0.001). The coefficients are color-coded with a gradient from light blue (negative) to white (zero) to dark blue (positive)."
library(broom)
library(MASS)

# Prepare data
hetero <- classroom_data %>%
  dplyr::select(income, poverty, charter_school, school_account, n_classes_by_teacher,
         mean_C1, mean_C2, mean_C3, gamma, alpha, tau) %>%
  dplyr::mutate(
    income = factor(income, ordered = TRUE),
    poverty = factor(poverty, ordered = TRUE)
  )

# Define predictors and responses
responses <- c("income", "poverty",
               "charter_school", "school_account", "n_classes_by_teacher")
predictors <- c("mean_C1", "mean_C2", "mean_C3", "gamma", "alpha", "tau")
hetero[predictors] <- scale(hetero[predictors])
# Create the formula for the model with all predictors
predictors_formula <- paste(predictors, collapse = " + ")

# Run models for each response, and extract coefficients
coef_matrix <- map_dfr(responses, function(response) {
  if (response == "income" | response == "poverty") {
    model <- polr(as.formula(paste0(response, " ~ ", predictors_formula)),
                  data = hetero, Hess = TRUE)
  } else if (response == "n_classes_by_teacher") {
    model <- glm(as.formula(paste0(response, " ~ ", predictors_formula)),
                 data = hetero, family = poisson(link = "log"))
  } else {
    model <- glm(as.formula(paste0(response, " ~ ", predictors_formula)),
                 data = hetero, family = binomial(link = "logit"))
  }
  
  tidy(model, p.values = TRUE) %>%
    filter(term %in% predictors) %>%
    dplyr::select(term, estimate, p.value) %>%
    mutate(response = response,
           sig = ifelse(p.value < 0.05, "***",
                        ifelse(p.value < 0.01, "**",
                               ifelse(p.value < 0.1, "*", ""))))
})

# Convert to wide format
coef_matrix <- coef_matrix %>%
  pivot_wider(names_from = term, values_from = c(estimate, p.value, sig))

# Reorder the rows to match the original order of responses
coef_matrix <- coef_matrix %>%
  mutate(response = factor(response, levels = responses)) %>%
  arrange(response)

# Reshape for plotting
coef_matrix_long <- coef_matrix %>%
  pivot_longer(cols = starts_with("estimate"), names_to = "term", values_to = "estimate") %>%
  pivot_longer(cols = starts_with("sig"), names_to = "term_sig", values_to = "sig")

# Clean up term names
coef_matrix_long$term <- str_replace(coef_matrix_long$term, "estimate_", "")
coef_matrix_long$term_sig <- str_replace(coef_matrix_long$term_sig, "sig_", "")

# Make sure the term columns match
coef_matrix_long <- coef_matrix_long[coef_matrix_long$term == coef_matrix_long$term_sig,]

# Rename the variables
var_names <- c("income" = "Income Level", "poverty" = "Poverty Level",
               "charter_school" = "Charter School",
               "school_account" = "Paid School Account",
               "n_classes_by_teacher" = "Number of Classes by Teacher",
               "mean_C1" = "Cost 1", "mean_C2" = "Cost 2", "mean_C3" = "Cost 3",
               "gamma" = "Discount Rate",
               "alpha" = "Learning Rate",
               "tau" = "Inverse Temperature")
coef_matrix_long$term <- var_names[coef_matrix_long$term]

responses_new <- c("Income\n Level", "Poverty\n Level",
                   "Charter\n School", "Paid School\n Account", 
                   "Classes\n per Teacher")

names(responses_new) <- responses

# Update variable labels
coef_matrix_long$response <- factor(coef_matrix_long$response, 
                                    levels = responses, 
                                    labels = responses_new)

# Create the plot
ggplot(coef_matrix_long, aes(x = response, y = term, fill = as.numeric(estimate))) +
  scale_y_discrete(limits = c("Cost 1", "Cost 2", "Cost 3",
                              "Inverse Temperature", "Discount Rate", "Learning Rate")) +
  geom_tile() +
  geom_text(aes(label = paste0(round(as.numeric(estimate), 2), sig)), size = 4) +
  scale_fill_gradient2(low = "lightblue", high = "darkblue", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), 
                       name = "Regression\n Coefficient") +
  scale_x_discrete(position = "top") + # This line moves the x-axis labels to the top
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, angle = 0, vjust = 1, hjust = 0.5), 
        axis.text.y = element_text(size = 12), 
        legend.title = element_text(size = 11), 
        legend.text = element_text(size = 10),
        legend.spacing.x = unit(0.5, "cm"),
        legend.position = "bottom") +
  labs(x = NULL, y = NULL, title = NULL)

```

# Discussion

Our study aimed to unravel the complex dynamics of teacher behavior in the context of Zearn Math, a popular online learning platform. We sought to understand the role of reinforcement learning (RL) in modeling these behaviors and their impact on student achievement. Our findings provide compelling evidence that RL models, particularly the hierarchical Q-learning model, can accurately characterize teacher behavior and offer valuable insights into the education field.

## Characterizing Teacher Behavior

Our first research question aimed to identify the RL model that best characterizes teacher behavior in the Zearn Math context. The hierarchical Q-learning model emerged as the most accurate, outperforming the simple logistic regression model. This result suggests that teachers are not merely following a static "education production function" but are actively learning and adapting their teaching strategies on Zearn Math.

The hierarchical Q-learning model's superiority underscores the importance of considering individual teacher differences. As an agent in the RL model, each teacher has unique parameters that reflect their learning rate and decision-making strategies. These parameters offer a rich source of information about teacher behavior, providing a more nuanced understanding than simpler models.

## Parameter Variations and Their Influence on Student Achievement

Our second research question explored how variations in the parameters of RL models (i.e., the learning rate and the exploration-exploitation trade-off) affect teacher behavior and, consequently, student achievement. The learning rate suggests a teacher's adaptability in altering teaching strategies based on feedback. The exploration-exploitation trade-off encapsulates a teacher's ability to balance using new teaching strategies (exploration) and sticking with known effective strategies (exploitation).

Our findings indicated that teachers exhibiting a higher learning rate, a marker of greater adaptability, were associated with higher student achievement. This finding echoes the sentiments expressed by teachers in [@knudsen2020], who found value in the multifaceted strategies provided by Zearn Math. A 3rd-grade Zearn teacher stated, "I like that Zearn provides several strategies to get to the answer...you see the problems; you see what you need to hit on and stress the first time around." This sentiment aligns with our observation that the ability to adapt teaching strategies based on feedback, akin to a higher learning rate in RL models, is a valuable attribute in promoting student achievement.

Similarly, teachers who effectively balanced exploration and exploitation had students with superior outcomes. [@morrison2019] noted that teachers exhibited varied levels of preparedness for implementing the Zearn Math curriculum, with just under half of the teachers reporting feeling adequately prepared for implementation. This lack of preparation could influence how effectively teachers navigate the exploration-exploitation trade-off, impacting student outcomes.

Our results also highlight the complex dynamics underlying teacher behaviors and their implications for student outcomes. For instance, [@knudsen2020], found that veteran teachers with strong PCK leaned heavily on traditional teaching methods, reducing their engagement with the novel practices proposed by Zearn Math. Conversely, new teachers exhibited a blend of traditional and innovative practices, despite their initial resistance to Zearn Math, underscoring their learning curve with the new curriculum.

Our findings point to the potential of RL parameters to provide valuable insights into teacher behaviors and their subsequent effects on student achievement. Understanding and leveraging these parameters could enhance teacher training programs and interventions, ultimately improving student outcomes. By incorporating insights from our study into such initiatives, we can help foster a more adaptive and effective teaching landscape that better supports student learning and achievement.

## Influence of Teacher Background, Training, and Experience

Our third research question asked how teacher background, training, and experience influence their adaptation to and implementation of the Zearn Math curriculum. Our findings confirm the fundamental role of teachers' background, training, and experience when implementing new educational platforms like Zearn Math. They also underscore the importance of differentiating support and training strategies to cater to teachers' unique backgrounds and needs, which may prove pivotal in fostering effective adoption and implementation of such platforms. A prominent theme that emerged from our investigation was the interaction between teacher behaviors, school characteristics, and broader socio-economic contexts.

Our analyses revealed a telling association between Poverty Level and Cost 3, denoting the estimated cost of applying PCK, suggesting that schools in higher poverty strata are more likely to encounter additional barriers in the form of resource constraints when implementing the Zearn Math curriculum.

This observation points towards a potential resource disparity where schools in lower-income areas grapple with the challenge of investing in critical teacher training and resources needed for effective curriculum implementation. Implementing a comprehensive math curriculum like Zearn becomes particularly daunting in economically disadvantaged areas where financial constraints may obstruct optimal pedagogical strategies.

Moreover, we unveiled a striking finding regarding charter schools. Our data showed a negative relationship between being a Charter School and the Learning Rate. This finding was unexpected, given the increased flexibility often associated with charter schools. It raises pertinent questions about the pedagogical dynamics within such institutions and points towards the necessity for further research into the support systems and professional development opportunities provided for teachers in these settings.

One of the more disconcerting findings was the negative relationship between Poverty Level and Discount Rate, indicating that teachers in schools with higher poverty levels might place less emphasis on long-term student outcomes. This phenomenon may occur due to the pressing short-term challenges related to student well-being and engagement, often prevalent in high-poverty settings, which demand immediate attention and resources.

These findings underscore the complex challenges facing schools in lower-income and high-poverty areas. They illuminate the complexities teachers must navigate when adapting to new teaching habits and implementing curricula, which are only magnified by demographic and school-specific factors.

Our Reinforcement Learning model uncovered the need for a deeper examination of systemic educational disparities and the development of targeted interventions and policies to bridge these gaps. By shedding light on these complex relationships, we contribute to the broader goal of creating a more equitable educational landscape.

## Implications for Teachers and Schools

Our findings have several important implications for teachers, schools, and the broader education field. First, they highlight the dynamic nature of teaching, with teachers continually learning and adapting their strategies based on feedback. This result underscores the importance of providing teachers with ongoing professional development opportunities and supportive learning environments.

Second, our results suggest that optimal teaching strategies vary among teachers, reflecting individual differences in learning rates and decision-making strategies. This feature points to the need for a more personalized approach to teacher training and support, considering individual teachers' strengths and areas for improvement.

Third, our findings highlight the potential of RL models as tools for understanding and improving teaching practices. By capturing the complex dynamics of teacher behavior, these models can inform the design of interventions to enhance student achievement. For instance, interventions could help teachers improve their learning rates or better balance exploration and exploitation in their teaching strategies.

Finally, our results have policy implications. They suggest that policies aimed at improving student achievement should consider not only the resources available to schools but also teachers' behaviors and decision-making processes. Policies should support teachers' learning and adaptation processes by providing professional development opportunities or creating supportive learning environments.

## Limitations

One of this study's limitations is the available data, including teachers only in Louisiana, which can limit the generalizability of our findings. Second, our data were at the weekly level, with approximately 40 weeks per classroom. To fit more optimally, RL models would need more trials, either over a more extended period or with smaller time units (e.g., twice a week or daily). Another major challenge of this study was finding the best characterization of actions, which required various techniques for dimensionality reduction. Third, we did not exhaust all possible RL models, and other models could better fit the data. Future research could explore other RL models and compare their performance in characterizing teacher behavior and predicting student achievement. Finally, our models made certain assumptions and simplifications, such as the characterization of actions, which could affect the accuracy of our results. For instance, our data did not include the variance of classroom scores, only the averages, which limits our ability to answer questions about how teachers adapt to the distribution of student achievement in their classrooms.

## Future Research

Our study opens several avenues for future research. First, our findings could be validated and extended in other educational contexts, such as different grade levels, subjects, or geographical locations. Second, our RL models could be integrated with other models and approaches to provide a more comprehensive understanding of teacher behavior and its impact on student outcomes. Third, future research could expand the scope of variables and data sources considered in the models, such as incorporating additional variables related to teacher background, training, and experience. Similarly, data from other sources and domains could enrich the models and provide a more nuanced understanding of human behavior in the field.

In conclusion, our study provides compelling evidence for the potential of reinforcement learning models to understand and improve teacher behavior and student achievement in the context of online learning platforms like Zearn Math. By shedding light on the complex dynamics of teacher behavior, our findings offer valuable insights for teachers, schools, policymakers, and researchers in the education field. Our work will inspire further research and practical applications of reinforcement learning in education.

# References

::: {#refs}
:::

# Supplemental Information {.appendix}

## Figures {#sec-supp-fig}

![Zearn Student Portal](images/student-feed.PNG){#fig-st-portal fig-align="center"}

![Professional Development Calendar](images/PD-calendar.jpg){#fig-prof-dev fig-align="center"}

## Zearn's Eye View of the Data

```{r}
#| eval: false
#| label: fig-raw-data
#| fig-cap: "The raw data from Zearn's platform, with each row corresponding to a different timestamp for a teacher action. The columns represent the different variables, including the number of minutes spent on the platform, the number of badges earned, and the number of boosts used. The data is organized by classroom, with each classroom having a different color. The data is also organized by week, with each week having a different shape. The data is organized by teacher, with each teacher having a different line type. The data is organized by school, with each school having a different line color."

```

## Pooled Correlations between Components and Student Outcomes

We meta-analyzed individual correlations within our dataset to reveal the pooled relationships between student outcomes (badges) and the components from the dimensionality reduction procedure. This approach, termed 'internal meta-analysis,' is advantageous when dealing with large datasets with hierarchical structures, as it allows for simultaneously considering multiple, potentially correlated outcomes.

To conduct this internal meta-analysis, we first transformed the correlations using Fisher's z-transformation, ensuring a normal distribution of the correlations. We then ran a random effects model with each unique combination of "Teacher" and "School." This multivariate meta-analysis offers the advantage of modeling multiple, potentially correlated outcomes, providing a comprehensive estimate of the correlations for each outcome, considering the hierarchical structure of the data. The outcome of this process is a robust understanding of the relationships between different outcomes and the Badges across diverse schools and teachers. This approach provides a more resilient analysis than simple correlations, as it accounts for the inherent variability and dependencies within the data. @fig-meta-analysis displays the results and illustrates the pooled correlations between the components and student outcomes.

```{r Meta-correlation prep}
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
# Importing results from Python
results_list <- py$results
gc(verbose = FALSE)
```

```{r}
#| cache: true
#| label: fig-meta-analysis
#| fig-cap: "Results of the correlation meta-analysis."
library(metafor)
library(ppcor)

n_comp <- 3
method <- "FrobeniusNNDSVD"
selected_cols <- c("Classroom.ID", "Teacher.User.ID",
                   "MDR.School.ID", "District.Rollup.ID",
                   "week", "Usage.Week",
                   # Main loadings of Components:
                   "tch_min", "tch_min_1",
                   paste0("FrobeniusNNDSVD", seq_len(n_comp)),
                   paste0("FrobeniusNNDSVD", seq_len(n_comp), "_1"),
                   "RD.optional_problem_sets",
                   "Guided.Practice.Completed", "Tower.Completed",
                   # Student Variables
                   "Active.Users...Total", "Minutes.per.Active.User",
                   "Badges.per.Active.User", "Boosts.per.Tower.Completion",
                   "Tower.Alerts.per.Tower.Completion",
                   # Classroom and Teacher Variables
                   "teacher_number_classes", "Grade.Level",
                   "Students...Total", "n_weeks",
                   # School Variables
                   "poverty", "income", "charter.school",
                   "school.account", "zipcode")  # Column to select

df_corr <- df %>%
  ungroup() %>%
  arrange(Classroom.ID, week) %>%
  dplyr::select(all_of(selected_cols)) %>%
  rename_with(~paste0("Frobenius", seq_len(n_comp)),
              paste0("FrobeniusNNDSVD", seq_len(n_comp))) %>%
  rename_with(~paste0("Frobenius", seq_len(n_comp), "_1"),
              paste0("FrobeniusNNDSVD", seq_len(n_comp), "_1"))

# Define a safe version of pcor.test that returns NA when there's an error
safe_pcor <- possibly(~pcor.test(..1, ..2, ..3, method = "spearman")$estimate,
                      otherwise = NA)
df_corr <- df_corr %>%
  group_by(Classroom.ID,Teacher.User.ID,MDR.School.ID,District.Rollup.ID) %>%
  summarise(
    n = n(),
    Frobenius1 = safe_pcor(Frobenius1, Badges.per.Active.User, Frobenius1_1),
    Frobenius2 = safe_pcor(Frobenius2, Badges.per.Active.User, Frobenius2_1),
    Frobenius3 = safe_pcor(Frobenius3, Badges.per.Active.User, Frobenius3_1),
    Minutes    = safe_pcor(tch_min, Badges.per.Active.User, tch_min_1),
    n_weeks = mean(n_weeks),
    teacher_number_classes = mean(teacher_number_classes),
    poverty = first(poverty),
    income = first(income),
    school.account = mean(school.account)
  ) %>%
  filter(!is.na(Frobenius1) &
           !is.na(Frobenius2) &
           !is.na(Frobenius3))

df_corr_sub <- df_corr %>%
  as_tibble() %>%
  slice_sample(prop = 0.1) %>%
  mutate_at(vars(paste0("Frobenius", seq_len(n_comp)), "Minutes"),
            list(~atanh(.))) %>%  # Fisher's z transformation
  mutate_at(vars(paste0("Frobenius", seq_len(n_comp)), "Minutes"),
            list(se = ~sqrt(1/(n - 2 - 2)))) %>%  # standard error sqrt(1/N2g)
  gather(key = "outcome", value = "correlation",
         c(paste0("Frobenius", seq_len(n_comp)), "Minutes")) %>%
  gather(key = "outcome_se", value = "se",
         c(paste0("Frobenius", seq_len(n_comp), "_se"), "Minutes_se")) %>%
  filter(str_replace(outcome, "_se", "") == str_replace(outcome_se, "_se", "")) %>%
  dplyr::select(-"outcome_se") %>%
  filter(!is.na(se))

# Run multivariate meta-analysis
res <- rma.mv(yi = correlation,
              V = se^2,
              random = ~ 1 | Classroom.ID/Teacher.User.ID/MDR.School.ID/District.Rollup.ID,
              mods = ~ -1 + outcome,
              data = df_corr_sub)

# Add columns for back-transformed effect sizes and their standard errors
res_df <- data.frame(
  estimate = coef(res),
  outcome = str_replace(names(coef(res)), "outcome", ""),
  # se = sqrt(diag(vcov(res)))
  ci.lb = res$ci.lb,
  ci.ub = res$ci.ub
  ) %>%
  mutate(
    estimate_r = tanh(estimate),
    ci.lb = tanh(ci.lb),
    ci.ub = tanh(ci.ub)
    # se_r = se / (1 - estimate^2)  # delta method for SE
  )
# Reverse the Fisher's Z transformation
df_corr_sub <- df_corr_sub %>%
  mutate(correlation = tanh(correlation))
# Create the plot
ggplot(df_corr_sub, aes(x = correlation, y = after_stat(density))) +
  geom_histogram(binwidth = 0.1, fill = "lightblue") +
  geom_density(aes(weight = weights(res)),
               alpha = 0.5) +
  geom_vline(data = res_df,
             aes(xintercept = estimate_r,
                 color = outcome),
             linetype = "dashed") +
  geom_segment(data = res_df,
               aes(x = ci.lb,
                   y = 0,
                   xend = ci.ub,
                   yend = 0,
                   color = outcome),
               linewidth = 1.5) +
  geom_text(data = res_df,
            aes(x = estimate_r,
                y = 0.15,
                label = sub('0\\.', '.', round(estimate_r, 2))),
            color = "black",
            size = 3.5,
            check_overlap = TRUE) +
  facet_wrap(~outcome, scales = "free",  ncol = 2) +
  labs(x = "Correlation", y = "Density") +
  theme_light() +
  scale_color_discrete(name = "Pooled effects \n (with 95% C.I.)")
  theme(legend.position = c(0.85, 0.24),
        legend.direction = "vertical")  # Set legend position and direction

```

```{r meta-analysis summary}
#| eval: false
summary(res)
```

## Bayesian Model Diagnostics

```{r}
#| eval: false
#| cache: true
#| label: tbl-diagnostics
#| tbl-cap: "Diagnostics for the Bayesian Models used in Q-learning and Q-learning with states. The tables show diagnostics for three Non-negative Matrix Factorization (NMF) models: Frobenius (initialized with a Nonnegative Double Singular Value Decomposition), Frobenius Average (initialized with a Nonnegative Double Singular Value Decomposition with zeros filled with the average of each variable), and Kullback-Leibler. The rows correspond to 'alpha,' the learning rate, 'gamma,' the discount factor, 'tau,' the temperature parameter, 'cost' for each of the components from the NMF, 'sensi,' the reward sensitivity parameter, and 'Log Posterior,' the logarithm of the posterior probability. 'Rhat' indicates the potential scale reduction factor on split chains. Values close to 1.0 imply model convergence. 'ESS Bulk' and 'ESS Tail' represent the effective sample size for the bulk of the posterior distribution and its tail, respectively. An ESS of at least a few hundred is often considered sufficient for estimation purposes."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]
results_files <- results_files[!grepl("hierarchical", results_files)]
diag_df <- data.frame()

for (result_file in results_files) {
  fit <- readRDS(result_file)
  
  # Get summary
  fit_summary <- fit$summary()
  
  # Create a data frame of the Rhat values and ESS
  result_df <- data.frame(
    Model = gsub("Bayesian/Results/||.RDS", "", result_file),
    Variable = fit_summary$variable,
    Rhat = fit_summary$rhat,
    ESS_Bulk = fit_summary$ess_bulk,
    ESS_Tail = fit_summary$ess_tail
  )
  
  # Add result to the overall diagnostics data frame
  diag_df <- rbind(diag_df, result_df)
}
# Reshape the data frame
diag_df <- diag_df %>%
  pivot_longer(cols = c(Rhat, ESS_Bulk, ESS_Tail),
               names_to = "Statistic") %>%
  pivot_wider(names_from = c(Model, Statistic),
              values_from = value) %>%
  mutate(Variable = if_else(Variable == "lp__", "Log Posterior", Variable))

# Split the data into two separate data frames
diag_df_actor_critic <- diag_df[,c(TRUE, grepl("Actor-Critic-", names(diag_df))[-1])]
diag_df_q_learning_states <- diag_df[,c(TRUE, grepl("Q-learning-states-", names(diag_df))[-1])]
diag_df_q_learning <- diag_df[,c(TRUE, (!names(diag_df) %in% names(diag_df_q_learning_states) &
                                          !names(diag_df) %in% names(diag_df_actor_critic))[-1])]

# Define the spanner labels for each model
diag_df_q_learning %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_q_learning_states %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_actor_critic %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

```

## Bayesian Model Estimates

This section presents the group parameter estimates for all Reinforcement Learning (RL) models, both non-hierarchical and hierarchical. To generate these group parameter estimates, we first extract the posterior samples from the fitted models and calculate the mean (M), standard deviation (SD), 25th percentile (Q1), and 75th percentile (Q3) for each parameter. @tbl-group-parameters summarizes the distribution of the parameter estimates across classrooms, revealing their typical values and variability.

```{r}
#| eval: false
#| cache: true
#| label: tbl-group-parameters
#| tbl-cap: "Group parameter estimates for the Bayesian Models used in Q-learning and Q-learning with states."

# Define a function that processes one file
process_file <- function(result_file) {
  # Load the fitted model
  fit <- readRDS(result_file)
  
  # Extract the posterior samples of the parameters
  post_samples <- fit$draws()
  
  # Calculate the group parameter estimates
  group_params <- post_samples %>%
    as.data.frame() %>%
    summarise(across(everything(), list(M = mean, SD = sd, Q1 = ~quantile(., 0.25), Q3 = ~quantile(., 0.75))))
  
  # Add the model name to the data frame
  group_params$Model <- gsub("Bayesian/Results/||.RDS", "", result_file)
  
  return(group_params)
}

# Apply the function to each results file and bind the results into one data frame
group_params_df <- purrr::map_dfr(results_files, process_file)

# Continue with the rest of your code
group_params_df %>%
  gt() %>%
  tab_header(
    title = "Group Parameter Estimates",
    subtitle = "The table shows the mean (M), standard deviation (SD), 25th percentile (Q1), and 75th percentile (Q3) of the learning rate (alpha), inverse temperature (beta), weights, and cost parameters for each model."
  ) %>%
  cols_label(
    Model = "Model",
    M = "Mean",
    SD = "Standard Deviation",
    Q1 = "25th Percentile",
    Q3 = "75th Percentile"
  ) %>%
  fmt_number(
    columns = c("M", "SD", "Q1", "Q3"),
    decimals = 2
  )


```

---
title: "Unveiling Adaptive Pedagogy: Reinforcement Learning Models Illuminate Teacher Decision-Making in an Online Math-Teaching Platform"
abstract: "In the rapidly evolving landscape of education, understanding the decision-making process of teachers is crucial. Based on a sample of over 2,000 classrooms from the online math-teaching platform Zearn, this study leverages reinforcement learning (RL) algorithms to model how teachers learn and adapt pedagogical choices. We conceptualize the teacher's role as a multi-armed bandit problem, where teachers balance their weekly effort against the number of lessons their students complete. This exploration-exploitation trade-off is dynamic, with teachers continually learning and adapting their strategies. Our findings reveal that teachers who favor exploration tend to be more adaptive and responsive to student needs, leading to a more personalized and effective learning experience. In contrast, those who lean towards exploitation often rely on tried-and-true methods, resulting in consistent but potentially less innovative teaching strategies. This work underscores the potential of RL in providing insights into human behavior in real-world settings, with significant implications for policy and practice in education."
keywords: "Reinforcement Learning, Pedagogical Decision-Making, Digital Education Platforms, Teacher Behavioral Dynamics, Empirical Field Data, Exploration-Exploitation Dilemma, Instructional Adaptation"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
format:
  elsevier-pdf:
    keep-tex: true
    include-in-header:
      - text: |
          \usepackage{dcolumn}
          \usepackage{typearea}
    journal:
      formatting: preprint
      model: 3p
      layout: onecolumn
      cite-style: number
      # graphical-abstract: "![](abstract.png)"
bibliography: zearnrefs.bib

execute:
  echo: false
  warning: false
  error: false
  
knitr:
  opts_chunk:
    cache.extra: set.seed(832399554)
---

```{r load packages}
library(data.table)
library(tidyverse)
library(ggpubr)
library(ggforce)
library(ggrepel)
library(RColorBrewer)
library(gridExtra)
library(scales)
library(gtsummary)
library(gtExtras)
library(PerformanceAnalytics)
library(doParallel)
library(foreach)
library(reticulate)
# library(cmdstanr)
# library(brms)
```

```{r}
set.seed(832399554)
random_py <- reticulate::import("random")
random_py$seed(832399554)
# https://www.random.org/integers/
# Timestamp: 2023-05-17 16:18:28 UTC
```

# Introduction

<!-- Needs around 5 more studies here. -->

Predicting repeated behavior has been a long-standing goal of the behavioral sciences, including economics, psychology, and neuroscience. Much of human behavior results from stimulus-response associations, which are context-sensitive and not always consciously deliberated [@buyalskaya2023]. Reinforcement learning (RL) algorithms have emerged as a prominent way of quantifying these relationships, assigning a mathematical relationship between contextual cues (states), behavior (actions), and reward [@sutton2018]. These algorithms have found wide application in neuroscience and cognitive psychology, where they are used on data sets to model agents in specific environments. However, these disciplines generally do not work with the practical and applied data typically used in social psychology and economics [@buyalskaya2023].

This gap presents a novel opportunity to use methods from one set of disciplines on data traditionally used in another. In this paper, we aim to further this integration by applying RL algorithms to model the decision-making process of teachers in the math-teaching platform Zearn. Reinforcement learning (RL) provides a system of rewards and punishments where the agent (in this case, the teacher) learns to make optimal decisions by maximizing the rewards and minimizing the punishments. By assuming every teacher has an objective function to balance with their potential rewards, we model the sequential behavior of a teacher throughout a school year. For instance, the teacher chooses which pedagogical actions to employ, such as assigning homework, checking student progress, or reviewing content, in anticipation of enhancing student achievement. Applying RL algorithms allows for flexibility in learning the best strategy given certain contextual information, revealing the intricate dynamics of the teaching-learning process. That is, we provide a model of how teachers adapt their strategies in response to student performance and other contextual factors. This approach offers a flexible and robust model for our available data and opens new avenues for understanding and enhancing human behavior in field settings.

## The Zearn Platform

Zearn is a digital platform for mathematics education designed to facilitate the teaching and learning of mathematics. About 25% of elementary school students and over 1 million middle school students across the United States use Zearn [@post-weblog]. Its unique blend of hands-on teaching and immersive digital learning, paired with its widespread adoption, provides the perfect setting for understanding how teachers adapt their strategies to optimize student achievement.

Zearn's pedagogical approach includes interactive digital lessons using visual aids (see @fig-zearn-poster) and real-time student feedback. The platform's approach to mathematical concepts, such as fractions, is particularly noteworthy. Students go through a series of representations: concrete, pictorial, and abstract, each designed to scaffold their understanding (i.e., "breaking down" problems, see [@jumaat2014; @reiser2014]) and prepare them for subsequent levels.

![Example of Teaching with Visual Models on Zearn](images/zearn-poster.jpg){#fig-zearn-poster fig-align="center"}

The platform's structure facilitates a personalized learning experience for students (see SI for a screenshot of the student portal), allowing teachers to track student progress and make informed decisions (see @fig-class-report for a sample class report). This structure includes self-paced online lessons and small group instruction, a rotational model that allows students to learn new grade-level content in two ways: independently through engaging digital lessons and in small groups with their teacher and classmates. This dual approach enables students to learn at their own pace, fostering a sense of autonomy and self-directed learning.

![Sample Class Report](images/class-report.png){#fig-class-report fig-align="center"}

A key feature of Zearn is its badge system, which tracks student progress and motivates continued learning (see @fig-badges-screen). Students earn badges upon mastery of specific skills, providing a tangible representation of their achievement. This system motivates students and provides teachers with valuable data on student performance, informing their decision-making process [@knudsen2020]. Zearn also incorporates notifications, known as Tower Alerts, sent to teachers when a student struggles with a specific concept. This feature allows teachers to provide timely support and address learning gaps, enhancing the platform's capacity for personalized learning.

![Badge System for Student Achievement](images/badges.PNG){#fig-badges-screen fig-align="center"}

Another noteworthy aspect is the platform's professional development component, which is available for schools with a paid account (see SI for a sample training schedule). Teachers explore each unit or mission through word problems, fluencies, and small group lessons, conducting collaborative analysis of student work and problem-solving strategies. This professional development revolves around each mission's big mathematical idea, visual representations to scaffold learning, and strategies to address unfinished learning from prior grades and preparation for future learning [@morrison2019].

The platform encapsulates a holistic methodology in mathematics education, intertwining a hybrid curriculum that leverages print and digital mediums, a rotational pedagogical model, targeted professional development programs, and detailed analytics at the classroom and institutional level to inform teaching and learning practices. This integrated framework furnishes a rich repository of data for comprehensive analysis. The variables delineated for investigation by the Zearn consortium encompass a variety of dimensions: (1) teacher engagement, quantified through a diverse set of actions; (2) student achievement, denoted by variables such as lesson completion (i.e., "badges" earned after each lesson is finished with full proficiency); and (3) student struggles, monitored through variables such as "tower alerts".

<!--Lit review here -->

## Research Questions

We propose the following research questions to explore the dynamics of teacher behavior, the role of reinforcement learning in modeling these behaviors, and the subsequent impact on student achievement. We hope to shed light on the potential of reinforcement learning for understanding human behavior in field data.

1\. Characterizing Teacher Behavior: Can reinforcement learning models fit teacher behavior better than a logistic regression model? Which reinforcement learning model most accurately characterizes the behavior of teachers in the context of Zearn Math? What insights about teacher behavior do these differences in model fit bring?

2\. Impact of RL Parameters on Teacher Behavior and Student Achievement: How do the parameters of reinforcement learning models vary with teacher behavior? What effects do these variations have on student achievement?

3\. Influence of Teacher and School Background: How do factors such as teacher background influence their adaptation to and implementation of the Zearn Math curriculum? How can these factors be effectively incorporated into the reinforcement learning model to provide a more comprehensive understanding of teacher behavior and its impact on student outcomes?

# Theory

## Reinforcement Learning to Capture Patterns in Repeated Behavior

In RL, an agent learns to make decisions over time to maximize a cumulative reward. At the heart of RL is the concept of a policy, which is a mapping from states to actions or, more commonly, a probability distribution over actions [@sutton2018]. The agent's goal is to learn an optimal policy, which maximizes the expected cumulative reward, even when the parameters of the environment are not known a priori.

The application of RL in the context of education and teaching is not new. One of the pioneers in the field of Markov decision processes, Ronald Howard, attempted to apply his mathematical framework to instruction theory as early as 1960 [@howard1960]. Later, in 1972, Richard Atkinson proposed a theory of instruction that encapsulates the key components of a Markov decision process, including states, actions, transition probabilities, reward functions, and a time horizon [@atkinson1972]. In Atkinson's framework, actions are instructional activities (e.g., assigning problem sets) that can change a given state (e.g., student learning level). These changes in states can yield rewards minus the associated cost of the action. For example, a teacher may be rewarded with an increase in the knowledge or skill of a student, but such reward must be balanced with its associated effort (e.g., labor cost). Atkinson and colleagues continued to test many parametrizations of this idea, contributing significantly to the development of RL theory in the context of education (see @doroudi2019 for a full review).

More recently, RL models have been used in the psychology of habit to explain learning and reward association. One common approach in human studies is to apply the "multi-armed bandit" task. <!-- CITE --> In this type of experiment, participants are presented with multiple actions, each with an unknown payoff. The subject's goal is to learn the best outcome through trial and error. In the beginning, the reward-action relationships are unknown, so the participant must explore or sample each action [@sutton2018] <!-- CITE page# -->. This exploration-exploitation trade-off is a central theme in RL and has the potential to provide valuable insights into how students learn and make decisions over time.

## Education production function

<!-- Needs CITE here. -->

Previous research has explored how teacher effort affects student achievement. In particular, economists have studied the "education production function," in which educational outcomes are a function of various inputs, including teacher effort, student effort, school resources, and family background [@hedges1994]. This function serves as a theoretical framework for understanding how different factors contribute to educational achievement and how interventions can be designed to improve outcomes.

One of the key inputs in the education production function is teacher effort, as teachers play a crucial role in shaping students' learning experiences and outcomes. Their effort, which encompasses their time, energy, dedication, and instructional strategies, can significantly influence students' academic achievement [@rivkin2005]. However, measuring teacher effort and its impact on student outcomes can be challenging due to the complex and multifaceted nature of teaching.

To address this challenge, researchers have employed various strategies to identify the effects of teacher effort on student achievement. One common approach is to manipulate the conditions under which teachers operate, thereby changing the levels of teacher effort. For instance, @duflo2011 conducted a randomized controlled trial in Kenya to examine the effects of tracking, a practice of grouping students based on their ability levels. Tracking has been a prominent tool in the sociology of education and assumes that teacher inputs will depend on the ability level of the students. This tailoring of teaching strategies and content potentially enhances the instructor's effectiveness.

However, traditional social science approaches to studying the education production function often lack the flexibility to account for changes in context and experience and individual-level differences <!-- CITE page# -->. Reinforcement learning (RL) offers a promising alternative approach. RL models incorporate a flexibility term (i.e., a learning rate) that allows for changes in behavior with experience and an exploration versus exploitation term (e.g., inverse temperature) that captures individual differences in decision-making strategies. Furthermore, the RL framework inherently accounts for the process of learning about rewards, making it a flexible and dynamic tool for studying teacher behavior [@sutton2018].

## Why Reinforcement Learning?

Reinforcement Learning (RL) presents a paradigm shift from models that employ a static approach to link teacher efforts with student outcomes, such as the education production function. RL embodies the flexibility to adapt and evolve strategies over time. This dynamic framework reflects the continuous learning process seen in biological systems and has the potential to concretize the evolving nature of educational interactions. In this study, our RL models position teachers as agents who navigate their environment (i.e., the classroom) by acting based on their observations and feedback. This process allows for a detailed understanding of the interplay between teacher actions, the classroom environment, and the outcomes of these interactions, as such:

$$
\text{Agent} \xrightarrow[\text{Actions}]{\text{Performs}} \text{Environment} \xrightarrow[\text{Observations, Rewards}]{\text{Provides}} \text{Agent}
$$

Further, RL algorithms also enable the modeling of individual teachers' decision-making processes. This approach renders the creation of detailed profiles for instructors, understanding how they adapt and respond to various states and rewards within the educational setting. The flexibility and robustness of RL make it an ideal tool for adapting to changing learning environments and addressing individual teacher and classroom needs. By incorporating a wide range of variables (i.e., states, actions, and rewards), RL models are customizable to diverse educational contexts and objectives. They can also account for the inherent uncertainties in teaching, guiding the formulation of optimal decision-making strategies under uncertain conditions.

The analytical capabilities of RL extend to identifying variations in teacher learning and behavior and how these differences influence student outcomes. By estimating individual teacher parameters, RL provides insights into aspects such as teacher flexibility, enabling targeted interventions by policymakers to enhance educational outcomes. Conducting "counterfactual analyses" opens avenues for innovative educational interventions. Beyond immediate study goals, RL models hold the potential for automating instructional decisions based on identified patterns, potentially alleviating the workload on teachers and optimizing the educational process.

<!-- ### Previous Applied RL Cases -->

<!-- -   Discuss existing models and where they fall short, justifying your approach. -->

<!-- -   Introduce the concept of data-driven exploration in RL models. -->

## Q-Learning Model

The Q-learning algorithm is the first class of models we employ to elucidate decision-making dynamics within an interactive environment. Q-learning involves the iterative refinement of Q-value functions, which map an agent's actions in a given state to evolving expectations of future rewards (analogous to subjective value or utility). This methodological approach is closely related to the classic "multi-armed bandit" problem, wherein the agent faces a finite set of choices (e.g., slot machines), each linked to a specific reward schedule, and aims to identify the optimal action that maximizes returns. Learning in this model depends on adjusting expectations to reduce the impact of prediction errors (the "surprise level," or the difference between expected and realized outcomes), using the Bellman equation to update Q-values iteratively [@rummery]. Through this iterative process, the equation calculates the value of each state-action pair by combining the immediate reward with the discounted value of subsequent optimal state-action configurations. This model frames decision-making as a result of accumulated experience and the anticipation of future rewards. Note that this framework is considered a model-free reinforcement learning algorithm, because it does not require a model of an environment's stochastic transitions to approximate expected rewards [@watkins1992].

The Q-function, $Q(s, a)$, is defined for all state-action pairs $(s, a)$, where $s$ is the state and $a$ is the action. The Q-function represents the expected return or future reward for taking action $a$ in state $s$ following a certain policy $\pi = \Pr(a)$. The Q-function is updated iteratively using the Bellman equation as follows:

$$
Q_\text{new}(s, a) = Q(s, a) + \alpha \delta
$$ {#eq-q-learn}

where $\alpha$ is the learning rate, which determines how much the Q-value is updated based on $\delta$, the reward prediction error. The reward prediction error is the difference between the estimated Q-value and the observed reward plus the discounted future Q-value. This error is used to update the Q-value in the direction of the observed reward, as follows:

$$
\delta = r + \gamma \max_{a'} Q(s', a') - Q(s, a)
$$ {#eq-RPE}

where:

-   $r$ is the immediate reward received after taking action $a$ in state $s$,

-   $\gamma$ is the discount factor,

-   $s'$ is the new state after taking action $a$,

-   $a'$ is the action to be taken in the new state $s'$,

-   $s$ is the current state,

-   $a$ is the action taken,

-   $\max_{a'} Q(s', a')$ is the maximum reward that can be obtained in the next state $s'$,

-   $Q(s, a)$ is the current estimate of the Q-value for action $a$ in state $s$.

The Q-learning algorithm uses this update rule to learn the Q-function and, hence, the optimal policy. The agent starts with an initial Q-function (which can be arbitrary) and then updates the Q-values based on the experiences it gathers from interactions with the environment. The update rule is applied every time the agent transitions from a state $s$ to a state $s'$ by taking an action $a$ and receiving a reward $r$. The agent selects actions based on a policy function of the Q-values. A common choice is the softmax action selection method, which which chooses actions probabilistically based on their Q-values. The probability of choosing a particular action is defined as follows:

$$
\Pr(a) = \frac{e^{Q(s, a)/\tau}}{\sum_{a'} e^{Q(s, a')/\tau}}
$$ {#eq-softmax}

where:

-   $\Pr(a)$ is the probability of choosing action $a$,

-   $Q(s, a)$ is the Q-value of action $a$ in state $s$,

-   $\tau$ is a parameter known as the temperature, which controls the level of exploration,

-   The denominator is the sum over all possible actions $a'$ of the exponential of their Q-values divided by the temperature.

One possible interpretation of the temperature parameter $\tau$ is the control of the trade-off between exploration and exploitation. When $\tau$ is high, the agent explores more because the action probabilities are more uniform. When $\tau$ is low, the agent exploits more because the action with the highest Q-value is more likely to be chosen than the others. As the agent learns, it can be beneficial to start with a high temperature to encourage exploration and then gradually decrease it to favor the exploitation of the learned policy.

### State-Free vs. State-Based Models

So far, we have defined a state-based model that accounts for different states of the environment into the Q-function. However, a special case of the Q-learning model does not require a state space, prescribing only an action-reward relationship, and the Q-value, $Q(a)$, becomes a function of the action alone. In our context, such an adaptation would assume that the best action in one week is the best action at any other week. This means a teacher learns the value of their action regardless of the history of their classroom or students. Generally, this assumption can be beneficial in scenarios where the state exerts minimal influence on the outcome of the action or when the state is difficult to define or observe [@sutton2018]. The Q-value update function, therefore, simplifies to:

$$
Q(a) = Q(a) + \alpha \left[ r(a) - Q(a) \right]
$$ {#eq-state-free}

where:

-   $Q(a)$ is the current estimate of the Q-value for action $a$,

-   $\alpha$ is the learning rate,

-   $r(a)$ is the immediate reward received after taking action $a$.

In this equation, the term in the brackets, $r(a) - Q(a)$, is the reward prediction error. It represents the difference between the observed reward and the current estimate of the Q-value. The Q-value is updated in the direction of this error, scaled by the learning rate $\alpha$.

## The Actor-Critic Model

For cases in which states do matter in teacher decisions, we choose a model that divides the action selection and action evaluation tasks into two components: the "actor" and the "critic" [@sutton2018b]. This division theoretically allows for more efficient learning, as the critic guides the actor's learning process.

The "actor" in this model selects actions based on a policy function, denoted as $\pi(a|s)$, which maps states to actions, determining the probability of taking each action in each state. The actor aims to learn an optimal policy that maximizes the expected cumulative reward. In our setting, we set the policy as a softmax function over action preferences:

$$
\pi(a|s) = \frac{e^{h(a, s)}}{\sum_{a'} e^{h(a', s)}}
$$

where $h(a, s)$ is the preference for action $a$ in state $s$.

The "critic," on the other hand, evaluates the actions taken by the actor by learning a value function, denoted as $V(s)$. Given the actor's current policy, the critic estimates the expected cumulative reward from each state. The critic's feedback, in the form of the value function, guides the actor's learning. We update the value function based on the Temporal Difference (TD) error, a measure of the difference between the estimated and actual return:

$$
\delta = r + \gamma V(s') - V(s)
$$

where $r$ is the reward, $\gamma$ is the discount factor, $s'$ is the new state, and $s$ is the current state.

This separation of action selection and evaluation distinguishes the Actor-Critic model. In Q-learning, a single Q-function selects and evaluates actions. Conversely, in the Actor-Critic case, the actor updates its policy to increase the probability of actions that lead to higher-than-expected returns and decrease the probability of actions that lead to lower-than-expected returns.

## Applying RL Models to the Zearn Context

In this study, we propose an application of the Reinforcement Learning (RL) algorithms for modeling teacher decision-making within the Zearn platform.

Consider a typical teaching scenario: the state is the students' current progress in the class, and the actions are a range of pedagogical strategies, such as assigning additional practice, providing personalized feedback, or adjusting lesson plans. Some relevant reward variables include improved student performance, increased student engagement, or reduced learning gaps.

In the Zearn context, we define the decision process as follows:

1.  Agents are the teachers.

2.  Actions include the choice of specific pedagogical strategies.

3.  The environment is the Zearn platform with its students.

4.  The state is the week-over-week change in student performance and/or struggle.

5.  The reward is a linear function of student performance.

Mathematically mapping the agent-environment interaction is flexible, with many models potentially satisfying our initial assumptions. We approach this problem as a competition of models, selecting a set of models applicable to our setting, fitting them to the data, and comparing their performances.

# Material and Methods

To accurately capture the intricate dynamics present within our data, we explored a large set of candidate models, progressively escalating in complexity, as shown in @tbl-methods. This methodological triangulation enabled us to refine our data processing, analysis, and interpretation methods, effectively addressing the drawbacks of relying solely on one analytical approach. Through this rigorous methodology, our goal was to strengthen the reliability of our findings and offer a detailed understanding of the underlying behavioral patterns.

+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------+
| **Step**                 | **Method**                                                                  | **Software/Tools**                                               |
+==========================+=============================================================================+==================================================================+
| Data Preprocessing       | Cleaning, normalization                                                     | R [@rcoreteam2024]                                               |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------+
| Dimensionality Reduction | Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF) | Python (`scikit-learn`) [@pedregosa2011]                         |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------+
| Feature Selection        | Regression analysis                                                         | R (`fixest` package) [@berge2018]                                |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------+
| Analytical Methods       | Q-learning, Actor-Critic Model Estimation                                   | R, Matlab (`CBM` package for Laplace approximation) [@piray2019] |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------+
| Statistical Analysis     | Hierarchical Bayesian Inference                                             | Matlab (`CBM` package for expectation-maximization algorithm)    |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------+
| Model Evaluation         | Hetereogeneity analyses of model performance across teachers                | R                                                                |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------+

: Analytical steps employed in the study. {#tbl-methods}

## Data

The data from the Zearn platform follows a time-series structure, spanning across an academic year, with the unit of analysis being the classroom-week. This level of granularity enables us to capture the temporal dynamics of teacher-student interactions and their subsequent influence on student achievement. In particular, we can model the decision-making process of teachers as they allocate their time and effort on the Zearn platform weekly and how these decisions translate into student outcomes, measured by the completion of lessons or "badges."

```{r data prep}

dt <- read.csv(file = "Data/df_clean.csv")

# Convert columns to appropriate data types
setDT(dt)
# Rename variable
dt[, `:=`(
  poverty = factor(poverty, ordered = TRUE, exclude = c("", NA)),
  income = factor(income, ordered = TRUE, exclude = c("", NA)),
  st_login = fifelse(Minutes.per.Active.User > 0, 1, 0, na=0),
  tch_login = fifelse(User.Session > 0, 1, 0, na=0)
  # Log Transform
  # Badges.per.Active.User = log(Badges.per.Active.User + 1),
  # Tower.Alerts.per.Tower.Completion = log(Tower.Alerts.per.Tower.Completion + 1),
  # tch_min = log(tch_min + 1)
)]

# Code datetime variables and compute additional metrics
setorder(dt, Teacher.User.ID, year, week, Classroom.ID)
dt[, isoweek := week]
dt[, week := week + 52*(year - 2019)]
# Fixing week == 1 (last week of 2019 counts as week 1 of 2020)
dt[week == 1, week := week + 52]
dt[, first_week := min(week), by = .(Teacher.User.ID)]
dt[, week := week - first_week + 1]
dt[, Tsubj := max(week), by = .(Classroom.ID)]

# Update the Grade.Level values and labels
dt <- dt[!(Grade.Level %in% c(-1, 11))] # Ignore -1 and 11
dt[, Grade.Level := factor(Grade.Level,
                           ordered = TRUE,
                           exclude = c(""))]
dt[, Grade.Level := factor(Grade.Level,
                           levels = c(0:8),
                           labels = c("Kindergarten", "1st", "2nd",
                                      "3rd", "4th", "5th",
                                      "6th", "7th", "8th"))]

df <- as.data.frame(dt) %>%
  ungroup()

# Convert year and isoweek to a date (Monday of that week)
df <- df %>%
  mutate(date = as.Date(paste(year, isoweek, 1, sep="-"), format="%Y-%U-%u"))

```

Zearn provided administrative data for teachers and students. Teacher activity is time-stamped to the second and includes the time spent on the platform and specific actions taken. On the other hand, student data is aggregated at the classroom-week level due to data privacy considerations. This aggregation included a variety of potentially interesting variables capturing student achievement (e.g., student lesson completion or "badges") and level of student struggle (e.g., tower alerts). These variables offer a comprehensive view of the dynamics of teacher-student interactions on the Zearn platform.

```{r}
#| label: tbl-summary
#| tbl-cap: "Summary statistics by school, detailing the number of teachers, total students, and weeks of active engagement."

df_summary <- df %>%
  group_by(MDR.School.ID) %>%
  summarise(
    Unique_Teacher_Count = n_distinct(Teacher.User.ID)
  ) %>%
  left_join(df %>%
              group_by(Classroom.ID, MDR.School.ID) %>%
              summarise(
                Students_Total = mean(Students...Total, na.rm = TRUE),
                Weeks_Total = max(week)
                ) %>%
              group_by(MDR.School.ID) %>%
              summarize(
                Students_Total = sum(Students_Total, na.rm = TRUE),
                Weeks_Total = mean(Weeks_Total)
                ),
            by = "MDR.School.ID") %>%
  ungroup() %>%
  summarise(
    Mean_Teachers = mean(Unique_Teacher_Count),
    SD_Teachers = sd(Unique_Teacher_Count),
    Min_Teachers = min(Unique_Teacher_Count),
    Max_Teachers = max(Unique_Teacher_Count),
    Mean_Students_Total = mean(Students_Total),
    SD_Students_Total = sd(Students_Total, na.rm = TRUE),
    Min_Students_Total = min(Students_Total),
    Max_Students_Total = max(Students_Total),
    Mean_Weeks_Total = mean(Weeks_Total),
    SD_Weeks_Total = sd(Weeks_Total, na.rm = TRUE),
    Min_Weeks_Total = min(Weeks_Total),
    Max_Weeks_Total = max(Weeks_Total)
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Measure", "Variable"),
           sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)
  
df_proportions <- df %>%
  filter(!is.na(poverty)) %>%
  group_by(poverty) %>%
  summarise(n = n()) %>%
  mutate(Percentage = paste0(
    round(n / sum(n) * 100, digits = 2), "%"
    )) %>%
  select(-"n") %>%
  na.omit() %>%
  rename(Variable = poverty) %>%
  mutate(Variable = as.character(Variable)) %>%
  bind_rows(df %>%
              filter(!is.na(income)) %>%
              group_by(income) %>%
              summarise(n = n()) %>%
              mutate(Percentage = paste0(
                round(n / sum(n) * 100, digits = 2), "%"
                )) %>%
              select(-"n") %>%
              na.omit() %>%
              rename(Variable = income) %>%
              mutate(Variable = as.character(Variable))
            ) %>%
  bind_rows(df %>%
              ungroup() %>%
              summarise(
                Charter_Schools = mean(charter.school, na.rm = T)*100,
                Schools_with_Paid_Account = mean(school.account, na.rm = T)*100
                ) %>%
              mutate(
                Charter_Schools = paste0(round(Charter_Schools, digits = 2), "%"),
                Schools_with_Paid_Account = paste0(round(Schools_with_Paid_Account,
                                                         digits = 2), "%")) %>%
              t() %>% as.data.frame() %>%
              rename(Percentage = V1) %>%
              mutate(Variable = row.names(.))) %>%
  add_row(Variable = "Poverty Level", Percentage = "", .before = 1) %>%
  add_row(Variable = "Income", Percentage = "", .before = 5) %>%
  add_row(Variable = "Other", Percentage = "", .before = 23)

# Summary statistics table
gt_school_sum <- df_summary %>%
  mutate(Variable = gsub("_Total", "", Variable)) %>%
  gt(rowname_col = "Variable") %>%
  cols_label(Mean = "Mean", SD = "Standard Deviation",
             Min = "Minimum", Max = "Maximum") %>%
  fmt_number(
    columns = c("Mean", "SD"),
    decimals = 2
  ) %>%
  fmt_number(
    columns = c("Min","Max"),
    decimals = 0
  )
gt_school_sum

```

The dataset includes `r length(unique(df$Classroom.ID))` classrooms and `r length(unique(df$Teacher.User.ID))` educators, with an average of `r round(mean(df$Students...Total, na.rm = T), 1)` students per classroom. @tbl-summary provides a snapshot of the schools' characteristics and reveals a broad spectrum of Zearn's user demographics including the average number of teachers, students, and weeks of data per school. The bimodal distribution of weekly data per classroom, as showcased in @fig-classroom-weeks, reveals that some classrooms have less than 3 to 4 months of data. The classrooms with less than 16 weeks of data are color-coded in red. The pattern of student logins over time, particularly around significant holidays, as seen in @fig-logins-week, underscore the temporal fluctuations in platform engagement.

```{r}
#| label: fig-classroom-weeks
#| fig-cap: "Histogram of number of weeks of data per classroom. A bimodal distribution highlights the majority of classrooms with data spanning over 50 weeks. A smaller but significant subset of classrooms have less than 18 weeks of data. The dashed line acts as a threshold that excludes a notable segment of classrooms from further analysis. The lack of data between these two peaks suggests distinct patterns of usage—some classrooms consistently use the platform throughout the academic year, while others show sporadic engagement, possibly reflecting trial periods or intermittent usage."

# Create the histogram
df %>%
  group_by(Classroom.ID) %>%
  summarize(Tsubj = n()) %>%
  mutate(Tsubj_category = if_else(Tsubj < 18, "less than 18", "18 or more")) %>%
  ggplot(aes(x = Tsubj, fill = Tsubj_category)) +
  geom_histogram(color = "black", breaks = seq(min(df$Tsubj, na.rm = T),
                                               max(df$Tsubj, na.rm = T) + 1,
                                               by = 2)) +
  geom_vline(xintercept = 17, color = "darkgray",
             linetype = "dashed", linewidth = 0.8) +
  annotate("text", x = 10, y = 4000, label = "Excluded\nClassrooms",
           vjust = 1, color = "red") +
  labs(x = "Total Number of Weeks",
       y = "Frequency (Classrooms)") +
  scale_fill_manual(values = c("less than 18" = "red",
                               "18 or more" = "steelblue")) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(1, seq(5, max(df$Tsubj, na.rm = T), by = 5)))

```

```{r}
#| label: fig-logins-week
#| fig-cap: "Total number of student logins over time. The chart depicts the connection between academic schedules and platform engagement. Each bar represents a week, revealing peaks coinciding with active school weeks and troughs aligning with holiday periods (e.g., Thanksgiving and Christmas)."

# Calculate the sum of login values by date and Teacher.User.ID
login_data <- df %>%
  group_by(date, Teacher.User.ID) %>%
  summarize(tch_login = max(tch_login),
            st_login  = max(st_login)) %>%
  group_by(date) %>%
  summarize(tch_logins = sum(tch_login),
            st_logins  = sum(st_login))
# Create bar plot
bar_plot <- ggplot() +
  geom_bar(data = login_data, aes(x = date, y = st_logins), stat = "identity") +
  # geom_point(data = login_data, aes(x = date, y = tch_logins), color = "blue") +
  labs(
    x = "Week",
    y = "Total Logins"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12)) +
  scale_x_date(date_breaks = "3 week", date_labels = "%Y-%m-%d")

# Add labels for Christmas and Thanksgiving
bar_plot +
  geom_text(aes(x = as.Date("2019-12-25"), y = 1250, label = "Christmas"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "red") +
  geom_text(aes(x = as.Date("2019-11-25"), y = 1250, label = "Thanksgiving"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "darkorange")

```

### Geographic Context and Distribution

Our investigation encompasses a diverse range of Louisiana schools. This region presents a distinct educational landscape and widespread adoption of the Zearn platform across its schools. @fig-teachers-map maps the distribution of teachers engaged with Zearn across Louisiana's parishes, highlighting areas with pronounced clusters of active teachers. The map also marks the top five cities leading in Zearn adoption among teachers.

```{r}
#| cache: true
#| label: fig-teachers-map
#| fig-cap: "Geographic distribution of Zearn teachers across Lousiana parishes. The color gradient indicates the density of teachers, with darker hues representing a greater number of educators using Zearn. The top five cities where Zearn is most prominent are identified and labeled."

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
# Set the library path to where proj.db is located
# Sys.setenv(PROJ_LIB = "")
unique_zipcodes <- unique(dt$zipcode) %>%
  as.list()
plan(strategy = "multisession", workers = availableCores())
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
dt <- merge(dt, address_geodata,
            by.x = "zipcode",
            by.y = "postalcode",
            all.x = TRUE)

# Aggregate the data to get the number of teachers in each county
dt_map <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID)
), by = .(county)]

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID),
  lat = mean(lat),
  long = mean(long)
), by = .(city, county)]
# Get the top 5 cities with the most teachers
top_cities <- top_cities[order(-num_teachers)][1:5,]

# Get the Louisiana county map data
df_map <- tigris::counties(cb = TRUE,
                           resolution = "20m",
                           class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    as.data.frame(dt_map),
    by = c("NAMELSAD" = "county")
  ) %>%
  sf::st_as_sf()

ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers", low = "white", high = "red", na.value = "gray90") +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city), size = 3, color = "black")
```

To enhance our analysis, we have included the socioeconomic profile detailed in @fig-income-dist. The distributions of median poverty and income levels highlight the heterogeneity in our data and the potential for understanding the impact of socioeconomic factors on the educational landscape.

```{r}
#| label: fig-income-dist
#| fig-cap: "Distributions of School Socioeconomic Profiles. The first graph shows school poverty levels based on the eligibility for free or reduced-price lunch (FRPL), a proxy for low-income student concentration. Schools are categorized by the percentage of eligible students: low-poverty (0-40%), mid-poverty (40-75%), and high-poverty (over 75%). The second graph depicts the median income distribution within the school districts."
#| fig-subcap: 
#|   - "School Poverty Distribution"
#|   - "Median Income Distribution"
#| layout-ncol: 2

# Splitting df_proportions into different categories for pie charts
df_poverty <- df_proportions[2:4,]
df_income <- df_proportions[6:22,]

# Convert Percentage to numeric
df_poverty$Percentage <- as.numeric(gsub("%", "", df_poverty$Percentage))/100

# Create Bar Graph for Poverty Level
ggplot(df_poverty, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Poverty Level",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Convert Percentage to numeric
df_income$Percentage <- as.numeric(gsub("%", "", df_income$Percentage))/100

# Create Bar Graph for Income Distribution
ggplot(df_income, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Income Range",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### Preprocessing and Exclusion criteria

In order to model interesting behavioral patterns, we focus our analysis on the teachers who most likely take advantage of a wide range of resources on the platform. Thus, we select teachers who consistently use the platform and work in traditional school settings. To achieve this, the raw data underwent rigorous preprocessing to ensure its suitability for analysis. This process included performing log transformations on our variables of interest (i.e., minutes, badges, and tower alerts) to normalize their distributions. Given the diverse user base of Zearn, we applied specific exclusion criteria to select teachers who most likely representat traditional classroom settings with consistent platform utilization. We selected virtual classrooms with at least five active students weekly, filtering out parents or tutors who may use Zearn outside the classroom setting. We removed teachers with more than four classrooms and those who logged in for less than 16 weeks. We excluded classrooms in the 6th to 8th grades, as they represent only a small proportion of our dataset. This deletion ensures a focus on traditional school settings, minimizing bias from teachers and schools that have not used Zearn consistently.

```{r preprocess data}
#| include: false

dt <- setDT(df)
dt[, `:=`(
  n_weeks = .N,
  mean_act_st = mean(Active.Users...Total, na.rm = TRUE)
  ), by = Classroom.ID]

dt <- dt[
  n_weeks > 18 & # At least 4.5 months cumulative activity
    Tsubj < 2*n_weeks & # At least activity twice a month on average
    teacher_number_classes < 5 &
    Students...Total > 5 &
    mean_act_st > 3 &
    !(Grade.Level %in% c("6th","7th","8th")) &
    !(month(date) %in% c(6, 7, 8)) &
    !is.na(District.Rollup.ID),
]

df <- as.data.frame(dt) %>%
  select(-X) %>%
  mutate(Minutes.per.Active.User = as.numeric(Minutes.per.Active.User)) %>%
  mutate(across(Active.Users...Total:Tower.Alerts.per.Tower.Completion,
         ~ ifelse(is.na(.), 0, .))) %>%
  arrange(Classroom.ID, week)
# Clean environment
rm(list = setdiff(ls(), c("df","random_py")))
gc(verbose = FALSE)

```

@tbl-classroom-summary summarizes the refined dataset, providing a snapshot of the key variables of interest. Their means and standard deviations (SD) are computed for each grade level and overall (across all grades).

```{r, results='asis'}
#| label: tbl-classroom-summary
#| tbl-cap: "Classroom engagement by grade level. Means (standard deviations) of minutes and badges per student, Tower Alerts per lesson completion, and teacher engagement minutes."

create_summary <- function(var_name, var_label, type = "continuous") {
  summary_stat <- df %>%
    select(Grade.Level, var_name) %>%
    tbl_summary(
      by = Grade.Level,
      missing = "no",
      type = list(var_name ~ type),
      statistic = var_name ~ ifelse(type == "continuous",
                                    "{mean} ({sd})",
                                    "{n} ({p})")
    ) %>%
    add_overall() %>%
    as_tibble() 
  summary_stat[1] <- var_label
  return(summary_stat)
}

summaries_list <- list(
  # create_summary("Sessions.per.Active.User", "Sessions per Student"),
  create_summary("Minutes.per.Active.User", "Minutes per Student"),
  create_summary("Badges.per.Active.User", "Badges per Student"),
  create_summary("Tower.Alerts.per.Tower.Completion",
                 "Tower Alerts per Lesson Completion"),
  # create_summary("tch_login", "Teacher Login (0/1)", type = "dichotomous"),
  create_summary("Minutes.on.Zearn...Total", "Minutes per Teacher")
)

school_summary <- bind_rows(summaries_list) %>%
  select(-c(9:11)) %>% t() %>% as.data.frame()
colnames(school_summary) <- school_summary[1,]
# Remove the first row and set row names
school_summary <- school_summary[-1, ]
school_summary$`Grade Level` <- rownames(school_summary)

# Create a gt table
gt_classroom_sum <- school_summary %>%
  mutate(`Grade Level` = gsub("\\*\\*", "", `Grade Level`)) %>%
  gt(rowname_col = "Grade Level") %>%
  cols_label(
    `Minutes per Student` = "Minutes",
    `Badges per Student` = "Badges",
    `Tower Alerts per Lesson Completion` = "Tower Alerts",
    `Minutes per Teacher` = "Teacher Minutes"
  )

# Display the table
gt_classroom_sum

```

### Operationalizing Actions, Rewards, and States

In analyzing Zearn data spanning an entire academic year, we must establish a framework for input (actions), output (rewards), and, optionally, state variables for state-based RL models. In this framework, teacher activities drive the educational process, student achievements result from these efforts, and the constantly changing educational environment represents the states. The following sections will further explore potential actions, rewards, and states in our dataset.

#### Teacher Actions

Teacher actions encompass a broad spectrum, from platform log-ins to resource downloads and specific instructional activities. Simultaneously, student engagement is captured through metrics such as lesson completions (badges earned) and time spent logged in. @tbl-teacher-variables provides a list of the actions available in the data.

+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Variable**                                               | **Description**                                                                                                                                                                                                                 |
+============================================================+=================================================================================================================================================================================================================================+
| PD Course Guide Download [@zearnaa; @zearnab]              | Detailed agenda for Professional Development (PD) courses focusing on classroom implementation, leadership, supporting diverse learners,using data to inform teaching practices, and accelerating student learning.             |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| PD Course Notes Download [@zearnaa; @zearnab]              | Professional development session notes offering insights into effectively using Zearn's curriculum.                                                                                                                             |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Curriculum Map Download [@zearna]                          | Detailed outline of learning objectives and content. Presents a sequence of interconnected math concepts across grades, aligning with states' instructional requirements.                                                       |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Assessments Download [@zearnb]                             | Assessments to evaluate student understanding of the material, including ongoing formative assessments, digital daily checks, and paper-based unit assessments.                                                                 |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Assessments Answer Key Download [@zearnc]                  | Solutions for assessments to aid in grading and feedback. Provides detailed rubrics for mission-level assessments.                                                                                                              |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Elementary Schedule Download [@zearne]                     | A recommended schedule for elementary school-level Zearn curriculum activities to guide daily and weekly instructional planning, ensuring comprehensive coverage of curriculum content.                                         |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Grade Level Overview Download [@zearnf]                    | Provides a summary of learning objectives, pacing guidance, key grade-level terminology, a list of required materials, and details on the standards covered by each lesson.                                                     |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Kindergarten Schedule Download [@zearng]                   | Recommended schedules for Kindergarten, supporting structured instruction planning.                                                                                                                                             |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Kindergarten Mission Download [@zearnh]                    | Details interactive activities focused on kindergarten-level concepts and their learning objectives.                                                                                                                            |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Mission Overview Download [@zearnf]                        | Outlines a mission's (i.e., learning module) flow of topics, lessons, and assessments; highlights foundational concepts introduced earlier; lists recently introduced terms and required materials for teacher-led instruction. |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Optional Homework Download [@zearnj]                       | Assignments for additional practice, enhancing student learning outside of class.                                                                                                                                               |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Optional Problem Sets Download [@zearnk]                   | Exercises for extra practice, tailored to reinforce lesson concepts.                                                                                                                                                            |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Small Group Lesson Download [@zearnl]                      | Lessons designed for small-group engagement.                                                                                                                                                                                    |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Student Notes and Exit Tickets Download [@zearnz; @zearny] | Student notes supplement digital lessons with paper-and-pencil activities. Exit tickets are lesson-level assessments for teachers to monitor daily learning.                                                                    |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Teaching and Learning Approach Download [@zearnn]          | Resources outlining Zearn's pedagogical methods.                                                                                                                                                                                |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Whole Group Fluency Download [@zearno]                     | Lesson-aligned practice activities to build math fluency through whole-class engagement.                                                                                                                                        |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Whole Group Word Problems Download [@zearnl]               | Word problem-solving activities intended for collaborative, whole-class engagement.                                                                                                                                             |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Fluency Completed [@lesson-a]                              | Indicates teacher completed a fluency activity, typically given to students before their daily digital lessons.                                                                                                                 |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Guided Practice Completed [@zearnr]                        | Indicates teacher completed a guided practice segment, where students learn new concepts. These include videos with on-screen teachers, interactive activities, and paper-and-pencil Student Notes.                             |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Kindergarten Activity Completed [@zearns]                  | Indicates teacher completed an activity within the Kindergarten curriculum.                                                                                                                                                     |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Number Gym Activity Completed [@zearnt]                    | Indicates teacher completed a Number Gym, an individually adaptive activity that builds number sense, reinforces previously learned skills, and addresses areas of unfinished learning.                                         |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Tower Completed [@zearnu]                                  | Indicates teacher completed a Tower of Power, an activity that requires full mastery of lesson objectives and that students must complete independently.                                                                        |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Tower Struggled [@zearnac]                                 | Indicates teacher committed a mistake when engaging with the Tower of Power activity in a student role, triggering a "boost" (scaffolding remediation).                                                                         |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Tower Stage Failed [@zearnad]                              | Indicates teacher received three consecutive "boosts" due to repeated errors when engaging with the Tower of Power in a student role.                                                                                           |
+------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: Catalog of Teacher Activities. This table presents teachers' actions, including curriculum engagement, downloads of pedagogical materials, and completion of various interactive components within the Zearn educational platform. {#tbl-teacher-variables}

#### Reward and State Variables

Reward and state variables in Reinforcement Learning (RL) models capture the dynamics of the environment in which learning and actions occur. The Zearn platform captures these variables from student activity and performance data, providing a quantifiable snapshot of classroom engagement and learning challenges. The student variables in our data are:

1.  "Active Students" directly measures classroom engagement, representing the number of students actively logging in to complete digital lessons within a given week [@zearn2022].

2.  "Student Logins" functions as an attendance roster, tallying the frequency of students entering the platform, potentially serving as an engagement metric [@zearnaf].

3.  "Badges (on grade)" and "Badges" per active user metrics offer insights into the level of curriculum mastery. They reflect the number of new lessons completed weekly at students' grade level and in general. Accumulating badges can serve as a reward signal in an RL model, indicating students' progress and pace through the curriculum [@zearnae].

4.  "Minutes per active student" variable measures students' time on the platform, potentially correlating with their focus and learning progress. Once routines are established, teachers can use this metric to monitor whether students meet their expected weekly minutes [@zearn2022].

5.  "Tower Alerts" signal instances when students repeatedly encounter difficulties within the same lesson, prompting the platform to notify teachers. In the context of RL models, Tower Alerts could be viewed as a negative reinforcement signal, highlighting areas where students may require additional support or intervention to improve learning outcomes [@zearnad].

### Dimensionality Reduction

While comprehensive, directly using the available variables presents certain challenges:

1.  **Complexity**: The sheer number of available variables complicates the identification of meaningful patterns and relationships.
2.  **Dimensionality**: The high-dimensional nature of the data risks diluting important signals due to the "curse of dimensionality."[^1]
3.  **Interpretability**: Directly interpreting the impact of specific actions or behaviors on outcomes can be obscured by the intertwined nature of the data.

[^1]: Richard Bellman coined this phrase to describe the challenge of optimizing a control process by searching over a discrete multidimensional grid, where the number of grid points increases exponentially with the number of dimensions. He wrote: “In view of all that we have said in the foregoing sections, the many obstacles we appear to have surmounted, what casts the pall over our victory celebration? It is the curse of dimensionality, a malediction that has plagued the scientist from the earliest days” [@bellman2015adaptive].

Given these considerations, we avoid relying solely on single, discrete variables and employ dimensionality reduction techniques.

By reducing the data to a manageable number of components, we can more readily identify underlying patterns of behavior and interaction. Components generated through this technique provide a distilled representation of the data, where each one reflects a combination of behaviors or activities with a potential thematic linkage. To achieve this, we applied Principal Component Analysis (PCA) and Nonnegative Matrix Factorization (NMF) techniques. We then used the results to define action, reward, and state variables rather than using individual metrics.

PCA is our first methodological choice. It is widely utilized but assumes data normality [@jolliffe2016] and maximizes variance explained, potentially overlooking subtle relationships between variables. Consequently, we also employ NMF, which, by contrast, imposes a non-negativity constraint and is more closely related to clustering algorithms, creating a more interpretable, sparse representation of behaviors [@ding2005; @lee1999]. This technique is particularly advantageous for data representing counts or frequencies. By trying different techniques, we can explore these trade-offs and discover the reduced-dimension representation best suited to our specific dataset and research questions.

First, we standardized the dataset by z-scoring the variables of interest at the school level (using school-wide means and standard deviations). We performed PCA and NMF, and we evaluated the data's reconstruction accuracy and cluster separation using, respectively, the sum of squared residuals (a measure of the difference between the original data and the reconstructed data) and silhouette scores (a measure of how similar an object is to its cluster compared to other clusters [@rousseeuw1987]).

We calculate the silhouette score with the formula $(b - a) / \max(a, b)$, where $a$ is the average distance within a cluster and $b$ is the average distance to the nearest neighboring cluster. This score ranges from -1 to 1, with higher values indicating a data point is well-matched to its cluster and poorly matched to neighboring clusters.

Note that our chosen RL models require discreet action variables. Thus, we choose to split teacher actions into binary variables, following a median split.

#### Nonnegative Matrix Factorization (NMF) Methodology

Let the original matrix ($\mathbf{V}$) be a detailed description of all the teachers' (or students') behaviors. Each row in the matrix represents a unique teacher-week (or classroom-week), and each column represents a specific behavior or action. The entry in a specific row and column corresponds to the frequency of that behavior for that particular teacher-week (or classroom-week). We then estimate $\mathbf{V} \simeq \mathbf{W}\mathbf{H}$, such that we minimize the following:

$$
\left\| \mathbf{V} - \mathbf{W}\mathbf{H} \right\| , \mathbf{W} \geq 0, \mathbf{H} \geq 0.
$$

We used two different loss functions (Frobenius norm and Kullback-Leibler divergence) and two different initialization methods (nonnegative double singular value decomposition (NNDSVD) and NNDSVD with zeros filled with the average of the input matrix (NNDSVDA)). The resulting matrices are:

1.  Basis Matrix ($\mathbf{W}$): This matrix represents underlying behavior patterns. Each column is a "meta-behavior" or a group of behaviors occurring together.
2.  Mixture Matrix ($\mathbf{H}$): This matrix shows the extent to which each "meta-behavior" is present in each teacher-week (or classroom-week). Each entry in this matrix represents the contribution of a "meta-behavior" to a particular behavior present in the data.

These matrices can reveal underlying patterns of behaviors (from the basis matrix) and how these patterns are mixed and matched in different teachers (from the mixture matrix). It allows us to assess the method's performance under varying configurations, with the sum of squared residuals and silhouette scores for comparison.

One desirable feature of NMF is that it produces sparse components. In most cases, a median split is equivalent to giving a value of 1 to any positive entry. This sparsity makes the results more interpretable.

## Analytical Methods

### Feature Selection

In order to pre-select the most appropriate action, reward, and state variables, we used a panel logistic regression model inspired by dynamic analysis [@lau2005]. This approach acted as a filter to capture the action-reward (or action-reward-state) configurations displaying characteristics reminiscent of reinforcement learning (RL). We followed four criteria: a) the influence of consistent rewards on the propensity of actions being repeated, b) the immediate impact of states on action selection, c) the strategic role of actions in navigating towards desirable states, and d) the identification of action auto-correlation as an indicator of incremental learning processes.

Unlike @lau2005, our model is not limited to the lagged effects of actions and rewards. We also capture action-reward interactions and the contemporaneous influence of states on actions. The general model formulation for state-free and state-based scenarios is as follows:

#### State-Free Model

```{=tex}
\begin{align*}
\text{Action}_t =& \ \sum_{i=1}^{L} \left( \beta_{i} \text{Reward}_{t-i} + \gamma_i \text{Action}_{t-i} + \sum_{j=i}^{L} \delta_{ij} (\text{Reward}_{t-i} \times \text{Action}_{t-j}) \right) \\
& + \mu_{\text{Teacher}} + \lambda_{\text{Week}} + \epsilon_t 
\end{align*}
```
#### State-Based Model

```{=tex}
\begin{align*}
\text{Action}_t =& \ \sum_{i=1}^{L} \left( \beta_{i} \text{Reward}_{t-i} + \gamma_i \text{Action}_{t-i} + \sum_{j=i}^{L} \delta_{ij} (\text{Reward}_{t-i} \times \text{Action}_{t-j}) \right) \\
& + \phi \text{State}_{t-1} + \psi (\text{State}_{t-1} \times \text{Action}_{t-2}) + \mu_{\text{Teacher}} + \lambda_{\text{Week}} + \epsilon_t
\end{align*}
```
where $\text{Action}_t$ denotes the binary outcome at time $t$, $\text{Reward}_{t-i}$ and $\text{Action}_{t-i}$ represent the reward and action variables lagged by $i$ periods, and $L$ is the maximum lag considered. $\mu_{\text{Teacher}}$ and $\lambda_{\text{Week}}$ represent fixed effects for teachers and weeks, respectively.

#### Varying Coefficients Model

We used a varying coefficients model to capture individual teacher effects on the dynamics between rewards, actions, and states. The model is specified as follows:

```{=tex}
\begin{align*}
\text{Action}_{kt} =& \ \sum_{i=1}^{L} \left( \beta_{ki} \text{Reward}_{k, t-i} + \gamma_{ki} \text{Action}_{k, t-i} + \sum_{j=i}^{L} \delta_{kij} (\text{Reward}_{k, t-i} \times \text{Action}_{k, t-j}) \right) \\
& + \phi_k \text{State}_{kt} + \psi_k (\text{State}_{kt} \times \text{Action}_{k, t-1}) + \mu_k + \lambda_{\text{Week}} + \epsilon_{kt}
\end{align*}
```
where $\beta_{ki}$, $\gamma_{ki}$, and $\delta_{kij}$ represent the reward, action, and their interaction coefficients that vary by teacher; $\phi_k$ and $\psi_k$ are the effects for the state and the interaction between state and lagged action, respectively; and $\mu_k$ and $\lambda_{\text{Week}}$ are fixed effects for teachers and weeks, respectively.

Herein, the interaction between lagged rewards and actions aimed to capture the reinforcement aspect (a), where prior rewards enhance the likelihood of repeating specific actions. Including current state variables addressed (b) and examining how present educational contexts inform action choices. The interaction between current states and lagged actions encapsulated (c) that actions are deliberately chosen to navigate towards or sustain preferable educational states. Lastly, considering lagged rewards alone, we sought to elucidate (d) the phenomenon where past successes influence future endeavors, indicative of a learning trajectory.

To operationalize these models, we constructed lagged versions of the variables, extending up to six weeks to determine the optimal lag. This lag value captured how far back rewards and past actions influenced teacher behavior.

Two types of metrics anchored model selection. First, we split the data into training (80%) and testing (20%) subsets. Then, we evaluated model fit and predictive accuracy using the Bayesian Information Criterion (BIC) and the Out-of-Sample fit, gauged through the Area Under the Receiver Operating Characteristic curve (AUC). As such, we balanced model parsimony with predictive power. Second, we aimed to delineate which regression coefficients most closely embodied RL-like dynamics. We focused on a) variables that consistently enhanced action probability in response to rewards (i.e., positive coefficient for the state-reward interaction), b) actions that changed according to a given state, c) actions chosen to achieve desired states (i.e., significant coefficient for the action-state interaction), and d) demonstrate action auto-correlation.

### Reinforcement Learning Model Estimation

#### State-Free Q-Learning Model

We use a state-free version of the Q-learning model to predict the actions of teachers based on their past actions and the rewards they received. Thus, the Q-value for action $a$ is updated based on the reward prediction error $\delta$:

$$
Q_{t+1}(a) = Q_{t}(a) + \alpha \left( \gamma (\text{Reward}_t - \text{cost}(a)) - Q_{t}(a) \right)
$$

where

-   $\alpha$ is the learning rate,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_t$ is the immediate reward received after taking action $a$ in state $s$,
-   $\text{cost}(a)$ is the estimated cost associated with action $a$,
-   $Q_{t}(a)$ is the estimate of the Q-value for action $a$ at time $t$.

The probability of choosing a particular action is determined by the softmax function and is defined as follows:

$$
\Pr_t(a) = \frac{1}{1+e^{-\tau Q_{t}(a)}}
$$

where:

-   $\Pr_{t}(a)$ is the probability of choosing action $a$ at time $t$,
-   $\tau$ is the temperature parameter.

We interpret each of these parameters as follows:

-   Cost: The perceived effort or inconvenience associated with the action, such as the effort required to complete a particular task or the inconvenience of deviating from a preferred teaching method.

-   Learning rate ($\alpha$): The extent to which the newly acquired information will override the old information. A factor of 0 will make the agent not learn anything.

-   Discount rate ($\gamma$): The degree to which future rewards are discounted compared to immediate rewards. A high discount rate means that future rewards are considered almost as valuable as immediate rewards, which encourages long-term planning. A low discount rate means that immediate rewards are much more valuable than future rewards, which encourages short-term thinking.

-   Inverse temperature ($\tau$): The degree of randomness in the choice behavior. A high inverse temperature means that the agent is more likely to choose the action with the highest expected reward, while a low inverse temperature means that the agent is more likely to choose actions randomly. This parameter can be interpreted as a measure of the agent's confidence in its Q-values, reflecting the trade-off between exploration (trying out new actions) and exploitation (sticking to known beneficial actions).

#### The Actor-Critic Model

In the Actor-Critic model, we update the value function, parameterized by weights $w$, and the policy function, defined by the parameter $\theta$. The model's update mechanism is minimizing the prediction error ($\delta$), which drives adjustments to both the actor and critic components. The equations for updating the weights and policy are as follows:

$$
w_{t+1} = w_{t} + \alpha_{v} \cdot \delta \cdot S_{t}
$$

$$
\theta_{t+1} = \theta_{t} + \alpha_{\pi} \cdot \delta \cdot S_{t}
$$

where $w_{t}, \theta_{t}$ are the vectors of policy and value weights, respectively, for action $a$, and $S_{t}$ is a vector that characterizes the current state, defined as $S_{t} = \begin{bmatrix} 1 \\ \text{State Variable}_{t} \end{bmatrix}$.

We define the parameterized policy as $\Pr_{t}(a)=\text{Logit}^{-1}(\theta_{t} \cdot S_{t})$, the values of each state as $v(S_{t},a) = w_{t} \cdot S_{t}$, and prediction error $\delta$ as the difference between the actual outcome and the estimated value of the state-action pair:

$$
\delta = (\text{Reward}_{t} - \text{cost}(a)) - \left( \gamma v(S_{t},a) - v(S_{t+1},a) \right)
$$

#### Hybrid Models

The hybrid models maintain parallel estimations of two models (Logistic regression, Q-learning, or Actor-Critic), combining them to estimate action selection. This integration is achieved through a weighting scheme that balances the contributions of each model. Specifically, the probability of selecting action ($a$) at time ($t$), denoted ($\Pr_t(a)$), is calculated through a linear combination of the softmax outputs of the selected models. This integration uses a weighting parameter ($\lambda$), which adjusts the relative influence of each model and is formulated as:

$$
\Pr_t(a) = \lambda \Pr_t(a)_{\text{Model 1}} + (1-\lambda) \Pr_t(a)_{\text{Model 2}},
$$

## Model Estimation

We adopt the Hierarchical Bayesian Inference (HBI) framework, as described by @piray2019a, to assess the fitness of our RL models and estimate their respective parameters across subjects. This approach uses Laplace approximations for efficient computation of posteriors by approximating the integrals involved in Bayesian inference. Subsequently, it leverages population-level distributions to refine individual parameter variation. Within this framework, we assume that for any given model $k$ and subject $n$, the individual parameters ($h_{k,n}$) are normally distributed across the population with $p(h_{k,n}) = N(h_{k,n} | \mu_k, V_k)$, where $\mu_k$ and $V_k$ represent the mean and variance of the prior distribution over $h_{k,n}$, respectively.

We use an expectation-maximization algorithm, iteratively performing the following two steps:

1.  Expectation Step: The algorithm calculates a posteriori estimates of the individual parameters ($h_{k,n}$) based on the existing group-level distributions.
2.  Maximization Step: The algorithm refines the group-level parameters ($\mu_k$ and $V_k$) using current individual parameter estimates. The updated mean group parameter $\mu_k$ is computed as the average of subject-level mean estimates ($\theta_{k,n}$) across all subjects, conforming to $\mu_k = \frac{1}{N}\sum_{n}\theta_{k,n}$, where $N$ is the total number of subjects.

With this approach, we can estimate the log-likelihood for each subject's data, given the proposed models and parameter estimates. Recognizing the constraint of normality, we transform constrained parameters (e.g., the learning rate and discount factor in the Q-learning model). For parameters within a (0,1) interval, we use the inverse logit function, $\text{Logit}^{-1}$. Parameters like temperature and costs, which are intrinsically non-negative, undergo an exponential transformation to accord with the presumed normality in their logarithmic space.

The parameters estimated for each model are as follows:

1.  Q-learning:
    -   Learning Rate: $\text{Logit}^{-1}(\alpha)$
    -   Discount Rate: $\text{Logit}^{-1}(\gamma)$
    -   Temperature: $\exp(\tau)$
    -   Cost: $\exp(\text{cost})$
2.  Actor-Critic:
    -   Weights $w$ and $\theta$: $\exp(w)$, $\exp(\theta)$
    -   Learning rates ($\alpha_w$, $\alpha_\theta$): $\text{Logit}^{-1}(\alpha_w)$, $\text{Logit}^{-1}(\alpha_\theta)$
    -   Discount Factor: $\text{Logit}^{-1}(\gamma)$
    -   Temperature: $\exp(\tau)$
    -   Initial Values: $\exp(\theta_{\text{init}})$, $\exp(w_{\text{init}})$
    -   Costs: $\exp(\text{cost})$
3.  Logistic Regression Model:
    -   Parameters: $\beta$

#### Top Model Selection

We used a Bayesian model comparison technique from @piray2019a to determine the best-fit model from our set of candidates by considering the exceedance probability (i.e., the likelihood that one model is more probable than others in explaining the data) and the protected exceedance probability (i.e., the exceedance probability adjusted for the chance that observed differences in model evidence are due to random fluctuations).

The model evidence $\Pr(D | M_k)$ for model $k$ represents the likelihood of the observed data $D$ given the model $M_k$, and is given by $\Pr(D|M_k) = \int_{\Theta} \Pr(D|\theta, M_k) \Pr(\theta|M_k) d\theta$ for all possible parameters $\theta \in \Theta$. This value allows us to assess the models' overall fit across the entire parameter space.

We then calculated the likelihood of each model being the best fit for the data. We do so by first computing the posterior probabilities $$
\Pr(M_k|D) = \frac{\Pr(D|M_k)\Pr(M_k)}{\Pr(D)},
$$ where $\Pr(D) = \sum_{k=1}^K \Pr(D|M_k)\Pr(M_k)$ serves as a normalizing constant. We then proceeded to compute the exceedance probabilities, $EP_k = \Pr(\Pr(M_k|D) > \Pr(M_j|D) \text{ for all } j \neq k)$. In simple terms, it is the likelihood of model $k$ having a posterior probability higher than any other model in our comparison.

Finally, to account for false positives, we estimated the protected exceedance probability $PEP_k = EP_k(1 - P_0) + P_0/K$, where $P_0 = 1/(1 + \exp(L - L_0))$, and $L$ and $L_0$ represent the log-likelihoods under the alternative and null hypotheses (i.e., the models are equally likely). The $PEP$ helps prevent overconfident selection of a model when the evidence is not sufficiently robust to distinguish model performance reliably.

### Heterogeneity Analysis

We explore the heterogeneity across schools and teachers by analyzing individual and group-level parameters as follows:

-   Classification of Individual Responses: Using the hierarchical Bayesian framework, we assign each teacher to the model that best captures their behavior, recognizing the individual differences that emerge from our population-level analysis.

-   Parameter Estimation Across Models: We estimate individual-specific parameters for each teacher and use them as a behavioral profile.

-   Analysis of Group-Level Trends: By aggregating the teacher data at the school level, we identify patterns and trends beyond individual variation. This aggregation allows us to investigate the influence of collective attributes (e.g., school income levels and classroom size) on educational outcomes and teacher performance.

-   Investigation of Influential Variables: We are particularly interested in how socioeconomic factors, such as zipcode median income, may impact the models' ability to describe teacher behavior. These variables provide insights into the heterogeneity in teacher classification and parameter estimates, offering a deeper understanding of the complex interplay between educational resources and pedagogical success.

# Results

In this section, we present the results of our analysis, as outlined in @tbl-methods.

## Dimensionality Reduction

To simplify the complexity of our data, we first conducted a dimensionality reduction exercise, comparing different methods to identify the most essential features. After carefully balancing reconstruction accuracy with clustering clarity, we ultimately selected Non-negative Matrix Factorization (NMF) with four components. We chose this configuration after carefully considering the Reconstruction $R^2$ and Silhouette Scores, as shown in @fig-nmf-pca-comparison. Although PCA provided a simple approach, NMF, particularly the Frobenius variant, outperformed it with superior silhouette scores (i.e., clustering) for teacher and student data.

```{r pca nmf data-prep}
#| include: false

df <- df %>%
  filter(!is.na(Minutes.on.Zearn...Total)) %>%
  group_by(Classroom.ID) %>%
  # train/test split
  mutate(set = sample(c("train", "test"), size = n(),
                      prob = c(0.8, 0.2), replace = TRUE)) %>%
  ungroup() %>% arrange(Classroom.ID, week)

columns <- names(
  df %>% select(RD.elementary_schedule:Minutes.on.Zearn...Total,
                Active.Users...Total:Tower.Alerts.per.Tower.Completion)
  )

# Create base data.table for models (faster than data.frame)
df_pca <- as.data.table(df)
# Convert columns to double to prevent precision loss
df_pca[, (columns) := lapply(.SD, as.numeric),
        .SDcols = columns]

# Apply the scaling operation
df_pca[, (columns) := lapply(.SD, function(x) {
  x[is.na(x)] <- 0
  sd_x <- sd(x, na.rm = TRUE)
  if(sd_x == 0 | is.na(sd_x)) return(rep(0, .N))
  x / sd_x
}), by = MDR.School.ID, .SDcols = columns]
setorder(df_pca, Classroom.ID, week)

# Calculate standard deviations
std_devs <- apply(df_pca %>% select(all_of(columns)), 2, sd, na.rm = T)
# Identify columns with defined standard deviations (not NaN or Inf)
invalid_cols <- names(std_devs[is.na(std_devs) |
                                 is.infinite(std_devs) |
                                 std_devs == 0])
df_pca <- df_pca %>% select(-all_of(invalid_cols))

# Clean environment
rm(list = setdiff(ls(), c("df", "df_pca", "random_py")))
gc(verbose = FALSE)

```

```{python load data}
#| include: false
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA, NMF
from sklearn.metrics import silhouette_score

## Basic Variables
# Import data from R
dfpca_py = pd.DataFrame(r.df_pca)
dfpca_py.sort_values(['Classroom.ID', 'week'], inplace=True)

# Split the data into teacher and student subsets
teacher_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "RD.elementary_schedule"
    ):dfpca_py.columns.get_loc(
      "RD.grade_level_teacher_materials"
      # "Minutes.on.Zearn...Total"
      )+1
  ]
student_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "Active.Users...Total"
    ):dfpca_py.columns.get_loc(
      "Tower.Alerts.per.Tower.Completion"
      )+1
  ]

X_teachers = dfpca_py[teacher_variables]
X_students = dfpca_py[student_variables]

# Dictionaries for export
components = {}
results = {}
residuals = {}
silhouette = {}

```

```{python pca-nmf}
#| cache: true
#| include: false

# Function for NMF
def nmf_method(n, method, initial, X_scaled, data_label, solv = 'mu', nomin = False):
  method_name = f"{method.title()} {initial.upper()}"
  
  if method == 'frobenius' and initial == 'nndsvd': solv = 'cd'
  if method != 'frobenius' and initial == 'nndsvd': return
  if method != 'frobenius': method_name = f"{method.title()}"
  if nomin: method_name = method_name + "_nomin"
  
  nmf = NMF(
    n_components=n,
    init=initial,
    beta_loss=method,
    solver=solv,
    max_iter=4_000
  )
  X_nmf, nmf_comp = nmf.fit_transform(X_scaled), nmf.components_
  X_hat = nmf.inverse_transform(X_nmf)
  labels = np.argmax(nmf_comp, axis=0)
  
  results.setdefault(f"{method_name}_{data_label}", {})[n] = X_nmf
  components.setdefault(f"{method_name}_{data_label}", {})[n] = nmf_comp
  residuals.setdefault(f"{method_name}_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
  silhouette.setdefault(f"{method_name}_{data_label}", {})[n] = silhouette_score(nmf_comp.transpose(), labels)

def perform_pca_nmf(X_scaled, data_label, n_comp):
  for n in range(2, n_comp):
    ## PCA
    pca = PCA(n_components=n)
    X_pca = pca.fit_transform(X_scaled)
    pca_comp = pca.components_
    X_hat = pca.inverse_transform(X_pca)
    labels = np.argmax(pca_comp, axis=0)
    results.setdefault(f"PCA_{data_label}", {})[n] = X_pca
    components.setdefault(f"PCA_{data_label}", {})[n] = pca_comp
    residuals.setdefault(f"PCA_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
    silhouette.setdefault(f"PCA_{data_label}", {})[n] = silhouette_score(pca_comp.transpose(), labels)
    
    ## Non-negative Matrix Factorization
    for method in {'frobenius', 'kullback-leibler'}:
      for initial in {'nndsvd', 'nndsvda'}:
        nmf_method(
          n, method, initial, X_scaled,
          data_label=data_label,
          nomin = True    # No teacher minutes included
          )

# Run PCA and NMF for teachers
perform_pca_nmf(X_teachers, "teachers", min(X_teachers.shape) // 3)
# Run PCA and NMF for students
perform_pca_nmf(X_students, "students", min(X_students.shape))

```

```{python clean environment}

# get the names of all variables in the global namespace
all_vars = list(globals().keys())

# list of variables to keep
keep_vars = ['residuals', 'silhouette', 'components', 'results', 'r']

# delete all variables that are not in keep_vars
for var in all_vars:
    if var not in keep_vars:
        del globals()[var]
del keep_vars
del all_vars
del var

# manually call the garbage collector
import gc
gc.collect()

```

```{r}
#| label: fig-nmf-pca-comparison
#| fig-cap: "Comparison of Dimensionality Reduction Techniques. This figure juxtaposes the performance of PCA against NMF variants characterized by Frobenius and Kullback-Leibler divergences across teacher and student datasets. The comparison is predicated on the reconstruction quality, depicted by R-squared residuals, and the clarity of data structuring, indicated by silhouette scores, for a range of component numbers."
#| fig-subcap: 
#| - "Teacher Data"
#| - "Student Data"
#| layout-ncol: 1
#| cache: true

# Importing from Python
residuals_list <- py$residuals
silhouette_list <- py$silhouette

teacher_variables <- names(
  df_pca[,RD.elementary_schedule:RD.grade_level_teacher_materials]
)
student_variables <- names(
  df_pca[,Active.Users...Total:Tower.Alerts.per.Tower.Completion]
)
TSS_teacher <- df_pca %>%
  select(all_of(teacher_variables)) %>%
  mutate(across(all_of(teacher_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(teacher_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()
TSS_student <- df_pca %>%
  select(all_of(student_variables)) %>%
  mutate(across(all_of(student_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(student_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()

# Creating dataframes
df_residuals <- do.call(rbind, lapply(names(residuals_list), function(method) {
  do.call(rbind, lapply(names(residuals_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Residuals = residuals_list[[method]][[n]]
    )
  }))
}))
df_residuals_teachers <- df_residuals[
  grepl("_teachers", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_teacher)
df_residuals_students <- df_residuals[
  grepl("_students", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_student)

df_silhouette <- do.call(rbind, lapply(names(silhouette_list), function(method) {
  do.call(rbind, lapply(names(silhouette_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Silhouette = silhouette_list[[method]][[n]]
    )
  }))
}))
df_silhouette_teachers <- df_silhouette[
  grepl("_teachers", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method))
df_silhouette_students <- df_silhouette[
  grepl("_students", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method))

# Plotting residuals
p1 <- ggplot() +
  geom_line(data = df_residuals_teachers,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R2",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_teachers$Components),
                                  max(df_residuals_teachers$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p2 <- ggplot(df_silhouette_teachers,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_teachers$Components),
                                  max(df_silhouette_teachers$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_teachers$Silhouette) +
                                  3*sd(df_silhouette_teachers$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting residuals
p3 <- ggplot() +
  geom_line(data = df_residuals_students,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R2",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_students$Components),
                                  max(df_residuals_students$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p4 <- ggplot(df_silhouette_students,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_students$Components),
                                  max(df_silhouette_students$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_students$Silhouette) +
                                  3*sd(df_silhouette_students$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Combine the plots and place the legend at the bottom
comparison_plot <- ggarrange(p1, p2,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
comparison_plot2 <- ggarrange(p3, p4,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")

comparison_plot
comparison_plot2

```

#### Interpreting Components

After analyzing the NMF data, we identified four significant components for teachers and students. @fig-nmf-heatmap displays these components as heatmaps, offering insight into the underlying behavioral structures. Given the loadings, we interpret the components as follows:

##### Teachers Components

**Component 1 (Assessments)**: This component has substantial weights on supplemental assessment materials, such as "Optional Problem Sets Download," "Optional Homework Download," and "Student Notes and Exit Tickets Download," indicating a proactive approach to evaluating and supporting student learning progress. It could also reflect a proactive approach to monitoring student understanding and providing feedback.

**Component 2 (Pedagogical Knowledge)**: The high weights on "Guided Practice Completed," "Tower Completed," "Tower Stage Failed," and "Fluency Completed" suggest that this component reflects when teachers are engaged in acquiring subject-matter-specific pedagogy, learning to scaffold and explain concepts in various ways.

**Component 3 (Group Instruction)**: This component, with prominent weights on "Small Group Lesson Download," "Whole Group Word Problems Download," and "Whole Group Fluency Download," suggests a pedagogical approach focused on fostering interactive and comprehensive classroom instruction. It implies engagement in activities that promote group learning dynamics and collective problem-solving skills.

**Component 4 (Curriculum Planning)**: The dominance of "Mission Overview Download" and "Grade Level Overview Download" in this component suggests that teachers are highly involved in strategic planning and curriculum mapping. It involves organizing the curriculum content and structuring lesson plans to align with grade-level objectives and mission overviews.

##### Student Components

**Component 1 (Badges)**: This component emphasizes "On-grade Badges" and "Badges," indicating that it measures students' overall engagement and advancement through the curriculum.

**Component 2 (Struggles)**: This component, which heavily weights "Boosts" and "Tower Alerts," seems to capture the frequency of occasions when students require additional scaffolding and assistance.

**Component 3 (Number of Students)**: This component mainly consists of "Active Students," which provides insight into what proportion of students regularly log in to complete Digital Lessons.

**Component 4 (Activity)**: Dominated by "Student Minutes" and "Student Logins," this component highlights the amount of time students invest in and the frequency of their interactions with Zearn.

```{r}
#| cache: true
#| label: fig-nmf-heatmap
#| fig-cap: "Heatmap of Non-negative Matrix Factorization (NMF) components for the Zearn data. The rows represent the original variables, and the columns represent the individual components. The varying colors from light to dark indicate the increasing weights of variables within each component."
#| fig-subcap:
#| - "Teacher Data. Component 1 (Assessments) focuses on using supplemental materials for student evaluation; Component 2 (Pedagogical Knowledge) emphasizes developing subject-specific teaching strategies; Component 3 (Group Instruction) centers on collaborative and whole-class teaching methods; Component 4 (Curriculum Planning) highlights planning and lesson preparation."
#| - "Student Data. Component 1 (Badges) measures curriculum engagement and progression; Component 2 (Struggles) indicates the need for additional academic support; Component 3 (Number of Students) tracks student participation within the platform; Component 4 (Activity) reflects the overall time spent and frequency of platform usage."
#| layout-ncol: 1

library(pheatmap)
library(grDevices)

components_list <- py$components
df_heatmap_teachers <- 
  components_list[["Frobenius NNDSVD_nomin_teachers"]][["4"]] %>%
  t() %>% as.data.frame()
df_heatmap_students <- 
  components_list[["Frobenius NNDSVD_nomin_students"]][["4"]] %>%
  t() %>% as.data.frame()

# Define a named vector to map old variable names to new ones
variable_names <- c(
  "Minutes.on.Zearn...Total" = "Teacher Minutes",
  "RD.optional_problem_sets" = "Optional Problem Sets Download",
  "RD.student_notes_and_exit_tickets" = "Student Notes and Exit Tickets Download",
  "RD.mission_overview" = "Mission Overview Download",
  "RD.pd_course_notes" = "Course Notes Download",
  "RD.elementary_schedule" = "Elementary Schedule Download",
  "RD.whole_group_fluency" = "Whole Group Fluency Download",
  "Guided.Practice.Completed" = "Guided Practice Completed",
  "RD.small_group_lessons" = "Small Group Lesson Download",
  "Tower.Completed" = "Tower Completed",
  "Fluency.Completed" = "Fluency Completed",
  "Number.Gym.Activity.Completed" = "Number Gym Activity Completed",
  "RD.grade_level_overview" = "Grade Level Overview Download",
  "Tower.Stage.Failed" = "Tower Stage Failed",
  "Kindergarten.Activity.Completed" = "Kindergarten Activity Completed",
  "Tower.Struggled" = "Tower Struggled",
  "RD.k_mission" = "Kindergarten Mission Download",
  "RD.whole_group_word_problems" = "Whole Group Word Problems Download",
  "RD.assessments" = "Assessments Download",
  "RD.teaching_and_learning_approach" = "Teaching and Learning Approach Download",
  "RD.optional_homework" = "Optional Homework Download",
  "RD.k_schedule" = "Kindergarten Schedule Download",
  "RD.curriculum_map" = "Curriculum Map Download",
  "RD.assessments_answer_key" = "Assessments Answer Key Download",
  "RD.pd_course_guide" = "Course Guide Download",
  "RD.grade_level_teacher_materials" = "Teacher Materials Download",
  "Active.Users...Total" = "Active Students",
  "Sessions.per.Active.User" = "Student Logins",
  "Badges.per.Active.User" = "Badges",
  "Badges..on.grade..per.Active.Student" = "On-grade Badges",
  "Minutes.per.Active.User" = "Student Minutes",
  "Tower.Alerts.per.Tower.Completion" = "Tower Alerts",
  "Boosts.per.Tower.Completion" = "Boosts"
)
# Rename the rows of the dataframe
row.names(df_heatmap_teachers) <- variable_names[teacher_variables]
row.names(df_heatmap_students) <- variable_names[student_variables]

names(df_heatmap_teachers) <- paste0("Comp ", 1:4)
names(df_heatmap_students) <- paste0("Comp ", 1:4)
df_heatmap_teachers <- df_heatmap_teachers %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)
df_heatmap_students <- df_heatmap_students %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)

color_scheme <- colorRampPalette(
  c("#F7F7F7",RColorBrewer::brewer.pal(n = 9, name = "YlOrRd"))
  )(100)
breaks <- seq(0, 1, by = 0.01)
minmax <- function(x) ((x - min(x))/(max(x) - min(x)))
pheatmap(df_heatmap_teachers %>%
           mutate_all(minmax),
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         angle_col = 0,
         # main = "Loadings of NMF components",
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE,
         show_rownames = TRUE)

pheatmap(df_heatmap_students %>%
           mutate_all(minmax),
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         angle_col = 0,
         # main = "Loadings of NMF components",
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE,
         show_rownames = TRUE)

```

```{r export dimension reduction}
#| eval: false
# Importing results from Python
results_list <- py$results

# List of method names
# methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler")
methods <- c("Frobenius NNDSVD")
# Initialize df_components
df_components <- df_pca

# Loop over each method, extract the results at 3 components and bind to df_components
for (method in methods) {
  method_teacher <- paste0(method, "_nomin_teachers")
  method_student <- paste0(method, "_nomin_students")
  result_teacher <- results_list[[method_teacher]][["4"]]
  result_student <- results_list[[method_student]][["4"]]
  df_components <- df_components %>%
    bind_cols(result_teacher, result_student, .name_repair = )
  
  # Adjust column names
  new_cols <- paste0("Frobenius NNDSVD",
                     rep(c("_teacher", "_student"), each = 4),
                     rep(1:4, 2))
  names(df_components)[
    (ncol(df_components) -
       ncol(result_teacher) -
       ncol(result_student) + 1):ncol(df_components)
    ] <- new_cols
}

# Write to csv
write.csv(df_components, "./Bayesian/df.csv")

```

## Feature Selection

```{r load dimension reduction}
#| include: false
df <- read.csv(file = "./Bayesian/df.csv") %>%
    mutate(across(where(is.numeric),
                  ~ ifelse(. < .Machine$double.eps, 0, .)))
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
gc(verbose = FALSE)
```

```{r helper-functions}

get_lag_value <- function(datatable, col, lag_period, n_comp = NULL) {
  # Add a column for week_lag
  datatable[, week_lag := c(0, diff(week)), by = Classroom.ID]

  if (is.null(n_comp)) {
    # Update the lag column with shift function
    datatable[, (paste0(col, "_", lag_period)) :=
                shift(get(col), lag_period, fill = 0, type = "lag"),
              by = Classroom.ID]
  } else {
    for (comp in 1:n_comp) {
      # Update the lag column with shift function
      datatable <- datatable[, (paste0(col, comp, "_", lag_period)) :=
                              shift(get(paste0(col, comp)), lag_period, fill = 0, type = "lag"),
                            by = Classroom.ID]
    }
  }
  
  return(datatable)
}

# Function to identify high-density regions for a given vector of values
find_hdr <- function(values) {
  IQR <- quantile(values, 0.75) - quantile(values, 0.25)
  return(c(quantile(values, 0.25) - 1.5*IQR,
           quantile(values, 0.75) + 1.5*IQR))
}

in_hdr <- function(values) {
  hdr <- find_hdr(values)
  return(hdr[1] <= values & values <= hdr[2])
}

```

```{r panel-model}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial)
  return(model)
}

create_formula <- function(action, reward, state = NULL, lag = 1) {
  terms <- c()

  for (i in 1:lag) {
    terms <- c(terms, paste0(reward, "_", i))
    terms <- c(terms, paste0(action, "_", i))
    # Interaction term for reward_i * action_i
    terms <- c(terms, paste0(reward, "_", i, ":", action, "_", i))
    if (i != lag) {
      for (j in (i + 1):lag) {
        # Interaction for reward_i * action_j when i < lag
        terms <- c(terms, paste0(reward, "_", i, ":", action, "_", j))
        
      }
    }
  }

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + week_lag | ", # Adding fixed effects part
                             "Teacher.User.ID + week")
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + ", paste0(state, "_1"),
                             " + ", paste0(state, "_1", ":",
                                           action, "_", (2:max(lag,2)),
                                           collapse = " + "),
                             " + week_lag | ",
                             "Teacher.User.ID + week")
  }
  return(formula_string)
}
#-------------
lags <- c(1:6)
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  # st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  # filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     # st <- as.character(params$st[i])
                     st <- NULL
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          # State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-results.RData")

```

```{r panel-model-states}
#| eval: false

#-------------
lags <- c(1:6)
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     st <- as.character(params$st[i])
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-state-results.RData")

```

We first explore reinforcement learning (RL)-like characteristics within the teacher and classroom usage data. We aimed to uncover patterns indicative of RL, where actors (teachers) select actions (teaching strategies) that historically yield higher rewards (improved student outcomes) and use states (classroom contexts) as signals for action selection. Further, we sought to understand how actions contribute to achieving or maintaining desired states and the extent to which actions exhibit auto-correlation due to incremental learning processes.

## Model Fit and Performance

In order to capture the temporal dynamics of actions influenced by lagged rewards and states, we employed panel logistic regression models across different combinations of variables and lags. We incorporate lagged variables (ranging from one to six weeks) into the models using the Dynamic Analysis approach proposed to account for temporal autocorrelation and potential delayed effects. We applied reward and state structures extracted from classroom data via non-negative matrix factorization (NMF) with the Frobenius Non-negative Double Singular Value Decomposition (NNDSVD) and actions derived similarly from teacher data. We evaluate these models using the Bayesian Information Criterion (BIC) for model complexity and fit and the Area Under the Receiver Operating Characteristic curve (AUC) for predictive accuracy.

More specifically, we select one teacher component as the action and one student component as the reward. When constructing state-based models, we incorporate an additional student component as the state variable. In total, this choice yields 16 state-free models and 48 state-based models.

### Temporal Dynamics

Our investigation into temporal dynamics confirmed the impact of lagged rewards and actions on decision-making: shaping future decisions by past experiences. @fig-panel-bic illustrates this relationship, showcasing the predictive accuracy and model fit across fixed-effects models with different lags, with BIC and AUC scores for the models with one week lags as the baseline. The results suggest a preference for a lag of two periods as optimal, based on the "elbow" in the AUC curves and the minima in BIC curves.

```{r}
#| label: fig-panel-bic
#| fig-cap: "BIC and AUC variations across lags for fixed-effects panel logistic regression models. The plot shows model prediction accuracy and fit for different lag periods, with one-week lag as the baseline. The optimal lag period is determined based on the 'elbow' in the AUC curves and the minima in the BIC curves."
#| fig-subcap: 
#|   - "BIC state-free"
#|   - "AUC state-free"
#|   - "BIC state-dependent"
#|   - "AUC state-dependent"
#| layout-ncol: 2

load("Regressions/fe-results.RData")

fe_results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = "None",
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
}))

teachers <- Reduce(intersect, sapply(results, function(x) {
  names(x$recoef$Teacher.User.ID)
}))

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    # State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         # bic = bic - bic_base,
         bic = bic/bic_base - 1,
         auc_base = auc[which(Lag == 1)],
         # auc = auc - auc_base,
         auc = auc/auc_base - 1) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "Percent Change in BIC",
                    "auc" = "Percent Change in AUC")
  
  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(Reward, Method),
                               # group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = "lightblue", alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, color = "blue") +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    scale_y_continuous(labels = percent_format()) +
    theme(axis.text.x = element_text(hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#------------------

load("Regressions/fe-state-results.RData")

fe_results_df <- fe_results_df %>%
  rbind(
    do.call(rbind, lapply(results, function(x) {
      data.frame(
        Method = x$Method,
        Lag = x$Lag,
        State = x$State,
        Reward = x$Reward,
        auc = x$AUC,
        bic = x$bic,
        stringsAsFactors = FALSE
        )
      }))
    )

teachers <- intersect(
  teachers, Reduce(intersect, sapply(results, function(x) {
    names(x$recoef$Teacher.User.ID)
    }))
  )

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward, State) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         # bic = bic - bic_base,
         bic = bic/bic_base - 1,
         auc_base = auc[which(Lag == 1)],
         # auc = auc - auc_base,
         auc = auc/auc_base - 1) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "Percent Change in BIC",
                    "auc" = "Percent Change in AUC")
  
  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = "lightblue", alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, color = "blue") +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    scale_y_continuous(labels = percent_format()) +
    theme(axis.text.x = element_text(hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#----------------

bic_plot_se
auc_plot_se
bic_plot_se_st
auc_plot_se_st

```

### Model Selection and Interpretability

```{r panel-model-subset}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial)
  return(model)
}

create_formula <- function(action, reward, state = NULL) {
  terms <- c()

  # Interaction term for reward_i * action_i
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_1"))
  terms <- c(terms, paste0(reward, "_1:", action, "_1"))
  
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(reward, "_2"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_2:", action, "_2"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + week_lag | ", # Adding fixed effects part
                             "Teacher.User.ID + week")
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + ", paste0(state, "_1"),
                             " + ", paste0(state, "_1", ":", action, "_2"),
                             " + week_lag | ",
                             "Teacher.User.ID + week")
  }
  return(formula_string)
}
#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  # st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  # filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data", "teachers",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     # st <- as.character(params$st[i])
                     st <- NULL
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          # State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-subset-results.RData")

```

```{r panel-model-states-subset}
#| eval: false

#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data", "teachers",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     st <- as.character(params$st[i])
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-state-subset-results.RData")

```

```{r panel-model-restricted}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial)
  return(model)
}

create_formula <- function(action, reward, state = NULL) {
  terms <- c()

  # Interaction term for reward_i * action_i
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(action, "_1"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + week_lag | ", # Adding fixed effects part
                             "Teacher.User.ID + week")
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + ", paste0(state, "_1"),
                             " + ", paste0(state, "_1", ":", action, "_2"),
                             " + week_lag | ",
                             "Teacher.User.ID + week")
  }
  return(formula_string)
}
#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  # st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  # filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data", "teachers",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     # st <- as.character(params$st[i])
                     st <- NULL
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          # State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-restricted-results.RData")

```

This section delves into the performance of our model within the framework of fixed effects (FE) and varying coefficients. Our analysis reveals the complexities and variations in the Zearn data on predicting teacher behavior based on previous student performances. @tbl-RL-exploration synthesizes our model performance scores, revealing that the fixed-effect area under the receiver operating characteristic curve (FE AUC) averages 0.81. We found that "Assessments" emerged as the most influential action and "Number of Students" as the leading reward in the state-free context. The Fixed Effects Bayesian Information Criterion (FE BIC) also reflects a mean of approximately 93.06 x 10^3. The lowest BIC values correspond with "Scaffolding" as a pivotal action and "Struggle" as a salient reward variable in the state-free approach.

Furthermore, the average teacher-specific AUC, a metric derived from the varying coefficient models, is 0.70. Notably, "Scaffolding" continues to be a consistent influencing factor for both state-free and state-based models, with "Number of Sessions" as the corresponding reward. The average out-of-sample teacher-specific log-likelihood, also derived from varying coefficients, shows an average of -262.63. In the state-based scenario, "Planning Guides" and "Activities" are the top-performing actions, aligning with "Struggle" and "Completion" as state variables.

```{r panel-model-states-restricted}
#| eval: false

#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data", "teachers",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     st <- as.character(params$st[i])
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-state-restricted-results.RData")

```

```{r RL-mixed-effects}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}
create_formula <- function(action, reward, state = NULL, lag = 1) {
  terms <- c()

  for (i in 1:lag) {
    # Base terms for action and reward
    terms <- c(terms, paste0(reward, "_", i), paste0(action, "_", i))

    # Interaction term for reward_i * action_i with varying slopes
    terms <- c(terms, paste0(reward, "_", i, ":", action, "_", i))

    if (i != lag) {
      for (j in (i + 1):lag) {
        # Add the interaction for reward_i * action_j when i < lag with varying slopes
        terms <- c(terms, paste0(reward, "_", i, ":", action, "_", j))
      }
    }
  }

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/me-results.RData")

```

```{r RL-mixed-effects-subset}
#| eval: false

library(fixest)

load("Regressions/me-results.RData")
teachers <- intersect(
  Reduce(union, sapply(results, function(x) {
    x$perform_df$Teacher.User.ID
    })),
  Reduce(intersect, sapply(results, function(x) {
    names(x$recoef$Teacher.User.ID)
    }))
  )

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}
create_formula <- function(action, reward, state = NULL, lag = 1) {
  terms <- c()

  for (i in 1:lag) {
    # Base terms for action and reward
    terms <- c(terms, paste0(reward, "_", i), paste0(action, "_", i))

    # Interaction term for reward_i * action_i with varying slopes
    terms <- c(terms, paste0(reward, "_", i, ":", action, "_", i))

    if (i != lag) {
      for (j in (i + 1):lag) {
        # Add the interaction for reward_i * action_j when i < lag with varying slopes
        terms <- c(terms, paste0(reward, "_", i, ":", action, "_", j))
      }
    }
  }

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/me-subset-results.RData")

```

```{r RL-mixed-effects-restricted}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}
create_formula <- function(action, reward, state = NULL) {
  terms <- c()
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(action, "_1"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/me-results-restricted.RData")

```

```{r}
#| label: table-RL-exploration

library(gtExtras)
load("Regressions/me-subset-results.RData")
# load("Regressions/me-results-restricted.RData")

top_fits_df <- do.call(rbind, lapply(results, function(x) {
  # Assuming 'perform_df' is correctly structured as shown in your summary
  top_fits <- x$perform_df %>%
    mutate(keep = case_when(!in_hdr(n_train) ~ F,
                            !in_hdr(n_test) ~ F,
                            !in_hdr(logLik_ind) ~ F,
                            auc_in < 0.5 ~ F,
                            auc_out < 0.5 ~ F,
                            auc_in  == 1 ~ F,
                            auc_out == 1 ~ F,
                            .default = T)) %>%
    filter(keep) %>%
    summarize(
      # across(c(auc_out, auc_in, logLik_ind),
      #        ~ sd(., na.rm = T)/sqrt(n()), .names = "{.col}_se"),
      across(c(auc_out, auc_in, logLik_ind),
             ~ list(mean(., na.rm = T))),
      total = n())
  # top_fits$AUC <- x$AUC
  # top_fits$BIC <- x$bic
  top_fits$Action <- x$Method
  top_fits$Reward <- x$Reward
  top_fits$State <- ifelse(is.null(x$State), "None", x$State)
  return(top_fits)
})) %>%
  mutate(across(c(auc_out, auc_in, logLik_ind), ~ unlist(.))) %>%
  na.omit()

top_fits_df <- top_fits_df %>%
  full_join(fe_results_df %>%
              filter(Lag == 2),
            by = c("Action" = "Method", "Reward", "State")) %>%
  mutate(across(c(Action, Reward, State),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Badges",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggles",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "Activity",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Pedagogical Knowledge",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Group Instruction",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Curriculum Planning",
                            .default = .)))
  
top_statefree <- rbind(
  top_fits_df %>% filter(State == "None") %>%
    arrange(desc(auc)) %>% slice_head(n = 1) %>% select(Action, Reward),
  top_fits_df %>% filter(State == "None") %>%
    arrange(bic) %>% slice_head(n = 1) %>% select(Action, Reward),
  top_fits_df %>% filter(State == "None") %>%
    arrange(desc(auc_out)) %>% slice_head(n = 1) %>% select(Action, Reward),
  top_fits_df %>% filter(State == "None") %>%
    arrange(desc(logLik_ind)) %>% slice_head(n = 1) %>% select(Action, Reward)
)
top_statebased <- rbind(
  top_fits_df %>% filter(State != "None") %>%
    arrange(desc(auc)) %>% slice_head(n = 1) %>% select(Action, Reward, State),
  top_fits_df %>% filter(State != "None") %>%
    arrange(bic) %>% slice_head(n = 1) %>% select(Action, Reward, State),
  top_fits_df %>% filter(State != "None") %>%
    arrange(desc(auc_out)) %>% slice_head(n = 1) %>% select(Action, Reward, State),
  top_fits_df %>% filter(State != "None") %>%
    arrange(desc(logLik_ind)) %>% slice_head(n = 1) %>% select(Action, Reward, State)
)

top_fits_df %>%
  mutate(bic = bic/1000) %>%
  select(auc, bic, auc_out, logLik_ind) %>%
  rename("FE AUC" = auc,
         "FE BIC (x10<sup>3</sup>)" = bic,
         "Avg. Hiearchical AUC" = auc_out,
         "Avg. Hiearchical Log Likelihood" = logLik_ind) %>%
  gt_plt_summary(title = "Model Performance") %>%
  fmt_markdown() %>%
  # Remove "Missing" column
  cols_hide(n_missing) %>%
  fmt_number(
    columns = c(Mean, Median, SD),
    decimals = 2
  ) %>%
  # Add columns for top models
  cols_add(
    top_action = top_statefree$Action,
    top_reward = top_statefree$Reward,
    top_action_st = top_statebased$Action,
    top_reward_st = top_statebased$Reward,
    top_state = top_statebased$State) %>%
  cols_label(
    top_action = "Top Action",
    top_reward = "Top Reward",
    top_action_st = "Top Action",
    top_reward_st = "Top Reward",
    top_state = "Top State"
  ) %>%
  tab_spanner(
    label = "State-Free",
    columns = c(top_action, top_reward)
  ) %>%
  tab_spanner(
    label = "State-Based",
    columns = c(top_action_st, top_reward_st, top_state)
  ) %>% gt::gtsave(filename = "images/tbl-RL-exploration.png")

```

![Summary of Fixed Effects and Varying Coefficients Models](images/tbl-RL-exploration.png){#tbl-RL-exploration}

#### Model Performance

Drawing on the resulting metrics, we strategically narrowed our focus to the action-reward-state configurations that most closely align with Reinforcement Learning (RL) principles. @fig-RL-exploration delineates the top 25 models based on their high fixed-effects Area Under the Curve (AUC) and mean teacher-specific AUC, as well as their parsimonious Bayesian Information Criterion (BIC) and higher mean teacher-specific log-likelihood.

Panel A of the figure highlights six state-free models. Each point on the plot represents a different model, identified by its BIC and average teacher-specific AUC. Within this framework, the actions of "Activities" and "Assessments" stand out, connected to rewards like "Completion," "No. Sessions," and "No. Students." Panel B spotlights 19 state-based models. In this plot, "Scaffolding" and "Activities" also emerge as the primary actions.

These visualizations highlight "Scaffolding" and "Activities" as best-fit actions. However, the choice of associated rewards and states is unclear, suggesting the need to fit RL models using all configurations featuring these two actions.

```{r}
#| label: fig-RL-exploration
#| fig-cap: "Selection of Actions, Rewards, and States for Reinforcement Learning Analysis. The plots show the performance metrics of logistic regression models designed to capture characteristics similar to reinforcement learning models. The AUC and BIC values are plotted separately for state-free and state-dependent contexts. Models closest to the upper-left corners indicate a more optimal balance between parsimony and accuracy in capturing RL-like teacher behaviors."
#| fig-subcap: 
#|   - "State-free"
#|   - "State-dependent"
#| layout-ncol: 2

# Select top models by BIC (lower is better) and AUC (higher is better)

# Keep models for later analysis
selected_models <- top_fits_df %>%
  filter(State == "None") %>%
  filter(auc_out > 0.7, bic < 90000)

## State-free
plot1 <- top_fits_df %>%
  filter(State == "None") %>%
  select(Action, Reward, bic, auc_out) %>%
  ggplot(aes(x = bic, y = auc_out, color = Reward)) +
  geom_point() +
  geom_mark_ellipse(
    aes(group = Action, label = Action, color = NULL),
    con.type = "straight",
    con.cap = 0,
    expand = .02,
    label.buffer = unit(0.1, "mm")
  ) +
  # Draw a segment for the x-line up to y = 90000
  geom_segment(x = 84000, y = 0.7, xend = 90000, yend = 0.7,
               linetype = "dashed", color = "darkgray") +
  # Draw a segment for the y-line starting from x = 0.7
  geom_segment(x = 90000, y = 0.7, xend = 90000, yend = 0.72,
               linetype = "dashed", color = "darkgray") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "BIC F.E.", y = "Avg. Teacher AUC",
       title = paste(nrow(selected_models), "Models Selected"))

# Join with selected_models
selected_models <- bind_rows(
  selected_models,
  top_fits_df %>%
    filter(State != "None") %>%
    filter(auc_out > 0.7, bic < 90000)
  )

## State-based
plot2 <- top_fits_df %>%
  filter(State != "None") %>%
  select(Action, Reward, State, bic, auc_out) %>%
  ggplot(aes(x = bic, y = auc_out, color = Reward, shape = State)) +
  geom_point() +
  theme(legend.position = "bottom") +
  geom_mark_ellipse(
    aes(group = Action, label = Action, color = NULL),
    con.type = "straight",
    con.cap = 0,
    expand = .02,
    label.buffer = unit(0.1, "mm")
  ) +
  # Draw a segment for the x-line up to y = 90000
  geom_segment(x = 84000, y = 0.7, xend = 90000, yend = 0.7,
               linetype = "dashed", color = "darkgray") +
  # Draw a segment for the y-line starting from x = 0.7
  geom_segment(x = 90000, y = 0.7, xend = 90000, yend = 0.72,
               linetype = "dashed", color = "darkgray") +
  theme_minimal() +
  labs(x = "BIC F.E.", y = "Avg. Teacher AUC",
       title = paste(nrow(selected_models %>% filter(State != "None")),
                     "Models Selected"))

plot1
plot2

```

#### Model Interpretability

```{r RL-me-interpretation}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}

# Use only reward_1:action_2 given the previous results
create_formula <- function(action, reward, state = NULL) {
  terms <- c()
  
  # Interaction term for reward_i * action_i
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_1"))
  terms <- c(terms, paste0(reward, "_1:", action, "_1"))
  
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(reward, "_2"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_2:", action, "_2"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique() %>%
  semi_join(
    selected_models %>%
      select(Action, Reward, State) %>%
      mutate(across(c(Action, Reward, State),
                    ~ case_when(. == "Badges" ~ "Frobenius.NNDSVD_student1",
                                . == "Struggles" ~ "Frobenius.NNDSVD_student2",
                                . == "No. Students" ~ "Frobenius.NNDSVD_student3",
                                . == "Activity" ~ "Frobenius.NNDSVD_student4",
                                . == "Assessments" ~ "Frobenius.NNDSVD_teacher1",
                                . == "Pedagogical Knowledge" ~ "Frobenius.NNDSVD_teacher2",
                                . == "Group Instruction" ~ "Frobenius.NNDSVD_teacher3",
                                . == "Curriculum Planning" ~ "Frobenius.NNDSVD_teacher4",
                                . == "None" ~ "NA",
                                .default = .))),
    by = c("act" = "Action", "rwd" = "Reward", "st" = "State"))

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    mutate(across(dplyr::starts_with(reward), ~ ./sd(., na.rm = TRUE))) %>%
    arrange(Classroom.ID, week) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(min(nrow(params2), detectCores()-1))
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/me-standardized-coef.RData")

```

Prior to fitting the reinforcement learning (RL) models, we analyzed the regression coefficients in our best-fitting logit models to identify patterns that align with RL principles. Our primary focus was on the relationship between rewards and actions. Their interaction should positively influence future actions for desired outcomes (e.g., lesson completion) and negatively for undesired outcomes (e.g., struggles). Additionally, we examined the effect of current states on strategic action selection and the interaction between states and lagged actions to indicate how actions attain and maintain desired states.

We re-estimated the models with scaled independent variables, allowing for a direct coefficient comparison. In this context, a unit increase in these variables equates to a one standard deviation increase. @tbl-re-estimation-statefree and @tbl-re-estimation summarize these results, presenting a coherent overview of the standardized coefficients and highlighting the significance of interactions between rewards, states, and lagged actions.

The results convey that no singular approach fits all educational contexts. Instead, a spectrum of pedagogical strategies exists, with specific teaching methods aligning more closely with the principles of RL. By focusing on coefficients that display RL-like effects, particularly the interaction term R(t-1) x A(t-2), we identified dominant strategies that include 1) Action: Group Instruction with Reward: Struggles, and 2) Action: Group Instruction with Reward: Activity and State: Badges.

In contrast, @fig-RL-exploration favors models that occupy the upper-left quadrant, indicative of an optimized balance between model complexity and predictive accuracy. Noteworthy configurations include 1) Action: Pedagogical Knowledge with Reward: Activity, and 2) Action: Pedagogical Knowledge with Reward: Activity, State: Number of Students.

These configurations highlight teacher individual differences and support the idea that using a hybrid modeling approach could improve the process of fitting RL.

```{=tex}
\newpage
\KOMAoptions{paper=landscape,pagesize}
\recalctypearea
{\areaset[current]{\dimexpr\textwidth\relax}{\textheight}
\setlength{\marginparwidth}{0pt}
\scriptsize
```
```{r}
#| label: tbl-re-estimation-statefree
#| tbl-cap: "State-Free Mixed Effects Logistic Regression Results"

load("Regressions/me-standardized-coef.RData")

filtered_results <- lapply(results, function(x) {
  if (!is.null(x$State)) return(NULL)
  temp <- data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = "None"
  ) %>%
  mutate(across(c(Action, Reward),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Badges",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggles",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "Activity",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Pedagogical Knowledge",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Group Instruction",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Curriculum Planning",
                            .default = .)))
  if(nrow(
    temp %>%
    semi_join(selected_models, by = c("Action", "Reward", "State"))
    ) == 0) return(NULL)
  if(x$AUC < 0.5) return(NULL)
  x$recoef <- do.call(cbind, x$recoef[-c(1, length(x$recoef))]) %>%
    as.data.frame() %>%
    filter(if_all(everything(),
                  ~ . >= find_hdr(.)[1] & . <= find_hdr(.)[2])) %>%
    rownames_to_column(var = "Teacher.User.ID")
  
  return(list(
    Action = temp$Action,
    Reward = temp$Reward,
    AUCmodel = x$AUC,
    BICmodel = x$bic,
    coef = data.frame(
      Teacher.User.ID = as.integer(x$recoef[,1]),
      lag1 = x$recoef[,3],
      lag2 = x$recoef[,7],
      rwd1 = x$recoef[,2],
      rwd2 = x$recoef[,6],
      rwd1_lag1 = x$recoef[,4],
      rwd1_lag2 = x$recoef[,5],
      rwd2_lag2 = x$recoef[,8]
    ) %>%
      inner_join(x$perform_df %>%
                   select(Teacher.User.ID, auc_out, logLik_ind) %>%
                   filter(if_all(everything(), ~ . >= find_hdr(.)[1] &
                                                 . <= find_hdr(.)[2])) %>%
                   filter(0.5 < auc_out & auc_out < 1),
                 by = "Teacher.User.ID")
  ))
})
filtered_results <- filtered_results[!sapply(filtered_results, is.null)]

model_summary <- do.call(rbind, lapply(filtered_results, function(x) {
  summary <- data.frame(
    Action = x$Action,
    Reward = x$Reward,
    AUC = x$AUCmodel,
    BIC = x$BICmodel
  )
  summary$lag1 = list(x$coef$lag1)
  summary$lag2 = list(x$coef$lag2)
  summary$rwd1_lag1 = list(x$coef$rwd1_lag1)
  summary$rwd1_lag2 = list(x$coef$rwd1_lag2)
  summary$rwd2_lag2 = list(x$coef$rwd2_lag2)
  summary$auc_ind = list(x$coef$auc_out)
  summary$logLik_ind = list(x$coef$logLik_ind)
  
  summary <- summary %>%
    # # Standardize the coefficients by dividing by the standard deviation
    # mutate(across(c(rwd1_lag1, rwd1_lag2, rwd2_lag2, st, st_lag1),
    #               ~ lapply(., function(x) x / sd(x, na.rm = T)))) %>%
    # # Fill in x$State == "None" with 0s
    # mutate(across(c(st, st_lag1), ~ if_else(is.na(.), list(0), .))) %>%
    mutate(n_teachers = lapply(summary$lag1, length)) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2, 
                    rwd2_lag2,
                    auc_ind, logLik_ind),
                  ~ lapply(., mean, na.rm = T),
                  .names = "{.col}.mean")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., sd, na.rm = T),
                  .names = "{.col}.sd")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., quantile, 0.25, names = F),
                  .names = "{.col}.q1")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., median, na.rm = T),
                  .names = "{.col}.median")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., quantile, 0.75, names = F),
                  .names = "{.col}.q3")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., function(x) mean(x > 0, na.rm = T)),
                  .names = "{.col}.pos")) %>%
    select(-c(lag1, lag2,
              rwd1_lag1,
              rwd1_lag2,
              rwd2_lag2,
              auc_ind, logLik_ind, auc_ind.mean, logLik_ind.mean))
  
  return(summary)
}))

# Transform the model_summary dataframe to a long format for plotting
model_summary_long <- model_summary %>%
  mutate(across(!c(Action, Reward), as.numeric)) %>%
  mutate(across(dplyr::ends_with(".pos") & dplyr::starts_with("rwd"),
                ~ case_when(Reward == "Struggle" ~ 1 - ., .default = .))) %>%
  select(!c(AUC, BIC)) %>%
  pivot_longer(cols = -c(Action, Reward), 
               names_to = c("variable", "statistic"), values_to = "value",
               names_sep = "\\.") %>%
  arrange(Action, Reward) %>%
  pivot_wider(names_from = c(Action, Reward), values_from = c(value))
  
max <- model_summary_long %>%
  filter(statistic == "pos" & !variable %in% c("lag1","lag2")) %>%
  rowwise() %>%
  # Remove columns that do not have the max of at least 1 row
  mutate(max_val = max(c_across(is.numeric), na.rm = T)) %>%
  ungroup() %>%
  mutate(across(is.numeric, ~ if_else(. == max_val, ., NA))) %>%
  select(where(~ !all(is.na(.)))) %>%
  select(-c("max_val","statistic")) %>%
  mutate(across(is.numeric, ~ !is.na(.))) %>%
  pivot_longer(cols = -variable, names_to = "model", values_to = "keep") %>%
  filter(keep) %>% select(-keep)

model_summary_long %>%
  filter(!variable %in% c("lag1", "lag2")) %>%
  mutate(statistic = factor(statistic,
                            levels = c("mean", "sd", "q1", "median", "q3", "pos"),
                            labels = c("Mean", "SD", "Q1", "Median", "Q3", "RL-like")),
         variable = factor(variable,
                           levels = c(
                             # "lag1", "lag2",
                             "rwd1_lag1",
                             "rwd1_lag2",
                             "rwd2_lag2",
                             "n_teachers"),
                           labels = c(
                             # "Action(t-1)",
                             # "Action(t-2)",
                             "R(t-1) x \n A(t-1)",
                             "R(t-1) x \n A(t-2)",
                             "R(t-2) x \n A(t-2)",
                             "N"
                             ))) %>%
  select(order(names(model_summary_long))) %>%
  group_by(variable) %>%
  gt(rowname_col = "statistic", row_group_as_column = T) %>%
  row_group_order(groups = c(
    # "Action(t-1)",
    # "Action(t-2)",
    "R(t-1) x \n A(t-1)",
    "R(t-1) x \n A(t-2)",
    "R(t-2) x \n A(t-2)",
    "N")) %>%
  fmt_markdown(columns = "variable") %>%
  fmt_number(decimals = 2) %>%
  fmt_integer(rows = variable == "No. Teachers") %>%
  fmt_percent(rows = statistic == "RL-like") %>%
  # cols_hide(starts_with("Assessments_No. Students")) %>%
  sub_missing() %>%
  tab_spanner_delim("_") %>%
  tab_header(
    title = "Summary of Model Coefficients"
  ) %>%
  tab_options(
    table.font.size = "small"
  )
  # as_latex()

```

{{< pagebreak >}}

```{r}
#| label: tbl-re-estimation
#| tbl-cap: "State-Based Mixed Effects Logistic Regression Results"

load("Regressions/me-standardized-coef.RData")

filtered_results <- lapply(results, function(x) {
  if (is.null(x$State)) return(NULL)
  temp <- data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = x$State
  ) %>%
  mutate(across(c(Action, Reward, State),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Badges",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggles",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "Activity",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Pedagogical Knowledge",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Group Instruction",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Curriculum Planning",
                            .default = .)))
  if(nrow(
    temp %>%
    semi_join(selected_models, by = c("Action", "Reward", "State"))
    ) == 0) return(NULL)
  if(x$AUC < 0.5) return(NULL)
  x$recoef <- do.call(cbind, x$recoef[-c(1, length(x$recoef))]) %>%
    as.data.frame() %>%
    filter(if_all(everything(),
                  ~ . >= find_hdr(.)[1] & . <= find_hdr(.)[2])) %>%
    rownames_to_column(var = "Teacher.User.ID")
  
  return(list(
    Action = temp$Action,
    Reward = temp$Reward,
    State = temp$State,
    AUCmodel = x$AUC,
    BICmodel = x$bic,
    coef = data.frame(
      Teacher.User.ID = as.integer(x$recoef[,1]),
      lag1 = x$recoef[,3],
      lag2 = x$recoef[,7],
      rwd1 = x$recoef[,2],
      rwd2 = x$recoef[,6],
      rwd1_lag1 = x$recoef[,4],
      rwd1_lag2 = x$recoef[,5],
      rwd2_lag2 = x$recoef[,8],
      st = x$recoef[,9],
      st_lag1 = x$recoef[,10]
    ) %>%
      inner_join(x$perform_df %>%
                   select(Teacher.User.ID, auc_out, logLik_ind) %>%
                   filter(auc_out >= 0.5 & auc_out < 1),
                 by = "Teacher.User.ID")
  ))
  })
filtered_results <- filtered_results[!sapply(filtered_results, is.null)]

model_summary <- do.call(rbind, lapply(filtered_results, function(x) {
  summary <- data.frame(
    Action = x$Action,
    Reward = x$Reward,
    State = x$State,
    AUC = x$AUCmodel,
    BIC = x$BICmodel
  )
  summary$lag1 = list(x$coef$lag1)
  summary$lag2 = list(x$coef$lag2)
  summary$rwd1_lag1 = list(x$coef$rwd1_lag1)
  summary$rwd1_lag2 = list(x$coef$rwd1_lag2)
  summary$rwd2_lag2 = list(x$coef$rwd2_lag2)
  summary$st = ifelse(x$State == "None", list(0), list(x$coef$st))
  summary$st_lag1 = ifelse(x$State == "None", list(0), list(x$coef$st_lag1))
  summary$auc_ind = list(x$coef$auc_out)
  summary$logLik_ind = list(x$coef$logLik_ind)
  
  summary <- summary %>%
    # # Standardize the coefficients by dividing by the standard deviation
    # mutate(across(c(rwd1_lag1, rwd1_lag2, rwd2_lag2, st, st_lag1),
    #               ~ lapply(., function(x) x / sd(x, na.rm = T)))) %>%
    # # Fill in x$State == "None" with 0s
    # mutate(across(c(st, st_lag1), ~ if_else(is.na(.), list(0), .))) %>%
    mutate(n_teachers = lapply(summary$lag1, length)) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1,
                    auc_ind, logLik_ind),
                  ~ lapply(., mean, na.rm = T),
                  .names = "{.col}.mean")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., sd, na.rm = T),
                  .names = "{.col}.sd")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., quantile, 0.25, names = F),
                  .names = "{.col}.q1")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., median, na.rm = T),
                  .names = "{.col}.median")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., quantile, 0.75, names = F),
                  .names = "{.col}.q3")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., function(x) mean(x > 0, na.rm = T)),
                  .names = "{.col}.pos")) %>%
    select(-c(lag1, lag2,
              rwd1_lag1,
              rwd1_lag2,
              rwd2_lag2,
              st, st_lag1,
              auc_ind, logLik_ind, auc_ind.mean, logLik_ind.mean))
  
  return(summary)
}))

# Transform the model_summary dataframe to a long format for plotting
model_summary_long <- model_summary %>%
  mutate(across(!c(Action, Reward, State), as.numeric)) %>%
  mutate(across(dplyr::ends_with(".pos") & dplyr::starts_with("st"),
                ~ case_when(State == "Struggle" ~ 1 - ., .default = .)),
         across(dplyr::ends_with(".pos") & dplyr::starts_with("rwd"),
                ~ case_when(Reward == "Struggle" ~ 1 - ., .default = .))) %>%
  select(!c(AUC, BIC)) %>%
  mutate(across(dplyr::starts_with("st", ignore.case = F),
                ~ if_else(State == "None", NA, .))) %>%
  pivot_longer(cols = -c(Action, Reward, State), 
               names_to = c("variable", "statistic"), values_to = "value",
               names_sep = "\\.") %>%
  arrange(Action, Reward, State) %>%
  pivot_wider(names_from = c(Action, Reward, State), values_from = c(value))
  
max <- model_summary_long %>%
  filter(statistic == "pos" & !variable %in% c("lag1","lag2")) %>%
  rowwise() %>%
  # Remove columns that do not have the max of at least 1 row
  mutate(max_val = max(c_across(is.numeric), na.rm = T)) %>%
  ungroup() %>%
  mutate(across(is.numeric, ~ if_else(. == max_val, ., NA))) %>%
  select(where(~ !all(is.na(.)))) %>%
  select(-c("max_val","statistic")) %>%
  mutate(across(is.numeric, ~ !is.na(.))) %>%
  pivot_longer(cols = -variable, names_to = "model", values_to = "keep") %>%
  filter(keep) %>% select(-keep)

model_summary_long %>%
  filter(!variable %in% c("lag1", "lag2")) %>%
  mutate(statistic = factor(statistic,
                            levels = c("mean", "sd", "q1", "median", "q3", "pos"),
                            labels = c("Mean", "SD", "Q1", "Median", "Q3", "RL-like")),
         variable = factor(variable,
                           levels = c(
                             # "lag1", "lag2",
                             "rwd1_lag1",
                             "rwd1_lag2",
                             "rwd2_lag2",
                             "st", "st_lag1",
                             "n_teachers"),
                           labels = c(
                             # "Action(t-1)",
                             # "Action(t-2)",
                             "R(t-1) x \n A(t-1)",
                             "R(t-1) x \n A(t-2)",
                             "R(t-2) x \n A(t-2)",
                             "S(t)",
                             "S(t) x \n A(t-1)",
                             "N"
                             ))) %>%
  select(order(names(model_summary_long))) %>%
  group_by(variable) %>%
  gt(rowname_col = "statistic", row_group_as_column = T) %>%
  row_group_order(groups = c(
    # "Action(t-1)",
    # "Action(t-2)",
    "R(t-1) x \n A(t-1)",
    "R(t-1) x \n A(t-2)",
    "R(t-2) x \n A(t-2)",
    "S(t)",
    "S(t) x \n A(t-1)",
    "N")) %>%
  fmt_markdown(columns = "variable") %>%
  fmt_number(decimals = 2) %>%
  fmt_integer(rows = variable == "No. Teachers") %>%
  fmt_percent(rows = statistic == "RL-like") %>%
  # cols_hide(starts_with("Assessments_No. Students")) %>%
  sub_missing() %>%
  tab_spanner_delim("_") %>%
  tab_header(
    title = "Summary of Model Coefficients"
  ) %>%
  tab_options(
    table.font.size = "small"
  )
  # as_latex()

```
```{=tex}
}
\newpage
\KOMAoptions{paper=portrait,pagesize}
\recalctypearea
```

## Estimating RL Models

This section aims to identify and evaluate reinforcement learning (RL) models that best represent the teaching strategies derived from our data. The performance of each model is quantified by computing the log evidence, a metric that balances model fit against complexity, similar to the Bayesian Information Criterion. This metric is critical as it provides a common ground for comparing models with varying degrees of freedom, ensuring that our selection process favors models that capture the essential patterns without overfitting. Our first step will be to choose the most suitable model based on this performance score. Then, we will carefully analyze it for any discernible patterns that may help us understand the teaching strategies present in our dataset.

#### Distribution of Model Performance

After estimating Q-learning and Actor-Critic Models for all potential action-reward-state configurations, specifically focusing on the actions "Pedagogical Knowledge" and "Group Instruction," we plotted their log evidence scores in @fig-log-evidence-histogram. The resulting histogram illustrates the variability in model fit for both Q-learning and Actor-Critic RL models. The diversity in log evidence scores highlights the distinct behaviors of each model. At the same time, the bimodal distribution suggests that the two action variables can yield significantly different model fits, as we saw in our logistic regression models.

```{r}
#| label: fig-log-evidence-histogram
#| fig-cap: "Histogram of Log Evidences for Q-learning and Actor-Critic Models. The graph illustrates the distribution of models based on their log evidence (i.e., penalized likelihood) across Actor-Critic and Q-learning models."

library(R.matlab)

# Initialize vectors to hold log-evidence values for each model type
ql_log_evidences <- c()
ac_log_evidences <- c()

# Path to the folders containing the .mat files for Q-learning and Actor-Critic models
ql_folder_path <- "CBM/zearn_results/ql_subj_results"
ac_folder_path <- "CBM/zearn_results/ac_subj_results"

# Function to read and accumulate log-evidences from given folder and model type
accumulate_log_evidences <- function(folder_path) {
  log_evidences <- c()
  files <- list.files(path = folder_path, pattern = "\\.mat$", full.names = TRUE)
  
  for (file in files) {
    mat_data <- readMat(file)
    log_evidences <- c(log_evidences, sum(mat_data[["cbm"]][[5]][[2]]))
  }
  
  return(log_evidences)
}

# Accumulate log-evidences for Q-learning and Actor-Critic models
ql_log_evidences <- accumulate_log_evidences(ql_folder_path)
ac_log_evidences <- accumulate_log_evidences(ac_folder_path)

# Create a combined vector of all log-evidences and a factor indicating model type
all_log_evidences <- c(ql_log_evidences, ac_log_evidences)
model_types <- factor(c(rep("QL", length(ql_log_evidences)), rep("AC", length(ac_log_evidences))))

# Create a data frame for ggplot
data <- data.frame(
  LogEvidence = c(ql_log_evidences, ac_log_evidences),
  ModelType = factor(c(rep("Q-learning", length(ql_log_evidences)), rep("Actor-Critic", length(ac_log_evidences))))
)

# Plot using ggplot
ggplot(data, aes(x = LogEvidence, fill = ModelType)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 20) +
  scale_fill_manual(values = c("Q-learning" = brewer.pal(3, "Dark2")[1],
                               "Actor-Critic" = brewer.pal(3, "Dark2")[2])) +
  labs(x = "Log Evidence",
       y = "Frequency") +
  theme_minimal() +
  theme(legend.title = element_blank())



```

#### Standout Models

Our computational analysis identified a distinct group of models that excelled in their high log evidence scores. Interestingly, all top-performing models featured "Pedagogical Knowledge" as their primary action. @tbl-top-CBM provides a breakdown of scores for different combinations of rewards and states under this action, offering insight into the comparative performance of various model setups. Notably, models with "Badges" and "Activity" as their rewards outperform others, as do state-free models, likely due to their higher simplicity.

```{r}
#| label: tbl-top-CBM
#| tbl-cap: "Best-fit models. The table lists the models with the highest log evidence scores by their rewards, states. All models are based on the action 'Pedagogical Knowledge.'"

ql_folder_path <- "CBM/zearn_results/ql_refine"
ac_folder_path <- "CBM/zearn_results/ac_refine"

# Create a data frame for ggplot
evidence_df <- data.frame(
  Action = as.character(NA),
  Reward = as.character(NA),
  State = as.character(NA),
  LogEvidence = as.numeric(NA)
)
files <- list.files(path = ql_folder_path,
                    pattern = "refine_ql", full.names = TRUE)
for (file in files) {
  mat_data <- readMat(file)
  evidence_df <- rbind(evidence_df, data.frame(
    Action = file,
    Reward = as.character(NA),
    State = as.character(NA),
    LogEvidence = sum(mat_data[["cbm"]][[5]][[2]])
  ))
}
files <- list.files(path = ac_folder_path,
                    pattern = "refine_ac", full.names = TRUE)
for (file in files) {
  mat_data <- readMat(file)
  evidence_df <- rbind(evidence_df, data.frame(
    Action = file,
    Reward = as.character(NA),
    State = as.character(NA),
    LogEvidence = sum(mat_data[["cbm"]][[5]][[2]])
  ))
}

evidence_df <- evidence_df[-1,]
evidence_df[1,1:3] <- c("Pedagogical Knowledge","Badges", NA) # QL 1
evidence_df[2,1:3] <- c("Pedagogical Knowledge","Struggles", NA) # QL 3
evidence_df[3,1:3] <- c("Pedagogical Knowledge","No. Students", NA) # QL 5
evidence_df[4,1:3] <- c("Pedagogical Knowledge","Activity", NA) # QL 7
evidence_df[5,1:3] <- c("Pedagogical Knowledge","Badges",
                        "Struggles, No. Students, Activity") # AC 13
evidence_df[6,1:3] <- c("Pedagogical Knowledge","Struggles",
                        "Activity") # AC 17
evidence_df[7,1:3] <- c("Pedagogical Knowledge","Badges",
                        "No. Students") # AC 2
evidence_df[8,1:3] <- c("Pedagogical Knowledge","Struggles",
                        "No. Students, Activity") # AC 23
evidence_df[9,1:3] <- c("Pedagogical Knowledge","Struggles",
                        "Badges, No. Students, Activity") # AC 27
evidence_df[10,1:3] <- c("Pedagogical Knowledge","No. Students",
                         "Badges, Struggles") # AC 35
evidence_df[11,1:3] <- c("Pedagogical Knowledge","Activity",
                         "Badges") # AC 43
evidence_df[12,1:3] <- c("Pedagogical Knowledge","Activity",
                         "Struggles No. Students") # AC 51
evidence_df[13,1:3] <- c("Pedagogical Knowledge","Activity",
                         "Badges, Struggles, No. Students") # AC 55
evidence_df[14,1:3] <- c("Pedagogical Knowledge","Badges",
                         "Struggles, No. Students") # AC 7

# Display the table
evidence_df %>%
  select(-Action) %>%
  group_by(Reward) %>%
  arrange(desc(LogEvidence)) %>%
  gt(row_group_as_column = TRUE) %>%
  tab_stubhead(label = "Reward") %>%
  cols_label(`LogEvidence` = "Log Evidence") %>%
  sub_missing() %>%
  fmt_number(columns = c(LogEvidence), decimals = 1)

```

#### Hierarchical Bayesian Inference (HBI) Results and Model Comparison

To further refine our selection, we employ Hierarchical Bayesian Inference (HBI) to perform a more detailed comparison of the selected models. This statistical approach allows us to compare models not just by their individual fits, but also by their ability to explain data across different subjects. In interpreting the results from HBI, we observed the differential performances of Q-learning (QL) and Actor-Critic (AC) models. @tbl-CBM-HBI provides quantifiable insights into how frequently each model emerged as the best fit across iterations (Model Frequency) and the sum log likelihood across subjects (Log Likelihood). Notably, we see that in both model types, models that used Badges and Activity as their rewards overperformed. The HBI results and corresponding model comparison statistics serve as our basis for selecting the most appropriate RL model.

```{r}
#| label: tbl-CBM-HBI
#| tbl-cap: "Top Q-learning and Actor-Critic models. The tables quantify performance under hierarchical bayesian inference through model frequencies and the sum of log likelihoods. All models are based on the action 'Pedagogical Knowledge.'"
#| tbl-subcap: 
#| - "Model frequencies"
#| - "Sum log likelihoods"

# Load the MATLAB files for the top QL and AC models
top_model_ql <- readMat('CBM/zearn_results/ql_refine/hbi_QL4_refined.mat')
top_model_ac <- readMat('CBM/zearn_results/ac_refine/hbi_AC5_refined.mat')

# Extract the relevant data from each model
model_freq_ql <- top_model_ql[["cbm"]][[5]][[5]]
loglik_ql <- apply(top_model_ql[["cbm"]][[4]][[1]][[1]], 1, sum)

model_freq_ac <- top_model_ac[["cbm"]][[5]][[5]]
loglik_ac <- apply(top_model_ac[["cbm"]][[4]][[1]][[1]], 1, sum)


# Create a data frame with each model
top_ql_models_table <- 
  data.frame(Statistic = c("Model Frequency", "Log Likelihood"),
             QL = matrix(c(model_freq_ql, t(loglik_ql)),
                         nrow = 2, ncol = 4, byrow = T))
colnames(top_ql_models_table)[2:5] <- c(
  "Pedagogical Knowledge_Badges", # QL 1
  "Pedagogical Knowledge_Struggles", # QL 3
  "Pedagogical Knowledge_No. Students", # QL 5
  "Pedagogical Knowledge_Activity" # QL 7
)
top_ac_models_table <- 
  data.frame(Statistic = c("Model Frequency", "Log Likelihood"),
             AC = matrix(c(model_freq_ac, t(loglik_ac)),
                         nrow = 2, ncol = 5, byrow = T))
colnames(top_ac_models_table)[2:6] <- c(
  "Pedagogical Knowledge_Badges_No. Students", # AC 2
  "Pedagogical Knowledge_Badges_Struggles, No. Students, Activity", # AC 13
  "Pedagogical Knowledge_No. Students_Badges, Struggles", # AC 35
  "Pedagogical Knowledge_Activity_Badges", # AC 43
  "Pedagogical Knowledge_Activity_Struggles, No. Students" # AC 51
)

# Display the table
top_ql_models_table %>%
  group_by(Statistic) %>%
  gt(row_group_as_column = TRUE) %>%
  tab_spanner_delim("_") %>%
  fmt_percent(rows = c(1), decimals = 1) %>%
  fmt_number(rows = c(2), decimals = 1)

top_ac_models_table %>%
  group_by(Statistic) %>%
  gt(row_group_as_column = TRUE) %>%
  tab_spanner_delim("_") %>%
  fmt_percent(rows = c(1), decimals = 1) %>%
  fmt_number(rows = c(2), decimals = 1)

```

#### Hybrid Models Comparison

In pursuit of a more comprehensive understanding of our model space, we turn to the investigation of hybrid models that blend elements from both the Logit and RL frameworks. These hybrids offer a potentially more nuanced approach to representing the complexities of learning behavior, aiming to combine the strengths of logistic regression with the dynamic adaptability inherent in reinforcement learning techniques.

The comparative analysis of these models is conducted via Hierarchical Bayesian Inference. @tbl-CBM-second shows the performance for different reward and state contexts across the logit model, the corresponding reinforcement learning model (Q-learning model in state-free cases and Actor-Critic in state-based cases), and two hybrid configurations (i.e., Logit and RL hybrids, Q-learning and Actor-Critic hybrids). Our evaluation suggests that any potential performance gain from hybridizing do not outweigh the cost of increased model complexity.

```{r}
#| label: tbl-CBM-second
#| tbl-cap: "Comparative fit of hybrid models. The table presents the proportion of data that is best explained by each model configuration, including hybrid models. All models are based on the action 'Pedagogical Knowledge.'"

# Load the MATLAB files for the comparative models and the top model comparison
comp_model_7 <- readMat('CBM/zearn_results/comp_aggr_results/hbi_compare_7.mat')
comp_model_1 <- readMat('CBM/zearn_results/comp_aggr_results/hbi_compare_1.mat')
comp_model_2 <- readMat('CBM/zearn_results/comp_aggr_results/hbi_compare_2.mat')
comp_model_51 <- readMat('CBM/zearn_results/comp_aggr_results/hbi_compare_51.mat')

# Extract model frequency data
model_freq <- data.frame(rbind(
  comp_model_7[["cbm"]][[5]][[5]],
  comp_model_1[["cbm"]][[5]][[5]],
  comp_model_2[["cbm"]][[5]][[5]],
  comp_model_51[["cbm"]][[5]][[5]]
  ))
names(model_freq) <- c("Logit", "RL", "Logit-RL Hybrid", "QL-AC Hybrid")
model_freq$Reward <- c(
  "Activity", # QL 7
  "Badges", # QL 1
  "Badges", # AC 2
  "Activity" # AC 51
)
model_freq$State <- c(
  NA, # QL 7
  NA, # QL 1
  "No. Students", # AC 2
  "Struggles, No. Students" # AC 51
)

model_freq %>%
  gt(groupname_col = "Reward",
     row_group_as_column = T) %>%
  tab_stubhead(label = "Reward") %>%
  cols_move_to_start(State) %>%
  sub_missing() %>%
  fmt_percent(decimals = 1) %>%
  tab_footnote(
    footnote = "Q-learning model in state-free case and Actor-Critic in state-based case",
    locations = cells_body(columns = RL)
  ) %>%
  tab_footnote(
    footnote = "Combines predictions of Logit and RL models",
    locations = cells_body(columns = `Logit-RL Hybrid`)
  ) %>%
  tab_footnote(
    footnote = "Combines predictions of Q-learning (state-free) and Actor-Critic (state-based) models",
    locations = cells_body(columns = `QL-AC Hybrid`)
  )

```

#### Top Model Selection

In the final stages of our model selection process, we converge on the identification of the top-performing models. By assessing the frequency with which each model provides the best explanation for the observed data, we make an informed decisions about their suitability for capturing teacher behavioral patterns. @tbl-CBM-final distills these results by juxtaposing the models based on the rewards 'Activity' and 'Badges.' The table reveals that the 'Badges' consistently outperforms 'Activity' across logit and actor-critic models, indicating its superior explanatory power in the context of pedagogical knowledge.

```{r}
#| label: tbl-CBM-final
#| tbl-cap: "Top Model Comparison. The table compares the leading models on their ability to explain the data, as indicated by the model frequencies. All models are based on the action 'Pedagogical Knowledge.'"

top_model_comp <- readMat('CBM/zearn_results/comp_aggr_results/top_model_comp.mat')
# Extract model frequency data
model_freq <- data.frame(
  top_model_comp[["cbm"]][[5]][[5]]
  )
names(model_freq) <- c(
  "Pedagogical Knowledge_Activity_None", # QL 7
  "Pedagogical Knowledge_Badges_None", # QL 1
  "Pedagogical Knowledge_Badges_No. Students", # AC 2
  "Pedagogical Knowledge_Activity_Struggles, No. Students" # AC 51
)
# model_freq <- rbind(
#   c("Logit", "Logit", "Actor-Critic", "Actor-Critic"),
#   model_freq)
# rownames(model_freq) <- c("Model Type","Model Frequency")
rownames(model_freq) <- c("Model Frequency")

model_freq %>%
  relocate(`Pedagogical Knowledge_Activity_Struggles, No. Students`,
           .after = `Pedagogical Knowledge_Activity_None`) %>%
  gt(rownames_to_stub = T) %>%
  tab_spanner_delim("_") %>%
  fmt_percent(decimals = 1)
  
```

## Heterogeneity Analyses

```{r}
#| label: fig-loglik-histogram
#| fig-cap: "Histogram of teacher-specific log likelihood"
 
# Load the full model data
full_model <- readMat("CBM/zearn_results/top_results/hbi_ac.mat")

# Extract log likelihoods
log_likelihoods <- as.numeric(full_model[["cbm"]][[4]][[1]][[1]])
median_log_likelihood <- round(median(log_likelihoods), digits = 1)

# Create a histogram of log likelihoods
ggplot(data = data.frame(LogLikelihoods = log_likelihoods),
       aes(x = LogLikelihoods)) +
  geom_histogram(fill = brewer.pal(3, "Set2")[1],
                 color = brewer.pal(3, "Dark2")[1]) + 
  geom_vline(xintercept = median_log_likelihood, color = brewer.pal(8, "Dark2")[8],
             linetype = "dashed", size = 0.7) +
  annotate("text", x = median_log_likelihood, y = 0,
           label = median_log_likelihood,
           color = "black", vjust = 1.5) +
  theme_minimal() +
  labs(x = "Log Likelihood",
       y = "Frequency")

```

```{r}
#| label: fig-params-histogram
#| fig-cap: "Histogram of Teacher-Specific Parameters"
#| fig-subcap:
#| - "Value weights learning rate"
#| - "Policy weights learning rate"
#| - "Discount factor"
#| - "Temperature"
#| - "Initial policy"
#| - "Initial value"
#| - "Cost"

group_mean <- full_model[["cbm"]][[5]][[3]][[1]][[1]]
error_bar <- full_model[["cbm"]][[5]][[4]][[1]][[1]]
teacher_specific <- full_model[["cbm"]][[5]][[1]][[1]][[1]]

para_labels <- c("alpha_w", "alpha_theta","gamma", "tau",
                 "theta_init", "w_init", "cost")

transformations <- list(
  function(x) 1 / (1 + exp(-x)), # sigmoid
  function(x) 1 / (1 + exp(-x)), # sigmoid
  function(x) 1 / (1 + exp(-x)), # sigmoid
  function(x) exp(x),            # exp
  function(x) x,                 # none
  function(x) x,                 # none
  function(x) exp(x)             # exp
)

# Apply transformations
group_mean_transformed <- mapply(function(x, transform) transform(x),
                                 group_mean, transformations)
# Error bar transformation - calculate lower and upper bounds after transformation
error_bar_lower <- sapply(1:length(group_mean),
  function(i) transformations[[i]](group_mean[i] - error_bar[i]))
error_bar_upper <- sapply(1:length(group_mean),
  function(i) transformations[[i]](group_mean[i] + error_bar[i]))

# Ensuring correct transformation of teacher-specific parameters
# The original attempt seemed to be incorrect; redoing transformation properly
teacher_specific_transformed <- t(apply(teacher_specific, 1, function(row) {
  mapply(function(value, transform) transform(value), row, transformations)
}))

# Correcting data frame for group means and error bars preparation
group_mean_df <- data.frame(
  Parameter = para_labels,
  Mean = group_mean_transformed,
  Lower = error_bar_lower,
  Upper = error_bar_upper
)

# Preparing long-format data for plotting
parameters_long <- data.frame(
  Parameter = rep(para_labels,
                  times = nrow(teacher_specific_transformed)),
  Value = as.vector(t(teacher_specific_transformed)),
  Teacher = rep(1:nrow(teacher_specific_transformed),
                each = length(para_labels))
)

individual_plots <- list()
# Plotting with improved readability
for(param in unique(parameters_long$Parameter)) {
  # Filter data for the current parameter
  data_current <- parameters_long[parameters_long$Parameter == param, ]
  group_mean_current <- group_mean_df[group_mean_df$Parameter == param, ]
  
  # Define the expression for the parameter name
  parameter_expression <- switch(param,
                                "alpha_w" = expression(alpha[w]),
                                "alpha_theta" = expression(alpha[theta]),
                                "gamma" = expression(gamma),
                                "tau" = expression(tau),
                                "theta_init" = expression(theta[init]),
                                "w_init" = expression(w[init]),
                                "cost" = expression(cost))
  
  # Generate the plot for the current parameter
  p <- ggplot(data_current) +
    geom_violin(
      aes(x = Parameter, y = Value),
      adjust = 2,
      fill = brewer.pal(3, "Set2")[1],
      color = brewer.pal(3, "Set2")[3],
      alpha = 0.5
    ) +
    geom_point(
      data = group_mean_current,
      aes(x = Parameter, y = Mean),
      color = brewer.pal(3, "Set2")[2], size = 4
    ) +
    geom_errorbar(
      data = group_mean_current,
      aes(x = Parameter, ymin = Lower, ymax = Upper),
      color = brewer.pal(3, "Set2")[2], width = 0.05, size = 1
    ) +
    theme_minimal() +
    theme(axis.text.x = element_blank()) +
    labs(y = "Parameter Value", x = parameter_expression)
  
  # Add the plot to the list
  individual_plots[[param]] <- p
}

individual_plots[[1]]
individual_plots[[2]]
individual_plots[[3]]
individual_plots[[4]]
individual_plots[[5]]
individual_plots[[6]]
individual_plots[[7]]

```

```{r}
#| label: tbl-CBM-teachers
#| tbl-cap: "Teacher Heterogeneity"

# Classify each teacher according to the best-fit model
teacher_classification <- apply(top_model_comp[["cbm"]][[5]][[2]],
                                1, which.max)
teacher_params <- top_model_comp[["cbm"]][[5]][[1]][[3]][[1]]
# Make min(teacher_classification) the baseline
load("CBM/data/classrooms.RData")
hetereogeneity <- classrooms %>%
  cbind(
    data.frame(
      actorcritic = teacher_classification - min(teacher_classification)
      )) %>%
  cbind(teacher_params)
names(hetereogeneity)[3:9] <- c(
  "alpha_w", "alpha_theta", "gamma", "tau", "theta_init", "w_init", "cost"
)
hetereogeneity <- hetereogeneity %>%
  mutate(across(c(alpha_w, alpha_theta, gamma), ~ 1/(1+exp(-.)))) %>%
  mutate(across(c(tau, cost), ~ exp(.))) %>%
  left_join(
    df %>%
      group_by(Classroom.ID, Teacher.User.ID) %>%
      summarize(
        across(c(charter.school, school.account),
               \(x) mean(x, na.rm = TRUE)),
        across(c(Frobenius.NNDSVD_teacher2,
                 Frobenius.NNDSVD_student1, Frobenius.NNDSVD_student3,
                 `Active.Users...Total`, Badges.per.Active.User,
                 Tower.Alerts.per.Tower.Completion),
               \(x) sum(x, na.rm = TRUE)),
        across(c(income, poverty), ~ unique(.)),
        n_weeks = n()
        ), by = "Classroom.ID"
    ) %>%
  mutate(across(c(income, poverty), as.ordered))

# T-tests for differences in means
hetereogeneity %>%
  select(actorcritic, charter.school, school.account, poverty, 
         n_weeks,
         `Active.Users...Total`, `Badges.per.Active.User`,
         `Tower.Alerts.per.Tower.Completion`,
         Frobenius.NNDSVD_teacher2,
         Frobenius.NNDSVD_student1, Frobenius.NNDSVD_student3) %>%
  tbl_summary(
    by = actorcritic,  # Grouping variable
    statistic = list(all_continuous() ~ "{mean} ({sd})",
                     all_categorical() ~ "{p}%"),
    missing = "no"
  ) %>%
  add_n() %>%
  add_difference() %>%
  add_q()

```

## Optimality

To understand the factors contributing to a teacher's ability to maximize lesson completion, we conducted an in-depth analysis of teachers' performance across various parameters from the previous hierarchical model. We focused specifically on the learning rate ("Alpha") and the inverse temperature ("Tau"). We examined their correlation with the average weekly badges earned per teacher, a proxy for lesson completion, and Tower Alerts, a measure of student struggle with the materials.

@tbl-optimality presents the coefficients of six different linear regression models. Each model predicts the average weekly badges earned per teacher (Models 1-3) or the average weekly Tower Alerts (Models 4-6) based on different combinations of the parameters and control variables. The control variables include the number of active students, the number of classes taught by the teacher, the grade level, the number of weeks, the poverty level, the income level, whether the school is a charter school, and whether the school has a paid account.

Alpha and Tau achieved statistical significance when adding all the control variables (columns 3 and 6, respectively). These results suggest that a higher learning rate may lead to fewer badges earned and more Tower Alerts. A higher inverse temperature is associated with a slight increase in badges earned.

```{r, results='asis'}
#| eval: false
#| cache: true
#| label: tbl-optimality
#| tbl-cap: "The impact of different parameters and control variables on average weekly badges and Tower Alerts. Six linear regression models were used to examine the correlations between a teacher's RL parameters (Cost 1, Cost 2, Cost 3, Gamma, Alpha, Tau) and two measures of student engagement: average weekly Badges earned per student (Models 1-3) and average weekly Tower Alerts per student (Models 4-6). Models 2-3 and 5-6 also control for other variables including number of active students, number of classes taught, grade level, weeks, poverty level, income level, whether the school is a charter school, and whether the school has a paid account. Coefficients and standard errors are provided for each parameter in each model."

library(tidybayes)
library(stargazer)

hierarchical_model <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
classroom_data <- read.csv("Bayesian/df_subset.csv") %>%
  group_by(Teacher.User.ID) %>%
  summarise(n_active_students = mean(Active.Users...Total),
            n_students = mean(Students...Total),
            minutes_students = mean(Minutes.per.Active.User),
            badges = mean(Badges.per.Active.User),
            boosts = mean(Boosts.per.Tower.Completion),
            tower_alers = mean(Tower.Alerts.per.Tower.Completion),
            n_classes_by_teacher = median(teacher_number_classes),
            grade = first(Grade.Level),
            n_weeks = mean(n_weeks),
            poverty = first(poverty),
            income = first(income),
            charter_school = first(charter.school),
            school_account = first(school.account))

posterior_samples <- hierarchical_model$draws() %>%
  spread_draws(cost[207,3],
               gamma[207],
               alpha[207], 
               tau[207])

summary_cost <- posterior_samples %>%
  unnest_wider(cost, names_sep = "_")
list_of_dfs <- lapply(seq_along(summary_cost)[grepl("cost", names(summary_cost))], function(x){
  temp_df <- as.data.frame(summary_cost[[x]])
  names(temp_df) <- paste0("C", 1:ncol(temp_df))
  temp_df$teacher <- paste0("teacher_", (x - 3))
  return(temp_df)
})
cost_df <- do.call(rbind, list_of_dfs)
cost_df$teacher <- as.numeric(gsub("teacher_", "", cost_df$teacher)) 
summary_cost <- cost_df %>% 
  group_by(teacher) %>% 
  summarise(across(starts_with("C"), mean, .names = "mean_{.col}"))

summary_gamma_alpha_tau <- posterior_samples %>%
  unnest_wider(gamma, names_sep = "_") %>%
  unnest_wider(alpha, names_sep = "_") %>%
  unnest_wider(tau, names_sep = "_") %>%
  summarise(across(c(starts_with("gamma"),
                     starts_with("alpha"),
                     starts_with("tau")),
                   mean, .names = "{.col}")) %>%
  pivot_longer(cols = c(starts_with("gamma"),
                        starts_with("alpha"),
                        starts_with("tau")),
               names_to = "variable",
               values_to = "value") %>%
  separate(variable, into = c("type", "teacher"), sep = "_", convert = TRUE) %>%
  pivot_wider(names_from = type, values_from = value)

# merge the two dataframes
summary_all <- merge(summary_cost, summary_gamma_alpha_tau, by = "teacher")

classroom_data <- classroom_data %>%
  arrange(Teacher.User.ID) %>%
  bind_cols(summary_all) %>%
  dplyr::mutate(
    income = factor(income, ordered = TRUE), 
    grade = factor(grade, ordered = TRUE,
                   levels = c("Kindergarten",
                              "1st", "2nd", "3rd", "4th", "5th")),
    poverty = factor(poverty, ordered = TRUE)
  )

ordered_factor <- function(fact_var) {
  categories <- levels(fact_var)
  n_cat <- length(categories)
  cont <- matrix(data = 0, nrow = n_cat, ncol = (n_cat - 1))
  cont[col(cont) < row(cont)] <- 1
  rownames(cont) <- categories
  colnames(cont) <- paste(categories[2:n_cat],
                          categories[1:(n_cat - 1)],
                          sep = " vs. ")
  contrasts(fact_var) <- cont
  return(fact_var)
}
classroom_data <- classroom_data %>%
  dplyr::mutate(
    income = ordered_factor(income), 
    grade = ordered_factor(grade),
    poverty = ordered_factor(poverty)
  )

model1 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau,
          classroom_data)

model2 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks,
          classroom_data)

model3 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks +
            poverty +
            income +
            charter_school +
            school_account,
          classroom_data)

model4 <- lm(tower_alers ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau,
          classroom_data)

model5 <- lm(tower_alers ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks,
          classroom_data)

model6 <- lm(tower_alers ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks +
            poverty +
            income +
            charter_school +
            school_account,
          classroom_data)

# Create table with stargazer
stargazer(model1, model2, model3, model4, model5, model6,
          type = "latex",
          header=FALSE,
          align = TRUE,
          covariate.labels = c("Cost 1", "Cost 2", "Cost 3",
                               "Gamma", "Alpha", "Tau",
                               "Number of Active Students", "Number of Classes",
                               "Number of Weeks", "Charter School", "Paid School Account"),
          omit = c("grade", "poverty", "income"),
          dep.var.caption = "Dependent variables:",
          dep.var.labels = c("Badges", "Tower Alerts"),
          star.cutoffs = c(.05, .01, .001), 
          star.char = c("*", "**", "***"),
          add.lines = list(c("Control for Grade Level",
                             "", "Yes", "Yes", "", "Yes", "Yes"),
                           c("Control for Poverty Level",
                             "", "", "Yes", "", "", "Yes"),
                           c("Control for Income Level",
                             "", "", "Yes", "", "", "Yes")))


```

@fig-heterogeneity presents a detailed summary of the associations between different factors and classroom variables. Among these, the interaction between "Cost 3" and "Poverty Level" is notable. Our data suggest that a higher estimated cost of applying Pedagogical Content Knowledge predicts a lower median income level in the school. The estimated Discount Rate is another variable significantly associated with a school's Poverty Level ($\beta = -0.39,\text{ } p<0.05$). Furthermore, the negative relationship between being a Charter School and the Learning Rate, with a coefficient of -0.67 (p\<0.001), suggests that teachers in charter schools are slower to adapt and modify their teaching methods in response to new information.

```{r}
#| eval: false
#| cache: true
#| label: fig-heterogeneity
#| fig-cap: "Predicting classroom variables with RL parameters. The data comes from a comprehensive set of classroom records, including income level, poverty level, charter school status, whether a school account is paid, and the number of classes each teaches. These classroom variables (columns) are regressed on the estimated cost parameters for each action (Cost 1, Cost 2, Cost 3), the discount rate, the learning rate, and the inverse temperature. Each cell in the heatmap represents the estimate of a linear, Poisson, or logistic regression model, depending on the variable type. For ordinal classroom variables (Income Level and Poverty Level), ordered logistic regression (proportional odds model) is used, while for binary variables (Charter School, Paid School Account), logistic regression is applied. The number of classes by each teacher, being a count data, is modeled with Poisson regression. The coefficients are scaled estimates of the effect of each parameter on the respective classroom variable. The asterisks indicate the level of statistical significance based on p-values (*p < 0.1; **p < 0.01; ***p < 0.001). The coefficients are color-coded with a gradient from light blue (negative) to white (zero) to dark blue (positive)."
library(broom)
library(MASS)

# Prepare data
hetero <- classroom_data %>%
  dplyr::select(income, poverty, charter_school, school_account, n_classes_by_teacher,
         mean_C1, mean_C2, mean_C3, gamma, alpha, tau) %>%
  dplyr::mutate(
    income = factor(income, ordered = TRUE),
    poverty = factor(poverty, ordered = TRUE)
  )

# Define predictors and responses
responses <- c("income", "poverty",
               "charter_school", "school_account", "n_classes_by_teacher")
predictors <- c("mean_C1", "mean_C2", "mean_C3", "gamma", "alpha", "tau")
hetero[predictors] <- scale(hetero[predictors])
# Create the formula for the model with all predictors
predictors_formula <- paste(predictors, collapse = " + ")

# Run models for each response, and extract coefficients
coef_matrix <- map_dfr(responses, function(response) {
  if (response == "income" | response == "poverty") {
    model <- polr(as.formula(paste0(response, " ~ ", predictors_formula)),
                  data = hetero, Hess = TRUE)
  } else if (response == "n_classes_by_teacher") {
    model <- glm(as.formula(paste0(response, " ~ ", predictors_formula)),
                 data = hetero, family = poisson(link = "log"))
  } else {
    model <- glm(as.formula(paste0(response, " ~ ", predictors_formula)),
                 data = hetero, family = binomial(link = "logit"))
  }
  
  tidy(model, p.values = TRUE) %>%
    filter(term %in% predictors) %>%
    dplyr::select(term, estimate, p.value) %>%
    mutate(response = response,
           sig = ifelse(p.value < 0.05, "***",
                        ifelse(p.value < 0.01, "**",
                               ifelse(p.value < 0.1, "*", ""))))
})

# Convert to wide format
coef_matrix <- coef_matrix %>%
  pivot_wider(names_from = term, values_from = c(estimate, p.value, sig))

# Reorder the rows to match the original order of responses
coef_matrix <- coef_matrix %>%
  mutate(response = factor(response, levels = responses)) %>%
  arrange(response)

# Reshape for plotting
coef_matrix_long <- coef_matrix %>%
  pivot_longer(cols = starts_with("estimate"), names_to = "term", values_to = "estimate") %>%
  pivot_longer(cols = starts_with("sig"), names_to = "term_sig", values_to = "sig")

# Clean up term names
coef_matrix_long$term <- str_replace(coef_matrix_long$term, "estimate_", "")
coef_matrix_long$term_sig <- str_replace(coef_matrix_long$term_sig, "sig_", "")

# Make sure the term columns match
coef_matrix_long <- coef_matrix_long[coef_matrix_long$term == coef_matrix_long$term_sig,]

# Rename the variables
var_names <- c("income" = "Income Level", "poverty" = "Poverty Level",
               "charter_school" = "Charter School",
               "school_account" = "Paid School Account",
               "n_classes_by_teacher" = "Number of Classes by Teacher",
               "mean_C1" = "Cost 1", "mean_C2" = "Cost 2", "mean_C3" = "Cost 3",
               "gamma" = "Discount Rate",
               "alpha" = "Learning Rate",
               "tau" = "Inverse Temperature")
coef_matrix_long$term <- var_names[coef_matrix_long$term]

responses_new <- c("Income\n Level", "Poverty\n Level",
                   "Charter\n School", "Paid School\n Account", 
                   "Classes\n per Teacher")

names(responses_new) <- responses

# Update variable labels
coef_matrix_long$response <- factor(coef_matrix_long$response, 
                                    levels = responses, 
                                    labels = responses_new)

# Create the plot
ggplot(coef_matrix_long, aes(x = response, y = term, fill = as.numeric(estimate))) +
  scale_y_discrete(limits = c("Cost 1", "Cost 2", "Cost 3",
                              "Inverse Temperature", "Discount Rate", "Learning Rate")) +
  geom_tile() +
  geom_text(aes(label = paste0(round(as.numeric(estimate), 2), sig)), size = 4) +
  scale_fill_gradient2(low = "lightblue", high = "darkblue", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), 
                       name = "Regression\n Coefficient") +
  scale_x_discrete(position = "top") + # This line moves the x-axis labels to the top
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, angle = 0, vjust = 1, hjust = 0.5), 
        axis.text.y = element_text(size = 12), 
        legend.title = element_text(size = 11), 
        legend.text = element_text(size = 10),
        legend.spacing.x = unit(0.5, "cm"),
        legend.position = "bottom") +
  labs(x = NULL, y = NULL, title = NULL)

```



<!--
## Posterior Predictive Checks
```{r}
#| eval: false
#| label: fig-timelines
#| fig-cap: ""

# Load the data
hbi_ac_data <- readMat('CBM/zearn_results/top_results/hbi_ac_data.mat')
model_fit <- readMat('CBM/zearn_results/top_results/hbi_ac.mat')
teacher_specific_params <- model_fit[["cbm"]][[5]][[1]][[1]][[1]]

simulate_predictions <- function(parameters, subj) {
  alpha_w <- 1 / (1 + exp(-parameters[1]))  # Learning rate for w (critic)
  alpha_theta <- 1 / (1 + exp(-parameters[2]))  # Learning rate for theta (actor)
  gamma <- 1 / (1 + exp(-parameters[3]))  # Discount factor
  tau <- exp(parameters[4])
  theta_init <- parameters[5]
  w_init <- parameters[6]
  cost <- exp(parameters[7])
  
  # Unpack data
  Tsubj <- length(subj[[1]][[2]])
  choice <- subj[[1]][[2]]
  outcome <- subj[[1]][[5]]
  state <- cbind(1, as.matrix(subj[[1]][[7]]))
  week <- subj[[1]][[9]][[1]]
  
  # Initialize
  w <- w_init
  theta <- theta_init
  log_probabilities <- rep(0, Tsubj)
  
  # Loop through trials
  for (t in 1:Tsubj) {
    if (week[t] == 0) break  # End loop if week is zero
    s <- state[t, ]
    a <- choice[t]
    o <- outcome[t]
    
    # Actor: Compute policy (log probability of taking the action)
    product <- sum(s * theta * tau)
    # Handle numerical issues for large values of theta
    if (product < -8) {
      log_probabilities[t] <- log_probabilities[t] - product * a
    } else if (product > 8) {
      log_probabilities[t] <- log_probabilities[t] + (1 - product) * (1 - a)
    } else {
      log_probabilities[t] <- log_probabilities[t] - log(1 + exp(-product)) * a - log(1 + exp(product)) * (1 - a)
    }
    
    # Critic: Compute TD error (delta)
    delta <- (o - cost)  # Basic TD error, without considering future states
    
    # Update weights
    theta <- theta + alpha_theta * gamma ^ (week[t] - week[1]) * (tau * s) / (1 + exp(product)) * delta
    w <- w + alpha_w * s * delta
  }
  
  # Sum log probabilities to get log-likelihood
  log_likelihood <- sum(log_probabilities)
  
  return(list(predictions = choice, log_likelihood = log_likelihood))
}

# Prepare the data for plotting (replace with actual variables and data)
long_df <- as.data.frame(hbi_ac_data$Variable) %>%
  mutate(Week = rep(1:nrow(.), each = ncol(.)),
         Value = as.vector(t(.)))

# Simulate predictions using group mean parameters
predictions <- simulate_predictions(group_mean_transformed, long_df)

# Variables to plot
variables_to_plot <- names(hbi_ac_data$Variable)

# Create individual plots
plots <- lapply(variables_to_plot, plot_variable, data = long_df)

# Reshape the data to long format
long_df <- df %>%
  select(MDR.School.ID, week, Active.Users...Total, Sessions.per.Active.User, 
         Minutes.per.Active.User, Badges.per.Active.User, 
         Boosts.per.Tower.Completion, Tower.Alerts.per.Tower.Completion, 
         FrobeniusNNDSVD1, FrobeniusNNDSVD2, FrobeniusNNDSVD3,
         poverty, school.account) %>%
  group_by(MDR.School.ID, week, poverty, school.account) %>%
  summarize(across(where(is.numeric), ~ mean(., na.rm = TRUE))) %>%
  gather(key = "Variable", value = "Value",
         -MDR.School.ID, -week, -poverty, -school.account)

# Function to create a plot for each variable with error ribbons
plot_variable <- function(variable_name, var) {
  # Calculate overall average and standard error
  avg_data <- long_df %>%
    filter(Variable == variable_name) %>%
    filter(!is.na({{var}})) %>%
    group_by({{var}}, week) %>%
    summarize(
      Avg = mean(Value, na.rm = TRUE),
      SE = sd(Value, na.rm = TRUE)/sqrt(n()),
      CI_low = Avg - SE,
      CI_high = Avg + SE
    )
  
  # Calculate 2 standard deviations from the overall data
  y_high <- quantile(avg_data$Value[avg_data$Variable == variable_name],
                     0.85, na.rm = T)
  y_low <- quantile(avg_data$Value[avg_data$Variable == variable_name],
                    0.15, na.rm = T)

  ggplot(avg_data, aes(x = week, y = Avg, group = {{var}})) +
    geom_line(aes(color = {{var}})) +
    geom_ribbon(data = avg_data,
                aes(x = week, y = Avg, group = {{var}}, alpha = 0.2,
                    fill = {{var}}, ymin = CI_low, ymax = CI_high)) +
    theme_minimal() +
    labs(title = variable_name, x = "Time", y = "Value") +
    theme(legend.position = "none") +
    coord_cartesian(ylim = c(y_low, y_high))  # Adjust y-axis range
}

# Variables to plot
variables_to_plot <- c(
  "Active.Users...Total", "Sessions.per.Active.User",
  "Minutes.per.Active.User", "Badges.per.Active.User",
  "Boosts.per.Tower.Completion", "Tower.Alerts.per.Tower.Completion", 
  "FrobeniusNNDSVD1", "FrobeniusNNDSVD2", "FrobeniusNNDSVD3"
  )

# Create individual plots
plots <- lapply(variables_to_plot, plot_variable, var = school.account)

# Combine the plots into a panel
do.call(grid.arrange, c(plots, ncol = 3))

```
-->

# Discussion

-- This pattern is consistent with the findings from [@knudsen2020] that teachers reported learning from a variety of Zearn Math resources and that the curriculum materials and their implementation are important sources of learning. **Component 2 (Resource Utilization)**: This component has high weights for variables related to different resources available on the Zearn platform, such as "Optional Problem Sets," "Student Notes and Exit Tickets," and "Mission Overview". High values in this component suggest that teachers are downloading and possibly using a variety of resources in their teaching. This pattern is consistent with the findings from [@knudsen2020] that teachers reported learning from a variety of Zearn Math resources and that the curriculum materials and their implementation are important sources of learning. **Component 3 (Pedagogical Content Knowledge)**: This component has high weights for variables related to student activities, such as "Guided Practice Completed," "Tower Completed," and "Fluency Completed," suggesting that teachers may be engaged in acquiring subject-matter-specific pedagogy, learning to scaffold and to explain concepts in a variety of ways. This finding aligns with [@morrison2019], where teachers were most likely to report using Independent Digital Lessons, student notes and workbooks, small-group lessons, and paper Exit Tickets frequently or very frequently.

Our study aimed to unravel the complex dynamics of teacher behavior in the context of Zearn Math, a popular online learning platform. We sought to understand the role of reinforcement learning (RL) in modeling these behaviors and their impact on student achievement. Our findings provide compelling evidence that RL models, particularly the hierarchical Q-learning model, can accurately characterize teacher behavior and offer valuable insights into the education field.

## Characterizing Teacher Behavior

Our first research question aimed to identify the RL model that best characterizes teacher behavior in the Zearn Math context. The hierarchical Q-learning model emerged as the most accurate, outperforming the simple logistic regression model. This result suggests that teachers are not merely following a static "education production function" but are actively learning and adapting their teaching strategies on Zearn Math.

The hierarchical Q-learning model's superiority underscores the importance of considering individual teacher differences. As an agent in the RL model, each teacher has unique parameters that reflect their learning rate and decision-making strategies. These parameters offer a rich source of information about teacher behavior, providing a more nuanced understanding than simpler models.

## Parameter Variations and Their Influence on Student Achievement

Our second research question explored how variations in the parameters of RL models (i.e., the learning rate and the exploration-exploitation trade-off) affect teacher behavior and, consequently, student achievement. The learning rate suggests a teacher's adaptability in altering teaching strategies based on feedback. The exploration-exploitation trade-off encapsulates a teacher's ability to balance using new teaching strategies (exploration) and sticking with known effective strategies (exploitation).

Our findings indicated that teachers exhibiting a higher learning rate, a marker of greater adaptability, were associated with higher student achievement. This finding echoes the sentiments expressed by teachers in [@knudsen2020], who found value in the multifaceted strategies provided by Zearn Math. A 3rd-grade Zearn teacher stated, "I like that Zearn provides several strategies to get to the answer...you see the problems; you see what you need to hit on and stress the first time around." This sentiment aligns with our observation that the ability to adapt teaching strategies based on feedback, akin to a higher learning rate in RL models, is a valuable attribute in promoting student achievement.

Similarly, teachers who effectively balanced exploration and exploitation had students with superior outcomes. [@morrison2019] noted that teachers exhibited varied levels of preparedness for implementing the Zearn Math curriculum, with just under half of the teachers reporting feeling adequately prepared for implementation. This lack of preparation could influence how effectively teachers navigate the exploration-exploitation trade-off, impacting student outcomes.

Our results also highlight the complex dynamics underlying teacher behaviors and their implications for student outcomes. For instance, @knudsen2020 found that veteran teachers with strong PCK leaned heavily on traditional teaching methods, reducing their engagement with the novel practices proposed by Zearn Math. Conversely, new teachers exhibited a blend of traditional and innovative practices, despite their initial resistance to Zearn Math, underscoring their learning curve with the new curriculum.

Our findings point to the potential of RL parameters to provide valuable insights into teacher behaviors and their subsequent effects on student achievement. Understanding and leveraging these parameters could enhance teacher training programs and interventions, ultimately improving student outcomes. By incorporating insights from our study into such initiatives, we can help foster a more adaptive and effective teaching landscape that better supports student learning and achievement.

## Influence of Teacher Background, Training, and Experience

Our third research question asked how teacher background, training, and experience influence their adaptation to and implementation of the Zearn Math curriculum. Our findings confirm the fundamental role of teachers' background, training, and experience when implementing new educational platforms like Zearn Math. They also underscore the importance of differentiating support and training strategies to cater to teachers' unique backgrounds and needs, which may prove pivotal in fostering effective adoption and implementation of such platforms. A prominent theme that emerged from our investigation was the interaction between teacher behaviors, school characteristics, and broader socio-economic contexts.

Our analyses revealed a telling association between Poverty Level and Cost 3, denoting the estimated cost of applying PCK, suggesting that schools in higher poverty strata are more likely to encounter additional barriers in the form of resource constraints when implementing the Zearn Math curriculum.

This observation points towards a potential resource disparity where schools in lower-income areas grapple with the challenge of investing in critical teacher training and resources needed for effective curriculum implementation. Implementing a comprehensive math curriculum like Zearn becomes particularly daunting in economically disadvantaged areas where financial constraints may obstruct optimal pedagogical strategies.

Moreover, we unveiled a striking finding regarding charter schools. Our data showed a negative relationship between being a Charter School and the Learning Rate. This finding was unexpected, given the increased flexibility often associated with charter schools. It raises pertinent questions about the pedagogical dynamics within such institutions and points towards the necessity for further research into the support systems and professional development opportunities provided for teachers in these settings.

One of the more disconcerting findings was the negative relationship between Poverty Level and Discount Rate, indicating that teachers in schools with higher poverty levels might place less emphasis on long-term student outcomes. This phenomenon may occur due to the pressing short-term challenges related to student well-being and engagement, often prevalent in high-poverty settings, which demand immediate attention and resources.

These findings underscore the complex challenges facing schools in lower-income and high-poverty areas. They illuminate the complexities teachers must navigate when adapting to new teaching habits and implementing curricula, which are only magnified by demographic and school-specific factors.

Our Reinforcement Learning model uncovered the need for a deeper examination of systemic educational disparities and the development of targeted interventions and policies to bridge these gaps. By shedding light on these complex relationships, we contribute to the broader goal of creating a more equitable educational landscape.

## Implications for Teachers and Schools

Our findings have several important implications for teachers, schools, and the broader education field. First, they highlight the dynamic nature of teaching, with teachers continually learning and adapting their strategies based on feedback. This result underscores the importance of providing teachers with ongoing professional development opportunities and supportive learning environments.

Second, our results suggest that optimal teaching strategies vary among teachers, reflecting individual differences in learning rates and decision-making strategies. This feature points to the need for a more personalized approach to teacher training and support, considering individual teachers' strengths and areas for improvement.

Third, our findings highlight the potential of RL models as tools for understanding and improving teaching practices. By capturing the complex dynamics of teacher behavior, these models can inform the design of interventions to enhance student achievement. For instance, interventions could help teachers improve their learning rates or better balance exploration and exploitation in their teaching strategies.

Finally, our results have policy implications. They suggest that policies aimed at improving student achievement should consider not only the resources available to schools but also teachers' behaviors and decision-making processes. Policies should support teachers' learning and adaptation processes by providing professional development opportunities or creating supportive learning environments.

## Limitations

The insights derived from this study potentially have broader implications, extending beyond the confines of Louisiana, given the universal teaching and learning principles underpinning our analysis.

Note that, although extensive, the set of actions was not comprehensive.

One of this study's limitations is the available data, including teachers only in Louisiana, which can limit the generalizability of our findings. Second, our data were at the weekly level, with approximately 40 weeks per classroom. To fit more optimally, RL models would need more trials, either over a more extended period or with smaller time units (e.g., twice a week or daily). Another major challenge of this study was finding the best characterization of actions, which required various techniques for dimensionality reduction. Third, we did not exhaust all possible RL models, and other models could better fit the data. Future research could explore other RL models and compare their performance in characterizing teacher behavior and predicting student achievement. Finally, our models made certain assumptions and simplifications, such as the characterization of actions, which could affect the accuracy of our results. For instance, our data did not include the variance of classroom scores, only the averages, which limits our ability to answer questions about how teachers adapt to the distribution of student achievement in their classrooms.

## Future Research

Our study opens several avenues for future research. First, our findings could be validated and extended in other educational contexts, such as different grade levels, subjects, or geographical locations. Second, our RL models could be integrated with other models and approaches to provide a more comprehensive understanding of teacher behavior and its impact on student outcomes. Third, future research could expand the scope of variables and data sources considered in the models, such as incorporating additional variables related to teacher background, training, and experience. Similarly, data from other sources and domains could enrich the models and provide a more nuanced understanding of human behavior in the field.

In conclusion, our study provides compelling evidence for the potential of reinforcement learning models to understand and improve teacher behavior and student achievement in the context of online learning platforms like Zearn Math. By shedding light on the complex dynamics of teacher behavior, our findings offer valuable insights for teachers, schools, policymakers, and researchers in the education field. Our work will inspire further research and practical applications of reinforcement learning in education.

# References

::: {#refs}
:::

{{< pagebreak >}}

# Supplemental Information {.appendix}

## Supplemental Methods {.appendix}

### Correlations Between Variables

We begin to unveil the intricate relationships among the variables under consideration through a comprehensive correlation analysis, as depicted in @fig-corr. This correlation matrix elucidates the magnitude and direction of associations among variables such as badges earned, minutes spent per student, tower alerts, the number of students, and teacher minutes. These interconnections inform the construction of our reinforcement learning models by suggesting the influence of teacher effort on student achievement. In this correlation matrix, each cell represents the Spearman correlation coefficient between a pair of variables. The color and size of the circles in each cell reflect the strength and direction of the correlation, with blue indicating positive correlations and red indicating negative correlations. The histograms along the diagonal provide a visual representation of the distribution of each variable.

```{r}
#| cache: true
#| label: fig-corr
#| fig-cap: "Correlation coefficients between variables after stardardization"
#| fig-format: png

df_corr <- df  %>%
  select(Badges.per.Active.User,
         Active.Users...Total,
         Minutes.per.Active.User,
         Tower.Alerts.per.Tower.Completion,
         Minutes.on.Zearn...Total) %>%
  rename("Badges" = Badges.per.Active.User,
         "Minutes per Student" = Minutes.per.Active.User,
         "Tower Alerts" = Tower.Alerts.per.Tower.Completion,
         "# of Students" = Active.Users...Total,
         "Teacher Minutes" = Minutes.on.Zearn...Total)

chart.Correlation(df_corr, histogram = TRUE, method = "spearman",
                  pch = 20, cex = 0.5, col = rgb(0, 0, 1, 0.5))

```

## Supplemental Tables {.appendix}

<!-- FE Logit Model Summary -->

<!-- FE Logit State-free -->

```{=tex}
\newpage
\KOMAoptions{paper=landscape,pagesize}
\recalctypearea
{\areaset[current]{\dimexpr\textwidth\relax}{\textheight}
\setlength{\marginparwidth}{0pt}
\scriptsize
```
```{r}
#| label: tbl-fe-results-statefree
#| tbl-cap: "State-Free Panel Logistic Regression Results"

# Load the results from both subset and restricted analyses
load("Regressions/fe-subset-results.RData")
original_results <- results
load("Regressions/fe-restricted-results.RData")
restricted_results <- results

# Combine the model stats from original and restricted results for comparison
original_models <- do.call(rbind, lapply(original_results, function(x) {
  if (x$Lag != 2) return(NULL)
  data.frame(
    Action = x$Method,
    Reward = x$Reward,
    AUC = x$AUC,
    BIC = x$bic,
    Type = "Original"
  )
}))

# Select top 2 AUC models and top 2 BIC models for both original and restricted
top_models <- original_models %>%
  arrange(desc(AUC)) %>%
  slice_head(n = 2)
top_models <- original_models %>%
  arrange(BIC) %>%
  slice_head(n = 2) %>%
  bind_rows(top_models)

# Prepare the results from selected top models for the table
tidy_fe_results <- lapply(original_results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (!(paste(x$Method, x$Reward) %in%
        paste(top_models$Action, top_models$Reward))) return(NULL)
  temp <- as.data.frame(x$fecoef) %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  as.numeric)) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")
  
  return(list(
    Model = paste(x$Method, x$Reward,"Full"),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results <- tidy_fe_results[!sapply(tidy_fe_results, is.null)]
tidy_fe_results_restrict <- lapply(restricted_results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (!(paste(x$Method, x$Reward) %in%
        paste(top_models$Action, top_models$Reward))) return(NULL)
  temp <- as.data.frame(x$fecoef) %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  as.numeric)) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")
  
  return(list(
    Model = paste(x$Method, x$Reward,"Restricted"),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results_restrict <- 
  tidy_fe_results_restrict[!sapply(tidy_fe_results_restrict, is.null)]
tidy_fe_results <- c(tidy_fe_results, tidy_fe_results_restrict)

# Combine all model summaries into one dataframe
model_summary <- do.call(rbind, lapply(tidy_fe_results, function(x) {
  x_name <- unlist(strsplit(x[[1]], " "))
  x_data <- x$Data %>%
    rename(Coefficient = Estimate,
           Std_Error = `Std. Error`) %>%
    mutate(
      Significance = case_when(
        `Pr(>|z|)` < .001 ~ "***",
        `Pr(>|z|)` < .01 ~ "**",
        `Pr(>|z|)` < .05 ~ "*",
        TRUE ~ ""),
      Term = gsub(x_name[1], "lag", Term),
      Term = gsub(x_name[2], "rwd", Term),
      Term = gsub("_", "",  Term),
      Term = gsub(":", "_",  Term)) %>%
    add_row(Term = "BIC", Coefficient = x[[3]]) %>%
    add_row(Term = "N", Coefficient = x[[4]])
  x_data$Model <- x[[1]]

  x_data
})) %>% select(Model, Term, Coefficient, Std_Error, Significance) %>%
  mutate(Model = gsub(" ", "&", Model),
         Model = gsub("Frobenius.NNDSVD_student1", "Badges", Model),
         Model = gsub("Frobenius.NNDSVD_student2", "Struggles", Model),
         Model = gsub("Frobenius.NNDSVD_student3", "No. Students", Model),
         Model = gsub("Frobenius.NNDSVD_student4", "Activity", Model),
         Model = gsub("Frobenius.NNDSVD_teacher1", "Assessments", Model),
         Model = gsub("Frobenius.NNDSVD_teacher2", "Pedagogical Knowledge", Model),
         Model = gsub("Frobenius.NNDSVD_teacher3", "Group Instruction", Model),
         Model = gsub("Frobenius.NNDSVD_teacher4", "Curriculum Planning", Model))

# Create the table using the 'gt' package
gt_table <- model_summary %>%
  filter(!Term %in% c("rwd1","rwd2","lag1","lag2")) %>%
  mutate(Estimate = ifelse(Term %in% c("BIC", "N"),
                           sprintf("%d", as.integer(Coefficient)),
                           sprintf("%1.3f%s\n(%1.3f)",
                                   Coefficient, Significance, Std_Error)),
         Term = factor(Term, levels = c("rwd1_lag1", "rwd1_lag2", "lag2_rwd2",
                                        # "lag1", "lag2",
                                        "st", "lag1_st",
                                        "BIC", "N"),
                       labels = c("R(t-1) x \n A(t-1)",
                                  "R(t-1) x \n A(t-2)",
                                  "R(t-2) x \n A(t-2)",
                                  # "Action(t-1)", "Action(t-2)",
                                  "S(t)", "S(t) x \n A(t-1)",
                                  "BIC", "N"))) %>%
  select(-Coefficient, -Std_Error, -Significance) %>%
  # Pivot wider to have one column per model
  pivot_wider(names_from = Model,
              values_from = Estimate,
              names_sep = "_") %>%
  select(order(names(.))) %>%
  arrange(Term) %>%
  gt(rowname_col = "Term") %>%
  sub_missing() %>%
  tab_spanner_delim("&") %>%
  tab_header(
    title = "Fixed Effects Logistic Regression Results"
  )

# Print the table
gt_table

```
```{=tex}
}
\newpage
\KOMAoptions{paper=portrait,pagesize}
\recalctypearea
```

<!-- FE Logit State-based -->

```{r}
#| label: tbl-fe-results
#| tbl-cap: "State-Based Panel Logistic Regression Results"

load("Regressions/fe-state-subset-results.RData")

# Combine the model stats from original and restricted results for comparison
results_df <- do.call(rbind, lapply(results, function(x) {
  if (x$Lag != 2) return(NULL)
  data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = x$State,
    AUC = x$AUC,
    BIC = x$bic,
    Type = "Original"
  )
}))
# Select top 2 AUC models and top 2 BIC models for both original and restricted
top_models <- results_df %>%
  arrange(desc(AUC)) %>%
  slice_head(n = 2)
top_models <- results_df %>%
  arrange(BIC) %>%
  slice_head(n = 2) %>%
  bind_rows(top_models)

# Filter and tidy the results
tidy_fe_results <- lapply(results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (top_models %>%
      filter(Action == x$Method &
             Reward == x$Reward &
             State  == x$State) %>%
      nrow() == 0) return(NULL)
  temp <- x$fecoef %>%
    as.data.frame() %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  ~ as.numeric(.))) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")
  
  return(list(
    Model = paste(x$Method, x$Reward, x$State),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results <- tidy_fe_results[!sapply(tidy_fe_results, is.null)]
	
# Combine all model summaries into one dataframe
model_summary <- do.call(rbind, lapply(tidy_fe_results, function(x) {
  x_name <- unlist(strsplit(x$Model, " "))
  x_data <- x$Data %>%
    rename(Coefficient = Estimate,
           Std_Error = `Std. Error`) %>%
    mutate(
      Significance = case_when(
        `Pr(>|z|)` < .001 ~ "***",
        `Pr(>|z|)` < .01 ~ "**",
        `Pr(>|z|)` < .05 ~ "*",
        TRUE ~ ""),
      Term = gsub(x_name[1], "lag", Term),
      Term = gsub(x_name[2], "rwd", Term),
      Term = gsub(x_name[3], "st",  Term),
      Term = gsub("_", "",  Term),
      Term = gsub(":", "_",  Term)) %>%
    add_row(Term = "BIC", Coefficient = x[[3]]) %>%
    add_row(Term = "N", Coefficient = x[[4]])
  x_data$Model <- x$Model

  x_data
})) %>% select(Model, Term, Coefficient, Std_Error, Significance) %>%
  mutate(Model = gsub(" ", "&", Model),
         Model = gsub("Frobenius.NNDSVD_student1", "Badges", Model),
         Model = gsub("Frobenius.NNDSVD_student2", "Struggles", Model),
         Model = gsub("Frobenius.NNDSVD_student3", "No. Students", Model),
         Model = gsub("Frobenius.NNDSVD_student4", "Activity", Model),
         Model = gsub("Frobenius.NNDSVD_teacher1", "Assessments", Model),
         Model = gsub("Frobenius.NNDSVD_teacher2", "Pedagogical Knowledge", Model),
         Model = gsub("Frobenius.NNDSVD_teacher3", "Group Instruction", Model),
         Model = gsub("Frobenius.NNDSVD_teacher4", "Curriculum Planning", Model))

# Create the table using the 'gt' package
gt_table <- model_summary %>%
  filter(!Term %in% c("rwd1","rwd2","lag1","lag2")) %>%
  mutate(Estimate = ifelse(Term %in% c("BIC", "N"),
                           sprintf("%d", as.integer(Coefficient)),
                           sprintf("%1.3f%s\n(%1.3f)",
                                   Coefficient, Significance, Std_Error)),
         Term = factor(Term, levels = c("rwd1_lag1", "rwd1_lag2", "lag2_rwd2",
                                        # "lag1", "lag2",
                                        "st1", "lag2_st1",
                                         "BIC", "N"),
                       labels = c("R(t-1) x \n A(t-1)",
                                  "R(t-1) x \n A(t-2)",
                                  "R(t-2) x \n A(t-2)",
                                  # "Action(t-1)", "Action(t-2)",
                                  "S(t-1)", "S(t-1) x \n A(t-2)",
                                  "BIC", "N"))) %>%
  select(-Coefficient, -Std_Error, -Significance) %>%
  # Pivot wider to have one column per model
  pivot_wider(names_from = Model,
              values_from = Estimate,
              names_sep = "_") %>%
  select(order(names(.))) %>%
  arrange(Term) %>%
  gt(rowname_col = "Term") %>%
  sub_missing() %>%
  tab_spanner_delim("&") %>%
  tab_header(
    title = "Fixed Effects Logistic Regression Results"
    )

# Print the table
gt_table

```

<!-- VC Logit Model Summary -->

<!-- QL Model Summary -->

<!-- AC Model Summary -->

## Supplemental Figures {#sec-supp-fig}

![Zearn Student Portal](images/student-feed.PNG){#fig-st-portal fig-align="center"}

![Professional Development Calendar](images/PD-calendar.jpg){#fig-prof-dev fig-align="center"}


```{r}
#| eval: false
#| label: fig-raw-data
#| fig-cap: "The raw data from Zearn's platform, with each row corresponding to a different timestamp for a teacher action. The columns represent the different variables, including the number of minutes spent on the platform, the number of badges earned, and the number of boosts used. The data is organized by classroom, with each classroom having a different color. The data is also organized by week, with each week having a different shape. The data is organized by teacher, with each teacher having a different line type. The data is organized by school, with each school having a different line color."

```

{{< pagebreak >}}

```{r}
#| eval: false
#| label: fig-panel-nmf-methods
#| fig-cap: ""

# Wrangling the results data frame for plotting
results_df <- map_dfr(results, ~as.data.frame(t(unlist(.))))
# Convert variables to appropriate data types
results_df <- results_df %>%
  mutate(Method = as.factor(Method),
         Component = as.factor(Component),
         Lag = as.numeric(Lag),
         nloglik = as.numeric(nloglik),
         bic = as.numeric(bic)) %>%
  group_by(Method, Component, Lag) %>%
  summarise(avg_bic = mean(bic, na.rm = TRUE),
            avg_nloglik = mean(nloglik, na.rm = TRUE),
            .groups = "drop")
# Filter methods for the plots
# methods_for_plots <- c("FrobeniusNNDSVD", "KullbackLeibler", "FrobeniusNNDSVDA")
methods_for_plots <- c("FrobeniusNNDSVD")
plot_data <- filter(results_df, Method %in% methods_for_plots)



# Shared aesthetics for the plots
aes <- ggplot2::aes(x = Lag, y = value, color = Component)

# Summarize methods not included in the plots and prepare tables
methods_for_summary <- setdiff(unique(results_df$Method), methods_for_plots)
summary_data <- filter(results_df, Method %in% methods_for_summary) %>%
  group_by(Method, Component) %>%
  summarise(BIC = mean(avg_bic, na.rm = TRUE),
            NLL = mean(avg_nloglik, na.rm = TRUE),
            .groups = "drop")
# Rename method
levels(summary_data$Method)[levels(summary_data$Method)=="tch_min"] <- "Minutes"
# Convert to scientific notation
summary_data$BIC <- format(summary_data$BIC, scientific = TRUE, digits = 2)
summary_data$NLL <- format(summary_data$NLL, scientific = TRUE, digits = 2)

# Create tables and remove headers
# bic_table <- summary_data %>% select(Method, BIC) %>%
#   tableGrob(theme = ttheme_minimal(base_size = 8, 
#                                    core=list(bg_params = list(fill = "white", col=NA),
#                                              fg_params=list(fontface=3))),
#              rows = NULL)
# bic_table <- bic_table[-1, ]  # Remove header
bic_data <- filter(plot_data, !is.na(avg_bic)) %>% 
  mutate(value = avg_bic, metric = "BIC")
# BIC Plot
bic_plot <- ggplot(bic_data, aes) + 
  geom_line() + 
  labs(title = "BIC", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none")
  # annotation_custom(bic_table, xmin = max(bic_data$Lag) - 3.5,
  #                   xmax = max(bic_data$Lag) - 0.5,
  #                   ymin = 1.01*max(bic_data$value),
  #                   ymax = Inf)

# nll_table <- summary_data %>% select(Method, NLL) %>%
#   tableGrob(theme = ttheme_minimal(base_size = 8, 
#                                    core=list(bg_params = list(fill = "white", col=NA),
#                                              fg_params=list(fontface=3))), 
#              rows = NULL)
# nll_table <- nll_table[-1, ]  # Remove header
nll_data <- filter(plot_data, !is.na(avg_nloglik)) %>% 
  mutate(value = avg_nloglik, metric = "Negative Log-Likelihood")
nll_plot <- ggplot(nll_data, aes) + 
  geom_line() + 
  labs(title = "Out-of-sample NLL", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none")
  # annotation_custom(nll_table, xmin = max(nll_data$Lag) - 3.5,
  #                   xmax = max(nll_data$Lag) - 0.5,
  #                   ymin = 1.01*max(nll_data$value),
  #                   ymax = Inf)

# Combine the plots and add a legend
combined_plot <- ggarrange(bic_plot, nll_plot,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
combined_plot

```

{{< pagebreak >}}

```{r}
#| eval: false
#| cache: true
#| label: fig-parameters-corr
#| fig-cap: "Correlations between the estimated parameters (Cost 1, Cost 2, Cost 3, Gamma, Alpha, Tau) and the outcome variables (average weekly Badges and average weekly Tower Alerts) across the six different linear regression models. Each circle represents a correlation coefficient; the size and shading of the circles indicate the magnitude and direction of the correlation, respectively. The first correlogram (top) relates to the outcome variable of average weekly Badges, while the second correlogram (bottom) relates to the outcome variable of average weekly Tower Alerts."

# Selecting relevant variables for the first model concerning badges
relevant_data_badges <- classroom_data %>%
  dplyr::select(mean_C1, mean_C2, mean_C3, gamma, alpha, tau, badges)

# Similarly, for the fourth model concerning tower alerts
relevant_data_tower <- classroom_data %>%
  dplyr::select(mean_C1, mean_C2, mean_C3, gamma, alpha, tau, tower_alers)

cor_matrix_badges <- cor(relevant_data_badges, use = "complete.obs")
cor_matrix_tower <- cor(relevant_data_tower, use = "complete.obs")

cor_matrix_badges <- cor(relevant_data_badges, use = "complete.obs")
cor_matrix_tower <- cor(relevant_data_tower, use = "complete.obs")

# For badges
corrplot(cor_matrix_badges, method = "circle")

# For tower alerts
corrplot(cor_matrix_tower, method = "circle")

```


<!--
## Markov Chain Monte Carlo (MCMC) Estimation

### Reinforcement Learning Model Fit

The reinforcement learning models were implemented using the Stan programming language, a probabilistic programming language designed for statistical inference [@stanmod2022; @gabry2022]. We trained the models on data that included the number of weeks, choices made, and outcomes (log badges) for each classroom. The model parameters, including cost, discount rate, learning rate, and inverse temperature, were estimated from the data. The models employed a Bernoulli logit model to compute action probabilities and updated the expected values of the actions based on prediction errors. The models also generated posterior predictions and computed the log-likelihood for each subject.

Stan employs the Hamiltonian Monte Carlo (HMC) algorithm, a state-of-the-art Markov Chain Monte Carlo (MCMC) method that is particularly well-suited for high-dimensional and complex posterior distributions [@betancourt2017]. We specified three independent MCMC chains to check for convergence of the MCMC algorithm by comparing them. Each chain had 2,500 warmup (burn-in) iterations and 2,500 sampling iterations. During the warmup phase, the HMC algorithm adapts its parameters to the shape of the posterior distribution. The samples drawn during the warmup phase were discarded, and the models ran until they achieved convergence, as assessed by the R-hat statistic, which compares the within-chain and between-chain variance of the MCMC samples; values close to 1 indicate that the chains have converged to the same distribution [@gelman1992].

#### Hierarchical RL Method

We extended the base reinforcement learning models by incorporating a hierarchical structure to account for individual commonalities and enhance robustness. This hierarchical framework defines individual-level parameters as random effects from a group-level distribution. We used the parameter values found by the non-hierarchical models to generate weakly informed priors for the hyper-parameters (group-level parameters). This approach was necessary to ensure the rapid convergence of the Hamiltonian Monte Carlo algorithm. We specified the priors as follows:

+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Parameter           | Group-level Prior                                 | Individual-level Prior                                                            |
+=====================+===================================================+===================================================================================+
| Cost                | $\mu_{\text{cost}} \sim \mathcal{N}(0.5, 1)$\     | $\text{cost}_{i,j} \sim \mathcal{N}(\mu_{\text{cost}_j}, \sigma_{\text{cost}_j})$ |
|                     | $\sigma_{\text{cost}} \sim \text{Cauchy}(0, 2.5)$ |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Discount Rate       | $\mu_{\gamma} \sim \mathcal{N}(0.7, 1)$\          | $\gamma_i \sim \mathcal{N}(\mu_{\gamma}, \sigma_{\gamma})$                        |
|                     | $\sigma_{\gamma} \sim \text{Cauchy}(0, 2.5)$      |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Step Size           | $\mu_{\alpha} \sim \mathcal{N}(0.5, 1)$\          | $\alpha_i \sim \mathcal{N}(\mu_{\alpha}, \sigma_{\alpha})$                        |
|                     | $\sigma_{\alpha} \sim \text{Cauchy}(0, 2.5)$      |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Inverse Temperature | $\mu_{\tau} \sim \mathcal{N}(1, 1)$\              | $\tau_i \sim \mathcal{N}(\mu_{\tau}, \sigma_{\tau})$                              |
|                     | $\sigma_{\tau} \sim \text{Cauchy}(0, 2.5)$        |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+

where $\mu$ and $\sigma$ denote the group-level hyperparameters, and the subscript $i$ signifies the individual-level parameters.

### Model Performance

To evaluate the performance of the Bayesian models, we used the Leave-One-Out Information Criterion (LOOIC), a robust measure of model quality. The LOOIC is a variant of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). However, unlike AIC and BIC, which use asymptotic approximations, LOOIC is a fully Bayesian criterion that provides a more accurate estimate of out-of-sample prediction error. We used the `loo` package in R to compute the LOOIC [@vehtari2023]. The package uses Pareto smoothed importance sampling (PSIS), a beneficial technique for models where standard cross-validation is computationally expensive or impractical [@vehtari2017].

```{r Stan Results prep}
#| eval: false
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
gc(verbose = FALSE)
```

```{r}
#| eval: false

# Load the MCMC results
files <- c(
  "Bayesian/Results/Actor-Critic-hierarchical-202403251140-1-914436.csv",
  "Bayesian/Results/Actor-Critic-hierarchical-202403251140-2-914436.csv",
  "Bayesian/Results/Actor-Critic-hierarchical-202403251140-3-914436.csv",
  "Bayesian/Results/Actor-Critic-hierarchical-202403251140-4-914436.csv"
  )
csv_contents <- as_cmdstan_fit(files)
draws_df <- csv_contents$draws()
sum1 <- csv_contents$summary()

library(bayesplot)

# Plotting trace plots for some parameters
color_scheme_set("brightblue")
mcmc_trace(draws_df, pars = c("lp__", "mu_cost", "mu_gamma", 
                              "mu_alpha[1]", "mu_alpha[2]", "mu_tau"),
           n_warmup = 500)

# R-hat statistics (values close to 1 indicate good convergence)
parameter_fit <- sum1 %>%
  filter(grepl("log_lik", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail)
cost_fit <- sum1 %>%
  filter(grepl("cost", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
gamma_fit <- sum1 %>%
  filter(grepl("gamma", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
alpha1_fit <- sum1 %>%
  filter(grepl("alpha\\[1", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
alpha2_fit <- sum1 %>%
  filter(grepl("alpha\\[2", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
tau_fit <- sum1 %>%
  filter(grepl("tau", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
# Combine all fit data frames into a single data frame
fit_list <- list(
  log_lik_fit, 
  cost_fit, 
  gamma_fit, 
  alpha1_fit, 
  alpha2_fit, 
  tau_fit
)
nrow <- min(sapply(fit_list, nrow))
good_fit <- do.call(
  cbind, sapply(fit_list, function(x) {
    apply(x[1:nrow, -1], 2, function(y) all(y < median(y)))
  }
  )
)
good_fit <- apply(
  sapply(fit_list, function(x) {
    apply(
      apply(x[1:nrow, -1], 2, function(y) y < median(y)),
      1, function(z) any(z))
    }),
  1, all)
  

print(rhats)


mcmc_areas(draws_df, pars = c("alpha", "beta"))




stan_data$outcome
observed_data <- c(...) # You need to define this based on your data
predicted_data <- posterior_predict(csv_contents, draws = 100)
bayesplot::ppc_dens_overlay(y = observed_data, yrep = predicted_data)






# Summarizing parameter estimates
parameter_summaries <- as.data.frame(sum1) %>%
  dplyr::select(variable, mean, sd, `50%`, `2.5%`, `97.5%`) %>%
  rename(Median = `50%`, `Lower CI` = `2.5%`, `Upper CI` = `97.5%`)

kable(parameter_summaries) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))



```

### Diagnostics

```{r}
#| eval: false
#| cache: true
#| label: tbl-diagnostics
#| tbl-cap: "Diagnostics for the Bayesian Models used in Q-learning and Q-learning with states. The tables show diagnostics for three Non-negative Matrix Factorization (NMF) models: Frobenius (initialized with a Nonnegative Double Singular Value Decomposition), Frobenius Average (initialized with a Nonnegative Double Singular Value Decomposition with zeros filled with the average of each variable), and Kullback-Leibler. The rows correspond to 'alpha,' the learning rate, 'gamma,' the discount factor, 'tau,' the temperature parameter, 'cost' for each of the components from the NMF, 'sensi,' the reward sensitivity parameter, and 'Log Posterior,' the logarithm of the posterior probability. 'Rhat' indicates the potential scale reduction factor on split chains. Values close to 1.0 imply model convergence. 'ESS Bulk' and 'ESS Tail' represent the effective sample size for the bulk of the posterior distribution and its tail, respectively. An ESS of at least a few hundred is often considered sufficient for estimation purposes."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]
results_files <- results_files[!grepl("hierarchical", results_files)]
diag_df <- data.frame()

for (result_file in results_files) {
  fit <- readRDS(result_file)
  
  # Get summary
  fit_summary <- fit$summary()
  
  # Create a data frame of the Rhat values and ESS
  result_df <- data.frame(
    Model = gsub("Bayesian/Results/||.RDS", "", result_file),
    Variable = fit_summary$variable,
    Rhat = fit_summary$rhat,
    ESS_Bulk = fit_summary$ess_bulk,
    ESS_Tail = fit_summary$ess_tail
  )
  
  # Add result to the overall diagnostics data frame
  diag_df <- rbind(diag_df, result_df)
}
# Reshape the data frame
diag_df <- diag_df %>%
  pivot_longer(cols = c(Rhat, ESS_Bulk, ESS_Tail),
               names_to = "Statistic") %>%
  pivot_wider(names_from = c(Model, Statistic),
              values_from = value) %>%
  mutate(Variable = if_else(Variable == "lp__", "Log Posterior", Variable))

# Split the data into two separate data frames
diag_df_actor_critic <- diag_df[,c(TRUE, grepl("Actor-Critic-", names(diag_df))[-1])]
diag_df_q_learning_states <- diag_df[,c(TRUE, grepl("Q-learning-states-", names(diag_df))[-1])]
diag_df_q_learning <- diag_df[,c(TRUE, (!names(diag_df) %in% names(diag_df_q_learning_states) &
                                          !names(diag_df) %in% names(diag_df_actor_critic))[-1])]

# Define the spanner labels for each model
diag_df_q_learning %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_q_learning_states %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_actor_critic %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

```

### Estimates

This section presents the group parameter estimates for all Reinforcement Learning (RL) models, both non-hierarchical and hierarchical. To generate these group parameter estimates, we first extract the posterior samples from the fitted models and calculate the mean (M), standard deviation (SD), 25th percentile (Q1), and 75th percentile (Q3) for each parameter. @tbl-group-parameters summarizes the distribution of the parameter estimates across classrooms, revealing their typical values and variability.

```{r}
#| eval: false
#| cache: true
#| label: tbl-group-parameters
#| tbl-cap: "Group parameter estimates for the Bayesian Models used in Q-learning and Q-learning with states."

# Define a function that processes one file
process_file <- function(result_file) {
  # Load the fitted model
  fit <- readRDS(result_file)
  
  # Extract the posterior samples of the parameters
  post_samples <- fit$draws()
  
  # Calculate the group parameter estimates
  group_params <- post_samples %>%
    as.data.frame() %>%
    summarise(across(everything(), list(M = mean, SD = sd, Q1 = ~quantile(., 0.25), Q3 = ~quantile(., 0.75))))
  
  # Add the model name to the data frame
  group_params$Model <- gsub("Bayesian/Results/||.RDS", "", result_file)
  
  return(group_params)
}

# Apply the function to each results file and bind the results into one data frame
group_params_df <- purrr::map_dfr(results_files, process_file)

# Continue with the rest of your code
group_params_df %>%
  gt() %>%
  tab_header(
    title = "Group Parameter Estimates",
    subtitle = "The table shows the mean (M), standard deviation (SD), 25th percentile (Q1), and 75th percentile (Q3) of the learning rate (alpha), inverse temperature (beta), weights, and cost parameters for each model."
  ) %>%
  cols_label(
    Model = "Model",
    M = "Mean",
    SD = "Standard Deviation",
    Q1 = "25th Percentile",
    Q3 = "75th Percentile"
  ) %>%
  fmt_number(
    columns = c("M", "SD", "Q1", "Q3"),
    decimals = 2
  )


```


The trade-off between model complexity and predictive accuracy justifies including four lags. Including more lags would increase the complexity of the model, potentially leading to overfitting and poorer predictive performance. Conversely, including fewer lags might result in a model that fails to capture critical temporal dependencies in the data. The selection of four lags represents an inflection point in the BIC and NLL, indicating an optimal balance between model complexity and predictive accuracy.

In @fig-lags, we present the estimated coefficients of the lagged variables as derived from the random effects models. The lines represent the regression coefficients of different variables and their standard errors. The grey line and shaded area correspond to the coefficients for the lagged Badges per Student variable. This graphical representation elucidates the diminishing influence of each variable as the lag increases.

Taking the Frobenius 1 component as an example, the coefficient for the first lag is approximately 0.30, accompanied by a standard error of 0.25. As the lag increases, there is a noticeable decrease in the coefficient, implying a waning influence of this component over time. In contrast, the Badges per Student variable demonstrates a different pattern. First, the magnitude of these coefficients is smaller and non-significant. The coefficient also fluctuates around zero as the lag increases, suggesting a relatively consistent influence over time.

In our pursuit to identify the most parsimonious model that optimally fits the data, we conducted a comparative analysis of the out-of-sample negative likelihood (NLL) across four distinct reinforcement learning models: State-Based Q-Learning, Kernalized Q-Learning, State-Free Q-Learning, and Actor-Critic. We evaluated these models using three different methods for non-negative matrix factorization: Frobenius (NNDSVD), Frobenius (NNDSVDA), and Kullback-Leibler. Each cell in @fig-panel-bic represents the median negative log-likelihood of a model given a specific method, with lower values signifying a superior model fit.

Our analysis reveals that the Kernalized Q-Learning model, when evaluated using the Frobenius (NNDSVD) method, provides the most optimal fit to the data, as evidenced by its lowest negative log-likelihood value. As a result, we select this combination of model and method for our remaining analyses.

```{r}
#| label: tbl-choose-RL-model
#| tbl-cap: "The table presents a comparison of the median negative log-likelihood values for posterior distributions across four reinforcement learning models: State-Based Q-Learning, Kernalized Q-Learning, State-Free Q-Learning, and Actor-Critic. These models are evaluated based on three methods for non-negative matrix factorization: Frobenius (NNDSVD), Frobenius (NNDSVDA), and Kullback-Leibler. Each cell in the table represents the median negative log-likelihood of a model's posterior given a method, with lower values indicating better model fit. The best performing combination of model and method (i.e., the one with the lowest negative log-likelihood value) is highlighted in light green."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]

# Function to extract median of lp__ from a model
extract_lp_median <- function(model_file) {
  fit <- readRDS(model_file)
  lp_median <- fit$summary("lp__", "median")  # Get the median of lp__
  data.frame(Model = gsub("Bayesian/Results/||.RDS", "", model_file),
             NLL_Median = -1 * as.numeric(lp_median))
}

# Apply function to all models and combine into one dataframe
lp_df <- purrr::map_df(results_files, extract_lp_median) %>%
  na.omit()
lp_df$Method <- gsub(".*-", "", lp_df$Model)
lp_df$ModelType <- stringr::str_extract(lp_df$Model, ".*(?=-)")

# Define descriptive names
model_names <- c("Q-learning-states" = "State-Based Q-Learning",
                 "Q-learning-kernel" = "Kernalized Q-Learning",
                 "Q-learning" = "State-Free Q-Learning",
                 "Actor-Critic" = "Actor-Critic")

method_names <- c("FR" = "Frobenius (NNDSVD)",
                  "FRa" = "Frobenius (NNDSVDA)",
                  "KL" = "Kullback-Leibler")

# Apply the descriptive names
lp_df$ModelType <- model_names[lp_df$ModelType]
lp_df$Method <- method_names[lp_df$Method]

# Create the table with models as rows and NMF methods as columns
table_df <- lp_df %>%
  dplyr::select(-Model) %>%
  tidyr::pivot_wider(names_from = Method, values_from = NLL_Median) %>%
  arrange(desc(ModelType)) %>%
  na.omit()

# Remove redundant 'ModelType' label
colnames(table_df)[colnames(table_df) == "ModelType"] <- "Model"

# To highlight the best value in the table
table_df %>%
  gt() %>%
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      columns = names(table_df)[colSums(table_df == min(table_df[,-1])) >= 1],
      rows = rowSums(table_df == min(table_df[,-1])) >= 1
    )
  )

```

We compared our base models, which used logistic regression, and our top-performing reinforcement learning (RL) model, which employed kernelized Q-learning, to identify which best fit the data. @tbl-RL-logit-comp presents the Leave-One-Out Information Criterion (LOOIC) estimates for these models, with lower values indicative of superior model performance.

Our analysis revealed that the hierarchical Q-learning model outperformed the others, as evidenced by its lowest LOOIC value. This finding suggests that reinforcement learning provides a more accurate representation of teacher behavior on Zearn when individual parameters are fitted. The hierarchical logistic regression model followed closely, demonstrating competitive performance. However, the models that did not incorporate a hierarchical structure yielded higher LOOIC values, indicating a lesser fit to the data. These findings highlight the significant heterogeneity in the data and emphasize the value of reinforcement learning models, particularly those with a hierarchical structure, in accurately capturing the dynamics of teacher behavior on Zearn.

```{r Bayesian LOOIC prep}
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py", "get_lag_value")))
gc(verbose = FALSE)
```

```{r}
#| eval: false
#| cache: true
#| label: tbl-RL-logit-comp
#| tbl-cap: "Model comparison using Leave-One-Out Information Criterion (LOOIC). LOOIC values, a measure of model quality, were calculated for each model type. Lower values indicate better model performance. Q-learning models were built using a kernel-based approach. Hierarchical models incorporate a hierarchical structure to account for classroom-level variations."

# Non-hierarchical models
## Q-learning model
# post <- read_rds("Bayesian/Results/Q-learning-kernel-FR.RDS")
post <- read_rds("Bayesian/Results/Q-learning-FR.RDS")
loo_qlearn <- post$loo()$estimates["looic", ]
## Logit
models_nh <- readRDS("Bayesian/Results/logit.RDS")
loo_nh <- models_nh$loo()$estimates["looic", ]
# nll_nh <- lapply(models_nh, log_lik)
# nll_nh <- lapply(nll_nh, mean)

# Hierarchical models
## Q-learning
post_hierarchical <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
loo_qhierarchical <- post_hierarchical$loo()$estimates["looic", ]
## Logit
models_h <- readRDS("Bayesian/Results/logit-hierarchical.RDS")
loo_h <- models_h$loo()$estimates["looic", ]

# Collect LOOIC values
looic_values <- c(loo_qlearn[1],loo_nh[1],
                  loo_qhierarchical[1], loo_h[1])
df_looic <- data.frame(Model = c("Q-learning", "Logistic regression", "Hierarchical Q-learning", "Hierarchical logistic regression"),
                       LOOIC = looic_values)

# Create gt table
df_looic %>%
  gt() %>%
  cols_label(
    Model = "Model",
    LOOIC = "LOOIC Value"
  )

# Create gt table
gt(df_looic)

```

We compared the hierarchical model's predictions and the original choice data, representing a distinct action. We averaged the model predictions across teachers weekly and overlayed them with the average action from the choice data. We also calculated the standard error around these averages accounting for missing data, which provided a measure of uncertainty around these values.

@fig-model-fit shows the model fit for each action where the y-axis represents the probability of a=1, and the x-axis represents the time in weeks. The line plot includes the mean probabilities, highlighted by colored lines, and their respective standard errors, represented by shaded ribbons surrounding the lines. The results section of the data for 'Action 1' revealed that the model predictions initially remained relatively close to the mean probability of 0.5, demonstrating some variability but remaining within a reasonable range. The model's variability increased over time, denoted by the broadening standard error ribbons, peaking at week 36 with a significant increase in the mean predicted probability. This peak corresponded to a drastic decline in the choice data, implying a divergence between the model's predictions and the data towards the end of the observed period.

In the case of 'Action 2' and 'Action 3,' the model prediction started at the mean probability of 0.5 and demonstrated a declining trend over the weeks. Although the declining trend was present in both model fit and data, the model predicted a less drastic decline. The standard error for this action also increased over time, albeit not as dramatically as in the case of 'Action 1', indicating that the model's predictions became more uncertain as time progressed. Despite this variability, the model provided a reasonable fit for the choice data across the weeks for the first months of the study period.

```{r}
#| eval: false
#| cache: true
#| label: fig-model-fit
#| fig-cap: "Model predictions for teacher actions compared with choice data over time. The y-axis denotes the probability of a particular action being taken (a=1), while the x-axis indicates time in weeks. The figure showcases three distinct actions (NMF components). The solid lines represent the weekly averaged model predictions for each action, while the shaded ribbons denote the respective standard errors."
stan_data <- read_rds("./Bayesian/Q-learn-data.RDS")
choice_data <- read_rds("./Bayesian/Results/Q-learning-FR.RDS")
choice_data <- choice_data$summary()

prediction_hierarchical <- read.csv("./Bayesian/Results/prediction_hierarchical.csv") %>%
  filter(grepl("y_pred", variable)) %>%
  dplyr::select(variable, mean)

prediction_hierarchical <- prediction_hierarchical %>%
  mutate(variable = str_extract(variable, "\\[.*\\]"),
         variable = str_replace_all(variable, "\\[|\\]", "")) %>%
  separate(variable, into = c("dim1", "dim2", "dim3"), sep = ",", convert = TRUE)

prediction_hierarchical_3d <- array(dim = c(max(prediction_hierarchical$dim1),
                                            max(prediction_hierarchical$dim2),
                                            max(prediction_hierarchical$dim3)))

for (i in 1:nrow(prediction_hierarchical)) {
  dim1 <- prediction_hierarchical$dim1[i]
  dim2 <- prediction_hierarchical$dim2[i]
  dim3 <- prediction_hierarchical$dim3[i]
  prediction_hierarchical_3d[dim1, dim2, dim3] <- prediction_hierarchical$mean[i]
}

# Loop through each subject
for (subject in seq_len(dim1)) {
  # Get the value of Tsubj for this subject
  Tsubj_value <- stan_data[["Tsubj"]][subject]
  
  # Set the values in prediction_hierarchical_3d to NA
  if (Tsubj_value != max(stan_data[["Tsubj"]])) {
    prediction_hierarchical_3d[subject, (Tsubj_value + 1):dim2, ] <- NA
    # Set the values in choice_data to NA
    choice_data[subject, (Tsubj_value + 1):dim2, ] <- NA
  }
}

# Get the number of layers
num_layers <- dim(prediction_hierarchical_3d)[3]
# Custom function to calculate standard error based on the number of non-NA elements
calc_se <- function(x) sd(x, na.rm = TRUE) / sqrt(sum(!is.na(x)))

df_compare <- data.frame()

# Generate a plot for each layer
for (k in 1:num_layers) {
  y_pred_avg <- apply(prediction_hierarchical_3d[, , k], 2, mean, na.rm = TRUE)
  y_pred_se  <- apply(prediction_hierarchical_3d[, , k], 2, calc_se)
  
  choice_data_avg <- apply(choice_data[, , k], 2, mean, na.rm = TRUE)
  choice_data_se  <- apply(choice_data[, , k], 2, calc_se)
  
  # Only consider weeks with valid SEs
  weeks <- seq_len(sum(!is.na(y_pred_se)))

  df_pred <- data.frame(weeks = weeks,
                        probability = y_pred_avg[weeks],
                        type = rep("Model Fit", max(weeks)),
                        se = y_pred_se[weeks],
                        action = rep(paste("Action", k), max(weeks)))

  df_real <- data.frame(weeks = weeks,
                        probability = choice_data_avg[weeks],
                        type = rep("Real Data", max(weeks)),
                        se = choice_data_se[weeks],
                        action = rep(paste("Action", k), max(weeks)))

  df_compare <- rbind(df_compare, df_pred, df_real)
}

p <- ggplot(df_compare, aes(x = weeks, y = probability, color = type)) +
  geom_line() +
  geom_ribbon(data = df_compare, aes(ymin = probability - se, ymax = probability + se, fill = type), alpha = 0.1) +
  labs(x = "Week", y = "Probability of a=1") +
  facet_wrap(~action, ncol = 1) +
  scale_color_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  scale_fill_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  theme_bw()

print(p)

```

```{r}
#| eval: false

logit_model <- readRDS("./Bayesian/Results/logit-hierarchical.RDS")
pred_logit_model <- lapply(logit_model, function(model) {
  fitted(model)
})
data_logit_model <- lapply(logit_model, function(model) {
  model$data[,1]
})
logit_pred_3d <- array(NA, dim = dim(prediction_hierarchical_3d))
choice_data_3d <- array(NA, dim = dim(prediction_hierarchical_3d))

# Loop through each subject
for (subject in seq_len(dim1)) {
  Tsubj_value <- stan_data[["Tsubj"]][subject]
  
  # Calculate the starting and ending indices for the current subject
  start_idx <- ifelse(subject == 1, 1,
                      sum(stan_data[["Tsubj"]][1:(subject-1)]) - subject + 2)
  end_idx <- sum(stan_data[["Tsubj"]][1:subject]) - subject
  
  # Loop through each model in pred_logit_model
  for (model_idx in seq_along(pred_logit_model)) {
    # Extract the estimated probabilities for the current model
    model_predictions <- pred_logit_model[[model_idx]][start_idx:end_idx,"Estimate"]
    real_data <- data_logit_model[[model_idx]][start_idx:end_idx]

    # Populate logit_pred_3d with the predicted values
    logit_pred_3d[subject, 2:(Tsubj_value), model_idx] <- model_predictions
    choice_data_3d[subject, 2:(Tsubj_value), model_idx] <- real_data
  }
}

# Generate a plot for each layer
df_compare <- data.frame()
for (k in 1:dim3) {
  y_pred_avg <- apply(logit_pred_3d[, , k], 2, mean, na.rm = TRUE)
  y_pred_se <- apply(logit_pred_3d[, , k], 2, calc_se)
  
  choice_data_avg <- apply(choice_data_3d[, , k], 2, mean, na.rm = TRUE)
  choice_data_se  <- apply(choice_data_3d[, , k], 2, calc_se)

  # Only consider weeks with valid SEs
  weeks <- seq_len(length(y_pred_se))[!is.na(y_pred_se)]
  idx_shift <- weeks[1] - 1

  df_pred <- data.frame(weeks = weeks,
                        probability = y_pred_avg[weeks],
                        type = rep("Model Fit", length(weeks)),
                        se = y_pred_se[weeks],
                        action = rep(paste("Action", k), length(weeks)))

  df_real <- data.frame(weeks = weeks,
                        probability = choice_data_avg[weeks],
                        type = rep("Real Data", length(weeks)),
                        se = choice_data_se[weeks],
                        action = rep(paste("Action", k), length(weeks)))

  df_compare <- rbind(df_compare, df_pred, df_real)
}

p <- ggplot(df_compare, aes(x = weeks, y = probability, color = type)) +
  geom_line() +
  geom_ribbon(data = df_compare, aes(ymin = probability - se, ymax = probability + se, fill = type), alpha = 0.1) +
  labs(x = "Week", y = "Probability of a=1") +
  facet_wrap(~action, ncol = 1) +
  scale_color_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  scale_fill_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  theme_bw()

print(p)

```


-->

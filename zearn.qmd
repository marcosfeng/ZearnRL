---
title: "Unveiling Adaptive Pedagogy: Reinforcement Learning Models Illuminate Teacher Decision-Making in an Online Math-Teaching Platform"
abstract: "In the rapidly evolving landscape of education, understanding the decision-making process of teachers is crucial. Based on a sample of over 2,000 classrooms from the online math-teaching platform Zearn, this study leverages reinforcement learning (RL) algorithms to model how teachers learn and adapt pedagogical choices. We conceptualize the teacher's role as a multi-armed bandit problem, where teachers balance their weekly effort against the number of lessons their students complete. This exploration-exploitation trade-off is dynamic, with teachers continually learning and adapting their strategies. Our findings reveal that teachers who favor exploration tend to be more adaptive and responsive to student needs, leading to a more personalized and effective learning experience. In contrast, those who lean towards exploitation often rely on tried-and-true methods, resulting in consistent but potentially less innovative teaching strategies. This work underscores the potential of RL in providing insights into human behavior in real-world settings, with significant implications for policy and practice in education."
keywords: "Reinforcement Learning, Pedagogical Decision-Making, Digital Education Platforms, Teacher Behavioral Dynamics, Empirical Field Data, Exploration-Exploitation Dilemma, Instructional Adaptation"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
format:
  elsevier-pdf:
    keep-tex: true
    include-in-header:
      - text: |
          \usepackage{dcolumn}
          \usepackage{typearea}
    journal:
      formatting: preprint
      model: 3p
      layout: onecolumn
      cite-style: number
      # graphical-abstract: "![](abstract.png)"
bibliography: zearnrefs.bib

execute:
  echo: false
  warning: false
  error: false
  
knitr:
  opts_chunk:
    cache.extra: set.seed(832399554)
---

```{r load packages}
library(data.table)
library(tidyverse)
library(ggpubr)
library(ggforce)
library(ggrepel)
library(gridExtra)
library(scales)
library(gtsummary)
library(gtExtras)
library(PerformanceAnalytics)
library(doParallel)
library(foreach)
library(reticulate)
library(cmdstanr)
library(brms)
```

```{r}
set.seed(832399554)
random_py <- reticulate::import("random")
random_py$seed(832399554)
# https://www.random.org/integers/
# Timestamp: 2023-05-17 16:18:28 UTC
```

# Introduction

<!-- Needs around 5 more studies here. -->

Predicting repeated behavior has been a long-standing goal of the behavioral sciences, including economics, psychology, and neuroscience. Much of human behavior results from stimulus-response associations, which are context-sensitive and not always consciously deliberated [@buyalskaya2023]. Reinforcement learning (RL) algorithms have emerged as a prominent way of quantifying these relationships, assigning a mathematical relationship between contextual cues (states), behavior (actions), and reward [@sutton2018]. These algorithms have found wide application in neuroscience and cognitive psychology, where they are used on data sets to model agents in specific environments. However, these disciplines generally do not work with the practical and applied data typically used in social psychology and economics [@buyalskaya2023].

This gap presents a novel opportunity to use methods from one set of disciplines on data traditionally used in another. In this paper, we aim to further this integration by applying RL algorithms to model the decision-making process of teachers in the math-teaching platform Zearn. Reinforcement learning (RL) provides a system of rewards and punishments where the agent (in this case, the teacher) learns to make optimal decisions by maximizing the rewards and minimizing the punishments. By assuming every teacher has an objective function to balance with their potential rewards, we model the sequential behavior of a teacher throughout a school year. For instance, the teacher chooses which pedagogical actions to employ, such as assigning homework, checking student progress, or reviewing content, in anticipation of enhancing student achievement. Applying the RL algorithm allows for flexibility in learning the best strategy given certain contextual information, revealing the intricate dynamics of the teaching-learning process. By modeling the tradeoff between teachers exploring unknown options and exploiting known information about the Zearn system, the RL algorithm provides a nuanced understanding of how teachers adapt their strategies in response to student performance and other contextual factors. This approach offers a flexible and robust model for our available data and opens new avenues for understanding and enhancing human behavior in field settings.

## The Zearn Platform

Zearn is a digital platform for mathematics education designed to facilitate the teaching and learning of mathematics, providing a rich environment for modeling the decision-making process of teachers. About 25% of elementary-school students and over 1 million middle-school students across the United States use Zearn [@post-weblog]. The platform's unique blend of hands-on teaching and immersive digital learning offers a fertile ground for understanding how teachers adapt their strategies to optimize student achievement.

Zearn's pedagogical approach includes interactive digital lessons using visual aids (see @fig-zearn-poster) and real-time student feedback. The platform's approach to mathematical concepts, such as fractions, is particularly noteworthy. Students go through a series of representations: concrete, pictorial, and abstract, each designed to scaffold (i.e., "breaking down" problems, see [@jumaat2014; @reiser2014]) their understanding and prepare them for subsequent levels. This approach aligns with the Common Core State Standards, enhancing relevance in the contemporary educational landscape.

![Example of Teaching with Visual Models on Zearn](images/zearn-poster.jpg){#fig-zearn-poster fig-align="center"}

The platform's structure facilitates a personalized learning experience for students (see SI for a screenshot of the student portal), allowing teachers to track student progress and make informed decisions (see @fig-class-report for a sample class report). This structure includes self-paced online lessons and small group instruction, a rotational model that allows students to learn new grade-level content in two ways: independently through engaging digital lessons and in small groups with their teacher and classmates. This dual approach enables students to learn at their own pace, fostering a sense of autonomy and self-directed learning.

![Sample Class Report](images/class-report.png){#fig-class-report fig-align="center"}

A key feature of Zearn is its badge system, which tracks student progress and motivates continued learning (see @fig-badges-screen). Students earn badges upon mastery of specific skills, providing a tangible representation of their achievement. This system motivates students and provides teachers with valuable data on student performance, informing their decision-making process [@knudsen2020]. Zearn also incorporates notifications, known as Tower Alerts, sent to teachers when a student struggles with a specific concept. This feature allows teachers to provide timely support and address learning gaps, enhancing the platform's capacity for personalized learning.

![Badge System for Student Achievement](images/badges.PNG){#fig-badges-screen fig-align="center"}

Another noteworthy aspect is the platform's professional development component, which is available for schools with a paid account (see SI for a sample training schedule). Teachers explore each unit or mission through word problems, fluencies, and small group lessons, conducting collaborative analysis of student work and problem-solving strategies. This professional development revolves around each mission's big mathematical idea, visual representations to scaffold learning, and strategies to address unfinished learning from prior grades and preparation for future learning [@morrison2019].

In order to model interesting behavioral patterns, we focus our analysis on the teachers who most likely take advantage of a wide range of resources on the platform. Thus, we select teachers who consistently use the platform and work in traditional school settings.

The platform encapsulates a holistic methodology in mathematics education, intertwining a hybrid curriculum that leverages print and digital mediums, a rotational pedagogical model, targeted professional development programs, and detailed analytics at the classroom and institutional level to inform teaching and learning practices. This integrated framework furnishes a rich repository of data for comprehensive analysis. The variables delineated for investigation by the Zearn consortium encompass a spectrum of dimensions: (1) teacher engagement, quantified through a diverse set of actions; (2) the completion of lessons, denoted by the acquisition of badges; (3) student struggles, monitored through tower alerts; and (4) the amount of time spent on the platform by both educators and learners.

<!--Lit review here -->

## Research Questions

We propose the following research questions to explore the dynamics of teacher behavior, the role of reinforcement learning in modeling these behaviors, and the subsequent impact on student achievement. We hope to shed light on the potential of reinforcement learning for understanding human behavior in field data.

1\. Characterizing Teacher Behavior: Can reinforcement learning models fit teacher behavior better than a logistic regression model? Which reinforcement learning model most accurately characterizes the behavior of teachers in the context of Zearn Math? What insights about teacher behavior do these differences in model fit bring?

2\. Impact of RL Parameters on Teacher Behavior and Student Achievement: How do the parameters of reinforcement learning models vary with teacher behavior? What effects do these variations have on student achievement?

3\. Influence of Teacher Background, Training, and Experience: How do factors such as teacher background, training, and experience influence their adaptation to and implementation of the Zearn Math curriculum? How can these factors be effectively incorporated into the reinforcement learning model to provide a more comprehensive understanding of teacher behavior and its impact on student outcomes?

# Theory

## Education production function

<!-- Needs citation here. -->

Previous research has explored how teacher effort affects student achievement. In particular, economists have studied the "education production function," in which educational outcomes are a function of various inputs, including teacher effort, student effort, school resources, and family background [@hedges1994]. This function serves as a theoretical framework for understanding how different factors contribute to educational achievement and how interventions can be designed to improve outcomes.

One of the key inputs in the education production function is teacher effort, as teachers play a crucial role in shaping students' learning experiences and outcomes. Their effort, which encompasses their time, energy, dedication, and instructional strategies, can significantly influence students' academic achievement [@rivkin2005]. However, measuring teacher effort and its impact on student outcomes can be challenging due to the complex and multifaceted nature of teaching.

To address this challenge, researchers have employed various strategies to identify the effects of teacher effort on student achievement. One common approach is to manipulate the conditions under which teachers operate, thereby changing the levels of teacher effort. For instance, Duflo, Dupas, and Kremer (2011) [@duflo2011] conducted a randomized controlled trial in Kenya to examine the effects of tracking, a practice of grouping students based on their ability levels. Tracking has been a prominent tool in the sociology of education and assumes that teacher inputs will depend on the ability level of the students. This tailoring of teaching strategies and content potentially enhances the instructor's effectiveness.

However, traditional social science approaches to studying the education production function often lack the flexibility to account for changes in context and experience and individual-level differences. Reinforcement learning (RL) offers a promising alternative approach. RL models incorporate a flexibility term (i.e., a learning rate) that allows for changes in behavior with experience and an exploration versus exploitation term (e.g., inverse temperature) that captures individual differences in decision-making strategies. Furthermore, the RL framework inherently accounts for the process of learning about rewards, making it a flexible and dynamic tool for studying teacher behavior [@sutton2018].

## Reinforcement Learning to Capture Patterns in Repeated Behavior

In RL, an agent learns to make decisions over time to maximize a cumulative reward. At the heart of RL is the concept of a policy, which is a mapping from states to actions or, more commonly, a probability distribution over actions [@sutton2018]. The agent's goal is to learn an optimal policy, which maximizes the expected cumulative reward, even when the parameters of the environment are not known a priori.

The application of RL in the context of education and teaching is not new. One of the pioneers in the field of Markov decision processes, Ronald Howard, attempted to apply his mathematical framework to instruction theory as early as 1960 [@howard1960]. Later, in 1972, Richard Atkinson proposed a theory of instruction that encapsulates the key components of a Markov decision process, including states, actions, transition probabilities, reward functions, and a time horizon [@atkinson1972]. In Atkinson's framework, actions are instructional activities (e.g., assigning problem sets) that can change a given state (e.g., student learning level). These changes in states can yield rewards minus the associated cost of the action. For example, a teacher may be rewarded with an increase in the knowledge or skill of a student, but such reward must be balanced with its associated effort (e.g., labor cost). Atkinson and colleagues continued to test many parametrizations of this idea, contributing significantly to the development of RL theory in the context of education (see [@doroudi2019] for a full review).

More recently, RL models have been used in the psychology of habit to explain learning and reward association. One common approach in human studies is to apply the "multi-armed bandit" task. In this type of experiment, participants are presented with multiple actions, each with an unknown payoff. The subject's goal is to learn the best outcome through trial and error. In the beginning, the reward-action relationships are unknown, so the participant must explore or sample each action [@sutton2018]. This exploration-exploitation trade-off is a central theme in RL and has the potential to provide valuable insights into how students learn and make decisions over time.

### Why Reinforcement Learning?

Reinforcement Learning (RL) presents a paradigm shift from traditional economic models, which often employ a static approach to link teacher efforts with student outcomes, to a dynamic framework reflecting educational interactions' evolving nature. Traditional models typically rely on a fixed education production function. In contrast, RL embodies the flexibility to adapt and evolve strategies over time, mirroring the continuous learning process seen in biological systems. Inspired by the adaptive learning processes observed in animals, RL positions teachers as agents who navigate their environment (i.e., the classroom) by taking actions based on their observations and the feedback received. This process allows for a detailed understanding of the interplay between teacher actions, the classroom environment, and the outcomes of these interactions, as such:

$$
\text{Agent} \xrightarrow[\text{Actions}]{\text{Performs}} \text{Environment} \xrightarrow[\text{Observations, Rewards}]{\text{Provides}} \text{Agent}
$$

Further, RL algorithms also enable the modeling of individual teachers' decision-making processes. This approach facilitates the creation of detailed profiles for instructors, understanding how they adapt and respond to various states and rewards within the educational setting. The flexibility and robustness of RL make it an ideal tool for adapting to changing learning environments and addressing individual teacher and classroom needs. By incorporating a wide range of variables (i.e., states, actions, and rewards), RL models are customizable to diverse educational contexts and objectives. They can also account for the inherent uncertainties in teaching, guiding the formulation of optimal decision-making strategies under uncertain conditions.

The analytical capabilities of RL extend to identifying variations in teacher learning and behavior and how these differences influence student outcomes. By estimating individual teacher parameters, RL provides insights into aspects such as teacher flexibility, enabling targeted interventions by policymakers to enhance educational outcomes. Conducting "counterfactual analyses" opens avenues for innovative educational interventions. Beyond immediate study goals, RL models hold the potential for automating instructional decisions based on identified patterns, potentially alleviating the workload on teachers and optimizing the educational process.

<!-- ### Previous Applied RL Cases -->

<!-- -   Discuss existing models and where they fall short, justifying your approach. -->

<!-- -   Introduce the concept of data-driven exploration in RL models. -->

### Q-Learning Model

The first class of models we apply to our data is the so-called Q-learning algorithm. Q-learning is a model-free reinforcement learning algorithm designed to learn a policy, which tells an agent what action to take under what circumstances [@watkins1992]. It does not require a model of the environment (hence the connotation "model-free"), and it can handle problems with stochastic transitions and rewards without requiring adaptations. This model is inspired by the so-called "multi-armed bandit" problem. In this paradigm, an agent has a finite number of choices, each associated with a given reward. The agent must simply learn to choose which action yields the highest reward. Learning in this setting occurs by adjusting expectations and minimizing "surprises" (i.e., prediction errors). Notice that this setting does not require us to define a given state: in our simplest model, Q-learning assumes that the best action in one week is the best action at any other week. Thus, the model only prescribes an action-reward relationship. The teacher here learns the value of their action regardless of the history of their classroom or students.

In the context of Q-learning, the agent interacts with the environment to gain experience and uses this experience to update its knowledge about the quality of particular actions, given the state of the environment. This knowledge is represented in the Q-values, a prediction of the future reward expected after taking an action in the current state (which in our context can be compared to subjective value or a utility). The goal of Q-learning is to accurately learn these Q-values by updating them iteratively [@rummery].

The Q-learning model is based on the concept of a Q-value function of both state and action representing the expected future reward for taking a particular action in a particular state. The Q-value is updated iteratively using the Bellman equation, which expresses the value of a state-action pair in terms of the immediate reward plus the discounted value of the best future state-action pair.

The Q-function, denoted as $Q(s, a)$, is defined for all state-action pairs $(s, a)$, where $s$ is the state and $a$ is the action. The Q-function represents the expected return or future reward for taking action $a$ in state $s$ following a certain policy $\pi$. The Q-function is updated iteratively using the Bellman equation as follows:

$$
Q_\text{new}(s, a) = Q(s, a) + \alpha \delta
$$ {#eq-q-learn}

where $\alpha$ is the learning rate, which determines how much the Q-value is updated based on $\delta$, the reward prediction error. The reward prediction error is the difference between the estimated Q-value and the observed reward plus the discounted future Q-value. This error is used to update the Q-value in the direction of the observed reward, as follows:

$$
\delta = r + \gamma \max_{a'} Q(s', a') - Q(s, a)
$$ {#eq-RPE}

where:

-   $r$ is the immediate reward received after taking action $a$ in state $s$,

-   $\gamma$ is the discount factor,

-   $s'$ is the new state after taking action $a$,

-   $a'$ is the action to be taken in the new state $s'$,

-   $s$ is the current state,

-   $a$ is the action taken,

-   $\max_{a'} Q(s', a')$ is the maximum reward that can be obtained in the next state $s'$,

-   $Q(s, a)$ is the current estimate of the Q-value for action $a$ in state $s$.

The Q-learning algorithm uses this update rule to learn the Q-function and, hence, the optimal policy. The agent starts with an initial Q-function (which can be arbitrary) and then updates the Q-values based on the experiences it gathers from interactions with the environment. The update rule is applied every time the agent transitions from a state $s$ to a state $s'$ by taking an action $a$ and receiving a reward $r$. The agent selects actions based on a policy derived from the Q-values. A common choice is the softmax action selection method, which is a way to balance exploration and exploitation. The softmax method chooses actions probabilistically based on their Q-values. The softmax function determines the probability of choosing a particular action and is defined as follows:

$$
P(a) = \frac{e^{Q(s, a)/\tau}}{\sum_{a'} e^{Q(s, a')/\tau}}
$$ {#eq-softmax}

where:

-   $P(a)$ is the probability of choosing action $a$,

-   $Q(s, a)$ is the Q-value of action $a$ in state $s$,

-   $\tau$ is a parameter known as the temperature, which controls the level of exploration,

-   the denominator is the sum over all possible actions $a'$ of the exponential of their Q-values divided by the temperature.

One possible interpretation of the temperature parameter $\tau$ is the control of the trade-off between exploration and exploitation. When $\tau$ is high, the agent explores more because the action probabilities are more uniform. When $\tau$ is low, the agent exploits more because the action with the highest Q-value is more likely to be chosen than the others. As the agent learns, it can be beneficial to start with a high temperature to encourage exploration and then gradually decrease it to favor the exploitation of the learned policy.

### State-Free vs. State-Based Models

So far, we have defined state-based models incorporating the environment into the Q-function. In state-free models, the Q-function, denoted as $Q(a)$, is solely a function of the action, $a$. The environmental state does not factor into the decision-making process. This simplifying assumption can be beneficial in scenarios where the environmental state exerts minimal influence on the outcome of the action. The agent learns a global policy that is independent of the specific state. This approach can be effective in environments with low state-action complexity or when the state is difficult to define or observe [@sutton2018]. The Q-value update function, therefore, simplifies to:

$$
Q(a) = Q(a) + \alpha \left[ r(a) - Q(a) \right]
$$ {#eq-state-free}

where:

-   $Q(a)$ is the current estimate of the Q-value for action $a$,

-   $\alpha$ is the learning rate,

-   $r(a)$ is the immediate reward received after taking action $a$.

In this equation, the term in the brackets, $r(a) - Q(a)$, is the reward prediction error. It represents the difference between the observed reward and the current estimate of the Q-value. The Q-value is updated in the direction of this error, scaled by the learning rate $\alpha$.

### The Actor-Critic Model

The next model we attempt to fit divides the action selection and action evaluation tasks into two components: the "actor" and the "critic" [@sutton2018b]. This division theoretically allows for more efficient learning, as the critic guides the actor's learning process.

The "actor" in this model selects actions based on a policy function, denoted as $\pi(a|s)$, which maps states to actions, determining the probability of taking each action in each state. The actor aims to learn an optimal policy that maximizes the expected cumulative reward. In our setting, we set the policy as a softmax function over action preferences:

$$
\pi(a|s) = \frac{e^{h(a, s)}}{\sum_{a'} e^{h(a', s)}}
$$

where $h(a, s)$ is the preference for action $a$ in state $s$.

The "critic," on the other hand, evaluates the actions taken by the actor by learning a value function, denoted as $V(s)$. Given the actor's current policy, the critic estimates the expected cumulative reward from each state. The critic's feedback, in the form of the value function, guides the actor's learning. We update the value function based on the Temporal Difference (TD) error, a measure of the difference between the estimated and actual return:

$$
\delta = r + \gamma V(s') - V(s)
$$

where $r$ is the reward, $\gamma$ is the discount factor, $s'$ is the new state, and $s$ is the current state.

This separation of action selection and evaluation distinguishes the Actor-Critic model. In Q-learning, a single Q-function selects and evaluates actions. Conversely, in the Actor-Critic case, the actor updates its policy to increase the probability of actions that lead to higher-than-expected returns and decrease the probability of actions that lead to lower-than-expected returns.

## Applying RL Models to the Zearn Context

In this study, we propose an application of the Reinforcement Learning (RL) algorithms for modeling teacher decision-making within the Zearn platform.

Consider a typical teaching scenario: the state is the students' current progress in the class, and the actions are a range of pedagogical strategies, such as assigning additional practice, providing personalized feedback, or adjusting lesson plans. Some relevant reward variables include improved student performance, increased student engagement, or reduced learning gaps. We represent this interaction as:

$$
State (S) \xrightarrow[\text{Action (A)}]{\text{Teacher Decides}} New State (S') \xrightarrow[\text{Reward (R)}]{\text{Resulting Outcome}} \text{Feedback}
$$ {#eq-zearn-RL}

In the Zearn context, we define the decision process as follows:

1.  Agents are the teachers.

2.  Actions include the choice of specific pedagogical strategies.

3.  The environment is the Zearn platform with its students.

4.  The state is the week-over-week change in student performance and/or struggle.

5.  The reward is a linear function of student performance.

Mathematically mapping the agent-environment interaction is flexible, with many models potentially satisfying our initial assumptions. We approach this problem as a competition of models, selecting a set of models applicable to our setting, fitting them to the data, and comparing their performances.

# Material and Methods

Our comparative analysis across these methods aimed to identify an optimal representation of our dataset, using principles of methodological triangulation to mitigate the limitations inherent to any single approach.

+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------+
| **Step**                 | **Method**                                                                  | **Software/Tools**                                   |
+==========================+=============================================================================+======================================================+
| Data Preprocessing       | Cleaning, normalization                                                     | R                                                    |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------+
| Dimensionality Reduction | Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF) | Python (`scikit-learn`) \|                           |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------+
| Feature Selection        | Regression analysis                                                         | R (`fixest` package) \|                              |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------+
| Analytical Methods       | Q-learning, Actor-Critic Model Estimation                                   | R, Matlab (Laplace approximation with `CBM` package) |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------+
| Statistical Analysis     | Bayesian inference                                                          | R (`Stan` package)                                   |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------+
| Model Evaluation         | Hetereogeneity analyses of model performance across teachers                | R                                                    |
+--------------------------+-----------------------------------------------------------------------------+------------------------------------------------------+

## Data

Our data from the Zearn platform follows a time-series structure, spanning across an academic year, with the unit of analysis being the classroom-week. This level of granularity enables us to capture the temporal dynamics of teacher-student interactions and their subsequent influence on student achievement. In particular, we can model the decision-making process of teachers as they allocate their time and effort on the Zearn platform weekly and how these decisions translate into student outcomes, measured by the completion of lessons or "badges."

```{r data prep}

dt <- read.csv(file = "Data/df_clean.csv")

# Convert columns to appropriate data types
setDT(dt)
# Rename variable
dt[, `:=`(
  poverty = factor(poverty, ordered = TRUE, exclude = c("", NA)),
  income = factor(income, ordered = TRUE, exclude = c("", NA)),
  st_login = fifelse(Minutes.per.Active.User > 0, 1, 0, na=0),
  tch_login = fifelse(User.Session > 0, 1, 0, na=0)
  # Log Transform
  # Badges.per.Active.User = log(Badges.per.Active.User + 1),
  # Tower.Alerts.per.Tower.Completion = log(Tower.Alerts.per.Tower.Completion + 1),
  # tch_min = log(tch_min + 1)
)]

# Code datetime variables and compute additional metrics
setorder(dt, Teacher.User.ID, year, week, Classroom.ID)
dt[, isoweek := week]
dt[, week := week + 52*(year - 2019)]
# Fixing week == 1 (last week of 2019 counts as week 1 of 2020)
dt[week == 1, week := week + 52]
dt[, first_week := min(week), by = .(Teacher.User.ID)]
dt[, week := week - first_week + 1]
dt[, Tsubj := max(week), by = .(Classroom.ID)]

# Update the Grade.Level values and labels
dt <- dt[!(Grade.Level %in% c(-1, 11))] # Ignore -1 and 11
dt[, Grade.Level := factor(Grade.Level,
                           ordered = TRUE,
                           exclude = c(""))]
dt[, Grade.Level := factor(Grade.Level,
                           levels = c(0:8),
                           labels = c("Kindergarten", "1st", "2nd",
                                      "3rd", "4th", "5th",
                                      "6th", "7th", "8th"))]

df <- as.data.frame(dt) %>%
  ungroup()

# Convert year and isoweek to a date (Monday of that week)
df <- df %>%
  mutate(date = as.Date(paste(year, isoweek, 1, sep="-"), format="%Y-%U-%u"))

```

Zearn provided administrative data for teachers and students at a granular level. Teacher activity was time-stamped to the second and included the time spent on the platform and specific actions taken. On the other hand, student data was aggregated at the classroom-week level due to data privacy considerations. This aggregation included a variety of potentially interesting variables capturing student achievement (e.g., student lesson completion or "badges") and level of student struggle (e.g., tower alerts). These variables form the basis of our reinforcement learning models, providing a comprehensive view of the dynamics of teacher-student interactions on the Zearn platform.

Representing a broad spectrum of Zearn's user demographics, the dataset details `r length(unique(df$Classroom.ID))` classrooms and `r length(unique(df$Teacher.User.ID))` educators, with an average of `r round(mean(df$Students...Total, na.rm = T), 1)` students per classroom setting. The school summary statistics, as presented in @tbl-summary, provide a snapshot of the schools' characteristics. The table reveals the average number of teachers, students, and weeks of data per school. The bimodal distribution of weekly data per classroom, as showcased in @fig-classroom-weeks, reveals that some classrooms have less than 3 to 4 months of data. The classrooms with less than 16 weeks of data are color-coded in red. The pattern of student logins over time, particularly around significant holidays as seen in @fig-logins-week, underscore the temporal fluctuations in platform engagement.

```{r}
#| label: fig-classroom-weeks
#| fig-cap: "Total number of weeks of data per classroom."

# Create the histogram
df %>%
  group_by(Classroom.ID) %>%
  summarize(Tsubj = n()) %>%
  mutate(Tsubj_category = if_else(Tsubj < 18, "less than 18", "18 or more")) %>%
  ggplot(aes(x = Tsubj, fill = Tsubj_category)) +
  geom_histogram(color = "black", breaks = seq(min(df$Tsubj, na.rm = T),
                                               max(df$Tsubj, na.rm = T) + 1,
                                               by = 2)) +
  geom_vline(xintercept = 17, color = "darkgray",
             linetype = "dashed", linewidth = 0.8) +
  annotate("text", x = 10, y = 4000, label = "Excluded\nClassrooms",
           vjust = 1, color = "red") +
  labs(title = "Histogram of Total Number of Weeks",
       x = "Total Number of Weeks",
       y = "Frequency") +
  scale_fill_manual(values = c("less than 18" = "red",
                               "18 or more" = "steelblue")) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(1, seq(5, max(df$Tsubj, na.rm = T), by = 5)))

```

```{r}
#| label: fig-logins-week
#| fig-cap: "Total number of student logins over time."

# Calculate the sum of login values by date and Teacher.User.ID
login_data <- df %>%
  group_by(date, Teacher.User.ID) %>%
  summarize(tch_login = max(tch_login),
            st_login  = max(st_login)) %>%
  group_by(date) %>%
  summarize(tch_logins = sum(tch_login),
            st_logins  = sum(st_login))
# Create bar plot
bar_plot <- ggplot() +
  geom_bar(data = login_data, aes(x = date, y = st_logins), stat = "identity") +
  # geom_point(data = login_data, aes(x = date, y = tch_logins), color = "blue") +
  labs(
    title = "Mean of Logins Across Teachers' Classrooms",
    x = "Week",
    y = "Total Logins"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12)) +
  scale_x_date(date_breaks = "3 week", date_labels = "%Y-%m-%d")

# Add labels for Christmas and Thanksgiving
bar_plot +
  geom_text(aes(x = as.Date("2019-12-25"), y = 1250, label = "Christmas"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "red") +
  geom_text(aes(x = as.Date("2019-11-25"), y = 1250, label = "Thanksgiving"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "darkorange")

```

### Geographic Context and Distribution

The data encompasses a diverse range of Louisiana schools, providing a fascinating context for our study. This region presents a unique educational landscape and widespread adoption of the Zearn platform across its schools. @fig-teachers-map depicts the geographic distribution of teachers using the Zearn platform across various parishes in Louisiana. This map visually represents the number of teachers per parish, revealing a heterogeneous distribution of Zearn usage across the state, with certain parishes demonstrating a higher concentration of teachers. The map also highlights the top five cities with the highest number of teachers using Zearn.

The dataset's socio-economic dimensions, illustrated in @fig-proportions, furnish a backdrop critical for interpreting the influence of external factors on educational engagement and achievement. Summary statistics elucidate the average profiles of participating schools, while the proportional distribution across varying poverty levels and income brackets contextualize the schools within Louisiana's broader socio-economic landscape.

```{r}
#| cache: true
#| label: fig-teachers-map
#| fig-cap: "Geographical distribution of teachers across various parishes in Louisiana, and the top 5 cities with the highest number of teachers."

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
# Set the library path to where proj.db is located
# Sys.setenv(PROJ_LIB = "")
unique_zipcodes <- unique(dt$zipcode) %>%
  as.list()
plan(strategy = "multisession", workers = availableCores())
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
dt <- merge(dt, address_geodata,
            by.x = "zipcode",
            by.y = "postalcode",
            all.x = TRUE)

# Aggregate the data to get the number of teachers in each county
dt_map <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID)
), by = .(county)]

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID),
  lat = mean(lat),
  long = mean(long)
), by = .(city, county)]
# Get the top 5 cities with the most teachers
top_cities <- top_cities[order(-num_teachers)][1:5,]

# Get the Louisiana county map data
df_map <- tigris::counties(cb = TRUE,
                           resolution = "20m",
                           class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    as.data.frame(dt_map),
    by = c("NAMELSAD" = "county")
  ) %>%
  sf::st_as_sf()

ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers", low = "white", high = "red", na.value = "gray90") +
  labs(
    title = "Number of Teachers by Parish in Louisiana"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city), size = 3, color = "black")
```

```{r}
#| label: tbl-summary
#| tbl-cap: "Summary statistics for the schools"

df_summary <- df %>%
  group_by(MDR.School.ID) %>%
  summarise(
    Unique_Teacher_Count = n_distinct(Teacher.User.ID)
  ) %>%
  left_join(df %>%
              group_by(Classroom.ID, MDR.School.ID) %>%
              summarise(
                Students_Total = mean(Students...Total, na.rm = TRUE),
                Weeks_Total = max(week)
                ) %>%
              group_by(MDR.School.ID) %>%
              summarize(
                Students_Total = sum(Students_Total, na.rm = TRUE),
                Weeks_Total = mean(Weeks_Total)
                ),
            by = "MDR.School.ID") %>%
  ungroup() %>%
  summarise(
    Mean_Teachers = mean(Unique_Teacher_Count),
    SD_Teachers = sd(Unique_Teacher_Count),
    Min_Teachers = min(Unique_Teacher_Count),
    Max_Teachers = max(Unique_Teacher_Count),
    Mean_Students_Total = mean(Students_Total),
    SD_Students_Total = sd(Students_Total, na.rm = TRUE),
    Min_Students_Total = min(Students_Total),
    Max_Students_Total = max(Students_Total),
    Mean_Weeks_Total = mean(Weeks_Total),
    SD_Weeks_Total = sd(Weeks_Total, na.rm = TRUE),
    Min_Weeks_Total = min(Weeks_Total),
    Max_Weeks_Total = max(Weeks_Total)
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Measure", "Variable"),
           sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)
  
df_proportions <- df %>%
  filter(!is.na(poverty)) %>%
  group_by(poverty) %>%
  summarise(n = n()) %>%
  mutate(Percentage = paste0(
    round(n / sum(n) * 100, digits = 2), "%"
    )) %>%
  select(-"n") %>%
  na.omit() %>%
  rename(Variable = poverty) %>%
  mutate(Variable = as.character(Variable)) %>%
  bind_rows(df %>%
              filter(!is.na(income)) %>%
              group_by(income) %>%
              summarise(n = n()) %>%
              mutate(Percentage = paste0(
                round(n / sum(n) * 100, digits = 2), "%"
                )) %>%
              select(-"n") %>%
              na.omit() %>%
              rename(Variable = income) %>%
              mutate(Variable = as.character(Variable))
            ) %>%
  bind_rows(df %>%
              ungroup() %>%
              summarise(
                Charter_Schools = mean(charter.school, na.rm = T)*100,
                Schools_with_Paid_Account = mean(school.account, na.rm = T)*100
                ) %>%
              mutate(
                Charter_Schools = paste0(round(Charter_Schools, digits = 2), "%"),
                Schools_with_Paid_Account = paste0(round(Schools_with_Paid_Account,
                                                         digits = 2), "%")) %>%
              t() %>% as.data.frame() %>%
              rename(Percentage = V1) %>%
              mutate(Variable = row.names(.))) %>%
  add_row(Variable = "Poverty Level", Percentage = "", .before = 1) %>%
  add_row(Variable = "Income", Percentage = "", .before = 5) %>%
  add_row(Variable = "Other", Percentage = "", .before = 23)

# Summary statistics table
gt_summary <- df_summary %>%
  gt(rowname_col = "Variable") %>%
  cols_label(Mean = "Mean", SD = "Standard Deviation",
             Min = "Minimum", Max = "Maximum") %>%
  fmt_number(
    columns = c("Mean", "SD"),
    decimals = 2
  ) %>%
  fmt_number(
    columns = c("Min","Max"),
    decimals = 0
  )
gt_summary
```

```{r}
#| label: fig-proportions
#| tbl-cap: ""

# Splitting df_proportions into different categories for pie charts
df_poverty <- df_proportions[2:4,]
df_income <- df_proportions[6:22,]

# Convert Percentage to numeric
df_poverty$Percentage <- as.numeric(gsub("%", "", df_poverty$Percentage))/100

# Create Bar Graph for Poverty Level
ggplot(df_poverty, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Poverty Level Distribution",
       x = "Poverty Level",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Convert Percentage to numeric
df_income$Percentage <- as.numeric(gsub("%", "", df_income$Percentage))/100

# Create Bar Graph for Income Distribution
ggplot(df_income, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Income Distribution",
       x = "Income Range",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

### Preprocessing and Exclusion criteria

The raw data underwent rigorous preprocessing to ensure its suitability for analysis. This process included performing log transformations on our variables of interest (i.e., minutes, badges, and tower alerts) to normalize their distributions. Given the diverse user base of Zearn, we applied specific exclusion criteria to select teachers who most likely representat traditional classroom settings with consistent platform utilization. We selected virtual classrooms with at least five active students weekly, filtering out parents or tutors who may use Zearn outside the classroom setting. We removed teachers with more than four classrooms and those who logged in for less than 16 weeks. We excluded classrooms in the 6th to 8th grades, as those are a small proportion of our dataset. This deletion ensures a focus on traditional school settings and consistent platform usage, minimizing bias from teachers and schools that have not used Zearn consistently.

```{r preprocess data}

dt <- setDT(df)
dt[, `:=`(
  n_weeks = .N,
  mean_act_st = mean(Active.Users...Total, na.rm = TRUE)
  ), by = Classroom.ID]

dt <- dt[
  n_weeks > 18 & # At least 4.5 months cumulative activity
    Tsubj < 2*n_weeks & # At least activity twice a month on average
    teacher_number_classes < 5 &
    Students...Total > 5 &
    mean_act_st > 3 &
    !(Grade.Level %in% c("6th","7th","8th")) &
    !(month(date) %in% c(6, 7, 8)) &
    !is.na(District.Rollup.ID),
]

df <- as.data.frame(dt) %>%
  select(-X) %>%
  mutate(Minutes.per.Active.User = as.numeric(Minutes.per.Active.User)) %>%
  mutate(across(Active.Users...Total:Tower.Alerts.per.Tower.Completion,
         ~ ifelse(is.na(.), 0, .))) %>%
  arrange(Classroom.ID, week)
# Clean environment
rm(list = setdiff(ls(), c("df","random_py")))
gc(verbose = FALSE)

```

@tbl-summary-statistics summarizes the refined dataset, providing a snapshot of the key variables of interest. Their means and standard deviations (SD) are computed for each grade level and overall (across all grades).

```{r, results='asis'}
#| label: tbl-summary-statistics
#| caption: "Means (SD) of classroom variables by grade level. Minutes and Badges are averaged per student. Tower Alerts are averaged per lesson completion. Teacher Minutes are divided by their number of classrooms."

create_summary <- function(var_name, var_label, type = "continuous") {
  summary_stat <- df %>%
    select(Grade.Level, var_name) %>%
    tbl_summary(
      by = Grade.Level,
      missing = "no",
      type = list(var_name ~ type),
      statistic = var_name ~ ifelse(type == "continuous",
                                    "{mean} ({sd})",
                                    "{n} ({p})")
    ) %>%
    add_overall() %>%
    as_tibble() 
  summary_stat[1] <- var_label
  return(summary_stat)
}

summaries_list <- list(
  # create_summary("Sessions.per.Active.User", "Sessions per Student"),
  create_summary("Minutes.per.Active.User", "Minutes per Student"),
  create_summary("Badges.per.Active.User", "Badges per Student"),
  create_summary("Tower.Alerts.per.Tower.Completion",
                 "Tower Alerts per Lesson Completion"),
  # create_summary("tch_login", "Teacher Login (0/1)", type = "dichotomous"),
  create_summary("Minutes.on.Zearn...Total", "Minutes per Teacher")
)

summary_table <- bind_rows(summaries_list) %>%
  select(-c(9:11)) %>% t() %>% as.data.frame()
colnames(summary_table) <- summary_table[1,]
# Remove the first row and set row names
summary_table <- summary_table[-1, ]
summary_table$`Grade Level` <- rownames(summary_table)

# Create a gt table
gt_table <- gt(summary_table,
               rowname_col = "Grade Level") %>%
  cols_label(
    `Minutes per Student` = "Minutes",
    `Badges per Student` = "Badges",
    `Tower Alerts per Lesson Completion` = "Tower Alerts",
    `Minutes per Teacher` = "Teacher Minutes"
  ) %>%
  as_latex()

# Display the table
gt_table

```

### Variables of interest

In analyzing time-series data from Zearn, spanning approximately 40 weeks of an academic year, we aim to elucidate the relationship between teacher effort and student achievement. This endeavor requires the delineation of an input (action), an output (reward), and, optionally, state variables for the deployment of Reinforcement Learning (RL) models. This structure is pivotal for the practical application of Reinforcement Learning (RL) models of how teacher behaviors (actions) influence student outcomes (rewards) within the evolving educational context (states).

#### Teacher Actions

Teacher actions encompass a broad spectrum, from platform log-ins to resource downloads and specific instructional activities. Simultaneously, student engagement is captured through metrics such as lesson completions (badges earned) and time spent logged in.  provides a list of the actions available in the data.

+-----------------------------------------+-------------------------------------------------------------------------------------------+
| **Variable**                            | **Description**                                                                           |
+=========================================+===========================================================================================+
| Course Guide Download                   | Provides a comprehensive overview, objectives, and structure of Zearn's PD courses.       |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Curriculum Map Download                 | A detailed map outlining learning objectives and content across grade levels.             |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Assessments Download                    | Includes assessments to evaluate student understanding of the material.                   |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Assessments Answer Key Download         | Key solutions for assessments, aiding in efficient grading and feedback.                  |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Course Notes Download                   | Notes from Zearn's professional development courses, offering insights and strategies.    |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Elementary Schedule Download            | A schedule outlining elementary-level Zearn curriculum activities.                        |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Grade Level Overview Download           | Provides a summary of the curriculum and learning goals for specific grade levels.        |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Kindergarten Schedule Download          | Detailed schedules for Kindergarten, supporting structured instruction planning.          |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Kindergarten Mission Download           | Describes specific learning objectives and activities in the Kindergarten curriculum.     |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Mission Overview Download               | Overview of particular learning missions within Zearn, guiding student learning paths.    |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Optional Homework Download              | Assignments for additional practice, enhancing student learning outside of class.         |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Optional Problem Sets Download          | Sets of problems for extra practice, tailored to reinforce lesson concepts.               |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Small Group Lesson Download             | Lessons designed for small-group engagement, promoting personalized learning.             |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Student Notes and Exit Tickets Download | Collection of lesson notes and quick assessments to gauge student understanding.          |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Teaching and Learning Approach Download | Resources outlining Zearn's pedagogical methods, supporting effective teaching.           |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Whole Group Fluency Download            | Fluency practice activities designed for whole-class participation.                       |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Whole Group Word Problems Download      | Word problem-solving activities intended for collaborative, whole-class engagement.       |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Fluency Completed                       | Indicates the completion of fluency activities, focusing on arithmetic skills.            |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Guided Practice Completed               | Signifies the completion of guided practice segments in lessons, reinforcing learning.    |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Kindergarten Activity Completed         | Marks the completion of specific activities within the Kindergarten curriculum.           |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Number Gym Activity Completed           | Completion of Number Gym activities, aimed at building numerical understanding.           |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Tower Completed                         | Completion of the "Tower of Power" activity, emphasizing mastery in lesson objectives.    |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Tower Stage Failed                      | Tracks instances where a student fails a stage in the "Tower of Power" activity.          |
+-----------------------------------------+-------------------------------------------------------------------------------------------+
| Tower Struggled                         | Identifies challenges or struggles students encounter within "Tower of Power" activities. |
+-----------------------------------------+-------------------------------------------------------------------------------------------+

: List of Teacher Actions

#### State Variables

For this paper's second class of Reinforcement Learning models, we require variables that can determine the state space of each week. Experienced teachers suggest that they strongly focus on measures of the level of difficulty encountered during the lessons. The Zearn platform provides some measures of this kind, such as "Tower Alerts." If a student is struggling in a given lesson, the platform automatically provides scaffolded remediation (i.e., breaking the problems step by step), and if they struggle multiple times in that same lesson, a "Tower Alert" is generated for their teacher. Effectively, we assume that the value of a given state (i.e., the week at hand) is a function of the previous week's state variable.

While comprehensive, directly using the available variables presents certain challenges:

1. **Complexity**: The sheer number of available variables complicates the identification of meaningful patterns and relationships.
2. **Dimensionality**: The high-dimensional nature of the data risks diluting important signals due to the "curse of dimensionality."
3. **Interpretability**: Directly interpreting the impact of specific actions or behaviors on outcomes can be obscured by the intertwined nature of the data.

Given these considerations, we avoid relying solely on single, discrete variables and employ dimensionality reduction techniques.

### Dimensionality Reduction

By reducing the data to a manageable number of components, we can more readily identify underlying patterns of behavior and interaction. Components generated through this technique provide a distilled representation of the data, where each component reflects a combination of behaviors or activities with a potential thematic linkage. To achieve this, we applied Principal Component Analysis (PCA) and Nonnegative Matrix Factorization (NMF) techniques. We then used the results to define action, reward, and state variables as components rather than individual metrics, facilitating a clearer understanding of their roles within the educational process.

PCA is our first methodological choice. While widely utilized, it assumes data normality [@jolliffe2016] and maximizes variance explained, potentially overlooking subtle relationships between variables. Consequently, we also employ NMF, which, by contrast, imposes a non-negativity constraint and is more closely related to clustering algorithms, creating a more interpretable, sparse representation of behaviors [@ding2005; @lee1999]. This technique is particularly advantageous for data representing counts or frequencies. By trying different techniques, we can explore these trade-offs and discover the reduced-dimension representation best suited to our specific dataset and research questions.

We performed these estimations with the `scikit-learn` library in python [@pedregosa2011]. First, we standardized the dataset by z-scoring the variables of interest at the school level (using school-wide means and standard deviations) and split it into training (80%) and testing (20%) subsets to produce an out-of-sample model evaluation. We performed PCA and evaluated the data's reconstruction accuracy and cluster separation using the sum of squared residuals (a measure of the difference between the original data and the data reconstructed from the PCA) and silhouette scores (a measure of how similar an object is to its cluster compared to other clusters). The silhouette score ranges from -1 to 1, with a high value indicating that the object is well-matched to its cluster and poorly matched to neighboring clusters.

We performed the Nonnegative Matrix Factorization (NMF) as follows:

The original matrix ($\mathbf{V}$) is a detailed description of all the teachers' (or students') behaviors. Each row in the matrix represents a unique teacher (or classroom), and each column represents a specific behavior or action. The entry in a specific row and column corresponds to the frequency of that behavior for that particular teacher (or classroom). We then estimate $\mathbf{V} \simeq \mathbf{W}\mathbf{H}$, such that

$$
\left\| \mathbf{V} - \mathbf{W}\mathbf{H} \right\|, \mathbf{W} \geq 0, \mathbf{H} \geq 0.
$$

We used two different loss functions (Frobenius norm and Kullback-Leibler divergence) and two different initialization methods (nonnegative double singular value decomposition (NNDSVD) and NNDSVD with zeros filled with the average of the input matrix (NNDSVDA)). The resulting matrices are:

1.  Basis Matrix ($\mathbf{W}$): This matrix represents underlying behavior patterns. Each column is a "meta-behavior" or a group of behaviors occurring together.
2.  Mixture Matrix ($\mathbf{H}$): This matrix shows the extent to which each "meta-behavior" is present in each teacher (or classroom). Each entry in this matrix represents the contribution of a "meta-behavior" to a particular behavior present in the data.

These matrices can reveal underlying patterns of behaviors (from the basis matrix) and how these patterns are mixed and matched in different teachers (from the mixture matrix). This methodology allowed us to assess the method's performance under varying configurations, again using the sum of squared residuals and silhouette scores for comparison.

## Analytical Methods

### Feature Selection

In order to find the appropriate action, reward, and state variables, we used a panel logistic regression model inspired by dynamic analysis [@lau2005]. This approach acted as a filter to capture the action-reward (or action-reward-state) configurations displaying characteristics reminiscent of reinforcement learning (RL). We followed four criteria: a) the influence of consistent rewards on the propensity of actions being repeated, b) the immediate impact of states on action selection, c) the strategic role of actions in navigating towards desirable states, and d) the identification of action auto-correlation as an indicator of incremental learning processes.

Unlike [@lau2005], our model captures both the lagged effects of actions and rewards and their interactions alongside the contemporaneous influence of states on actions. The general model formulation for both state-free and state-based scenarios is as follows:

#### State-Free Model

```{=tex}
\begin{align*}
\text{Action}_t =& \ \sum_{i=1}^{L} \left( \beta_{i} \text{Reward}_{t-i} + \gamma_i \text{Action}_{t-i} + \sum_{j=i}^{L} \delta_{ij} (\text{Reward}_{t-i} \times \text{Action}_{t-j}) \right) \\
& + \mu_{\text{Teacher}} + \lambda_{\text{Week}} + \epsilon_t 
\end{align*}
```

#### State-Based Model

```{=tex}
\begin{align*}
\text{Action}_t =& \ \sum_{i=1}^{L} \left( \beta_{i} \text{Reward}_{t-i} + \gamma_i \text{Action}_{t-i} + \sum_{j=i}^{L} \delta_{ij} (\text{Reward}_{t-i} \times \text{Action}_{t-j}) \right) \\
& + \phi \text{State}_{t-1} + \psi (\text{State}_{t-1} \times \text{Action}_{t-2}) + \mu_{\text{Teacher}} + \lambda_{\text{Week}} + \epsilon_t
\end{align*}
```

where $\text{Action}_t$ denotes the binary outcome at time $t$, $\text{Reward}_{t-i}$ and $\text{Action}_{t-i}$ represent the reward and action variables lagged by $i$ periods, and $L$ is the maximum lag considered. $\mu_{\text{Teacher}}$ and $\lambda_{\text{Week}}$ represent fixed effects for teachers and weeks, respectively.

#### Varying Coefficients Model

We used a varying coefficients model to capture individual teacher effects on the dynamics between rewards, actions, and states. The model is specified as follows:

```{=tex}
\begin{align*}
\text{Action}_{kt} =& \ \sum_{i=1}^{L} \left( \beta_{ki} \text{Reward}_{k, t-i} + \gamma_{ki} \text{Action}_{k, t-i} + \sum_{j=i}^{L} \delta_{kij} (\text{Reward}_{k, t-i} \times \text{Action}_{k, t-j}) \right) \\
& + \phi_k \text{State}_{kt} + \psi_k (\text{State}_{kt} \times \text{Action}_{k, t-1}) + \mu_k + \lambda_{\text{Week}} + \epsilon_{kt}
\end{align*}
```

where $\beta_{ki}$, $\gamma_{ki}$, and $\delta_{kij}$ represent the reward, action, and their interaction coefficients that vary by teacher; $\phi_k$ and $\psi_k$ are the fixed effects for the state and the interaction between state and lagged action, respectively; and $\mu_k$ and $\lambda_{\text{Week}}$ are fixed effects for teachers and weeks, respectively.

Herein, the interaction between lagged rewards and actions aimed to capture the reinforcement aspect (a), where prior rewards enhance the likelihood of repeating specific actions. Including current state variables addressed (b) and examining how present educational contexts inform action choices. The interaction between current states and lagged actions encapsulated (c) that actions are deliberately chosen to navigate towards or sustain preferable educational states. Lastly, considering lagged rewards alone, we sought to elucidate (d) the phenomenon where past successes influence future endeavors, indicative of a learning trajectory.

To operationalize these models, we used the NMF components. Since our RL models have discrete actions, we split the teacher NMF components into a binary variable, with 1 for all positive values (note that NMF produces sparse components). We then, constructed the lagged versions of these variables to capture temporal dynamics, extending up to six weeks to determine the optimal lag. This lag value captured how far back rewards and past actions influenced teacher behavior.

Two types of metrics anchored model selection. First, we evaluated model fit and predictive accuracy using the Bayesian Information Criterion (BIC) and the Out-of-Sample fit, gauged through the Area Under the Receiver Operating Characteristic curve (AUC). As such, we balanced model parsimony with predictive power. Second, we aimed to delineate which regression coefficients most closely embodied RL-like dynamics. We focused on 1) variables that consistently enhanced action probability in response to rewards (i.e., positive coefficient for the state-reward interaction), 2) actions that changed according to a given state, 3) actions chosen to achieve desired states (i.e., significant coefficient for the action-state interaction), and 4) demonstrate action auto-correlation.

### Reinforcement Learning Model Estimation

#### State-Free Q-Learning Model

We use a state-free version of the Q-learning model to predict the actions of teachers based on their past actions and the rewards they received. Thus, the Q-value for action $a$ is updated based on the reward prediction error $\delta$:

$$
Q_{t+1}(a) = Q_{t}(a) + \alpha \left( \gamma (\text{Reward}_t - \text{cost}(a)) - Q_{t}(a) \right)
$$

where

-   $\alpha$ is the learning rate,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_t$ is the immediate reward received after taking action $a$ in state $s$,
-   $\text{cost}(a)$ is the cost associated with action $a$,
-   $Q_{t}(a)$ is the estimate of the Q-value for action $a$ at time $t$.

The probability of choosing a particular action is determined by the softmax function and is defined as follows:

$$
P_{t}(a) = \frac{1}{1+e^{-Q_{t}(a)/\tau}}
$$

where:

-   $P_{t}(a)$ is the probability of choosing action $a$ at time $t$,
-   $\tau$ is the temperature parameter.

We interpret each of these parameters as follows:

-   Cost: The perceived effort or inconvenience associated with the action, such as the effort required to complete a particular task or the inconvenience of deviating from a preferred teaching method.

-   Learning rate ($\alpha$): The extent to which the newly acquired information will override the old information. A factor of 0 will make the agent not learn anything.

-   Discount rate ($\gamma$): The degree to which future rewards are discounted compared to immediate rewards. A high discount rate means that future rewards are considered almost as valuable as immediate rewards, which encourages long-term planning. A low discount rate means that immediate rewards are much more valuable than future rewards, which encourages short-term thinking.

-   Inverse temperature ($\tau$): The degree of randomness in the choice behavior. A high inverse temperature means that the agent is more likely to choose the action with the highest expected reward, while a low inverse temperature means that the agent is more likely to choose actions randomly. This parameter can be interpreted as a measure of the agent's confidence in its Q-values, reflecting the trade-off between exploration (trying out new actions) and exploitation (sticking to known beneficial actions).

#### The Actor-Critic Model

In the Actor-Critic model, we update the value function, parameterized by weights $w$, and the policy function, defined by the parameter $\theta$. The model's update mechanism is minimizing the prediction error \(\delta\), which drives adjustments to both the actor and critic components. The equations for updating the weights and policy are as follows:

$$
w_{t+1} = w_{t} + \alpha_{v} \cdot \delta \cdot S_{t} \\
\theta_{t+1} = \theta_{t} + \alpha_{\pi} \cdot \delta \cdot S_{t}
$$

where $w_{t}, \theta_{t}$ are the vectors of policy and value weights, respectively, for action $a$, and $S_{t}$ is a vector that characterizes the current state, defined as $S_{t} = \begin{bmatrix} 1 \\ \text{State Variable}_{t} \end{bmatrix}$.

We define the parameterized policy as $\text{Prob}_{t}(a)=\text{Logit}^{-1}(\theta_{t} \cdot S_{t})$, the values of each state as $v(S_{t},a) = w_{t} \cdot S_{t}$, and prediction error $\delta$ as the difference between the actual outcome and the estimated value of the state-action pair:

$$
\delta = (\text{Reward}_{t} - \text{cost}(a)) - \left( \gamma v(S_{t},a) - v(S_{t+1},a) \right)
$$

#### Hybrid Models

The hybrid models maintain parallel estimations of two models (Logistic regression, Q-learning, or Actor-Critic), combining them to estimate action selection. This integration is achieved through a weighting scheme that balances the contributions of each model. Specifically, the probability of selecting action \(a\) at time \(t\), denoted \(P_{t}(a)\), is calculated by integrating the softmax outputs of the selected models. This integration uses a weighting parameter \(\lambda\), which adjusts the relative influence of each model and is formulated as:

$$
P_{t}(a) = \lambda P_{t}(a)_{\text{Model 1}} + (1-\lambda) P_{t}(a)_{\text{Model 2}},
$$

## Model Evaluation

### Laplace Approximations

### Top Model Selection

### Full Markov chain Monte Carlo (MCMC) Sampling



## Hetereogeneity Analysis




# Results

## Dimensionality Reduction

We compare the PCA and NMF results by plotting the sum of squared residuals and silhouette scores for each method and the number of components used. @fig-nmf-pca-comparison visually compares the performance of the different methods and allows us to choose the one that provides the best balance between reconstruction accuracy (as measured by the sum of squared residuals) and cluster separation (as measured by the silhouette score). As such, we select the NMF with three components as our preferred method.

```{r pca nmf data-prep}
#| include: false

df <- df %>%
  filter(!is.na(Minutes.on.Zearn...Total)) %>%
  group_by(Classroom.ID) %>%
  # train/test split
  mutate(set = sample(c("train", "test"), size = n(),
                      prob = c(0.8, 0.2), replace = TRUE)) %>%
  ungroup() %>% arrange(Classroom.ID, week)

columns <- names(
  df %>% select(RD.elementary_schedule:Minutes.on.Zearn...Total,
                Active.Users...Total:Tower.Alerts.per.Tower.Completion)
  )

# Create base data.table for models (faster than data.frame)
df_pca <- as.data.table(df)
# Convert columns to double to prevent precision loss
df_pca[, (columns) := lapply(.SD, as.numeric),
        .SDcols = columns]

# Apply the scaling operation
df_pca[, (columns) := lapply(.SD, function(x) {
  x[is.na(x)] <- 0
  sd_x <- sd(x, na.rm = TRUE)
  if(sd_x == 0 | is.na(sd_x)) return(rep(0, .N))
  x / sd_x
}), by = MDR.School.ID, .SDcols = columns]
setorder(df_pca, Classroom.ID, week)

# Calculate standard deviations
std_devs <- apply(df_pca %>% select(all_of(columns)), 2, sd, na.rm = T)
# Identify columns with defined standard deviations (not NaN or Inf)
invalid_cols <- names(std_devs[is.na(std_devs) |
                                 is.infinite(std_devs) |
                                 std_devs == 0])
df_pca <- df_pca %>% select(-all_of(invalid_cols))

# Clean environment
rm(list = setdiff(ls(), c("df", "df_pca", "random_py")))
gc(verbose = FALSE)

```

```{python load data}

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA, NMF
from sklearn.metrics import silhouette_score

## Basic Variables
# Import data from R
dfpca_py = pd.DataFrame(r.df_pca)
dfpca_py.sort_values(['Classroom.ID', 'week'], inplace=True)

# Split the data into teacher and student subsets
teacher_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "RD.elementary_schedule"
    ):dfpca_py.columns.get_loc(
      "RD.grade_level_teacher_materials"
      # "Minutes.on.Zearn...Total"
      )+1
  ]
student_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "Active.Users...Total"
    ):dfpca_py.columns.get_loc(
      "Tower.Alerts.per.Tower.Completion"
      )+1
  ]

X_teachers = dfpca_py[teacher_variables]
X_students = dfpca_py[student_variables]

# Dictionaries for export
components = {}
results = {}
residuals = {}
silhouette = {}

```

```{python pca-nmf}
#| cache: true
# Function for NMF
def nmf_method(n, method, initial, X_scaled, data_label, solv = 'mu', nomin = False):
  method_name = f"{method.title()} {initial.upper()}"
  
  if method == 'frobenius' and initial == 'nndsvd': solv = 'cd'
  if method != 'frobenius' and initial == 'nndsvd': return
  if method != 'frobenius': method_name = f"{method.title()}"
  if nomin: method_name = method_name + "_nomin"
  
  nmf = NMF(
    n_components=n,
    init=initial,
    beta_loss=method,
    solver=solv,
    max_iter=4_000
  )
  X_nmf, nmf_comp = nmf.fit_transform(X_scaled), nmf.components_
  X_hat = nmf.inverse_transform(X_nmf)
  labels = np.argmax(nmf_comp, axis=0)
  
  results.setdefault(f"{method_name}_{data_label}", {})[n] = X_nmf
  components.setdefault(f"{method_name}_{data_label}", {})[n] = nmf_comp
  residuals.setdefault(f"{method_name}_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
  silhouette.setdefault(f"{method_name}_{data_label}", {})[n] = silhouette_score(nmf_comp.transpose(), labels)

def perform_pca_nmf(X_scaled, data_label, n_comp):
  for n in range(2, n_comp):
    ## PCA
    pca = PCA(n_components=n)
    X_pca = pca.fit_transform(X_scaled)
    pca_comp = pca.components_
    X_hat = pca.inverse_transform(X_pca)
    labels = np.argmax(pca_comp, axis=0)
    results.setdefault(f"PCA_{data_label}", {})[n] = X_pca
    components.setdefault(f"PCA_{data_label}", {})[n] = pca_comp
    residuals.setdefault(f"PCA_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
    silhouette.setdefault(f"PCA_{data_label}", {})[n] = silhouette_score(pca_comp.transpose(), labels)
    
    ## Non-negative Matrix Factorization
    for method in {'frobenius', 'kullback-leibler'}:
      for initial in {'nndsvd', 'nndsvda'}:
        nmf_method(
          n, method, initial, X_scaled,
          data_label=data_label,
          nomin = True    # No teacher minutes included
          )

# Run PCA and NMF for teachers
perform_pca_nmf(X_teachers, "teachers", min(X_teachers.shape) // 3)
# Run PCA and NMF for students
perform_pca_nmf(X_students, "students", min(X_students.shape))

```

```{python clean environment}

# get the names of all variables in the global namespace
all_vars = list(globals().keys())

# list of variables to keep
keep_vars = ['residuals', 'silhouette', 'components', 'results', 'r']

# delete all variables that are not in keep_vars
for var in all_vars:
    if var not in keep_vars:
        del globals()[var]
del keep_vars
del all_vars
del var

# manually call the garbage collector
import gc
gc.collect()

```

```{r}
#| cache: true
#| label: fig-nmf-pca-comparison
#| fig-cap: "Comparison of residuals and silhouette scores for PCA, Frobenius, and Kullback-Leibler methods."
# Importing from Python
residuals_list <- py$residuals
silhouette_list <- py$silhouette

teacher_variables <- names(
  df_pca[,RD.elementary_schedule:RD.grade_level_teacher_materials]
)
student_variables <- names(
  df_pca[,Active.Users...Total:Tower.Alerts.per.Tower.Completion]
)
TSS_teacher <- df_pca %>%
  select(all_of(teacher_variables)) %>%
  mutate(across(all_of(teacher_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(teacher_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()
TSS_student <- df_pca %>%
  select(all_of(student_variables)) %>%
  mutate(across(all_of(student_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(student_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()

# Creating dataframes
df_residuals <- do.call(rbind, lapply(names(residuals_list), function(method) {
  do.call(rbind, lapply(names(residuals_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Residuals = residuals_list[[method]][[n]]
    )
  }))
}))
df_residuals_teachers <- df_residuals[
  grepl("_teachers", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_teacher)
df_residuals_students <- df_residuals[
  grepl("_students", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_student)

df_silhouette <- do.call(rbind, lapply(names(silhouette_list), function(method) {
  do.call(rbind, lapply(names(silhouette_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Silhouette = silhouette_list[[method]][[n]]
    )
  }))
}))
df_silhouette_teachers <- df_silhouette[
  grepl("_teachers", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method))
df_silhouette_students <- df_silhouette[
  grepl("_students", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method))

# Plotting residuals
p1 <- ggplot() +
  geom_line(data = df_residuals_teachers,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R2",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_teachers$Components),
                                  max(df_residuals_teachers$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p2 <- ggplot(df_silhouette_teachers,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_teachers$Components),
                                  max(df_silhouette_teachers$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_teachers$Silhouette) +
                                  3*sd(df_silhouette_teachers$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting residuals
p3 <- ggplot() +
  geom_line(data = df_residuals_students,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R2",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_students$Components),
                                  max(df_residuals_students$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p4 <- ggplot(df_silhouette_students,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_students$Components),
                                  max(df_silhouette_students$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_students$Silhouette) +
                                  3*sd(df_silhouette_students$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Combine the plots and place the legend at the bottom
comparison_plot <- ggarrange(p1, p2,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
comparison_plot2 <- ggarrange(p3, p4,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")

comparison_plot
comparison_plot2

```

#### Interpreting Components

@fig-nmf-heatmap shows the loadings of the NMF components using a heatmap, depicting how each original feature contributes to each component. In other words, each component combines the original features to explain a substantial portion of the variance, potentially improving the efficiency and interpretability of the reinforcement learning models. Given the loadings, we interpret the components as follows:

1.  **Component 1 (Teacher Engagement)**: This component seems to be heavily influenced by the variable "Teacher Minutes," suggesting teacher engagement with the Zearn platform. The higher the value in this component, the more time teachers are spending on the platform, which could indicate a higher level of engagement with the curriculum and resources. High values in this component suggest that teachers are actively using the platform, spending time reviewing reports and possibly engaging with other features [@morrison2019].

2.  **Component 2 (Resource Utilization)**: This component has high weights for variables related to different resources available on the Zearn platform, such as "Optional Problem Sets," "Student Notes and Exit Tickets," and "Mission Overview". High values in this component suggest that teachers are downloading and possibly using a variety of resources in their teaching. This pattern is consistent with the findings from [@knudsen2020] that teachers reported learning from a variety of Zearn Math resources and that the curriculum materials and their implementation are important sources of learning.

3.  **Component 3 (Pedagogical Content Knowledge)**: This component has high weights for variables related to student activities, such as "Guided Practice Completed," "Tower Completed," and "Fluency Completed," suggesting that teachers may be engaged in acquiring subject-matter-specific pedagogy, learning to scaffold and to explain concepts in a variety of ways. This finding aligns with [@morrison2019], where teachers were most likely to report using Independent Digital Lessons, student notes and workbooks, small-group lessons, and paper Exit Tickets frequently or very frequently.

```{r}
#| cache: true
#| label: fig-nmf-heatmap
#| fig-cap: "Heatmap of Non-negative Matrix Factorization (NMF) components for Zearn data. Each row represents a variable, and each column represents a component. The color intensity indicates the weight of each variable in each component, with darker colors indicating higher weights. Component 1 represents Teacher Engagement, Component 2 represents Resource Utilization, and Component 3 represents Pedagogical Content Knowledge."

library(pheatmap)
library(grDevices)

components_list <- py$components
df_heatmap_teachers <- 
  components_list[["Frobenius NNDSVD_nomin_teachers"]][["4"]] %>%
  t() %>% as.data.frame()
df_heatmap_students <- 
  components_list[["Frobenius NNDSVD_nomin_students"]][["4"]] %>%
  t() %>% as.data.frame()

# Define a named vector to map old variable names to new ones
variable_names <- c(
  "Minutes.on.Zearn...Total" = "Teacher Minutes",
  "RD.optional_problem_sets" = "Optional Problem Sets Download",
  "RD.student_notes_and_exit_tickets" = "Student Notes and Exit Tickets Download",
  "RD.mission_overview" = "Mission Overview Download",
  "RD.pd_course_notes" = "Course Notes Download",
  "RD.elementary_schedule" = "Elementary Schedule Download",
  "RD.whole_group_fluency" = "Whole Group Fluency Download",
  "Guided.Practice.Completed" = "Guided Practice Completed",
  "RD.small_group_lessons" = "Small Group Lesson Download",
  "Tower.Completed" = "Tower Completed",
  "Fluency.Completed" = "Fluency Completed",
  "Number.Gym.Activity.Completed" = "Number Gym Activity Completed",
  "RD.grade_level_overview" = "Grade Level Overview Download",
  "Tower.Stage.Failed" = "Tower Stage Failed",
  "Kindergarten.Activity.Completed" = "Kindergarten Activity Completed",
  "Tower.Struggled" = "Tower Struggled",
  "RD.k_mission" = "Kindergarten Mission Download",
  "RD.whole_group_word_problems" = "Whole Group Word Problems Download",
  "RD.assessments" = "Assessments Download",
  "RD.teaching_and_learning_approach" = "Teaching and Learning Approach Download",
  "RD.optional_homework" = "Optional Homework Download",
  "RD.k_schedule" = "Kindergarten Schedule Download",
  "RD.curriculum_map" = "Curriculum Map Download",
  "RD.assessments_answer_key" = "Assessments Answer Key Download",
  "RD.pd_course_guide" = "Course Guide Download",
  "RD.grade_level_teacher_materials" = "Teacher Materials Download",
  "Active.Users...Total" = "Active Students",
  "Sessions.per.Active.User" = "Student Logins",
  "Badges.per.Active.User" = "Badges",
  "Badges..on.grade..per.Active.Student" = "On-grade Badges",
  "Minutes.per.Active.User" = "Student Minutes",
  "Tower.Alerts.per.Tower.Completion" = "Tower Alerts",
  "Boosts.per.Tower.Completion" = "Boosts"
)
# Rename the rows of the dataframe
row.names(df_heatmap_teachers) <- variable_names[teacher_variables]
row.names(df_heatmap_students) <- variable_names[student_variables]

names(df_heatmap_teachers) <- paste0("Comp ", 1:4)
names(df_heatmap_students) <- paste0("Comp ", 1:4)
df_heatmap_teachers <- df_heatmap_teachers %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)
df_heatmap_students <- df_heatmap_students %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)

color_scheme <- colorRampPalette(
  c("#F7F7F7",RColorBrewer::brewer.pal(n = 9, name = "YlOrRd"))
  )(100)
breaks <- seq(0, 1, by = 0.01)
minmax <- function(x) ((x - min(x))/(max(x) - min(x)))
pheatmap(df_heatmap_teachers %>%
           mutate_all(minmax),
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         angle_col = 0,
         # main = "Loadings of NMF components",
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE,
         show_rownames = TRUE)

pheatmap(df_heatmap_students %>%
           mutate_all(minmax),
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         angle_col = 0,
         # main = "Loadings of NMF components",
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE,
         show_rownames = TRUE)

```

```{r export dimension reduction}
#| eval: false
# Importing results from Python
results_list <- py$results

# List of method names
# methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler")
methods <- c("Frobenius NNDSVD")
# Initialize df_components
df_components <- df_pca

# Loop over each method, extract the results at 3 components and bind to df_components
for (method in methods) {
  method_teacher <- paste0(method, "_nomin_teachers")
  method_student <- paste0(method, "_nomin_students")
  result_teacher <- results_list[[method_teacher]][["4"]]
  result_student <- results_list[[method_student]][["4"]]
  df_components <- df_components %>%
    bind_cols(result_teacher, result_student, .name_repair = )
  
  # Adjust column names
  new_cols <- paste0("Frobenius NNDSVD",
                     rep(c("_teacher", "_student"), each = 4),
                     rep(1:4, 2))
  names(df_components)[
    (ncol(df_components) -
       ncol(result_teacher) -
       ncol(result_student) + 1):ncol(df_components)
    ] <- new_cols
}

# Write to csv
write.csv(df_components, "./Bayesian/df.csv")

```

## Feature Selection

```{r load dimension reduction}
#| include: false
df <- read.csv(file = "./Bayesian/df.csv") %>%
    mutate(across(where(is.numeric),
                  ~ ifelse(. < .Machine$double.eps, 0, .)))
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
gc(verbose = FALSE)
```

```{r helper-functions}

get_lag_value <- function(datatable, col, lag_period, n_comp = NULL) {
  # Add a column for week_lag
  datatable[, week_lag := c(0, diff(week)), by = Classroom.ID]

  if (is.null(n_comp)) {
    # Update the lag column with shift function
    datatable[, (paste0(col, "_", lag_period)) :=
                shift(get(col), lag_period, fill = 0, type = "lag"),
              by = Classroom.ID]
  } else {
    for (comp in 1:n_comp) {
      # Update the lag column with shift function
      datatable <- datatable[, (paste0(col, comp, "_", lag_period)) :=
                              shift(get(paste0(col, comp)), lag_period, fill = 0, type = "lag"),
                            by = Classroom.ID]
    }
  }
  
  return(datatable)
}

# Function to identify high-density regions for a given vector of values
find_hdr <- function(values) {
  IQR <- quantile(values, 0.75) - quantile(values, 0.25)
  return(c(quantile(values, 0.25) - 1.5*IQR,
           quantile(values, 0.75) + 1.5*IQR))
}

in_hdr <- function(values) {
  hdr <- find_hdr(values)
  return(hdr[1] <= values & values <= hdr[2])
}

```

```{r panel-model}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial)
  return(model)
}

create_formula <- function(action, reward, state = NULL, lag = 1) {
  terms <- c()

  for (i in 1:lag) {
    terms <- c(terms, paste0(reward, "_", i))
    terms <- c(terms, paste0(action, "_", i))
    # Interaction term for reward_i * action_i
    terms <- c(terms, paste0(reward, "_", i, ":", action, "_", i))
    if (i != lag) {
      for (j in (i + 1):lag) {
        # Interaction for reward_i * action_j when i < lag
        terms <- c(terms, paste0(reward, "_", i, ":", action, "_", j))
        
      }
    }
  }

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + week_lag | ", # Adding fixed effects part
                             "Teacher.User.ID + week")
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + ", paste0(state, "_1"),
                             " + ", paste0(state, "_1", ":",
                                           action, "_", (2:max(lag,2)),
                                           collapse = " + "),
                             " + week_lag | ",
                             "Teacher.User.ID + week")
  }
  return(formula_string)
}
#-------------
lags <- c(1:6)
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  # st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  # filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     # st <- as.character(params$st[i])
                     st <- NULL
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          # State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "fe-results.RData")

```

```{r panel-model-states}
#| eval: false

#-------------
lags <- c(1:6)
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     st <- as.character(params$st[i])
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "fe-state-results.RData")

```

We first explore reinforcement learning (RL)-like characteristics within the teacher and classroom usage data. We aimed to uncover patterns indicative of RL, where actors (teachers) select actions (teaching strategies) that historically yield higher rewards (improved student outcomes) and use states (classroom contexts) as signals for action selection. Further, we sought to understand how actions contribute to achieving or maintaining desired states and the extent to which actions exhibit auto-correlation due to incremental learning processes.

## Model Fit and Performance

In order to capture the temporal dynamics of actions influenced by lagged rewards and states, we employed panel logistic regression models across different combinations of variables and lags. We incorporate lagged variables (ranging from one to six weeks) into the models using the Dynamic Analysis approach proposed to account for temporal autocorrelation and potential delayed effects. We applied reward and state structures extracted from classroom data via non-negative matrix factorization (NMF) with the Frobenius Non-negative Double Singular Value Decomposition (NNDSVD), and actions derived similarly from teacher data. We evaluate these models on the Bayesian Information Criterion (BIC) for model complexity and fit and the Area Under the Receiver Operating Characteristic curve (AUC) for predictive accuracy.

### Temporal Dynamics

Our investigation into temporal dynamics confirmed the impact of lagged rewards and actions on decision-making, embodying a foundational RL principle: shaping future decisions by past experiences. @fig-panel-bic illustrates this relationship, showcasing the predictive accuracy and model fit across different lag models, with a clear preference for a lag of two periods as optimal, based on the "elbow" in the AUC curves and the minima in BIC curves. This finding underscores the significance of immediate and preceding influences on future actions, aligning with the delayed reinforcement principle in RL.

```{r}
#| label: fig-panel-bic
#| fig-cap: "Average BIC and AUC across Lags"
#| fig-subcap: 
#|   - "BIC state-free"
#|   - "AUC state-free"
#|   - "BIC state-dependent"
#|   - "AUC state-dependent"
#| layout-ncol: 2

load("fe-results.RData")

fe_results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = "None",
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
}))

teachers <- Reduce(intersect, sapply(results, function(x) {
  names(x$recoef$Teacher.User.ID)
}))

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    # State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         bic = bic - bic_base,
         auc_base = auc[which(Lag == 1)],
         auc = auc - auc_base) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "BIC",
                    "auc" = "AUC")
  
  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(Reward, Method),
                               # group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = "lightblue", alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, color = "blue") +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#------------------

load("fe-state-results.RData")

fe_results_df <- fe_results_df %>%
  rbind(
    do.call(rbind, lapply(results, function(x) {
      data.frame(
        Method = x$Method,
        Lag = x$Lag,
        State = x$State,
        Reward = x$Reward,
        auc = x$AUC,
        bic = x$bic,
        stringsAsFactors = FALSE
        )
      }))
    )

teachers <- intersect(
  teachers, Reduce(intersect, sapply(results, function(x) {
    names(x$recoef$Teacher.User.ID)
    }))
  )

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward, State) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         bic = bic - bic_base,
         auc_base = auc[which(Lag == 1)],
         auc = auc - auc_base) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "BIC",
                    "auc" = "AUC")
  
  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = "lightblue", alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, color = "blue") +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#----------------

bic_plot_se
auc_plot_se
bic_plot_se_st
auc_plot_se_st

```

```{r panel-model-subset}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial)
  return(model)
}

create_formula <- function(action, reward, state = NULL) {
  terms <- c()

  # Interaction term for reward_i * action_i
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_1"))
  terms <- c(terms, paste0(reward, "_1:", action, "_1"))
  
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(reward, "_2"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_2:", action, "_2"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + week_lag | ", # Adding fixed effects part
                             "Teacher.User.ID + week")
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + ", paste0(state, "_1"),
                             " + ", paste0(state, "_1", ":", action, "_2"),
                             " + week_lag | ",
                             "Teacher.User.ID + week")
  }
  return(formula_string)
}
#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  # st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  # filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data", "teachers",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     # st <- as.character(params$st[i])
                     st <- NULL
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          # State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "fe-subset-results.RData")

```

```{r panel-model-states-subset}
#| eval: false

#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data", "teachers",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     st <- as.character(params$st[i])
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "fe-state-subset-results.RData")

```

```{r panel-model-restricted}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial)
  return(model)
}

create_formula <- function(action, reward, state = NULL) {
  terms <- c()

  # Interaction term for reward_i * action_i
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(action, "_1"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + week_lag | ", # Adding fixed effects part
                             "Teacher.User.ID + week")
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + ", paste0(state, "_1"),
                             " + ", paste0(state, "_1", ":", action, "_2"),
                             " + week_lag | ",
                             "Teacher.User.ID + week")
  }
  return(formula_string)
}
#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  # st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  # filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data", "teachers",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     # st <- as.character(params$st[i])
                     st <- NULL
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          # State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "fe-restricted-results.RData")

```

```{=tex}
\newpage
\KOMAoptions{paper=landscape,pagesize}
\recalctypearea
{\areaset[current]{\dimexpr\textwidth\relax}{\textheight}
\setlength{\marginparwidth}{0pt}
\scriptsize
```
```{r}
#| label: tbl-fe-results-statefree
#| tbl-cap: "State-Free Panel Logistic Regression Results"

# Load the results from both subset and restricted analyses
load("fe-subset-results.RData")
original_results <- results
load("fe-restricted-results.RData")
restricted_results <- results

# Combine the model stats from original and restricted results for comparison
original_models <- do.call(rbind, lapply(original_results, function(x) {
  if (x$Lag != 2) return(NULL)
  data.frame(
    Action = x$Method,
    Reward = x$Reward,
    AUC = x$AUC,
    BIC = x$bic,
    Type = "Original"
  )
}))

# Select top 2 AUC models and top 2 BIC models for both original and restricted
top_models <- original_models %>%
  arrange(desc(AUC)) %>%
  slice_head(n = 2)
top_models <- original_models %>%
  arrange(BIC) %>%
  slice_head(n = 2) %>%
  bind_rows(top_models)

# Prepare the results from selected top models for the table
tidy_fe_results <- lapply(original_results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (!(paste(x$Method, x$Reward) %in%
        paste(top_models$Action, top_models$Reward))) return(NULL)
  temp <- as.data.frame(x$fecoef) %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  as.numeric)) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")
  
  return(list(
    Model = paste(x$Method, x$Reward,"Full"),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results <- tidy_fe_results[!sapply(tidy_fe_results, is.null)]
tidy_fe_results_restrict <- lapply(restricted_results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (!(paste(x$Method, x$Reward) %in%
        paste(top_models$Action, top_models$Reward))) return(NULL)
  temp <- as.data.frame(x$fecoef) %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  as.numeric)) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")
  
  return(list(
    Model = paste(x$Method, x$Reward,"Restricted"),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results_restrict <- 
  tidy_fe_results_restrict[!sapply(tidy_fe_results_restrict, is.null)]
tidy_fe_results <- c(tidy_fe_results, tidy_fe_results_restrict)

# Combine all model summaries into one dataframe
model_summary <- do.call(rbind, lapply(tidy_fe_results, function(x) {
  x_name <- unlist(strsplit(x[[1]], " "))
  x_data <- x$Data %>%
    rename(Coefficient = Estimate,
           Std_Error = `Std. Error`) %>%
    mutate(
      Significance = case_when(
        `Pr(>|z|)` < .001 ~ "***",
        `Pr(>|z|)` < .01 ~ "**",
        `Pr(>|z|)` < .05 ~ "*",
        TRUE ~ ""),
      Term = gsub(x_name[1], "lag", Term),
      Term = gsub(x_name[2], "rwd", Term),
      Term = gsub("_", "",  Term),
      Term = gsub(":", "_",  Term)) %>%
    add_row(Term = "BIC", Coefficient = x[[3]]) %>%
    add_row(Term = "N", Coefficient = x[[4]])
  x_data$Model <- x[[1]]

  x_data
})) %>% select(Model, Term, Coefficient, Std_Error, Significance) %>%
  mutate(Model = gsub(" ", "&", Model),
         Model = gsub("Frobenius.NNDSVD_student1", "Completion", Model),
         Model = gsub("Frobenius.NNDSVD_student2", "Struggle", Model),
         Model = gsub("Frobenius.NNDSVD_student3", "No. Students", Model),
         Model = gsub("Frobenius.NNDSVD_student4", "No. Sessions", Model),
         Model = gsub("Frobenius.NNDSVD_teacher1", "Assessments", Model),
         Model = gsub("Frobenius.NNDSVD_teacher2", "Scaffolding", Model),
         Model = gsub("Frobenius.NNDSVD_teacher3", "Activities", Model),
         Model = gsub("Frobenius.NNDSVD_teacher4", "Planning Guides", Model))

# Create the table using the 'gt' package
gt_table <- model_summary %>%
  filter(!Term %in% c("rwd1","rwd2","lag1","lag2")) %>%
  mutate(Estimate = ifelse(Term %in% c("BIC", "N"),
                           sprintf("%d", as.integer(Coefficient)),
                           sprintf("%1.3f%s\n(%1.3f)",
                                   Coefficient, Significance, Std_Error)),
         Term = factor(Term, levels = c("rwd1_lag1", "rwd1_lag2", "lag2_rwd2",
                                        # "lag1", "lag2",
                                        "st", "lag1_st",
                                        "BIC", "N"),
                       labels = c("R(t-1) x \n A(t-1)",
                                  "R(t-1) x \n A(t-2)",
                                  "R(t-2) x \n A(t-2)",
                                  # "Action(t-1)", "Action(t-2)",
                                  "S(t)", "S(t) x \n A(t-1)",
                                  "BIC", "N"))) %>%
  select(-Coefficient, -Std_Error, -Significance) %>%
  # Pivot wider to have one column per model
  pivot_wider(names_from = Model,
              values_from = Estimate,
              names_sep = "_") %>%
  select(order(names(.))) %>%
  arrange(Term) %>%
  gt(rowname_col = "Term") %>%
  sub_missing() %>%
  tab_spanner_delim("&") %>%
  tab_header(
    title = "Fixed Effects Logistic Regression Results"
  )

# Print the table
gt_table

```
```{=tex}
}
\newpage
\KOMAoptions{paper=portrait,pagesize}
\recalctypearea
```

```{r panel-model-states-restricted}
#| eval: false

#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
# Clean environment
rm(list = setdiff(ls(), c("df", "params", "train_data", "test_data", "teachers",
                          "bic_plm", "compute_nloglik","create_formula",
                          "create_model", "get_lag_value", "model_selection")))
gc(verbose = FALSE)

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     st <- as.character(params$st[i])
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "fe-state-restricted-results.RData")

```

```{r}
#| label: tbl-fe-results
#| tbl-cap: "State-Based Panel Logistic Regression Results"

load("fe-state-subset-results.RData")

# Combine the model stats from original and restricted results for comparison
results_df <- do.call(rbind, lapply(results, function(x) {
  if (x$Lag != 2) return(NULL)
  data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = x$State,
    AUC = x$AUC,
    BIC = x$bic,
    Type = "Original"
  )
}))
# Select top 2 AUC models and top 2 BIC models for both original and restricted
top_models <- results_df %>%
  arrange(desc(AUC)) %>%
  slice_head(n = 2)
top_models <- results_df %>%
  arrange(BIC) %>%
  slice_head(n = 2) %>%
  bind_rows(top_models)

# Filter and tidy the results
tidy_fe_results <- lapply(results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (top_models %>%
      filter(Action == x$Method &
             Reward == x$Reward &
             State  == x$State) %>%
      nrow() == 0) return(NULL)
  temp <- x$fecoef %>%
    as.data.frame() %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  ~ as.numeric(.))) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")
  
  return(list(
    Model = paste(x$Method, x$Reward, x$State),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results <- tidy_fe_results[!sapply(tidy_fe_results, is.null)]
	
# Combine all model summaries into one dataframe
model_summary <- do.call(rbind, lapply(tidy_fe_results, function(x) {
  x_name <- unlist(strsplit(x$Model, " "))
  x_data <- x$Data %>%
    rename(Coefficient = Estimate,
           Std_Error = `Std. Error`) %>%
    mutate(
      Significance = case_when(
        `Pr(>|z|)` < .001 ~ "***",
        `Pr(>|z|)` < .01 ~ "**",
        `Pr(>|z|)` < .05 ~ "*",
        TRUE ~ ""),
      Term = gsub(x_name[1], "lag", Term),
      Term = gsub(x_name[2], "rwd", Term),
      Term = gsub(x_name[3], "st",  Term),
      Term = gsub("_", "",  Term),
      Term = gsub(":", "_",  Term)) %>%
    add_row(Term = "BIC", Coefficient = x[[3]]) %>%
    add_row(Term = "N", Coefficient = x[[4]])
  x_data$Model <- x$Model

  x_data
})) %>% select(Model, Term, Coefficient, Std_Error, Significance) %>%
  mutate(Model = gsub(" ", "&", Model),
         Model = gsub("Frobenius.NNDSVD_student1", "Completion", Model),
         Model = gsub("Frobenius.NNDSVD_student2", "Struggle", Model),
         Model = gsub("Frobenius.NNDSVD_student3", "No. Students", Model),
         Model = gsub("Frobenius.NNDSVD_student4", "No. Sessions", Model),
         Model = gsub("Frobenius.NNDSVD_teacher1", "Assessments", Model),
         Model = gsub("Frobenius.NNDSVD_teacher2", "Scaffolding", Model),
         Model = gsub("Frobenius.NNDSVD_teacher3", "Activities", Model),
         Model = gsub("Frobenius.NNDSVD_teacher4", "Planning Guides", Model))

# Create the table using the 'gt' package
gt_table <- model_summary %>%
  filter(!Term %in% c("rwd1","rwd2","lag1","lag2")) %>%
  mutate(Estimate = ifelse(Term %in% c("BIC", "N"),
                           sprintf("%d", as.integer(Coefficient)),
                           sprintf("%1.3f%s\n(%1.3f)",
                                   Coefficient, Significance, Std_Error)),
         Term = factor(Term, levels = c("rwd1_lag1", "rwd1_lag2", "lag2_rwd2",
                                        # "lag1", "lag2",
                                        "st1", "lag2_st1",
                                         "BIC", "N"),
                       labels = c("R(t-1) x \n A(t-1)",
                                  "R(t-1) x \n A(t-2)",
                                  "R(t-2) x \n A(t-2)",
                                  # "Action(t-1)", "Action(t-2)",
                                  "S(t-1)", "S(t-1) x \n A(t-2)",
                                  "BIC", "N"))) %>%
  select(-Coefficient, -Std_Error, -Significance) %>%
  # Pivot wider to have one column per model
  pivot_wider(names_from = Model,
              values_from = Estimate,
              names_sep = "_") %>%
  select(order(names(.))) %>%
  arrange(Term) %>%
  gt(rowname_col = "Term") %>%
  sub_missing() %>%
  tab_spanner_delim("&") %>%
  tab_header(
    title = "Fixed Effects Logistic Regression Results"
    )

# Print the table
gt_table

```

### Variability Among Teachers

```{r RL-mixed-effects}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}
create_formula <- function(action, reward, state = NULL, lag = 1) {
  terms <- c()

  for (i in 1:lag) {
    # Base terms for action and reward
    terms <- c(terms, paste0(reward, "_", i), paste0(action, "_", i))

    # Interaction term for reward_i * action_i with varying slopes
    terms <- c(terms, paste0(reward, "_", i, ":", action, "_", i))

    if (i != lag) {
      for (j in (i + 1):lag) {
        # Add the interaction for reward_i * action_j when i < lag with varying slopes
        terms <- c(terms, paste0(reward, "_", i, ":", action, "_", j))
      }
    }
  }

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "me-results.RData")

```

```{r RL-mixed-effects-subset}
#| eval: false

library(fixest)

load("me-results.RData")
teachers <- intersect(
  Reduce(union, sapply(results, function(x) {
    x$perform_df$Teacher.User.ID
    })),
  Reduce(intersect, sapply(results, function(x) {
    names(x$recoef$Teacher.User.ID)
    }))
  )

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}
create_formula <- function(action, reward, state = NULL, lag = 1) {
  terms <- c()

  for (i in 1:lag) {
    # Base terms for action and reward
    terms <- c(terms, paste0(reward, "_", i), paste0(action, "_", i))

    # Interaction term for reward_i * action_i with varying slopes
    terms <- c(terms, paste0(reward, "_", i, ":", action, "_", i))

    if (i != lag) {
      for (j in (i + 1):lag) {
        # Add the interaction for reward_i * action_j when i < lag with varying slopes
        terms <- c(terms, paste0(reward, "_", i, ":", action, "_", j))
      }
    }
  }

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "me-subset-results.RData")

```

```{r RL-mixed-effects-restricted}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}
create_formula <- function(action, reward, state = NULL) {
  terms <- c()
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(action, "_1"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "me-results-restricted.RData")

```

With a two-week lag established as optimal, we explored variability in model performance across teachers through a varying coefficients approach, which allows for individual slope estimations for each teacher. @tbl-RL-exploration shows pronounced variability, suggesting idiosyncratic pedagogical differences across teachers or diverse strategies tailored to classroom-specific needs. This finding highlights the need for hybrid models to accommodate teachers' diverse pedagogical approaches, strategies, and reinforcement patterns.

```{r}
#| label: table-RL-exploration

library(gtExtras)
load("me-subset-results.RData")
# load("me-results-restricted.RData")

top_fits_df <- do.call(rbind, lapply(results, function(x) {
  # Assuming 'perform_df' is correctly structured as shown in your summary
  top_fits <- x$perform_df %>%
    mutate(keep = case_when(!in_hdr(n_train) ~ F,
                            !in_hdr(n_test) ~ F,
                            !in_hdr(logLik_ind) ~ F,
                            auc_in < 0.5 ~ F,
                            auc_out < 0.5 ~ F,
                            auc_in  == 1 ~ F,
                            auc_out == 1 ~ F,
                            .default = T)) %>%
    filter(keep) %>%
    summarize(
      # across(c(auc_out, auc_in, logLik_ind),
      #        ~ sd(., na.rm = T)/sqrt(n()), .names = "{.col}_se"),
      across(c(auc_out, auc_in, logLik_ind),
             ~ list(mean(., na.rm = T))),
      total = n())
  # top_fits$AUC <- x$AUC
  # top_fits$BIC <- x$bic
  top_fits$Action <- x$Method
  top_fits$Reward <- x$Reward
  top_fits$State <- ifelse(is.null(x$State), "None", x$State)
  return(top_fits)
})) %>%
  mutate(across(c(auc_out, auc_in, logLik_ind), ~ unlist(.))) %>%
  na.omit()

top_fits_df <- top_fits_df %>%
  full_join(fe_results_df %>%
              filter(Lag == 2),
            by = c("Action" = "Method", "Reward", "State")) %>%
  mutate(across(c(Action, Reward, State),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Completion",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggle",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "No. Sessions",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Scaffolding",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Activities",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Planning Guides",
                            .default = .)))
  
top_statefree <- rbind(
  top_fits_df %>% filter(State == "None") %>%
    arrange(desc(auc)) %>% slice_head(n = 1) %>% select(Action, Reward),
  top_fits_df %>% filter(State == "None") %>%
    arrange(bic) %>% slice_head(n = 1) %>% select(Action, Reward),
  top_fits_df %>% filter(State == "None") %>%
    arrange(desc(auc_out)) %>% slice_head(n = 1) %>% select(Action, Reward),
  top_fits_df %>% filter(State == "None") %>%
    arrange(desc(logLik_ind)) %>% slice_head(n = 1) %>% select(Action, Reward)
)
top_statebased <- rbind(
  top_fits_df %>% filter(State != "None") %>%
    arrange(desc(auc)) %>% slice_head(n = 1) %>% select(Action, Reward, State),
  top_fits_df %>% filter(State != "None") %>%
    arrange(bic) %>% slice_head(n = 1) %>% select(Action, Reward, State),
  top_fits_df %>% filter(State != "None") %>%
    arrange(desc(auc_out)) %>% slice_head(n = 1) %>% select(Action, Reward, State),
  top_fits_df %>% filter(State != "None") %>%
    arrange(desc(logLik_ind)) %>% slice_head(n = 1) %>% select(Action, Reward, State)
)

top_fits_df %>%
  mutate(bic = bic/1000) %>%
  select(auc, bic, auc_out, logLik_ind) %>%
  rename("FE AUC" = auc,
         "FE BIC (x10<sup>3</sup>)" = bic,
         "Avg. Hiearchical AUC" = auc_out,
         "Avg. Hiearchical Log Likelihood" = logLik_ind) %>%
  gt_plt_summary(title = "Model Performance") %>%
  fmt_markdown() %>%
  # Remove "Missing" column
  cols_hide(n_missing) %>%
  fmt_number(
    columns = c(Mean, Median, SD),
    decimals = 2
  ) %>%
  # Add columns for top models
  cols_add(
    top_action = top_statefree$Action,
    top_reward = top_statefree$Reward,
    top_action_st = top_statebased$Action,
    top_reward_st = top_statebased$Reward,
    top_state = top_statebased$State) %>%
  cols_label(
    top_action = "Top Action",
    top_reward = "Top Reward",
    top_action_st = "Top Action",
    top_reward_st = "Top Reward",
    top_state = "Top State"
  ) %>%
  tab_spanner(
    label = "State-Free",
    columns = c(top_action, top_reward)
  ) %>%
  tab_spanner(
    label = "State-Based",
    columns = c(top_action_st, top_reward_st, top_state)
  ) %>% gt::gtsave(filename = "images/tbl-RL-exploration.png")

```

![](images/tbl-RL-exploration.png){#tbl-RL-exploration}

### Model Selection and Interpretability

#### Model Performance

We then refined our analysis to highlight models that demonstrated robustness and high predictive accuracy and provided meaningful insights into the interactions between rewards, actions, and states.

@fig-RL-exploration showcases 11 models selected with the highest a) fixed effects AUC, b) mean teacher-specific AUC, c) lowest fixed effects BIC, and d) highest mean teacher-specific log-likelihood, with the top state-free model of each category selected along with the top two state-based models from each category. Models closest to the bottom right corner of the graph (lowest BIC, highest out-of-sample AUC) do best in balancing parsimony in the fixed effects model and predictability in the varying coefficients model. In general, @fig-RL-exploration suggests that the best-fitting models use scaffolding as their action, but the best rewards and states are not clearly defined.

```{r}
#| label: fig-RL-exploration
#| fig-cap: "Top AUC x BIC Models"
#| fig-subcap: 
#|   - "State-free"
#|   - "State-dependent"
#| layout-ncol: 2

# Select top models by BIC (lower is better) and AUC (higher is better)

# Keep models for later analysis
selected_models <- top_fits_df %>%
  filter(State == "None") %>%
  filter(auc_out > 0.7, bic < 90000)

## State-free
plot1 <- top_fits_df %>%
  filter(State == "None") %>%
  select(Action, Reward, bic, auc_out) %>%
  ggplot(aes(x = bic, y = auc_out, color = Reward)) +
  geom_point() +
  theme(legend.position = "bottom") +
  geom_mark_ellipse(
    aes(group = Action, label = Action, color = NULL),
    con.type = "straight",
    con.cap = 0,
    expand = .02,
    label.buffer = unit(0.1, "mm")
  ) +
  # Draw a segment for the x-line up to y = 90000
  geom_segment(x = 84000, y = 0.7, xend = 90000, yend = 0.7,
               linetype = "dashed", color = "darkgray") +
  # Draw a segment for the y-line starting from x = 0.7
  geom_segment(x = 90000, y = 0.7, xend = 90000, yend = 0.72,
               linetype = "dashed", color = "darkgray") +
  theme_minimal() +
  labs(x = "BIC F.E.", y = "Avg. AUC M.E.",
       title = paste(nrow(selected_models), "Models Selected"))

# Join with selected_models
selected_models <- bind_rows(
  selected_models,
  top_fits_df %>%
    filter(State != "None") %>%
    filter(auc_out > 0.7, bic < 90000)
  )

## State-based
plot2 <- top_fits_df %>%
  filter(State != "None") %>%
  select(Action, Reward, State, bic, auc_out) %>%
  ggplot(aes(x = bic, y = auc_out, color = Reward, shape = State)) +
  geom_point() +
  theme(legend.position = "bottom") +
  geom_mark_ellipse(
    aes(group = Action, label = Action, color = NULL),
    con.type = "straight",
    con.cap = 0,
    expand = .02,
    label.buffer = unit(0.1, "mm")
  ) +
  # Draw a segment for the x-line up to y = 90000
  geom_segment(x = 84000, y = 0.7, xend = 90000, yend = 0.7,
               linetype = "dashed", color = "darkgray") +
  # Draw a segment for the y-line starting from x = 0.7
  geom_segment(x = 90000, y = 0.7, xend = 90000, yend = 0.72,
               linetype = "dashed", color = "darkgray") +
  theme_minimal() +
  labs(x = "BIC F.E.", y = "Avg. AUC M.E.",
       title = paste(nrow(selected_models %>% filter(State != "None")),
                     "Models Selected"))

plot1
plot2

```

```{r RL-me-interpretation}
#| eval: false

library(fixest)

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}

# Use only reward_1:action_2 given the previous results
create_formula <- function(action, reward, state = NULL) {
  terms <- c()
  
  # Interaction term for reward_i * action_i
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_1"))
  terms <- c(terms, paste0(reward, "_1:", action, "_1"))
  
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(reward, "_2"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_2:", action, "_2"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique() %>%
  semi_join(
    selected_models %>%
      select(Action, Reward, State) %>%
      mutate(across(c(Action, Reward, State),
                    ~ case_when(. == "Completion" ~ "Frobenius.NNDSVD_student1",
                                . == "Struggle" ~ "Frobenius.NNDSVD_student2",
                                . == "No. Students" ~ "Frobenius.NNDSVD_student3",
                                . == "No. Sessions" ~ "Frobenius.NNDSVD_student4",
                                . == "Assessments" ~ "Frobenius.NNDSVD_teacher1",
                                . == "Scaffolding" ~ "Frobenius.NNDSVD_teacher2",
                                . == "Activities" ~ "Frobenius.NNDSVD_teacher3",
                                . == "Planning Guides" ~ "Frobenius.NNDSVD_teacher4",
                                . == "None" ~ "NA",
                                .default = .))),
    by = c("act" = "Action", "rwd" = "Reward", "st" = "State"))

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > median(., na.rm = TRUE), 1, 0))) %>%
    mutate(across(dplyr::starts_with(reward), ~ ./sd(., na.rm = TRUE))) %>%
    arrange(Classroom.ID, week) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(min(nrow(params2), detectCores()-1))
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "me-standardized-coef.RData")

```

#### Model Interpretability

We applied further scrutiny to the top models through re-estimation with scaled independent variables, enabling a direct comparison of coefficients. In this case, an increase of 1 unit in the independent variable represents an increase of one standard deviation. @tbl-re-estimation summarizes these results, revealing the standardized coefficients for reward and state variables and their interactions with lagged actions.

Here, we sought the models with coefficients presenting attributes most resembling RL. Specifically, the interaction between rewards and actions positively influenced future actions for desired outcomes (e.g., lesson completion) and negatively for undesired outcomes (e.g., struggles). Further, state-based models should incorporate the effects of current states on strategic action selection. The interaction term between states and lagged actions further points to the role of actions in achieving and maintaining desired states.

Notably, the models underscore the absence of a one-size-fits-all approach, indicating a spectrum of strategies where some teachers' practices align more closely with RL principles than others. Ranking the models by the proportion of teachers whose coefficients present RL-like effects, this analysis favors 1) Action: Scaffolding, Reward: No. Sessions, State: No. Students; 2) Action: Activities, Reward: Completion, State: No. Students; and 3) Action: Assessments, Reward: No. Students, State: Completion. @fig-RL-exploration, on the other hand, highlights the models closest to the bottom right corner: 1) Action: Scaffolding, Reward: No. Students, State: No. Sessions; 2) Action: Scaffolding, Reward: No. Sessions, State-free; 3) Action: Scaffolding, Reward: No. Sessions, State: Completion; 4) Action: Activities, Reward: Completion, State: No. Students; and 5) Action: Activities, Reward: Struggle, State-free. Overall, the complexity of these individual differences and the model performance calls for hybrid modeling in future RL-fitting steps.

```{=tex}
\newpage
\KOMAoptions{paper=landscape,pagesize}
\recalctypearea
{\areaset[current]{\dimexpr\textwidth\relax}{\textheight}
\setlength{\marginparwidth}{0pt}
\scriptsize
```
```{r}
#| label: tbl-re-estimation-statefree
#| tbl-cap: "State-Free Mixed Effects Logistic Regression Results"

load("me-standardized-coef.RData")

filtered_results <- lapply(results, function(x) {
  if (!is.null(x$State)) return(NULL)
  temp <- data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = "None"
  ) %>%
  mutate(across(c(Action, Reward),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Completion",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggle",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "No. Sessions",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Scaffolding",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Activities",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Planning Guides",
                            .default = .)))
  if(nrow(
    temp %>%
    semi_join(selected_models, by = c("Action", "Reward", "State"))
    ) == 0) return(NULL)
  if(x$AUC < 0.5) return(NULL)
  x$recoef <- do.call(cbind, x$recoef[-c(1, length(x$recoef))]) %>%
    as.data.frame() %>%
    filter(if_all(everything(),
                  ~ . >= find_hdr(.)[1] & . <= find_hdr(.)[2])) %>%
    rownames_to_column(var = "Teacher.User.ID")
  
  return(list(
    Action = temp$Action,
    Reward = temp$Reward,
    AUCmodel = x$AUC,
    BICmodel = x$bic,
    coef = data.frame(
      Teacher.User.ID = as.integer(x$recoef[,1]),
      lag1 = x$recoef[,3],
      lag2 = x$recoef[,7],
      rwd1 = x$recoef[,2],
      rwd2 = x$recoef[,6],
      rwd1_lag1 = x$recoef[,4],
      rwd1_lag2 = x$recoef[,5],
      rwd2_lag2 = x$recoef[,8]
    ) %>%
      inner_join(x$perform_df %>%
                   select(Teacher.User.ID, auc_out, logLik_ind) %>%
                   filter(if_all(everything(), ~ . >= find_hdr(.)[1] &
                                                 . <= find_hdr(.)[2])) %>%
                   filter(0.5 < auc_out & auc_out < 1),
                 by = "Teacher.User.ID")
  ))
})
filtered_results <- filtered_results[!sapply(filtered_results, is.null)]

model_summary <- do.call(rbind, lapply(filtered_results, function(x) {
  summary <- data.frame(
    Action = x$Action,
    Reward = x$Reward,
    AUC = x$AUCmodel,
    BIC = x$BICmodel
  )
  summary$lag1 = list(x$coef$lag1)
  summary$lag2 = list(x$coef$lag2)
  summary$rwd1_lag1 = list(x$coef$rwd1_lag1)
  summary$rwd1_lag2 = list(x$coef$rwd1_lag2)
  summary$rwd2_lag2 = list(x$coef$rwd2_lag2)
  summary$auc_ind = list(x$coef$auc_out)
  summary$logLik_ind = list(x$coef$logLik_ind)
  
  summary <- summary %>%
    # # Standardize the coefficients by dividing by the standard deviation
    # mutate(across(c(rwd1_lag1, rwd1_lag2, rwd2_lag2, st, st_lag1),
    #               ~ lapply(., function(x) x / sd(x, na.rm = T)))) %>%
    # # Fill in x$State == "None" with 0s
    # mutate(across(c(st, st_lag1), ~ if_else(is.na(.), list(0), .))) %>%
    mutate(n_teachers = lapply(summary$lag1, length)) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2, 
                    rwd2_lag2,
                    auc_ind, logLik_ind),
                  ~ lapply(., mean, na.rm = T),
                  .names = "{.col}.mean")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., sd, na.rm = T),
                  .names = "{.col}.sd")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., quantile, 0.25, names = F),
                  .names = "{.col}.q1")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., median, na.rm = T),
                  .names = "{.col}.median")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., quantile, 0.75, names = F),
                  .names = "{.col}.q3")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., function(x) mean(x > 0, na.rm = T)),
                  .names = "{.col}.pos")) %>%
    select(-c(lag1, lag2,
              rwd1_lag1,
              rwd1_lag2,
              rwd2_lag2,
              auc_ind, logLik_ind, auc_ind.mean, logLik_ind.mean))
  
  return(summary)
}))

# Transform the model_summary dataframe to a long format for plotting
model_summary_long <- model_summary %>%
  mutate(across(!c(Action, Reward), as.numeric)) %>%
  mutate(across(dplyr::ends_with(".pos") & dplyr::starts_with("rwd"),
                ~ case_when(Reward == "Struggle" ~ 1 - ., .default = .))) %>%
  select(!c(AUC, BIC)) %>%
  pivot_longer(cols = -c(Action, Reward), 
               names_to = c("variable", "statistic"), values_to = "value",
               names_sep = "\\.") %>%
  arrange(Action, Reward) %>%
  pivot_wider(names_from = c(Action, Reward), values_from = c(value))
  
max <- model_summary_long %>%
  filter(statistic == "pos" & !variable %in% c("lag1","lag2")) %>%
  rowwise() %>%
  # Remove columns that do not have the max of at least 1 row
  mutate(max_val = max(c_across(is.numeric), na.rm = T)) %>%
  ungroup() %>%
  mutate(across(is.numeric, ~ if_else(. == max_val, ., NA))) %>%
  select(where(~ !all(is.na(.)))) %>%
  select(-c("max_val","statistic")) %>%
  mutate(across(is.numeric, ~ !is.na(.))) %>%
  pivot_longer(cols = -variable, names_to = "model", values_to = "keep") %>%
  filter(keep) %>% select(-keep)

model_summary_long %>%
  filter(!variable %in% c("lag1", "lag2")) %>%
  mutate(statistic = factor(statistic,
                            levels = c("mean", "sd", "q1", "median", "q3", "pos"),
                            labels = c("Mean", "SD", "Q1", "Median", "Q3", "RL-like")),
         variable = factor(variable,
                           levels = c(
                             # "lag1", "lag2",
                             "rwd1_lag1",
                             "rwd1_lag2",
                             "rwd2_lag2",
                             "n_teachers"),
                           labels = c(
                             # "Action(t-1)",
                             # "Action(t-2)",
                             "R(t-1) x \n A(t-1)",
                             "R(t-1) x \n A(t-2)",
                             "R(t-2) x \n A(t-2)",
                             "N"
                             ))) %>%
  select(order(names(model_summary_long))) %>%
  group_by(variable) %>%
  gt(rowname_col = "statistic", row_group_as_column = T) %>%
  row_group_order(groups = c(
    # "Action(t-1)",
    # "Action(t-2)",
    "R(t-1) x \n A(t-1)",
    "R(t-1) x \n A(t-2)",
    "R(t-2) x \n A(t-2)",
    "N")) %>%
  fmt_markdown(columns = "variable") %>%
  fmt_number(decimals = 2) %>%
  fmt_integer(rows = variable == "No. Teachers") %>%
  fmt_percent(rows = statistic == "RL-like") %>%
  # cols_hide(starts_with("Assessments_No. Students")) %>%
  sub_missing() %>%
  tab_spanner_delim("_") %>%
  tab_header(
    title = "Summary of Model Coefficients"
  ) %>%
  tab_options(
    table.font.size = "small"
  )
  # as_latex()

```
\newpage
```{r}
#| label: tbl-re-estimation
#| tbl-cap: "State-Based Mixed Effects Logistic Regression Results"

load("me-standardized-coef.RData")

filtered_results <- lapply(results, function(x) {
  if (is.null(x$State)) return(NULL)
  temp <- data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = x$State
  ) %>%
  mutate(across(c(Action, Reward, State),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Completion",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggle",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "No. Sessions",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Scaffolding",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Activities",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Planning Guides",
                            .default = .)))
  if(nrow(
    temp %>%
    semi_join(selected_models, by = c("Action", "Reward", "State"))
    ) == 0) return(NULL)
  if(x$AUC < 0.5) return(NULL)
  x$recoef <- do.call(cbind, x$recoef[-c(1, length(x$recoef))]) %>%
    as.data.frame() %>%
    filter(if_all(everything(),
                  ~ . >= find_hdr(.)[1] & . <= find_hdr(.)[2])) %>%
    rownames_to_column(var = "Teacher.User.ID")
  
  return(list(
    Action = temp$Action,
    Reward = temp$Reward,
    State = temp$State,
    AUCmodel = x$AUC,
    BICmodel = x$bic,
    coef = data.frame(
      Teacher.User.ID = as.integer(x$recoef[,1]),
      lag1 = x$recoef[,3],
      lag2 = x$recoef[,7],
      rwd1 = x$recoef[,2],
      rwd2 = x$recoef[,6],
      rwd1_lag1 = x$recoef[,4],
      rwd1_lag2 = x$recoef[,5],
      rwd2_lag2 = x$recoef[,8],
      st = x$recoef[,9],
      st_lag1 = x$recoef[,10]
    ) %>%
      inner_join(x$perform_df %>%
                   select(Teacher.User.ID, auc_out, logLik_ind) %>%
                   filter(auc_out >= 0.5 & auc_out < 1),
                 by = "Teacher.User.ID")
  ))
  })
filtered_results <- filtered_results[!sapply(filtered_results, is.null)]

model_summary <- do.call(rbind, lapply(filtered_results, function(x) {
  summary <- data.frame(
    Action = x$Action,
    Reward = x$Reward,
    State = x$State,
    AUC = x$AUCmodel,
    BIC = x$BICmodel
  )
  summary$lag1 = list(x$coef$lag1)
  summary$lag2 = list(x$coef$lag2)
  summary$rwd1_lag1 = list(x$coef$rwd1_lag1)
  summary$rwd1_lag2 = list(x$coef$rwd1_lag2)
  summary$rwd2_lag2 = list(x$coef$rwd2_lag2)
  summary$st = ifelse(x$State == "None", list(0), list(x$coef$st))
  summary$st_lag1 = ifelse(x$State == "None", list(0), list(x$coef$st_lag1))
  summary$auc_ind = list(x$coef$auc_out)
  summary$logLik_ind = list(x$coef$logLik_ind)
  
  summary <- summary %>%
    # # Standardize the coefficients by dividing by the standard deviation
    # mutate(across(c(rwd1_lag1, rwd1_lag2, rwd2_lag2, st, st_lag1),
    #               ~ lapply(., function(x) x / sd(x, na.rm = T)))) %>%
    # # Fill in x$State == "None" with 0s
    # mutate(across(c(st, st_lag1), ~ if_else(is.na(.), list(0), .))) %>%
    mutate(n_teachers = lapply(summary$lag1, length)) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1,
                    auc_ind, logLik_ind),
                  ~ lapply(., mean, na.rm = T),
                  .names = "{.col}.mean")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., sd, na.rm = T),
                  .names = "{.col}.sd")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., quantile, 0.25, names = F),
                  .names = "{.col}.q1")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., median, na.rm = T),
                  .names = "{.col}.median")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., quantile, 0.75, names = F),
                  .names = "{.col}.q3")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., function(x) mean(x > 0, na.rm = T)),
                  .names = "{.col}.pos")) %>%
    select(-c(lag1, lag2,
              rwd1_lag1,
              rwd1_lag2,
              rwd2_lag2,
              st, st_lag1,
              auc_ind, logLik_ind, auc_ind.mean, logLik_ind.mean))
  
  return(summary)
}))

# Transform the model_summary dataframe to a long format for plotting
model_summary_long <- model_summary %>%
  mutate(across(!c(Action, Reward, State), as.numeric)) %>%
  mutate(across(dplyr::ends_with(".pos") & dplyr::starts_with("st"),
                ~ case_when(State == "Struggle" ~ 1 - ., .default = .)),
         across(dplyr::ends_with(".pos") & dplyr::starts_with("rwd"),
                ~ case_when(Reward == "Struggle" ~ 1 - ., .default = .))) %>%
  select(!c(AUC, BIC)) %>%
  mutate(across(dplyr::starts_with("st", ignore.case = F),
                ~ if_else(State == "None", NA, .))) %>%
  pivot_longer(cols = -c(Action, Reward, State), 
               names_to = c("variable", "statistic"), values_to = "value",
               names_sep = "\\.") %>%
  arrange(Action, Reward, State) %>%
  pivot_wider(names_from = c(Action, Reward, State), values_from = c(value))
  
max <- model_summary_long %>%
  filter(statistic == "pos" & !variable %in% c("lag1","lag2")) %>%
  rowwise() %>%
  # Remove columns that do not have the max of at least 1 row
  mutate(max_val = max(c_across(is.numeric), na.rm = T)) %>%
  ungroup() %>%
  mutate(across(is.numeric, ~ if_else(. == max_val, ., NA))) %>%
  select(where(~ !all(is.na(.)))) %>%
  select(-c("max_val","statistic")) %>%
  mutate(across(is.numeric, ~ !is.na(.))) %>%
  pivot_longer(cols = -variable, names_to = "model", values_to = "keep") %>%
  filter(keep) %>% select(-keep)

model_summary_long %>%
  filter(!variable %in% c("lag1", "lag2")) %>%
  mutate(statistic = factor(statistic,
                            levels = c("mean", "sd", "q1", "median", "q3", "pos"),
                            labels = c("Mean", "SD", "Q1", "Median", "Q3", "RL-like")),
         variable = factor(variable,
                           levels = c(
                             # "lag1", "lag2",
                             "rwd1_lag1",
                             "rwd1_lag2",
                             "rwd2_lag2",
                             "st", "st_lag1",
                             "n_teachers"),
                           labels = c(
                             # "Action(t-1)",
                             # "Action(t-2)",
                             "R(t-1) x \n A(t-1)",
                             "R(t-1) x \n A(t-2)",
                             "R(t-2) x \n A(t-2)",
                             "S(t)",
                             "S(t) x \n A(t-1)",
                             "N"
                             ))) %>%
  select(order(names(model_summary_long))) %>%
  group_by(variable) %>%
  gt(rowname_col = "statistic", row_group_as_column = T) %>%
  row_group_order(groups = c(
    # "Action(t-1)",
    # "Action(t-2)",
    "R(t-1) x \n A(t-1)",
    "R(t-1) x \n A(t-2)",
    "R(t-2) x \n A(t-2)",
    "S(t)",
    "S(t) x \n A(t-1)",
    "N")) %>%
  fmt_markdown(columns = "variable") %>%
  fmt_number(decimals = 2) %>%
  fmt_integer(rows = variable == "No. Teachers") %>%
  fmt_percent(rows = statistic == "RL-like") %>%
  # cols_hide(starts_with("Assessments_No. Students")) %>%
  sub_missing() %>%
  tab_spanner_delim("_") %>%
  tab_header(
    title = "Summary of Model Coefficients"
  ) %>%
  tab_options(
    table.font.size = "small"
  )
  # as_latex()

```
```{=tex}
}
\newpage
\KOMAoptions{paper=portrait,pagesize}
\recalctypearea
```


## Estimating RL Models

### Laplace Approximations

<!-- - Present the 4 models that stood out during the first estimation. -->

<!-- - Discuss the HBI results and model comparison statistics. -->

#### Top Q-Learning and Actor-Critic Models

```{r}
#| label: tbl-CBM-first
#| tbl-cap: "Top QL and AC Models"
library(R.matlab)

# Load the MATLAB files for the top QL and AC models
top_model_ql <- readMat('CBM/zearn_results/hbi_QL4_refined.mat')
top_model_ac <- readMat('CBM/zearn_results/hbi_AC5_refined.mat')

# Extract the relevant data from each model
model_freq_ql <- top_model_ql[["cbm"]][[5]][[5]]
loglik_ql <- apply(top_model_ql[["cbm"]][[4]][[1]][[1]], 1, sum)

model_freq_ac <- top_model_ac[["cbm"]][[5]][[5]]
loglik_ac <- apply(top_model_ac[["cbm"]][[4]][[1]][[1]], 1, sum)


# Create a data frame with each model
top_models_table <- rbind(
  data.frame(Statistic = "Frequency",
             QL = model_freq_ql, 
             AC = model_freq_ac),
  data.frame(Statistic = "Log Likelihood",
             QL = t(loglik_ql),
             AC = t(loglik_ac))
  )

# Display the table
gt(top_models_table) %>%
  tab_spanner_delim(".") %>%
  fmt_percent(rows = c(1), decimals = 1) %>%
  fmt_number(rows = c(2), decimals = 1)

```

#### Hybrid Models Comparison

```{r}
#| label: tbl-CBM-second
#| tbl-cap: "Hybrid Models"

# Load the MATLAB files for the comparative models and the top model comparison
comp_model_7 <- readMat('CBM/zearn_results/hbi_compare_7.mat')
comp_model_1 <- readMat('CBM/zearn_results/hbi_compare_1.mat')
comp_model_2 <- readMat('CBM/zearn_results/hbi_compare_2.mat')
comp_model_51 <- readMat('CBM/zearn_results/hbi_compare_51.mat')

# Extract model frequency data
model_freq <- data.frame(rbind(
  comp_model_7[["cbm"]][[5]][[5]],
  comp_model_1[["cbm"]][[5]][[5]],
  comp_model_2[["cbm"]][[5]][[5]],
  comp_model_51[["cbm"]][[5]][[5]]
  ))
row.names(model_freq) <- 
  c("Model 7", "Model 1", "Model 2", "Model 51")
names(model_freq) <- c("Logit", "RL", "Logit-RL Hybrid", "QL-AC Hybrid")

gt(model_freq, rownames_to_stub = T) %>%
  fmt_percent(decimals = 1)

```

#### Top Model

```{r}
#| label: tbl-CBM-final
#| tbl-cap: "Top Model Comparison"

top_model_comp <- readMat('CBM/zearn_results/top_model_comp.mat')
# Extract model frequency data
model_freq <- data.frame(
  top_model_comp[["cbm"]][[5]][[5]]
  )
names(model_freq) <- c("Logit 7", "Logit 1", "AC 2", "AC 51")

gt(model_freq) %>%
  fmt_percent(decimals = 1)
```

### Full Markov chain Monte Carlo (MCMC) Estimation

## Heterogeneity Analyses



<!--  ****************************************   -->


```{r}
#| label: tbl-CBM-teachers
#| tbl-cap: "Teacher Heterogeneity"

# Classify each teacher according to the best-fit model
teacher_classification <- apply(top_model_comp[["cbm"]][[5]][[2]],
                                1, which.max)
teacher_params <- top_model_comp[["cbm"]][[5]][[1]][[3]][[1]]
# Make min(teacher_classification) the baseline
load("CBM/data/classrooms.RData")
hetereogeneity <- classrooms %>%
  cbind(
    data.frame(
      actorcritic = teacher_classification - min(teacher_classification)
      )) %>%
  cbind(teacher_params)
names(hetereogeneity)[3:9] <- c(
  "alpha_w", "alpha_theta", "gamma", "tau", "theta_init", "w_init", "cost"
)
hetereogeneity <- hetereogeneity %>%
  mutate(across(c(alpha_w, alpha_theta, gamma), ~ 1/(1+exp(-.)))) %>%
  mutate(across(c(tau, cost), ~ exp(.))) %>%
  left_join(
    df %>%
      group_by(Classroom.ID, Teacher.User.ID) %>%
      summarize(
        across(c(charter.school, school.account),
               \(x) mean(x, na.rm = TRUE)),
        across(c(Frobenius.NNDSVD_teacher2,
                 Frobenius.NNDSVD_student1, Frobenius.NNDSVD_student3,
                 `Active.Users...Total`, Badges.per.Active.User,
                 Tower.Alerts.per.Tower.Completion),
               \(x) sum(x, na.rm = TRUE)),
        across(c(income, poverty), ~ unique(.)),
        n_weeks = n()
        ), by = "Classroom.ID"
    ) %>%
  mutate(across(c(income, poverty), as.ordered))

# T-tests for differences in means
hetereogeneity %>%
  select(actorcritic, charter.school, school.account, poverty, 
         n_weeks,
         `Active.Users...Total`, `Badges.per.Active.User`,
         `Tower.Alerts.per.Tower.Completion`,
         Frobenius.NNDSVD_teacher2,
         Frobenius.NNDSVD_student1, Frobenius.NNDSVD_student3) %>%
  tbl_summary(
    by = actorcritic,  # Grouping variable
    statistic = list(all_continuous() ~ "{mean} ({sd})",
                     all_categorical() ~ "{p}%"),
    missing = "no"
  ) %>%
  add_n() %>%
  add_difference() %>%
  add_q()

```

```{r}
# Performance

fit <- qs::qread("Bayesian/Results/ACfit.qs")
loo1 <- qs::qread("Bayesian/Results/ACloo.qs")



```








The trade-off between model complexity and predictive accuracy justifies including four lags. Including more lags would increase the complexity of the model, potentially leading to overfitting and poorer predictive performance. Conversely, including fewer lags might result in a model that fails to capture critical temporal dependencies in the data. The selection of four lags represents an inflection point in the BIC and NLL, indicating an optimal balance between model complexity and predictive accuracy.

In @fig-lags, we present the estimated coefficients of the lagged variables as derived from the random effects models. The lines represent the regression coefficients of different variables and their standard errors. The grey line and shaded area correspond to the coefficients for the lagged Badges per Student variable. This graphical representation elucidates the diminishing influence of each variable as the lag increases.

Taking the Frobenius 1 component as an example, the coefficient for the first lag is approximately 0.30, accompanied by a standard error of 0.25. As the lag increases, there is a noticeable decrease in the coefficient, implying a waning influence of this component over time. In contrast, the Badges per Student variable demonstrates a different pattern. First, the magnitude of these coefficients is smaller and non-significant. The coefficient also fluctuates around zero as the lag increases, suggesting a relatively consistent influence over time.

```{r}
#| eval: false
#| label: fig-timelines
#| fig-cap: ""

# Reshape the data to long format
long_df <- df %>%
  select(MDR.School.ID, week, Active.Users...Total, Sessions.per.Active.User, 
         Minutes.per.Active.User, Badges.per.Active.User, 
         Boosts.per.Tower.Completion, Tower.Alerts.per.Tower.Completion, 
         FrobeniusNNDSVD1, FrobeniusNNDSVD2, FrobeniusNNDSVD3,
         poverty, school.account) %>%
  group_by(MDR.School.ID, week, poverty, school.account) %>%
  summarize(across(where(is.numeric), ~ mean(., na.rm = TRUE))) %>%
  gather(key = "Variable", value = "Value",
         -MDR.School.ID, -week, -poverty, -school.account)

# Function to create a plot for each variable with error ribbons
plot_variable <- function(variable_name, var) {
  # Calculate overall average and standard error
  avg_data <- long_df %>%
    filter(Variable == variable_name) %>%
    filter(!is.na({{var}})) %>%
    group_by({{var}}, week) %>%
    summarize(
      Avg = mean(Value, na.rm = TRUE),
      SE = sd(Value, na.rm = TRUE)/sqrt(n()),
      CI_low = Avg - SE,
      CI_high = Avg + SE
    )
  
  # Calculate 2 standard deviations from the overall data
  y_high <- quantile(avg_data$Value[avg_data$Variable == variable_name],
                     0.85, na.rm = T)
  y_low <- quantile(avg_data$Value[avg_data$Variable == variable_name],
                    0.15, na.rm = T)

  ggplot(avg_data, aes(x = week, y = Avg, group = {{var}})) +
    geom_line(aes(color = {{var}})) +
    geom_ribbon(data = avg_data,
                aes(x = week, y = Avg, group = {{var}}, alpha = 0.2,
                    fill = {{var}}, ymin = CI_low, ymax = CI_high)) +
    theme_minimal() +
    labs(title = variable_name, x = "Time", y = "Value") +
    theme(legend.position = "none") +
    coord_cartesian(ylim = c(y_low, y_high))  # Adjust y-axis range
}

# Variables to plot
variables_to_plot <- c(
  "Active.Users...Total", "Sessions.per.Active.User",
  "Minutes.per.Active.User", "Badges.per.Active.User",
  "Boosts.per.Tower.Completion", "Tower.Alerts.per.Tower.Completion", 
  "FrobeniusNNDSVD1", "FrobeniusNNDSVD2", "FrobeniusNNDSVD3"
  )

# Create individual plots
plots <- lapply(variables_to_plot, plot_variable, var = school.account)

# Combine the plots into a panel
do.call(grid.arrange, c(plots, ncol = 3))

```

In our pursuit to identify the most parsimonious model that optimally fits the data, we conducted a comparative analysis of the out-of-sample negative likelihood (NLL) across four distinct reinforcement learning models: State-Based Q-Learning, Kernalized Q-Learning, State-Free Q-Learning, and Actor-Critic. We evaluated these models using three different methods for non-negative matrix factorization: Frobenius (NNDSVD), Frobenius (NNDSVDA), and Kullback-Leibler. Each cell in @fig-panel-bic represents the median negative log-likelihood of a model given a specific method, with lower values signifying a superior model fit.

Our analysis reveals that the Kernalized Q-Learning model, when evaluated using the Frobenius (NNDSVD) method, provides the most optimal fit to the data, as evidenced by its lowest negative log-likelihood value. As a result, we select this combination of model and method for our remaining analyses.

```{r}
#| label: tbl-choose-RL-model
#| tbl-cap: "The table presents a comparison of the median negative log-likelihood values for posterior distributions across four reinforcement learning models: State-Based Q-Learning, Kernalized Q-Learning, State-Free Q-Learning, and Actor-Critic. These models are evaluated based on three methods for non-negative matrix factorization: Frobenius (NNDSVD), Frobenius (NNDSVDA), and Kullback-Leibler. Each cell in the table represents the median negative log-likelihood of a model's posterior given a method, with lower values indicating better model fit. The best performing combination of model and method (i.e., the one with the lowest negative log-likelihood value) is highlighted in light green."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]

# Function to extract median of lp__ from a model
extract_lp_median <- function(model_file) {
  fit <- readRDS(model_file)
  lp_median <- fit$summary("lp__", "median")  # Get the median of lp__
  data.frame(Model = gsub("Bayesian/Results/||.RDS", "", model_file),
             NLL_Median = -1 * as.numeric(lp_median))
}

# Apply function to all models and combine into one dataframe
lp_df <- purrr::map_df(results_files, extract_lp_median) %>%
  na.omit()
lp_df$Method <- gsub(".*-", "", lp_df$Model)
lp_df$ModelType <- stringr::str_extract(lp_df$Model, ".*(?=-)")

# Define descriptive names
model_names <- c("Q-learning-states" = "State-Based Q-Learning",
                 "Q-learning-kernel" = "Kernalized Q-Learning",
                 "Q-learning" = "State-Free Q-Learning",
                 "Actor-Critic" = "Actor-Critic")

method_names <- c("FR" = "Frobenius (NNDSVD)",
                  "FRa" = "Frobenius (NNDSVDA)",
                  "KL" = "Kullback-Leibler")

# Apply the descriptive names
lp_df$ModelType <- model_names[lp_df$ModelType]
lp_df$Method <- method_names[lp_df$Method]

# Create the table with models as rows and NMF methods as columns
table_df <- lp_df %>%
  dplyr::select(-Model) %>%
  tidyr::pivot_wider(names_from = Method, values_from = NLL_Median) %>%
  arrange(desc(ModelType)) %>%
  na.omit()

# Remove redundant 'ModelType' label
colnames(table_df)[colnames(table_df) == "ModelType"] <- "Model"

# To highlight the best value in the table
table_df %>%
  gt() %>%
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      columns = names(table_df)[colSums(table_df == min(table_df[,-1])) >= 1],
      rows = rowSums(table_df == min(table_df[,-1])) >= 1
    )
  )

```

We compared our base models, which used logistic regression, and our top-performing reinforcement learning (RL) model, which employed kernelized Q-learning, to identify which best fit the data. @tbl-RL-logit-comp presents the Leave-One-Out Information Criterion (LOOIC) estimates for these models, with lower values indicative of superior model performance.

Our analysis revealed that the hierarchical Q-learning model outperformed the others, as evidenced by its lowest LOOIC value. This finding suggests that reinforcement learning provides a more accurate representation of teacher behavior on Zearn when individual parameters are fitted. The hierarchical logistic regression model followed closely, demonstrating competitive performance. However, the models that did not incorporate a hierarchical structure yielded higher LOOIC values, indicating a lesser fit to the data. These findings highlight the significant heterogeneity in the data and emphasize the value of reinforcement learning models, particularly those with a hierarchical structure, in accurately capturing the dynamics of teacher behavior on Zearn.

```{r Bayesian LOOIC prep}
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py", "get_lag_value")))
gc(verbose = FALSE)
```

```{r}
#| eval: false
#| cache: true
#| label: tbl-RL-logit-comp
#| tbl-cap: "Model comparison using Leave-One-Out Information Criterion (LOOIC). LOOIC values, a measure of model quality, were calculated for each model type. Lower values indicate better model performance. Q-learning models were built using a kernel-based approach. Hierarchical models incorporate a hierarchical structure to account for classroom-level variations."

# Non-hierarchical models
## Q-learning model
# post <- read_rds("Bayesian/Results/Q-learning-kernel-FR.RDS")
post <- read_rds("Bayesian/Results/Q-learning-FR.RDS")
loo_qlearn <- post$loo()$estimates["looic", ]
## Logit
models_nh <- readRDS("Bayesian/Results/logit.RDS")
loo_nh <- models_nh$loo()$estimates["looic", ]
# nll_nh <- lapply(models_nh, log_lik)
# nll_nh <- lapply(nll_nh, mean)

# Hierarchical models
## Q-learning
post_hierarchical <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
loo_qhierarchical <- post_hierarchical$loo()$estimates["looic", ]
## Logit
models_h <- readRDS("Bayesian/Results/logit-hierarchical.RDS")
loo_h <- models_h$loo()$estimates["looic", ]

# Collect LOOIC values
looic_values <- c(loo_qlearn[1],loo_nh[1],
                  loo_qhierarchical[1], loo_h[1])
df_looic <- data.frame(Model = c("Q-learning", "Logistic regression", "Hierarchical Q-learning", "Hierarchical logistic regression"),
                       LOOIC = looic_values)

# Create gt table
df_looic %>%
  gt() %>%
  cols_label(
    Model = "Model",
    LOOIC = "LOOIC Value"
  )

# Create gt table
gt(df_looic)

```

We compared the hierarchical model's predictions and the original choice data, representing a distinct action. We averaged the model predictions across teachers weekly and overlayed them with the average action from the choice data. We also calculated the standard error around these averages accounting for missing data, which provided a measure of uncertainty around these values.

@fig-model-fit shows the model fit for each action where the y-axis represents the probability of a=1, and the x-axis represents the time in weeks. The line plot includes the mean probabilities, highlighted by colored lines, and their respective standard errors, represented by shaded ribbons surrounding the lines. The results section of the data for 'Action 1' revealed that the model predictions initially remained relatively close to the mean probability of 0.5, demonstrating some variability but remaining within a reasonable range. The model's variability increased over time, denoted by the broadening standard error ribbons, peaking at week 36 with a significant increase in the mean predicted probability. This peak corresponded to a drastic decline in the choice data, implying a divergence between the model's predictions and the data towards the end of the observed period.

In the case of 'Action 2' and 'Action 3,' the model prediction started at the mean probability of 0.5 and demonstrated a declining trend over the weeks. Although the declining trend was present in both model fit and data, the model predicted a less drastic decline. The standard error for this action also increased over time, albeit not as dramatically as in the case of 'Action 1', indicating that the model's predictions became more uncertain as time progressed. Despite this variability, the model provided a reasonable fit for the choice data across the weeks for the first months of the study period.

```{r}
#| eval: false
#| cache: true
#| label: fig-model-fit
#| fig-cap: "Model predictions for teacher actions compared with choice data over time. The y-axis denotes the probability of a particular action being taken (a=1), while the x-axis indicates time in weeks. The figure showcases three distinct actions (NMF components). The solid lines represent the weekly averaged model predictions for each action, while the shaded ribbons denote the respective standard errors."
stan_data <- read_rds("./Bayesian/Q-learn-data.RDS")
choice_data <- read_rds("./Bayesian/Results/Q-learning-FR.RDS")
choice_data <- choice_data$summary()

prediction_hierarchical <- read.csv("./Bayesian/Results/prediction_hierarchical.csv") %>%
  filter(grepl("y_pred", variable)) %>%
  dplyr::select(variable, mean)

prediction_hierarchical <- prediction_hierarchical %>%
  mutate(variable = str_extract(variable, "\\[.*\\]"),
         variable = str_replace_all(variable, "\\[|\\]", "")) %>%
  separate(variable, into = c("dim1", "dim2", "dim3"), sep = ",", convert = TRUE)

prediction_hierarchical_3d <- array(dim = c(max(prediction_hierarchical$dim1),
                                            max(prediction_hierarchical$dim2),
                                            max(prediction_hierarchical$dim3)))

for (i in 1:nrow(prediction_hierarchical)) {
  dim1 <- prediction_hierarchical$dim1[i]
  dim2 <- prediction_hierarchical$dim2[i]
  dim3 <- prediction_hierarchical$dim3[i]
  prediction_hierarchical_3d[dim1, dim2, dim3] <- prediction_hierarchical$mean[i]
}

# Loop through each subject
for (subject in seq_len(dim1)) {
  # Get the value of Tsubj for this subject
  Tsubj_value <- stan_data[["Tsubj"]][subject]
  
  # Set the values in prediction_hierarchical_3d to NA
  if (Tsubj_value != max(stan_data[["Tsubj"]])) {
    prediction_hierarchical_3d[subject, (Tsubj_value + 1):dim2, ] <- NA
    # Set the values in choice_data to NA
    choice_data[subject, (Tsubj_value + 1):dim2, ] <- NA
  }
}

# Get the number of layers
num_layers <- dim(prediction_hierarchical_3d)[3]
# Custom function to calculate standard error based on the number of non-NA elements
calc_se <- function(x) sd(x, na.rm = TRUE) / sqrt(sum(!is.na(x)))

df_compare <- data.frame()

# Generate a plot for each layer
for (k in 1:num_layers) {
  y_pred_avg <- apply(prediction_hierarchical_3d[, , k], 2, mean, na.rm = TRUE)
  y_pred_se  <- apply(prediction_hierarchical_3d[, , k], 2, calc_se)
  
  choice_data_avg <- apply(choice_data[, , k], 2, mean, na.rm = TRUE)
  choice_data_se  <- apply(choice_data[, , k], 2, calc_se)
  
  # Only consider weeks with valid SEs
  weeks <- seq_len(sum(!is.na(y_pred_se)))

  df_pred <- data.frame(weeks = weeks,
                        probability = y_pred_avg[weeks],
                        type = rep("Model Fit", max(weeks)),
                        se = y_pred_se[weeks],
                        action = rep(paste("Action", k), max(weeks)))

  df_real <- data.frame(weeks = weeks,
                        probability = choice_data_avg[weeks],
                        type = rep("Real Data", max(weeks)),
                        se = choice_data_se[weeks],
                        action = rep(paste("Action", k), max(weeks)))

  df_compare <- rbind(df_compare, df_pred, df_real)
}

p <- ggplot(df_compare, aes(x = weeks, y = probability, color = type)) +
  geom_line() +
  geom_ribbon(data = df_compare, aes(ymin = probability - se, ymax = probability + se, fill = type), alpha = 0.1) +
  labs(x = "Week", y = "Probability of a=1") +
  facet_wrap(~action, ncol = 1) +
  scale_color_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  scale_fill_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  theme_bw()

print(p)

```

```{r}
#| eval: false

logit_model <- readRDS("./Bayesian/Results/logit-hierarchical.RDS")
pred_logit_model <- lapply(logit_model, function(model) {
  fitted(model)
})
data_logit_model <- lapply(logit_model, function(model) {
  model$data[,1]
})
logit_pred_3d <- array(NA, dim = dim(prediction_hierarchical_3d))
choice_data_3d <- array(NA, dim = dim(prediction_hierarchical_3d))

# Loop through each subject
for (subject in seq_len(dim1)) {
  Tsubj_value <- stan_data[["Tsubj"]][subject]
  
  # Calculate the starting and ending indices for the current subject
  start_idx <- ifelse(subject == 1, 1,
                      sum(stan_data[["Tsubj"]][1:(subject-1)]) - subject + 2)
  end_idx <- sum(stan_data[["Tsubj"]][1:subject]) - subject
  
  # Loop through each model in pred_logit_model
  for (model_idx in seq_along(pred_logit_model)) {
    # Extract the estimated probabilities for the current model
    model_predictions <- pred_logit_model[[model_idx]][start_idx:end_idx,"Estimate"]
    real_data <- data_logit_model[[model_idx]][start_idx:end_idx]

    # Populate logit_pred_3d with the predicted values
    logit_pred_3d[subject, 2:(Tsubj_value), model_idx] <- model_predictions
    choice_data_3d[subject, 2:(Tsubj_value), model_idx] <- real_data
  }
}

# Generate a plot for each layer
df_compare <- data.frame()
for (k in 1:dim3) {
  y_pred_avg <- apply(logit_pred_3d[, , k], 2, mean, na.rm = TRUE)
  y_pred_se <- apply(logit_pred_3d[, , k], 2, calc_se)
  
  choice_data_avg <- apply(choice_data_3d[, , k], 2, mean, na.rm = TRUE)
  choice_data_se  <- apply(choice_data_3d[, , k], 2, calc_se)

  # Only consider weeks with valid SEs
  weeks <- seq_len(length(y_pred_se))[!is.na(y_pred_se)]
  idx_shift <- weeks[1] - 1

  df_pred <- data.frame(weeks = weeks,
                        probability = y_pred_avg[weeks],
                        type = rep("Model Fit", length(weeks)),
                        se = y_pred_se[weeks],
                        action = rep(paste("Action", k), length(weeks)))

  df_real <- data.frame(weeks = weeks,
                        probability = choice_data_avg[weeks],
                        type = rep("Real Data", length(weeks)),
                        se = choice_data_se[weeks],
                        action = rep(paste("Action", k), length(weeks)))

  df_compare <- rbind(df_compare, df_pred, df_real)
}

p <- ggplot(df_compare, aes(x = weeks, y = probability, color = type)) +
  geom_line() +
  geom_ribbon(data = df_compare, aes(ymin = probability - se, ymax = probability + se, fill = type), alpha = 0.1) +
  labs(x = "Week", y = "Probability of a=1") +
  facet_wrap(~action, ncol = 1) +
  scale_color_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  scale_fill_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  theme_bw()

print(p)

```




## Optimality

To understand the factors contributing to a teacher's ability to maximize lesson completion, we conducted an in-depth analysis of teachers' performance across various parameters from the previous hierarchical model. We focused specifically on the learning rate ("Alpha") and the inverse temperature ("Tau"). We examined their correlation with the average weekly badges earned per teacher, a proxy for lesson completion, and Tower Alerts, a measure of student struggle with the materials.

@tbl-optimality presents the coefficients of six different linear regression models. Each model predicts the average weekly badges earned per teacher (Models 1-3) or the average weekly Tower Alerts (Models 4-6) based on different combinations of the parameters and control variables. The control variables include the number of active students, the number of classes taught by the teacher, the grade level, the number of weeks, the poverty level, the income level, whether the school is a charter school, and whether the school has a paid account.

Alpha and Tau achieved statistical significance when adding all the control variables (columns 3 and 6, respectively). These results suggest that a higher learning rate may lead to fewer badges earned and more Tower Alerts. A higher inverse temperature is associated with a slight increase in badges earned.

```{r, results='asis'}
#| eval: false
#| cache: true
#| label: tbl-optimality
#| tbl-cap: "The impact of different parameters and control variables on average weekly badges and Tower Alerts. Six linear regression models were used to examine the correlations between a teacher's RL parameters (Cost 1, Cost 2, Cost 3, Gamma, Alpha, Tau) and two measures of student engagement: average weekly Badges earned per student (Models 1-3) and average weekly Tower Alerts per student (Models 4-6). Models 2-3 and 5-6 also control for other variables including number of active students, number of classes taught, grade level, weeks, poverty level, income level, whether the school is a charter school, and whether the school has a paid account. Coefficients and standard errors are provided for each parameter in each model."

library(tidybayes)
library(stargazer)

hierarchical_model <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
classroom_data <- read.csv("Bayesian/df_subset.csv") %>%
  group_by(Teacher.User.ID) %>%
  summarise(n_active_students = mean(Active.Users...Total),
            n_students = mean(Students...Total),
            minutes_students = mean(Minutes.per.Active.User),
            badges = mean(Badges.per.Active.User),
            boosts = mean(Boosts.per.Tower.Completion),
            tower_alers = mean(Tower.Alerts.per.Tower.Completion),
            n_classes_by_teacher = median(teacher_number_classes),
            grade = first(Grade.Level),
            n_weeks = mean(n_weeks),
            poverty = first(poverty),
            income = first(income),
            charter_school = first(charter.school),
            school_account = first(school.account))

posterior_samples <- hierarchical_model$draws() %>%
  spread_draws(cost[207,3],
               gamma[207],
               alpha[207], 
               tau[207])

summary_cost <- posterior_samples %>%
  unnest_wider(cost, names_sep = "_")
list_of_dfs <- lapply(seq_along(summary_cost)[grepl("cost", names(summary_cost))], function(x){
  temp_df <- as.data.frame(summary_cost[[x]])
  names(temp_df) <- paste0("C", 1:ncol(temp_df))
  temp_df$teacher <- paste0("teacher_", (x - 3))
  return(temp_df)
})
cost_df <- do.call(rbind, list_of_dfs)
cost_df$teacher <- as.numeric(gsub("teacher_", "", cost_df$teacher)) 
summary_cost <- cost_df %>% 
  group_by(teacher) %>% 
  summarise(across(starts_with("C"), mean, .names = "mean_{.col}"))

summary_gamma_alpha_tau <- posterior_samples %>%
  unnest_wider(gamma, names_sep = "_") %>%
  unnest_wider(alpha, names_sep = "_") %>%
  unnest_wider(tau, names_sep = "_") %>%
  summarise(across(c(starts_with("gamma"),
                     starts_with("alpha"),
                     starts_with("tau")),
                   mean, .names = "{.col}")) %>%
  pivot_longer(cols = c(starts_with("gamma"),
                        starts_with("alpha"),
                        starts_with("tau")),
               names_to = "variable",
               values_to = "value") %>%
  separate(variable, into = c("type", "teacher"), sep = "_", convert = TRUE) %>%
  pivot_wider(names_from = type, values_from = value)

# merge the two dataframes
summary_all <- merge(summary_cost, summary_gamma_alpha_tau, by = "teacher")

classroom_data <- classroom_data %>%
  arrange(Teacher.User.ID) %>%
  bind_cols(summary_all) %>%
  dplyr::mutate(
    income = factor(income, ordered = TRUE), 
    grade = factor(grade, ordered = TRUE,
                   levels = c("Kindergarten",
                              "1st", "2nd", "3rd", "4th", "5th")),
    poverty = factor(poverty, ordered = TRUE)
  )

ordered_factor <- function(fact_var) {
  categories <- levels(fact_var)
  n_cat <- length(categories)
  cont <- matrix(data = 0, nrow = n_cat, ncol = (n_cat - 1))
  cont[col(cont) < row(cont)] <- 1
  rownames(cont) <- categories
  colnames(cont) <- paste(categories[2:n_cat],
                          categories[1:(n_cat - 1)],
                          sep = " vs. ")
  contrasts(fact_var) <- cont
  return(fact_var)
}
classroom_data <- classroom_data %>%
  dplyr::mutate(
    income = ordered_factor(income), 
    grade = ordered_factor(grade),
    poverty = ordered_factor(poverty)
  )

model1 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau,
          classroom_data)

model2 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks,
          classroom_data)

model3 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks +
            poverty +
            income +
            charter_school +
            school_account,
          classroom_data)

model4 <- lm(tower_alers ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau,
          classroom_data)

model5 <- lm(tower_alers ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks,
          classroom_data)

model6 <- lm(tower_alers ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau +
            n_active_students +
            n_classes_by_teacher +
            grade +
            n_weeks +
            poverty +
            income +
            charter_school +
            school_account,
          classroom_data)

# Create table with stargazer
stargazer(model1, model2, model3, model4, model5, model6,
          type = "latex",
          header=FALSE,
          align = TRUE,
          covariate.labels = c("Cost 1", "Cost 2", "Cost 3",
                               "Gamma", "Alpha", "Tau",
                               "Number of Active Students", "Number of Classes",
                               "Number of Weeks", "Charter School", "Paid School Account"),
          omit = c("grade", "poverty", "income"),
          dep.var.caption = "Dependent variables:",
          dep.var.labels = c("Badges", "Tower Alerts"),
          star.cutoffs = c(.05, .01, .001), 
          star.char = c("*", "**", "***"),
          add.lines = list(c("Control for Grade Level",
                             "", "Yes", "Yes", "", "Yes", "Yes"),
                           c("Control for Poverty Level",
                             "", "", "Yes", "", "", "Yes"),
                           c("Control for Income Level",
                             "", "", "Yes", "", "", "Yes")))


```

```{r}
#| eval: false
#| cache: true
#| label: fig-parameters-corr
#| fig-cap: "Correlations between the estimated parameters (Cost 1, Cost 2, Cost 3, Gamma, Alpha, Tau) and the outcome variables (average weekly Badges and average weekly Tower Alerts) across the six different linear regression models. Each circle represents a correlation coefficient; the size and shading of the circles indicate the magnitude and direction of the correlation, respectively. The first correlogram (top) relates to the outcome variable of average weekly Badges, while the second correlogram (bottom) relates to the outcome variable of average weekly Tower Alerts."

# Selecting relevant variables for the first model concerning badges
relevant_data_badges <- classroom_data %>%
  dplyr::select(mean_C1, mean_C2, mean_C3, gamma, alpha, tau, badges)

# Similarly, for the fourth model concerning tower alerts
relevant_data_tower <- classroom_data %>%
  dplyr::select(mean_C1, mean_C2, mean_C3, gamma, alpha, tau, tower_alers)

cor_matrix_badges <- cor(relevant_data_badges, use = "complete.obs")
cor_matrix_tower <- cor(relevant_data_tower, use = "complete.obs")

cor_matrix_badges <- cor(relevant_data_badges, use = "complete.obs")
cor_matrix_tower <- cor(relevant_data_tower, use = "complete.obs")

# For badges
corrplot(cor_matrix_badges, method = "circle")

# For tower alerts
corrplot(cor_matrix_tower, method = "circle")

```

### Feature Selection Model Fit

-   Discuss the best-performing model using students online as a reward, and alerts, badges, and minutes as state variables.

## Hierarchical Models

Our dataset comprises classrooms spanning a period of around 40 weeks. Given the limited amount of data per classroom, individual-level estimation using maximum likelihood estimation would yield noisy results. Although group-level estimation provides reliable estimates, it overlooks individual differences, which are crucial to our analysis.

To address this, we employed a hierarchical Bayesian analysis, which allows for information pooling across individuals while accommodating individual differences. In this approach, individual-level parameters are functions of group-level hyperparameters, and this anchoring enhances our power by assuming commonalities among individuals [@ahn2017]. A notable feature of this estimation technique is that the extent of pooling is reflected in the hyperparameter variance. Strong pooling corresponds to low hierarchical variance, and vice versa for weak pooling.

## Heterogeneity

Building on these results, we investigated how teachers' adaptation and implementation of the Zearn Math curriculum vary with various school-specific and demographic factors. The goal was to uncover potential disparities and leverage points that could inform intervention strategies and policy decisions to reduce educational inequities.

Our multi-pronged statistical approach matched the diverse nature of our classroom variables. We selected each model to best account for the distribution and characteristics of the dependent variable. We used an ordered logistic regression for ordinal classroom variables, such as Income Level and Poverty Level. Binary variables, which included Charter School and Paid School Account status, naturally lent themselves to logistic regression modeling. Finally, we used Poisson regression to model the variable representing the number of classes taught by each teacher.

@fig-heterogeneity presents a detailed summary of the associations between different factors and classroom variables. Among these, the interaction between "Cost 3" and "Poverty Level" is notable. Our data suggest that a higher estimated cost of applying Pedagogical Content Knowledge predicts a lower median income level in the school. The estimated Discount Rate is another variable significantly associated with a school's Poverty Level ($\beta = -0.39,\text{ } p<0.05$). Furthermore, the negative relationship between being a Charter School and the Learning Rate, with a coefficient of -0.67 (p\<0.001), suggests that teachers in charter schools are slower to adapt and modify their teaching methods in response to new information.

```{r}
#| eval: false
#| cache: true
#| label: fig-heterogeneity
#| fig-cap: "Predicting classroom variables with RL parameters. The data comes from a comprehensive set of classroom records, including income level, poverty level, charter school status, whether a school account is paid, and the number of classes each teaches. These classroom variables (columns) are regressed on the estimated cost parameters for each action (Cost 1, Cost 2, Cost 3), the discount rate, the learning rate, and the inverse temperature. Each cell in the heatmap represents the estimate of a linear, Poisson, or logistic regression model, depending on the variable type. For ordinal classroom variables (Income Level and Poverty Level), ordered logistic regression (proportional odds model) is used, while for binary variables (Charter School, Paid School Account), logistic regression is applied. The number of classes by each teacher, being a count data, is modeled with Poisson regression. The coefficients are scaled estimates of the effect of each parameter on the respective classroom variable. The asterisks indicate the level of statistical significance based on p-values (*p < 0.1; **p < 0.01; ***p < 0.001). The coefficients are color-coded with a gradient from light blue (negative) to white (zero) to dark blue (positive)."
library(broom)
library(MASS)

# Prepare data
hetero <- classroom_data %>%
  dplyr::select(income, poverty, charter_school, school_account, n_classes_by_teacher,
         mean_C1, mean_C2, mean_C3, gamma, alpha, tau) %>%
  dplyr::mutate(
    income = factor(income, ordered = TRUE),
    poverty = factor(poverty, ordered = TRUE)
  )

# Define predictors and responses
responses <- c("income", "poverty",
               "charter_school", "school_account", "n_classes_by_teacher")
predictors <- c("mean_C1", "mean_C2", "mean_C3", "gamma", "alpha", "tau")
hetero[predictors] <- scale(hetero[predictors])
# Create the formula for the model with all predictors
predictors_formula <- paste(predictors, collapse = " + ")

# Run models for each response, and extract coefficients
coef_matrix <- map_dfr(responses, function(response) {
  if (response == "income" | response == "poverty") {
    model <- polr(as.formula(paste0(response, " ~ ", predictors_formula)),
                  data = hetero, Hess = TRUE)
  } else if (response == "n_classes_by_teacher") {
    model <- glm(as.formula(paste0(response, " ~ ", predictors_formula)),
                 data = hetero, family = poisson(link = "log"))
  } else {
    model <- glm(as.formula(paste0(response, " ~ ", predictors_formula)),
                 data = hetero, family = binomial(link = "logit"))
  }
  
  tidy(model, p.values = TRUE) %>%
    filter(term %in% predictors) %>%
    dplyr::select(term, estimate, p.value) %>%
    mutate(response = response,
           sig = ifelse(p.value < 0.05, "***",
                        ifelse(p.value < 0.01, "**",
                               ifelse(p.value < 0.1, "*", ""))))
})

# Convert to wide format
coef_matrix <- coef_matrix %>%
  pivot_wider(names_from = term, values_from = c(estimate, p.value, sig))

# Reorder the rows to match the original order of responses
coef_matrix <- coef_matrix %>%
  mutate(response = factor(response, levels = responses)) %>%
  arrange(response)

# Reshape for plotting
coef_matrix_long <- coef_matrix %>%
  pivot_longer(cols = starts_with("estimate"), names_to = "term", values_to = "estimate") %>%
  pivot_longer(cols = starts_with("sig"), names_to = "term_sig", values_to = "sig")

# Clean up term names
coef_matrix_long$term <- str_replace(coef_matrix_long$term, "estimate_", "")
coef_matrix_long$term_sig <- str_replace(coef_matrix_long$term_sig, "sig_", "")

# Make sure the term columns match
coef_matrix_long <- coef_matrix_long[coef_matrix_long$term == coef_matrix_long$term_sig,]

# Rename the variables
var_names <- c("income" = "Income Level", "poverty" = "Poverty Level",
               "charter_school" = "Charter School",
               "school_account" = "Paid School Account",
               "n_classes_by_teacher" = "Number of Classes by Teacher",
               "mean_C1" = "Cost 1", "mean_C2" = "Cost 2", "mean_C3" = "Cost 3",
               "gamma" = "Discount Rate",
               "alpha" = "Learning Rate",
               "tau" = "Inverse Temperature")
coef_matrix_long$term <- var_names[coef_matrix_long$term]

responses_new <- c("Income\n Level", "Poverty\n Level",
                   "Charter\n School", "Paid School\n Account", 
                   "Classes\n per Teacher")

names(responses_new) <- responses

# Update variable labels
coef_matrix_long$response <- factor(coef_matrix_long$response, 
                                    levels = responses, 
                                    labels = responses_new)

# Create the plot
ggplot(coef_matrix_long, aes(x = response, y = term, fill = as.numeric(estimate))) +
  scale_y_discrete(limits = c("Cost 1", "Cost 2", "Cost 3",
                              "Inverse Temperature", "Discount Rate", "Learning Rate")) +
  geom_tile() +
  geom_text(aes(label = paste0(round(as.numeric(estimate), 2), sig)), size = 4) +
  scale_fill_gradient2(low = "lightblue", high = "darkblue", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), 
                       name = "Regression\n Coefficient") +
  scale_x_discrete(position = "top") + # This line moves the x-axis labels to the top
  theme_minimal() +
  theme(axis.text.x = element_text(size = 12, angle = 0, vjust = 1, hjust = 0.5), 
        axis.text.y = element_text(size = 12), 
        legend.title = element_text(size = 11), 
        legend.text = element_text(size = 10),
        legend.spacing.x = unit(0.5, "cm"),
        legend.position = "bottom") +
  labs(x = NULL, y = NULL, title = NULL)

```







# Discussion

Our study aimed to unravel the complex dynamics of teacher behavior in the context of Zearn Math, a popular online learning platform. We sought to understand the role of reinforcement learning (RL) in modeling these behaviors and their impact on student achievement. Our findings provide compelling evidence that RL models, particularly the hierarchical Q-learning model, can accurately characterize teacher behavior and offer valuable insights into the education field.

## Characterizing Teacher Behavior

Our first research question aimed to identify the RL model that best characterizes teacher behavior in the Zearn Math context. The hierarchical Q-learning model emerged as the most accurate, outperforming the simple logistic regression model. This result suggests that teachers are not merely following a static "education production function" but are actively learning and adapting their teaching strategies on Zearn Math.

The hierarchical Q-learning model's superiority underscores the importance of considering individual teacher differences. As an agent in the RL model, each teacher has unique parameters that reflect their learning rate and decision-making strategies. These parameters offer a rich source of information about teacher behavior, providing a more nuanced understanding than simpler models.

## Parameter Variations and Their Influence on Student Achievement

Our second research question explored how variations in the parameters of RL models (i.e., the learning rate and the exploration-exploitation trade-off) affect teacher behavior and, consequently, student achievement. The learning rate suggests a teacher's adaptability in altering teaching strategies based on feedback. The exploration-exploitation trade-off encapsulates a teacher's ability to balance using new teaching strategies (exploration) and sticking with known effective strategies (exploitation).

Our findings indicated that teachers exhibiting a higher learning rate, a marker of greater adaptability, were associated with higher student achievement. This finding echoes the sentiments expressed by teachers in [@knudsen2020], who found value in the multifaceted strategies provided by Zearn Math. A 3rd-grade Zearn teacher stated, "I like that Zearn provides several strategies to get to the answer...you see the problems; you see what you need to hit on and stress the first time around." This sentiment aligns with our observation that the ability to adapt teaching strategies based on feedback, akin to a higher learning rate in RL models, is a valuable attribute in promoting student achievement.

Similarly, teachers who effectively balanced exploration and exploitation had students with superior outcomes. [@morrison2019] noted that teachers exhibited varied levels of preparedness for implementing the Zearn Math curriculum, with just under half of the teachers reporting feeling adequately prepared for implementation. This lack of preparation could influence how effectively teachers navigate the exploration-exploitation trade-off, impacting student outcomes.

Our results also highlight the complex dynamics underlying teacher behaviors and their implications for student outcomes. For instance, [@knudsen2020], found that veteran teachers with strong PCK leaned heavily on traditional teaching methods, reducing their engagement with the novel practices proposed by Zearn Math. Conversely, new teachers exhibited a blend of traditional and innovative practices, despite their initial resistance to Zearn Math, underscoring their learning curve with the new curriculum.

Our findings point to the potential of RL parameters to provide valuable insights into teacher behaviors and their subsequent effects on student achievement. Understanding and leveraging these parameters could enhance teacher training programs and interventions, ultimately improving student outcomes. By incorporating insights from our study into such initiatives, we can help foster a more adaptive and effective teaching landscape that better supports student learning and achievement.

## Influence of Teacher Background, Training, and Experience

Our third research question asked how teacher background, training, and experience influence their adaptation to and implementation of the Zearn Math curriculum. Our findings confirm the fundamental role of teachers' background, training, and experience when implementing new educational platforms like Zearn Math. They also underscore the importance of differentiating support and training strategies to cater to teachers' unique backgrounds and needs, which may prove pivotal in fostering effective adoption and implementation of such platforms. A prominent theme that emerged from our investigation was the interaction between teacher behaviors, school characteristics, and broader socio-economic contexts.

Our analyses revealed a telling association between Poverty Level and Cost 3, denoting the estimated cost of applying PCK, suggesting that schools in higher poverty strata are more likely to encounter additional barriers in the form of resource constraints when implementing the Zearn Math curriculum.

This observation points towards a potential resource disparity where schools in lower-income areas grapple with the challenge of investing in critical teacher training and resources needed for effective curriculum implementation. Implementing a comprehensive math curriculum like Zearn becomes particularly daunting in economically disadvantaged areas where financial constraints may obstruct optimal pedagogical strategies.

Moreover, we unveiled a striking finding regarding charter schools. Our data showed a negative relationship between being a Charter School and the Learning Rate. This finding was unexpected, given the increased flexibility often associated with charter schools. It raises pertinent questions about the pedagogical dynamics within such institutions and points towards the necessity for further research into the support systems and professional development opportunities provided for teachers in these settings.

One of the more disconcerting findings was the negative relationship between Poverty Level and Discount Rate, indicating that teachers in schools with higher poverty levels might place less emphasis on long-term student outcomes. This phenomenon may occur due to the pressing short-term challenges related to student well-being and engagement, often prevalent in high-poverty settings, which demand immediate attention and resources.

These findings underscore the complex challenges facing schools in lower-income and high-poverty areas. They illuminate the complexities teachers must navigate when adapting to new teaching habits and implementing curricula, which are only magnified by demographic and school-specific factors.

Our Reinforcement Learning model uncovered the need for a deeper examination of systemic educational disparities and the development of targeted interventions and policies to bridge these gaps. By shedding light on these complex relationships, we contribute to the broader goal of creating a more equitable educational landscape.

## Implications for Teachers and Schools

Our findings have several important implications for teachers, schools, and the broader education field. First, they highlight the dynamic nature of teaching, with teachers continually learning and adapting their strategies based on feedback. This result underscores the importance of providing teachers with ongoing professional development opportunities and supportive learning environments.

Second, our results suggest that optimal teaching strategies vary among teachers, reflecting individual differences in learning rates and decision-making strategies. This feature points to the need for a more personalized approach to teacher training and support, considering individual teachers' strengths and areas for improvement.

Third, our findings highlight the potential of RL models as tools for understanding and improving teaching practices. By capturing the complex dynamics of teacher behavior, these models can inform the design of interventions to enhance student achievement. For instance, interventions could help teachers improve their learning rates or better balance exploration and exploitation in their teaching strategies.

Finally, our results have policy implications. They suggest that policies aimed at improving student achievement should consider not only the resources available to schools but also teachers' behaviors and decision-making processes. Policies should support teachers' learning and adaptation processes by providing professional development opportunities or creating supportive learning environments.

## Limitations

The insights derived from this study potentially have broader implications, extending beyond the confines of Louisiana, given the universal teaching and learning principles underpinning our analysis.

Note that, although extensive, the set of actions was not comprehensive.

One of this study's limitations is the available data, including teachers only in Louisiana, which can limit the generalizability of our findings. Second, our data were at the weekly level, with approximately 40 weeks per classroom. To fit more optimally, RL models would need more trials, either over a more extended period or with smaller time units (e.g., twice a week or daily). Another major challenge of this study was finding the best characterization of actions, which required various techniques for dimensionality reduction. Third, we did not exhaust all possible RL models, and other models could better fit the data. Future research could explore other RL models and compare their performance in characterizing teacher behavior and predicting student achievement. Finally, our models made certain assumptions and simplifications, such as the characterization of actions, which could affect the accuracy of our results. For instance, our data did not include the variance of classroom scores, only the averages, which limits our ability to answer questions about how teachers adapt to the distribution of student achievement in their classrooms.

## Future Research

Our study opens several avenues for future research. First, our findings could be validated and extended in other educational contexts, such as different grade levels, subjects, or geographical locations. Second, our RL models could be integrated with other models and approaches to provide a more comprehensive understanding of teacher behavior and its impact on student outcomes. Third, future research could expand the scope of variables and data sources considered in the models, such as incorporating additional variables related to teacher background, training, and experience. Similarly, data from other sources and domains could enrich the models and provide a more nuanced understanding of human behavior in the field.

In conclusion, our study provides compelling evidence for the potential of reinforcement learning models to understand and improve teacher behavior and student achievement in the context of online learning platforms like Zearn Math. By shedding light on the complex dynamics of teacher behavior, our findings offer valuable insights for teachers, schools, policymakers, and researchers in the education field. Our work will inspire further research and practical applications of reinforcement learning in education.

# References

::: {#refs}
:::

# Supplemental Information {.appendix}

## Correlations Between Variables

We begin to unveil the intricate relationships among the variables under consideration through a comprehensive correlation analysis, as depicted in @fig-corr. This correlation matrix elucidates the magnitude and direction of associations among variables such as badges earned, minutes spent per student, tower alerts, the number of students, and teacher minutes. These interconnections inform the construction of our reinforcement learning models by suggesting the influence of teacher effort on student achievement. In this correlation matrix, each cell represents the Spearman correlation coefficient between a pair of variables. The color and size of the circles in each cell reflect the strength and direction of the correlation, with blue indicating positive correlations and red indicating negative correlations. The histograms along the diagonal provide a visual representation of the distribution of each variable.

```{r}
#| cache: true
#| label: fig-corr
#| fig-cap: "Correlation coefficients between variables after stardardization"
#| fig-format: png

df_corr <- df  %>%
  select(Badges.per.Active.User,
         Active.Users...Total,
         Minutes.per.Active.User,
         Tower.Alerts.per.Tower.Completion,
         Minutes.on.Zearn...Total) %>%
  rename("Badges" = Badges.per.Active.User,
         "Minutes per Student" = Minutes.per.Active.User,
         "Tower Alerts" = Tower.Alerts.per.Tower.Completion,
         "# of Students" = Active.Users...Total,
         "Teacher Minutes" = Minutes.on.Zearn...Total)

chart.Correlation(df_corr, histogram = TRUE, method = "spearman",
                  pch = 20, cex = 0.5, col = rgb(0, 0, 1, 0.5))

```

## Software and Tools

### Dynamic Analysis

We performed the dynamic analysis using the `plm` package in R [@croissant2008], which estimates linear models for panel data. We estimated both within and random effects models and used the Hausman test to select the most appropriate model based on its p-value [@hausman1978]. We estimated each model with different numbers of lags (from 1 to 8) and different methods of matrix factorization (PCA, NMF, and Autoencoder). We divided the data into training (80%) and testing (20%) sets to compute the Bayesian Information Criterion (BIC) and the negative log-likelihood (NLL) for each model. We then selected the number of lags based on the model with the lowest BIC and NLL.

### Base Models

#### Panel Model Estimation for Lag Selection

We specified and estimated panel logistic regression models to capture the dynamics of actions influenced by lagged rewards and actions, including their interactions. The general model formulation for both state-free scenario is presented as follows:

$$
\text{Action}_t = \sum_{i=1}^{L} \left( \beta_{i} \text{Reward}_{t-i} + \gamma_i \text{Action}_{t-i} + \sum_{j=i}^{L} \delta_{ij} (\text{Reward}_{t-i} \times \text{Action}_{t-j}) \right) + \mu_{\text{Teacher}} + \lambda_{\text{Week}} + \epsilon_t 
$$

Model performance was assessed using the area under the receiver operating characteristic curve (AUC) and Bayesian Information Criterion (BIC). Optimal lag structures were identified based on predictive accuracy and model parsimony.

We used a varying coefficients model to capture individual teacher effects on the dynamics between rewards, actions, and states. The model is specified as follows:

```{=tex}
\begin{align*}
\text{Action}_{kt} =& \ \sum_{i=1}^{L} \left( \beta_{ki} \text{Reward}_{k, t-i} + \gamma_{ki} \text{Action}_{k, t-i} + \sum_{j=i}^{L} \delta_{kij} (\text{Reward}_{k, t-i} \times \text{Action}_{k, t-j}) \right) \\
& + \phi_k \text{State}_{kt} + \psi_k (\text{State}_{kt} \times \text{Action}_{k, t-1}) + \mu_k + \lambda_{\text{Week}} + \epsilon_{kt}
\end{align*}
```
where $\beta_{ki}$, $\gamma_{ki}$, and $\delta_{kij}$ represent the reward, action, and their interaction coefficients that vary by teacher; $\phi_k$ and $\psi_k$ are the fixed effects for the state and the interaction between state and lagged action, respectively; and $\mu_k$ and $\lambda_{\text{Week}}$ are fixed effects for teachers and weeks, respectively.

To assess the model's predictive accuracy and generalizability, we used the out-of-sample AUC as a performance metric, calculated separately for each teacher.

### Hierarchical Models

Our initial approach involved fitting non-hierarchical models for three binary action variables using a simple logistic regression model. The dependent variables were derived from the Non-negative Matrix Factorization (NMF) using the Frobenius Nonnegative Double Singular Value Decomposition (NNDSVD) method. For each of these variables, we generated a lagged version and incorporated it into the model alongside the 'Badges' and 'State' variables. The models were constructed using the `brms` package, specifying the Bernoulli family and employing the `cmdstanr` backend [@bates2015; @bürkner2017; @gabry2022; @stanmod2022; @bürkner2021; @croissant2008]. The formula for these models are represented as follows:

$$
a_t = a_{t-1} + Badges_t + s_t + \epsilon_t
$$

where $a_t$ represents the action variable at time $t$, $a_{t-1}$ is the lagged version of the action variable, $Badges_t$ is the reward variable, $s_t$ is the 'State' variable, and $ε_t$ is the error term.

#### Hierarchical Base Models

Subsequently, we constructed hierarchical models for the same set of action variables. We assume Classroom, Teacher, and School are the three levels of the hierarchy. The hierarchical models were similar to their non-hierarchical counterparts but included random intercepts and random slopes for the predictors.

### Reinforcement Learning Model Fit

The reinforcement learning models were implemented using the Stan programming language, a probabilistic programming language designed for statistical inference [@stanmod2022; @gabry2022]. We trained the models on data that included the number of weeks, choices made, and outcomes (log badges) for each classroom. The model parameters, including cost, discount rate, learning rate, and inverse temperature, were estimated from the data. The models employed a Bernoulli logit model to compute action probabilities and updated the expected values of the actions based on prediction errors. The models also generated posterior predictions and computed the log-likelihood for each subject.

Stan employs the Hamiltonian Monte Carlo (HMC) algorithm, a state-of-the-art Markov Chain Monte Carlo (MCMC) method that is particularly well-suited for high-dimensional and complex posterior distributions [@betancourt2017]. We specified three independent MCMC chains to check for convergence of the MCMC algorithm by comparing them. Each chain had 2,500 warmup (burn-in) iterations and 2,500 sampling iterations. During the warmup phase, the HMC algorithm adapts its parameters to the shape of the posterior distribution. The samples drawn during the warmup phase were discarded, and the models ran until they achieved convergence, as assessed by the R-hat statistic, which compares the within-chain and between-chain variance of the MCMC samples; values close to 1 indicate that the chains have converged to the same distribution [@gelman1992].

#### Hierarchical RL Method

We extended the base reinforcement learning models by incorporating a hierarchical structure to account for individual commonalities and enhance robustness. This hierarchical framework defines individual-level parameters as random effects from a group-level distribution. We used the parameter values found by the non-hierarchical models to generate weakly informed priors for the hyper-parameters (group-level parameters). This approach was necessary to ensure the rapid convergence of the Hamiltonian Monte Carlo algorithm. We specified the priors as follows:

+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Parameter           | Group-level Prior                                 | Individual-level Prior                                                            |
+=====================+===================================================+===================================================================================+
| Cost                | $\mu_{\text{cost}} \sim \mathcal{N}(0.5, 1)$\     | $\text{cost}_{i,j} \sim \mathcal{N}(\mu_{\text{cost}_j}, \sigma_{\text{cost}_j})$ |
|                     | $\sigma_{\text{cost}} \sim \text{Cauchy}(0, 2.5)$ |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Discount Rate       | $\mu_{\gamma} \sim \mathcal{N}(0.7, 1)$\          | $\gamma_i \sim \mathcal{N}(\mu_{\gamma}, \sigma_{\gamma})$                        |
|                     | $\sigma_{\gamma} \sim \text{Cauchy}(0, 2.5)$      |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Step Size           | $\mu_{\alpha} \sim \mathcal{N}(0.5, 1)$\          | $\alpha_i \sim \mathcal{N}(\mu_{\alpha}, \sigma_{\alpha})$                        |
|                     | $\sigma_{\alpha} \sim \text{Cauchy}(0, 2.5)$      |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+
| Inverse Temperature | $\mu_{\tau} \sim \mathcal{N}(1, 1)$\              | $\tau_i \sim \mathcal{N}(\mu_{\tau}, \sigma_{\tau})$                              |
|                     | $\sigma_{\tau} \sim \text{Cauchy}(0, 2.5)$        |                                                                                   |
+---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+

where $\mu$ and $\sigma$ denote the group-level hyperparameters, and the subscript $i$ signifies the individual-level parameters.

### Feature Selection

-   Explain how you created different combinations of variables for rewards and state variables.
-   Elaborate on data-driven exploration, model estimation, and HBI.
-   Discuss the code used for actor-critic models and variable combinations.

### Model Performance

To evaluate the performance of the Bayesian models, we used the Leave-One-Out Information Criterion (LOOIC), a robust measure of model quality. The LOOIC is a variant of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). However, unlike AIC and BIC, which use asymptotic approximations, LOOIC is a fully Bayesian criterion that provides a more accurate estimate of out-of-sample prediction error. We used the `loo` package in R to compute the LOOIC [@vehtari2023]. The package uses Pareto smoothed importance sampling (PSIS), a beneficial technique for models where standard cross-validation is computationally expensive or impractical [@vehtari2017].

### Heterogeneity

To investigate the heterogeneity across teachers, we first extracted the posterior samples from the hierarchical model that demonstrated the best performance. For each teacher, we calculated the mean of each parameter: the estimated cost for each action, the discount rate, the learning rate, and the inverse temperature. We then correlated these parameters with the classroom-level variables: the average number of students, average minutes per student, average number of badges per student, average number of tower alerts, the number of classes per teacher, the grade level, the total number of weeks, the proportion of students under the poverty level, the average income level in the school, and whether the school had a paid Zearn account. Subsequently, we fit a linear regression model to predict the average number of badges per active user, using the estimated parameters as predictors.

#### Across Demographics

We extended our analysis to examine the relationship between the estimated parameters and demographic variables for each classroom to reveal whether learning and decision-making processes captured by our model vary across different demographic groups.

## Figures {#sec-supp-fig}

![Zearn Student Portal](images/student-feed.PNG){#fig-st-portal fig-align="center"}

![Professional Development Calendar](images/PD-calendar.jpg){#fig-prof-dev fig-align="center"}

## Zearn's Eye View of the Data

```{r}
#| eval: false
#| label: fig-raw-data
#| fig-cap: "The raw data from Zearn's platform, with each row corresponding to a different timestamp for a teacher action. The columns represent the different variables, including the number of minutes spent on the platform, the number of badges earned, and the number of boosts used. The data is organized by classroom, with each classroom having a different color. The data is also organized by week, with each week having a different shape. The data is organized by teacher, with each teacher having a different line type. The data is organized by school, with each school having a different line color."

```

## Lag Comparison Across NMF Methods

```{r}
#| eval: false
#| label: fig-panel-nmf-methods
#| fig-cap: ""

# Wrangling the results data frame for plotting
results_df <- map_dfr(results, ~as.data.frame(t(unlist(.))))
# Convert variables to appropriate data types
results_df <- results_df %>%
  mutate(Method = as.factor(Method),
         Component = as.factor(Component),
         Lag = as.numeric(Lag),
         nloglik = as.numeric(nloglik),
         bic = as.numeric(bic)) %>%
  group_by(Method, Component, Lag) %>%
  summarise(avg_bic = mean(bic, na.rm = TRUE),
            avg_nloglik = mean(nloglik, na.rm = TRUE),
            .groups = "drop")
# Filter methods for the plots
# methods_for_plots <- c("FrobeniusNNDSVD", "KullbackLeibler", "FrobeniusNNDSVDA")
methods_for_plots <- c("FrobeniusNNDSVD")
plot_data <- filter(results_df, Method %in% methods_for_plots)



# Shared aesthetics for the plots
aes <- ggplot2::aes(x = Lag, y = value, color = Component)

# Summarize methods not included in the plots and prepare tables
methods_for_summary <- setdiff(unique(results_df$Method), methods_for_plots)
summary_data <- filter(results_df, Method %in% methods_for_summary) %>%
  group_by(Method, Component) %>%
  summarise(BIC = mean(avg_bic, na.rm = TRUE),
            NLL = mean(avg_nloglik, na.rm = TRUE),
            .groups = "drop")
# Rename method
levels(summary_data$Method)[levels(summary_data$Method)=="tch_min"] <- "Minutes"
# Convert to scientific notation
summary_data$BIC <- format(summary_data$BIC, scientific = TRUE, digits = 2)
summary_data$NLL <- format(summary_data$NLL, scientific = TRUE, digits = 2)

# Create tables and remove headers
# bic_table <- summary_data %>% select(Method, BIC) %>%
#   tableGrob(theme = ttheme_minimal(base_size = 8, 
#                                    core=list(bg_params = list(fill = "white", col=NA),
#                                              fg_params=list(fontface=3))),
#              rows = NULL)
# bic_table <- bic_table[-1, ]  # Remove header
bic_data <- filter(plot_data, !is.na(avg_bic)) %>% 
  mutate(value = avg_bic, metric = "BIC")
# BIC Plot
bic_plot <- ggplot(bic_data, aes) + 
  geom_line() + 
  labs(title = "BIC", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none")
  # annotation_custom(bic_table, xmin = max(bic_data$Lag) - 3.5,
  #                   xmax = max(bic_data$Lag) - 0.5,
  #                   ymin = 1.01*max(bic_data$value),
  #                   ymax = Inf)

# nll_table <- summary_data %>% select(Method, NLL) %>%
#   tableGrob(theme = ttheme_minimal(base_size = 8, 
#                                    core=list(bg_params = list(fill = "white", col=NA),
#                                              fg_params=list(fontface=3))), 
#              rows = NULL)
# nll_table <- nll_table[-1, ]  # Remove header
nll_data <- filter(plot_data, !is.na(avg_nloglik)) %>% 
  mutate(value = avg_nloglik, metric = "Negative Log-Likelihood")
nll_plot <- ggplot(nll_data, aes) + 
  geom_line() + 
  labs(title = "Out-of-sample NLL", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none")
  # annotation_custom(nll_table, xmin = max(nll_data$Lag) - 3.5,
  #                   xmax = max(nll_data$Lag) - 0.5,
  #                   ymin = 1.01*max(nll_data$value),
  #                   ymax = Inf)

# Combine the plots and add a legend
combined_plot <- ggarrange(bic_plot, nll_plot,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
combined_plot

```

## Autoencoder

```{python train autoencoder}
#| eval: false
import os # To fix: https://github.com/tensorflow/tensorflow/issues/59779
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from keras.models import Model
from keras.layers import Input, Dense
from kerastuner.tuners import Hyperband
from kerastuner.engine.hyperparameters import HyperParameters
from keras.constraints import NonNeg
from keras.callbacks import EarlyStopping
from keras.regularizers import l1, l2

# Create training and testing data frames
train_df = dfpca_py[dfpca_py['set'] == 'train']
test_df = dfpca_py[dfpca_py['set'] == 'test']

# Predictors
X_train = train_df.drop([
  'Badges.per.Active.User', 'Classroom.ID', 'MDR.School.ID',
  'set', 'week'
  ], axis=1)
X_test = test_df.drop([
  'Badges.per.Active.User', 'Classroom.ID', 'MDR.School.ID',
  'set', 'week'
  ], axis=1)

X_train = pd.DataFrame(X_train, columns=X_cols)
X_test = pd.DataFrame(X_test, columns=X_cols)

# Get the target variable
Y = dfpca_py[['Badges.per.Active.User']]
Y_train = train_df[['Badges.per.Active.User']]
Y_test = test_df[['Badges.per.Active.User']]

# Define the number of components and features
n_features = X_scaled.shape[1]
n_labels = 1  # For regression, we usually have just one output node

# Determine loss weights according to the data structure:
decoding_weight = Y.std() / (Y.std() + X_scaled.std(numeric_only=True).mean())
prediction_weight = 1 - decoding_weight
def build_model(hp):
  # Define the input layer
  input_data = Input(shape=(n_features,))
  
  # Define the encoding layer(s)
  n_layers = hp.Int('n_layers', min_value=1, max_value=4, step=1)
  n_units = [
    hp.Choice('units_' + str(i), values=[8, 16, 32, 64, 128, 256, 512])
    for i in range(n_layers)
  ]
  encoded = input_data
  for i in range(n_layers):
      encoded = Dense(
        units=n_units[i],
        kernel_regularizer=l2(0.001),
        activation='relu')(encoded)
        
  # Generate the latent vector
  latent_dim = hp.Int('encoding_units', min_value=2, max_value=n_comp, step=1)
  l1_value = hp.Float('l1_value', min_value=0.0001, max_value=0.001, default=0.0005, step=0.0001)
  latent = Dense(
    units=latent_dim,
    activation='linear',
    # activity_regularizer= L1(l1=l1_value),
    kernel_constraint=NonNeg(),
    name='latent')(encoded)      

  # Decoder
  decoded = latent
  for i in range(n_layers):
      decoded = Dense(
        units=n_units[n_layers - i - 1],
        activation='relu')(decoded)
  decoded = Dense(n_features, activation='sigmoid', name='decoded')(decoded)
  
  # Define the label output layer
  label_output = Dense(n_labels, activation='linear', name='label_output')(latent)

  # Define the autoencoder model
  autoencoder = Model(inputs=input_data, outputs=[decoded, label_output])
  # Compile the model
  autoencoder.compile(optimizer='adadelta',
                  loss={
                    'decoded': 'mean_squared_error',
                    'label_output': 'mean_squared_error'
                  },
                  loss_weights={
                    'decoded': decoding_weight,
                    'label_output': prediction_weight
                  })
                  
  return autoencoder

# Set up hyperparameter tuner
tuner = Hyperband(build_model,
                  objective='val_loss',
                  max_epochs=20,
                  directory='autoencoder_tuning',
                  project_name='autoencoder_3rd_tuning')

# Perform hyperparameter search
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)
tuner.search(x=X_train,
            y=[X_train, Y_train],
            epochs=50,
            validation_data=(X_test, [X_test, Y_test]),
            callbacks=[early_stopping_callback])
# tuner.results_summary()
# Get the optimal hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=2)[1]
model = tuner.hypermodel.build(best_hyperparameters)
history = model.fit(x=X_train, 
                    y=[X_train, Y_train],
                    epochs=2_000,
                    validation_data=(X_test, [X_test, Y_test]),
                    callbacks=[early_stopping_callback])
best_model = model


# Function to get encoded representation and components
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

# Get encoded representation and components
get_encoded_representation_and_components(best_model, X_scaled)

# save the model
model.save('./autoencoder_tuning/final_model.h5')

```

```{python load autoencoder}
#| eval: false
from tensorflow.keras.models import load_model
from keras.models import Model
from keras.layers import Input

# Function to get encoded representation and components
loaded_model = load_model('./autoencoder_tuning/final_model.h5')
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

n_features = X_scaled.shape[1]
# Get encoded representation and components
get_encoded_representation_and_components(loaded_model, X_scaled)
```

## Bayesian Model Diagnostics

```{r}
#| eval: false
#| cache: true
#| label: tbl-diagnostics
#| tbl-cap: "Diagnostics for the Bayesian Models used in Q-learning and Q-learning with states. The tables show diagnostics for three Non-negative Matrix Factorization (NMF) models: Frobenius (initialized with a Nonnegative Double Singular Value Decomposition), Frobenius Average (initialized with a Nonnegative Double Singular Value Decomposition with zeros filled with the average of each variable), and Kullback-Leibler. The rows correspond to 'alpha,' the learning rate, 'gamma,' the discount factor, 'tau,' the temperature parameter, 'cost' for each of the components from the NMF, 'sensi,' the reward sensitivity parameter, and 'Log Posterior,' the logarithm of the posterior probability. 'Rhat' indicates the potential scale reduction factor on split chains. Values close to 1.0 imply model convergence. 'ESS Bulk' and 'ESS Tail' represent the effective sample size for the bulk of the posterior distribution and its tail, respectively. An ESS of at least a few hundred is often considered sufficient for estimation purposes."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]
results_files <- results_files[!grepl("hierarchical", results_files)]
diag_df <- data.frame()

for (result_file in results_files) {
  fit <- readRDS(result_file)
  
  # Get summary
  fit_summary <- fit$summary()
  
  # Create a data frame of the Rhat values and ESS
  result_df <- data.frame(
    Model = gsub("Bayesian/Results/||.RDS", "", result_file),
    Variable = fit_summary$variable,
    Rhat = fit_summary$rhat,
    ESS_Bulk = fit_summary$ess_bulk,
    ESS_Tail = fit_summary$ess_tail
  )
  
  # Add result to the overall diagnostics data frame
  diag_df <- rbind(diag_df, result_df)
}
# Reshape the data frame
diag_df <- diag_df %>%
  pivot_longer(cols = c(Rhat, ESS_Bulk, ESS_Tail),
               names_to = "Statistic") %>%
  pivot_wider(names_from = c(Model, Statistic),
              values_from = value) %>%
  mutate(Variable = if_else(Variable == "lp__", "Log Posterior", Variable))

# Split the data into two separate data frames
diag_df_actor_critic <- diag_df[,c(TRUE, grepl("Actor-Critic-", names(diag_df))[-1])]
diag_df_q_learning_states <- diag_df[,c(TRUE, grepl("Q-learning-states-", names(diag_df))[-1])]
diag_df_q_learning <- diag_df[,c(TRUE, (!names(diag_df) %in% names(diag_df_q_learning_states) &
                                          !names(diag_df) %in% names(diag_df_actor_critic))[-1])]

# Define the spanner labels for each model
diag_df_q_learning %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_q_learning_states %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_actor_critic %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

```

## Bayesian Model Estimates

This section presents the group parameter estimates for all Reinforcement Learning (RL) models, both non-hierarchical and hierarchical. To generate these group parameter estimates, we first extract the posterior samples from the fitted models and calculate the mean (M), standard deviation (SD), 25th percentile (Q1), and 75th percentile (Q3) for each parameter. @tbl-group-parameters summarizes the distribution of the parameter estimates across classrooms, revealing their typical values and variability.

```{r}
#| eval: false
#| cache: true
#| label: tbl-group-parameters
#| tbl-cap: "Group parameter estimates for the Bayesian Models used in Q-learning and Q-learning with states."

# Define a function that processes one file
process_file <- function(result_file) {
  # Load the fitted model
  fit <- readRDS(result_file)
  
  # Extract the posterior samples of the parameters
  post_samples <- fit$draws()
  
  # Calculate the group parameter estimates
  group_params <- post_samples %>%
    as.data.frame() %>%
    summarise(across(everything(), list(M = mean, SD = sd, Q1 = ~quantile(., 0.25), Q3 = ~quantile(., 0.75))))
  
  # Add the model name to the data frame
  group_params$Model <- gsub("Bayesian/Results/||.RDS", "", result_file)
  
  return(group_params)
}

# Apply the function to each results file and bind the results into one data frame
group_params_df <- purrr::map_dfr(results_files, process_file)

# Continue with the rest of your code
group_params_df %>%
  gt() %>%
  tab_header(
    title = "Group Parameter Estimates",
    subtitle = "The table shows the mean (M), standard deviation (SD), 25th percentile (Q1), and 75th percentile (Q3) of the learning rate (alpha), inverse temperature (beta), weights, and cost parameters for each model."
  ) %>%
  cols_label(
    Model = "Model",
    M = "Mean",
    SD = "Standard Deviation",
    Q1 = "25th Percentile",
    Q3 = "75th Percentile"
  ) %>%
  fmt_number(
    columns = c("M", "SD", "Q1", "Q3"),
    decimals = 2
  )


```

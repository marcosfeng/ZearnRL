---
title: "Unveiling Adaptive Pedagogy: Reinforcement Learning Models Illuminate Teacher Decision-Making in an Online Math-Teaching Platform"
keywords: "Reinforcement Learning, Pedagogical Decision-Making, Digital Education Platforms, Instructional Adaptation, Q-learning, Actor-Critic"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
format:
  elsevier-pdf:
    keep-tex: true
    include-in-header:
      - text: |
          \usepackage{dcolumn}
          \usepackage{typearea}
          \usepackage{longtable}
    journal:
      formatting: preprint
      model: 3p
      layout: onecolumn
      cite-style: number
      # graphical-abstract: "![](abstract.png)"
bibliography: zearnrefs.bib

execute:
  echo: false
  warning: false
  error: false

knitr:
  opts_chunk:
    cache.extra: set.seed(832399554)
---

```{r load packages}
# Figures
library(ggforce)
library(pheatmap)
library(scales)
library(RColorBrewer)
library(ggrepel)
library(ggpubr)

# Tables
library(stargazer)
library(gtsummary)
library(gtExtras)
library(kableExtra)

# Other platforms
library(R.matlab)
library(reticulate)

# Statistics
library(broom)
library(PerformanceAnalytics)
library(ppcor)
library(fixest)
library(pROC)
library(glmmTMB)
library(broom.mixed)

# Basic packages
library(doParallel)
library(data.table)
library(tidyverse)

# library(cmdstanr)
# library(brms)
# library(bayesplot)
```

```{r}
set.seed(832399554)
random_py <- reticulate::import("random")
random_py$seed(832399554)
# https://www.random.org/integers/
# Timestamp: 2023-05-17 16:18:28 UTC
```

# Theory

## Q-Learning Model

Consider a teacher using the Zearn platform. Each week, they must decide between two actions: assigning additional homework (action 1) or spending more time reviewing the material in class (action 2). At first, the teacher is uncertain about the best action to take. They start with initial beliefs about each action's long-term value (Q-value). However, they know that these beliefs may not be accurate and that they need to learn from experience.

Each week, the teacher chooses an action based on the current Q-value estimates. For example, in week 1, the teacher believes that assigning homework (action 1) has a slightly higher Q-value than reviewing in class (action 2). So, they assign homework and observe the outcome.

Then, the teacher receives a reward signal (e.g., the students' performance after the homework assignment). They use this reward to update their estimate of the Q-value for assigning homework, following the Q-learning update rule. This rule adjusts the Q-value estimate based on the difference between the observed reward and the previous estimate multiplied by a learning rate parameter.

Over the following weeks, the teacher continues to make decisions and update their estimates based on the outcomes observed. Sometimes, they explore new actions to gather more information, even if these actions do not seem optimal based on the current estimates. Other times, the teacher exploits their experience by choosing the action with the highest estimated Q-value.

As the teacher learns from experience, the Q-value estimates gradually converge toward the true values for each action.

| Week | Q-value Difference | Policy | Choice | Reward | Prediction Error | Updated Q-value |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| $t$ | $Q_t$ | $\text{Pr}_t(a = 1)$ | $a$ | $R_t$ | $\delta_t = \gamma R_t-c-Q_t$ | $Q_{t+1}=Q_{t}+\alpha \delta_t$ |
| 1 | 1.533 | 1 | 1 | 0.052 | -2.891 | 0.815 |
| 2 | 0.815 | 1 | 1 | 0.163 | -2.122 | 0.288 |
| 3 | 0.288 | 0.955 | 1 | 0.047 | -1.649 | -0.121 |
| 4 | -0.121 | 0.218 | 2 | 0.039 | -0.018 | -0.125 |
| 5 | -0.125 | 0.210 | 2 | 0.064 | -0.030 | -0.133 |
| ... |  |  |  |  |  |  |

: Example of a Q-learning algorithm for a teacher on the Zearn platform. Each week ($t$), the teacher decides between action 1 and action 2 based on the difference in Q-values ($Q_t$) for each action. The teacher's choice ($a$) is determined by the policy ($\text{Pr}_t(a = 1)=1/(1+e^{-\tau Q_t})$). After observing the reward ($R_t$) associated with the chosen action, the teacher computes the prediction error ($\delta_t$) using the discount factor ($\gamma$) and the cost ($c$) of the action. The Q-value is then updated using the learning rate ($\alpha$) and the prediction error. As the teacher learns from experience, the Q-value converges toward the value that yields the highest reward. The values used here were drawn from an actual Zearn account ($\alpha = 0.25$, $\tau = 10.57$, $\gamma = 0.46$, $\text{cost} = 1.38$). {#tbl-qvalue-example}

This model frames decision-making as a result of accumulated experience and the anticipation of future rewards. In other words, Q-learning involves the iterative refinement of Q-value functions, which map an agent's actions to evolving expectations of future rewards (analogous to subjective value or utility). This methodological approach is closely related to the classic "multi-armed bandit" problem, wherein the agent faces a finite set of choices (e.g., slot machines), each linked to a specific reward schedule, and aims to learn the action that yields the highest returns. Learning in this model depends on adjusting expectations to reduce the impact of prediction errors (the "surprise level," or the difference between expected and realized outcomes), using the Bellman equation to update Q-values iteratively [@rummery].

In this study, we opt for a state-independent version of Q-learning. That is, the Q-values do not vary with a contextual variable but are learned for each action only. This assumption is useful in scenarios where the state exerts minimal influence on the outcome of the action or when the state is difficult to define or observe [@sutton2018]. As such, the Q-function represents the expected return or future reward for taking action $a \in A = \{a_1,a_2,...\}$ following a certain policy $\pi = \Pr(a)$. The updating rule uses the Bellman equation as follows:

$$
Q_{t}(a) = Q_{t-1}(a) + \alpha \delta_t
$$ {#eq-q-learn}

where $\alpha$ is the learning rate, which determines how much the Q-value is updated based on $\delta$, the reward prediction error. The reward prediction error is the difference between the estimated Q-value and the observed reward. This error is used to update the Q-value of the chosen action in the direction of the observed reward, scaled by the learning rate $\alpha$, as follows:

$$
\delta_t = \gamma R_t - Q_{t-1}(a)
$$ {#eq-RPE}

where:

-   $a$ is the chosen action,

-   $R_t$ is the immediate reward received after taking action $a$,

-   $\gamma$ is the discount factor[^1],

-   $Q_{t-1}(a)$ is the estimate of the Q-value for action $a$ in the previous period.

[^1]: Commonly, this parameter captures the degree to which future rewards are discounted compared to immediate rewards. In this example, it could also act as a scaling factor of net reward.

In other words, $\alpha$ is the extent to which the newly acquired information will override the old information. A value of 0 means the agent does not learn anything. The agent starts with an initial Q-value (which can be arbitrary) and then updates the Q-values based on the experiences it gathers from interactions with the environment. The update rule is applied every time the agent takes action $a$ and receives a reward $R$. The agent selects actions based on a policy function of the Q-values. A common choice is the softmax action selection method, which chooses actions probabilistically based on their Q-values, as follows:

$$
\text{Pr}_t(a) = \frac{e^{\tau U_t(a)}}{\sum_{a'} e^{\tau U_t(a')}}
$$ {#eq-softmax}

where:

-   $\Pr_t(a)$ is the probability of choosing action $a$ at time $t$,

-   $U_t(a) = Q_t(a) - \text{cost}(a)$ is the utility of action $a$ at time $t$, which is the difference between the Q-value of action $a$ ($Q_t(a)$) and the cost associated with taking that action,

-   $\tau$ is a parameter known as the inverse temperature, or the degree of randomness in the choice behavior,[^2]

-   $\text{cost}(a)$ is the perceived effort or inconvenience associated with action $a$,

-   The denominator is the sum over all possible actions $a' \in A$ of the exponential of their Q-values multiplied by the inverse temperature, and it functions as a normalizing value.

[^2]: One possible interpretation of the inverse temperature parameter $\tau$ is the agent's confidence in its Q-values, which controls the trade-off between exploration and exploitation. When $\tau$ is high, the agent explores more because the action probabilities are more uniform. When $\tau$ is low, the agent exploits more because the action with the highest Q-value is more likely to be chosen than the others. Some models may also allow for agents to start with a high inverse temperature to encourage exploration and then gradually decrease it to favor the exploitation of the learned policy.

### Binary Actions

For cases in which actions are binary (i.e., only two options $a_1$ and $a_2$), we set one of the actions as an outside option with a Q-value and cost of zero (i.e., base value $Q(a_2)=0$ and $\text{cost}(a_2)=0$). In this case, we update the difference in Q-values, using the same Bellman equation but a modified prediction error:

$$
\delta_t =
\begin{cases}
\gamma R_t - \Delta Q_{t-1} & \text{if } a_1 \text{ is chosen} \\
-\gamma R_t & \text{if } a_2 \text{ is chosen}
\end{cases}
$$ {#eq-state-free}

where:

-   $\Delta Q_{t}=Q_{t}(a_1)-Q_{t}(a_2)$ is the estimate of the difference in Q-values for the two actions,

-   $\alpha$ is the learning rate,

-   $R_t$ is the immediate reward received after taking the action.

Thus, the probability of choosing a particular action is determined by the logistic function as follows:

$$
\text{Pr}_t(a_1) = \frac{1}{1+e^{-\tau \Delta U_{t}(a)}}
\\
\text{Pr}_t(a_2) = 1 - \text{Pr}_t(a_1)
$$

where:

-   $\Delta U_{t}=\Delta Q_{t} - \text{cost}(a_1)$ is the estimate of the difference in Q-values for the two actions.

# Results

## Selecting a model specification

To analyze Zearn data spanning an entire academic year, we first establish a framework for actions and rewards. In this framework, teacher activities drive the educational process, and student achievements result from these efforts. Instead of relying solely on one analytical approach, our strategy involves a large set of candidate models, as shown in @tbl-methods. Our overarching goal was to strengthen the reliability of our findings and offer a detailed understanding of the underlying behavioral patterns.

```{r data prep}

dt <- read.csv(file = "Data/df_clean.csv")

# Convert columns to appropriate data types
setDT(dt)
# Rename variable
dt[, `:=`(
  poverty = factor(poverty, ordered = TRUE, exclude = c("", NA)),
  income = factor(income, ordered = TRUE, exclude = c("", NA)),
  st_login = fifelse(Minutes.per.Active.User > 0, 1, 0, na=0),
  tch_login = fifelse(User.Session > 0, 1, 0, na=0)
  # Log Transform
  # Badges.per.Active.User = log(Badges.per.Active.User + 1),
  # Tower.Alerts.per.Tower.Completion = log(Tower.Alerts.per.Tower.Completion + 1),
  # tch_min = log(tch_min + 1)
)]

# Code datetime variables and compute additional metrics
setorder(dt, Teacher.User.ID, year, week, Classroom.ID)
dt[, isoweek := week]
dt[, week := week + 52*(year - 2019)]
# Fixing week == 1 (last week of 2019 counts as week 1 of 2020)
dt[week == 1, week := week + 52]
dt[, first_week := min(week), by = .(Teacher.User.ID)]
dt[, week := week - first_week + 1]
dt[, Tsubj := max(week), by = .(Classroom.ID)]

# Update the Grade.Level values and labels
dt <- dt[!(Grade.Level %in% c(-1, 11))] # Ignore -1 and 11
dt[, Grade.Level := factor(Grade.Level,
                           ordered = TRUE,
                           exclude = c(""))]
dt[, Grade.Level := factor(Grade.Level,
                           levels = c(0:8),
                           labels = c("Kindergarten", "1st", "2nd",
                                      "3rd", "4th", "5th",
                                      "6th", "7th", "8th"))]

df <- as.data.frame(dt) %>%
  ungroup()

# Convert year and isoweek to a date (Monday of that week)
df <- df %>%
  mutate(date = as.Date(paste(year, isoweek, 1, sep="-"), format="%Y-%U-%u"))

```

```{r table-summary-code}

df_summary <- df %>%
  group_by(MDR.School.ID) %>%
  summarise(
    Unique_Teacher_Count = n_distinct(Teacher.User.ID)
  ) %>%
  na.omit() %>%
  left_join(df %>%
              group_by(Classroom.ID, MDR.School.ID) %>%
              summarise(
                Students_Total = mean(Students...Total, na.rm = TRUE),
                Weeks_Total = max(week)
                ) %>%
              group_by(MDR.School.ID) %>%
              summarize(
                Students_Total = sum(Students_Total, na.rm = TRUE),
                Weeks_Total = mean(Weeks_Total)
                ),
            by = "MDR.School.ID") %>%
  ungroup()

# Summary statistics table
gt_school_sum <- df_summary %>%
  summarise(
    Mean_Teachers = mean(Unique_Teacher_Count),
    Median_Teachers = median(Unique_Teacher_Count),
    SD_Teachers = sd(Unique_Teacher_Count),
    Min_Teachers = min(Unique_Teacher_Count),
    Max_Teachers = max(Unique_Teacher_Count),
    Mean_Students_Total = mean(Students_Total),
    Median_Students_Total = median(Students_Total),
    SD_Students_Total = sd(Students_Total, na.rm = TRUE),
    Min_Students_Total = min(Students_Total),
    Max_Students_Total = max(Students_Total),
    Mean_Weeks_Total = mean(Weeks_Total),
    Median_Weeks_Total = median(Weeks_Total),
    SD_Weeks_Total = sd(Weeks_Total, na.rm = TRUE),
    Min_Weeks_Total = min(Weeks_Total),
    Max_Weeks_Total = max(Weeks_Total)
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Measure", "Variable"),
           sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value) %>%
  mutate(Variable = gsub("_Total", "", Variable)) %>%
  gt(rowname_col = "Variable") %>%
  # cols_label(Mean = "Mean", SD = "Standard Deviation",
  #            Min = "Minimum", Max = "Maximum") %>%
  fmt_number(
    columns = c("Mean", "SD"),
    decimals = 2
  ) %>%
  fmt_number(
    columns = c("Median", "Min","Max"),
    decimals = 0
  )

# Pre-exclusion variables
N_class_pre = length(unique(df$Classroom.ID))
N_teachers_pre = length(unique(df$Teacher.User.ID))
avg_std_pre = round(mean(df$Students...Total, na.rm = T), 1)

```

```{r draw-classroom-weeks}

# Create the histogram
fig_classroom_weeks <- df %>%
  group_by(Classroom.ID) %>%
  summarize(Tsubj = n()) %>%
  mutate(Tsubj_category = if_else(Tsubj < 18, "less than 18", "18 or more")) %>%
  ggplot(aes(x = Tsubj, fill = Tsubj_category)) +
  geom_histogram(color = "black", breaks = seq(min(df$Tsubj, na.rm = T),
                                               max(df$Tsubj, na.rm = T) + 1,
                                               by = 2)) +
  geom_vline(xintercept = 17, color = "darkgray",
             linetype = "dashed", linewidth = 0.8) +
  annotate("text", x = 10, y = 4000, label = "Excluded\nClassrooms",
           vjust = 1, color = "red") +
  labs(x = "Total Number of Weeks",
       y = "Frequency (Classrooms)") +
  scale_fill_manual(values = c("less than 18" = "red",
                               "18 or more" = "steelblue")) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(1, seq(5, max(df$Tsubj, na.rm = T), by = 5)))

```

```{r draw-logins-week}

# Calculate the sum of login values by date and Teacher.User.ID
login_data <- df %>%
  group_by(date, Teacher.User.ID) %>%
  summarize(tch_login = max(tch_login),
            st_login  = max(st_login)) %>%
  group_by(date) %>%
  summarize(tch_logins = sum(tch_login),
            st_logins  = sum(st_login))
# Create bar plot
bar_plot <- ggplot() +
  geom_bar(data = login_data, aes(x = date, y = st_logins), stat = "identity") +
  # geom_point(data = login_data, aes(x = date, y = tch_logins), color = "blue") +
  labs(
    x = "Week",
    y = "Total Logins"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12)) +
  scale_x_date(date_breaks = "3 week", date_labels = "%Y-%m-%d")

# Add labels for Christmas and Thanksgiving
logins_week <- bar_plot +
  geom_text(aes(x = as.Date("2019-12-25"), y = 1250, label = "Christmas"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "red") +
  geom_text(aes(x = as.Date("2019-11-25"), y = 1250, label = "Thanksgiving"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "darkorange")

```

```{r draw-map}
#| cache: true

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
# Set the library path to where proj.db is located
# Sys.setenv(PROJ_LIB = "")
unique_zipcodes <- unique(dt$zipcode) %>%
  na.omit() %>%
  as.list()
# plan(strategy = "multisession", workers = availableCores())
plan(strategy = "multisession", workers = 1)
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes,
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
dt <- merge(dt, address_geodata,
            by.x = "zipcode",
            by.y = "postalcode",
            all.x = TRUE)
dt <- dt[!is.na(zipcode)]

# Aggregate the data to get the number of teachers in each county
dt_map <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID)
), by = .(county)]

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID),
  lat = mean(lat),
  long = mean(long)
), by = .(city, county)]
# Get the top 5 cities with the most teachers
top_cities <- top_cities[order(-num_teachers)][1:5,]

# Get the Louisiana county map data
df_map <- tigris::counties(cb = TRUE,
                           resolution = "20m",
                           class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    as.data.frame(dt_map),
    by = c("NAMELSAD" = "county")
  ) %>%
  sf::st_as_sf() %>%
  mutate(num_teachers = ifelse(is.na(num_teachers), 0, num_teachers))

map_LA <- ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers", low = "white", high = "red", na.value = "gray90") +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city), size = 3, color = "black")
```

```{r draw-income-dist}

df_proportions <- df %>%
  filter(!is.na(poverty)) %>%
  group_by(poverty) %>%
  summarise(n = n()) %>%
  mutate(Percentage = paste0(
    round(n / sum(n) * 100, digits = 2), "%"
    )) %>%
  select(-"n") %>%
  na.omit() %>%
  rename(Variable = poverty) %>%
  mutate(Variable = as.character(Variable)) %>%
  bind_rows(df %>%
              filter(!is.na(income)) %>%
              group_by(income) %>%
              summarise(n = n()) %>%
              mutate(Percentage = paste0(
                round(n / sum(n) * 100, digits = 2), "%"
                )) %>%
              select(-"n") %>%
              na.omit() %>%
              rename(Variable = income) %>%
              mutate(Variable = as.character(Variable))
            ) %>%
  bind_rows(df %>%
              ungroup() %>%
              summarise(
                Charter_Schools = mean(charter.school, na.rm = T)*100,
                Schools_with_Paid_Account = mean(school.account, na.rm = T)*100
                ) %>%
              mutate(
                Charter_Schools = paste0(round(Charter_Schools, digits = 2), "%"),
                Schools_with_Paid_Account = paste0(round(Schools_with_Paid_Account,
                                                         digits = 2), "%")) %>%
              t() %>% as.data.frame() %>%
              rename(Percentage = V1) %>%
              mutate(Variable = row.names(.))) %>%
  add_row(Variable = "Poverty Level", Percentage = "", .before = 1) %>%
  add_row(Variable = "Income", Percentage = "", .before = 5) %>%
  add_row(Variable = "Other", Percentage = "", .before = 23)

# Splitting df_proportions into different categories for pie charts
df_poverty <- df_proportions[2:4,]
df_income <- df_proportions[6:22,]

# Convert Percentage to numeric
df_poverty$Percentage <- as.numeric(gsub("%", "", df_poverty$Percentage))/100

# Create Bar Graph for Poverty Level
poverty_plot <- ggplot(df_poverty, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Poverty Level",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Convert Percentage to numeric
df_income$Percentage <- as.numeric(gsub("%", "", df_income$Percentage))/100

# Create Bar Graph for Income Distribution
income_plot <- ggplot(df_income, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Income Range",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r preprocess data}
#| include: false

dt <- setDT(df)
dt[, `:=`(
  n_weeks = .N,
  mean_act_st = mean(Active.Users...Total, na.rm = TRUE)
  ), by = Classroom.ID]

dt <- dt[
  n_weeks > 18 & # At least 4.5 months cumulative activity
    Tsubj < 2*n_weeks & # At least activity twice a month on average
    teacher_number_classes < 5 &
    Students...Total > 5 &
    mean_act_st > 3 &
    !(Grade.Level %in% c("6th","7th","8th")) &
    !(month(date) %in% c(6, 7, 8)) &
    !is.na(District.Rollup.ID),
]

df <- as.data.frame(dt) %>%
  select(-X) %>%
  mutate(Minutes.per.Active.User = as.numeric(Minutes.per.Active.User)) %>%
  mutate(across(Active.Users...Total:Tower.Alerts.per.Tower.Completion,
         ~ ifelse(is.na(.), 0, .))) %>%
  arrange(Classroom.ID, week)

```

```{r table-classroom-summary-code}

create_summary <- function(var_name, var_label, type = "continuous") {
  summary_stat <- df %>%
    select(Grade.Level, var_name) %>%
    tbl_summary(
      by = Grade.Level,
      missing = "no",
      type = list(var_name ~ type),
      statistic = var_name ~ ifelse(type == "continuous",
                                    "{mean} ({sd})",
                                    "{n} ({p})")
    ) %>%
    add_overall() %>%
    as_tibble()
  summary_stat[1] <- var_label
  return(summary_stat)
}

summaries_list <- list(
  # create_summary("Sessions.per.Active.User", "Sessions per Student"),
  create_summary("Minutes.per.Active.User", "Minutes per Student"),
  create_summary("Badges.per.Active.User", "Badges per Student"),
  create_summary("Tower.Alerts.per.Tower.Completion",
                 "Tower Alerts per Lesson Completion"),
  # create_summary("tch_login", "Teacher Login (0/1)", type = "dichotomous"),
  create_summary("Minutes.on.Zearn...Total", "Minutes per Teacher")
)

school_summary <- bind_rows(summaries_list) %>%
  select(-c(9:11)) %>% t() %>% as.data.frame()
colnames(school_summary) <- school_summary[1,]
# Remove the first row and set row names
school_summary <- school_summary[-1, ]
school_summary$`Grade Level` <- rownames(school_summary)

# Create a gt table
gt_classroom_sum <- school_summary %>%
  mutate(`Grade Level` = gsub("\\*\\*", "", `Grade Level`)) %>%
  gt(rowname_col = "Grade Level") %>%
  cols_label(
    `Minutes per Student` = "Minutes",
    `Badges per Student` = "Badges",
    `Tower Alerts per Lesson Completion` = "Tower Alerts",
    `Minutes per Teacher` = "Teacher Minutes"
  )


```

## Dimensionality Reduction

We first conducted a dimensionality reduction with Non-negative Matrix Factorization (NMF) and four components (see @fig-nmf-pca-comparison for a comparison of different methods by balancing reconstruction accuracy, i.e., R-squared, with clustering clarity, i.e., Silhouette Scores). Given that our chosen RL models require discrete action variables, we choose to split teacher actions into binary variables, following a median split. In our case, a median split is equivalent to giving a value of 1 to any positive value.

```{r pca nmf data-prep}
#| include: false

df <- df %>%
  filter(!is.na(Minutes.on.Zearn...Total)) %>%
  group_by(Classroom.ID) %>%
  # train/test split
  mutate(set = sample(c("train", "test"), size = n(),
                      prob = c(0.8, 0.2), replace = TRUE)) %>%
  ungroup() %>% arrange(Classroom.ID, week)

columns <- names(
  df %>% select(RD.elementary_schedule:Minutes.on.Zearn...Total,
                Active.Users...Total:Tower.Alerts.per.Tower.Completion)
  )

# Create base data.table for models (faster than data.frame)
df_pca <- as.data.table(df)
# Convert columns to double to prevent precision loss
df_pca[, (columns) := lapply(.SD, as.numeric),
        .SDcols = columns]

# Apply the scaling operation
df_pca[, (columns) := lapply(.SD, function(x) {
  x[is.na(x)] <- 0
  sd_x <- sd(x, na.rm = TRUE)
  if(sd_x == 0 | is.na(sd_x)) return(rep(0, .N))
  x / sd_x
}), by = MDR.School.ID, .SDcols = columns]
setorder(df_pca, Classroom.ID, week)

# Calculate standard deviations
std_devs <- apply(df_pca %>% select(all_of(columns)), 2, sd, na.rm = T)
# Identify columns with defined standard deviations (not NaN or Inf)
invalid_cols <- names(std_devs[is.na(std_devs) |
                                 is.infinite(std_devs) |
                                 std_devs == 0])
df_pca <- df_pca %>% select(-all_of(invalid_cols))

```

```{python load data}
#| include: false
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA, NMF
from sklearn.metrics import silhouette_score

## Basic Variables
# Import data from R
dfpca_py = pd.DataFrame(r.df_pca)
dfpca_py.sort_values(['Classroom.ID', 'week'], inplace=True)

# Split the data into teacher and student subsets
teacher_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "RD.elementary_schedule"
    ):dfpca_py.columns.get_loc(
      "RD.grade_level_teacher_materials"
      # "Minutes.on.Zearn...Total"
      )+1
  ]
student_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "Active.Users...Total"
    ):dfpca_py.columns.get_loc(
      "Tower.Alerts.per.Tower.Completion"
      )+1
  ]

X_teachers = dfpca_py[teacher_variables]
X_students = dfpca_py[student_variables]

# Dictionaries for export
components = {}
results = {}
residuals = {}
silhouette = {}

```

```{python pca-nmf}
#| cache: true
#| include: false

# Function for NMF
def nmf_method(n, method, initial, X_scaled, data_label, solv = 'mu', nomin = False):
  method_name = f"{method.title()} {initial.upper()}"

  if method == 'frobenius' and initial == 'nndsvd': solv = 'cd'
  if method != 'frobenius' and initial == 'nndsvd': return
  if method != 'frobenius': method_name = f"{method.title()}"
  if nomin: method_name = method_name + "_nomin"

  nmf = NMF(
    n_components=n,
    init=initial,
    beta_loss=method,
    solver=solv,
    max_iter=4_000
  )
  X_nmf, nmf_comp = nmf.fit_transform(X_scaled), nmf.components_
  X_hat = nmf.inverse_transform(X_nmf)
  labels = np.argmax(nmf_comp, axis=0)

  results.setdefault(f"{method_name}_{data_label}", {})[n] = X_nmf
  components.setdefault(f"{method_name}_{data_label}", {})[n] = nmf_comp
  residuals.setdefault(f"{method_name}_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
  silhouette.setdefault(f"{method_name}_{data_label}", {})[n] = silhouette_score(nmf_comp.transpose(), labels)

def perform_pca_nmf(X_scaled, data_label, n_comp):
  for n in range(2, n_comp):
    ## PCA
    pca = PCA(n_components=n)
    X_pca = pca.fit_transform(X_scaled)
    pca_comp = pca.components_
    X_hat = pca.inverse_transform(X_pca)
    labels = np.argmax(pca_comp, axis=0)
    results.setdefault(f"PCA_{data_label}", {})[n] = X_pca
    components.setdefault(f"PCA_{data_label}", {})[n] = pca_comp
    residuals.setdefault(f"PCA_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
    silhouette.setdefault(f"PCA_{data_label}", {})[n] = silhouette_score(pca_comp.transpose(), labels)

    ## Non-negative Matrix Factorization
    for method in {'frobenius', 'kullback-leibler'}:
      for initial in {'nndsvd', 'nndsvda'}:
        nmf_method(
          n, method, initial, X_scaled,
          data_label=data_label,
          nomin = True    # No teacher minutes included
          )

# Run PCA and NMF for teachers
perform_pca_nmf(X_teachers, "teachers", min(X_teachers.shape) // 3)
# Run PCA and NMF for students
perform_pca_nmf(X_students, "students", min(X_students.shape))

```

```{python clean environment}

# get the names of all variables in the global namespace
all_vars = list(globals().keys())

# list of variables to keep
keep_vars = ['residuals', 'silhouette', 'components', 'results', 'r']

# delete all variables that are not in keep_vars
for var in all_vars:
    if var not in keep_vars:
        del globals()[var]
del keep_vars
del all_vars
del var

# manually call the garbage collector
import gc
gc.collect()

```

```{r nmf-pca-comparison-code}
#| cache: true

# Importing from Python
residuals_list <- py$residuals
silhouette_list <- py$silhouette

teacher_variables <- names(
  df_pca[,RD.elementary_schedule:RD.grade_level_teacher_materials]
)
student_variables <- names(
  df_pca[,Active.Users...Total:Tower.Alerts.per.Tower.Completion]
)
TSS_teacher <- df_pca %>%
  select(all_of(teacher_variables)) %>%
  mutate(across(all_of(teacher_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(teacher_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()
TSS_student <- df_pca %>%
  select(all_of(student_variables)) %>%
  mutate(across(all_of(student_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(student_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()

# Creating dataframes
df_residuals <- do.call(rbind, lapply(names(residuals_list), function(method) {
  do.call(rbind, lapply(names(residuals_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Residuals = residuals_list[[method]][[n]]
    )
  }))
}))
df_residuals_teachers <- df_residuals[
  grepl("_teachers", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_teacher)
df_residuals_students <- df_residuals[
  grepl("_students", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_student)

df_silhouette <- do.call(rbind, lapply(names(silhouette_list), function(method) {
  do.call(rbind, lapply(names(silhouette_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Silhouette = silhouette_list[[method]][[n]]
    )
  }))
}))
df_silhouette_teachers <- df_silhouette[
  grepl("_teachers", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method))
df_silhouette_students <- df_silhouette[
  grepl("_students", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method))

# Plotting residuals
p1 <- ggplot() +
  geom_line(data = df_residuals_teachers,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R-squared",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_teachers$Components),
                                  max(df_residuals_teachers$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p2 <- ggplot(df_silhouette_teachers,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_teachers$Components),
                                  max(df_silhouette_teachers$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_teachers$Silhouette) +
                                  3*sd(df_silhouette_teachers$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting residuals
p3 <- ggplot() +
  geom_line(data = df_residuals_students,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R-squared",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_students$Components),
                                  max(df_residuals_students$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p4 <- ggplot(df_silhouette_students,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_students$Components),
                                  max(df_silhouette_students$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_students$Silhouette) +
                                  3*sd(df_silhouette_students$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Combine the plots and place the legend at the bottom
comparison_plot <- ggarrange(p1, p2,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
comparison_plot2 <- ggarrange(p3, p4,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")

```

#### Interpreting Components

After analyzing the NMF data, we identified four significant components for teachers and students. @fig-nmf-heatmap displays these components as heatmaps, offering insight into the underlying behavioral structures. Given the loadings, we interpret the components as follows:

##### Teachers Components

**Component 1 (Assessments)**: This component has substantial weights on supplemental assessment materials, such as "Optional Problem Sets Download," "Optional Homework Download," and "Student Notes and Exit Tickets Download," indicating a proactive approach to evaluating and supporting student learning progress. It could also reflect a proactive approach to monitoring student understanding and providing feedback.

**Component 2 (Pedagogical Knowledge)**: The high weights on "Guided Practice Completed," "Tower Completed," "Tower Stage Failed," and "Fluency Completed" suggest that this component reflects when teachers are engaged in acquiring subject-matter-specific pedagogy, learning to scaffold and explain concepts in various ways.

**Component 3 (Group Instruction)**: This component, with prominent weights on "Small Group Lesson Download," "Whole Group Word Problems Download," and "Whole Group Fluency Download," suggests a pedagogical approach focused on fostering interactive and comprehensive classroom instruction. It implies engagement in activities that promote group learning dynamics and collective problem-solving skills.

**Component 4 (Curriculum Planning)**: The dominance of "Mission Overview Download" and "Grade Level Overview Download" in this component suggests that teachers are highly involved in strategic planning and curriculum mapping. It involves organizing the curriculum content and structuring lesson plans to align with grade-level objectives and mission overviews.

##### Student Components

**Component 1 (Badges)**: This component emphasizes "On-grade Badges" and "Badges," indicating that it measures students' overall engagement and advancement through the curriculum.

**Component 2 (Struggles)**: This component, which heavily weights "Boosts" and "Tower Alerts," seems to capture the frequency of occasions when students require additional scaffolding and assistance.

**Component 3 (Number of Students)**: This component mainly consists of "Active Students," which provides insight into what proportion of students regularly log in to complete Digital Lessons.

**Component 4 (Activity)**: Dominated by "Student Minutes" and "Student Logins," this component highlights the amount of time students invest in and the frequency of their interactions with Zearn.

```{r}
#| cache: true
#| label: fig-nmf-heatmap
#| fig-cap: "Heatmap of Non-negative Matrix Factorization (NMF) components for teacher and student data. The rows represent the original variables, and the columns correspond to the components. The color gradient indicates the relative importance of each variable within a component based on the proportion of the component's total weight attributed to that variable. These proportions were calculated by normalizing each variable weight within a component so that they all sum to 1. To provide context, the heatmaps label examples of low, moderate, and high proportion values."
#| fig-subcap:
#| - "Teacher Data. Component 1 (Assessments) focuses on using supplemental materials for student evaluation; Component 2 (Pedagogical Knowledge) emphasizes developing subject-specific teaching strategies; Component 3 (Group Instruction) centers on collaborative and whole-class teaching methods; Component 4 (Curriculum Planning) highlights planning and lesson preparation."
#| - "Student Data. Component 1 (Badges) measures curriculum engagement and progression; Component 2 (Struggles) indicates the need for additional academic support; Component 3 (Number of Students) tracks student participation within the platform; Component 4 (Activity) reflects the overall time spent and frequency of platform usage."
#| layout-ncol: 1

components_list <- py$components
df_heatmap_teachers <-
  components_list[["Frobenius NNDSVD_nomin_teachers"]][["4"]] %>%
  t() %>% as.data.frame()
df_heatmap_students <-
  components_list[["Frobenius NNDSVD_nomin_students"]][["4"]] %>%
  t() %>% as.data.frame()

# Define a named vector to map old variable names to new ones
variable_names <- c(
  "Minutes.on.Zearn...Total" = "Teacher Minutes",
  "RD.optional_problem_sets" = "Optional Problem Sets Download",
  "RD.student_notes_and_exit_tickets" = "Student Notes and Exit Tickets Download",
  "RD.mission_overview" = "Mission Overview Download",
  "RD.pd_course_notes" = "Course Notes Download",
  "RD.elementary_schedule" = "Elementary Schedule Download",
  "RD.whole_group_fluency" = "Whole Group Fluency Download",
  "Guided.Practice.Completed" = "Guided Practice Completed",
  "RD.small_group_lessons" = "Small Group Lesson Download",
  "Tower.Completed" = "Tower Completed",
  "Fluency.Completed" = "Fluency Completed",
  "Number.Gym.Activity.Completed" = "Number Gym Activity Completed",
  "RD.grade_level_overview" = "Grade Level Overview Download",
  "Tower.Stage.Failed" = "Tower Stage Failed",
  "Kindergarten.Activity.Completed" = "Kindergarten Activity Completed",
  "Tower.Struggled" = "Tower Struggled",
  "RD.k_mission" = "Kindergarten Mission Download",
  "RD.whole_group_word_problems" = "Whole Group Word Problems Download",
  "RD.assessments" = "Assessments Download",
  "RD.teaching_and_learning_approach" = "Teaching and Learning Approach Download",
  "RD.optional_homework" = "Optional Homework Download",
  "RD.k_schedule" = "Kindergarten Schedule Download",
  "RD.curriculum_map" = "Curriculum Map Download",
  "RD.assessments_answer_key" = "Assessments Answer Key Download",
  "RD.pd_course_guide" = "Course Guide Download",
  "RD.grade_level_teacher_materials" = "Teacher Materials Download",
  "Active.Users...Total" = "Active Students",
  "Sessions.per.Active.User" = "Student Logins",
  "Badges.per.Active.User" = "Badges",
  "Badges..on.grade..per.Active.Student" = "On-grade Badges",
  "Minutes.per.Active.User" = "Student Minutes",
  "Tower.Alerts.per.Tower.Completion" = "Tower Alerts",
  "Boosts.per.Tower.Completion" = "Boosts"
)
# Rename the rows of the dataframe
row.names(df_heatmap_teachers) <- variable_names[teacher_variables]
row.names(df_heatmap_students) <- variable_names[student_variables]

names(df_heatmap_teachers) <- paste0("Comp ", 1:4)
names(df_heatmap_students) <- paste0("Comp ", 1:4)
df_heatmap_teachers <- df_heatmap_teachers %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)
df_heatmap_students <- df_heatmap_students %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)

# Normalize by column sums to show proportion of contribution from each variable
df_heatmap_teachers <- df_heatmap_teachers %>%
  mutate(across(everything(), ~ round(.x / sum(.x), 3)))
df_heatmap_students <- df_heatmap_students %>%
  mutate(across(everything(), ~ round(.x / sum(.x), 3)))

# Define color scheme and breaks
breaks <- seq(0, max(df_heatmap_students,df_heatmap_teachers),
              by = round(max(df_heatmap_students,df_heatmap_teachers)/50,2))
color_scheme <- colorRampPalette(
  c("#F7F7F7", brewer.pal(n = 11, name = "RdYlBu")[6:1])
  )(length(breaks))

# Calculate annotations for both dataframes
calculate_annotations <- function(df) {
  data <- unlist(df)
  data <- data[data > breaks[4]]
  high <- max(data)
  low <- min(data)
  median <- ifelse(any(data == median(data)), median(data),
                   data[which.min(abs(data - median(data)))])
  return(df %>%
           mutate(across(everything(),
                         ~ if_else(. == low | . == high | . == median,
                                   ., 0))) %>%
           mutate(across(everything(), ~ as.character(.))) %>%
           mutate(across(everything(), ~ if_else(. == "0", "", .))) %>%
           as.matrix())
}

pheatmap(df_heatmap_teachers,
         cluster_rows = FALSE, cluster_cols = FALSE,
         angle_col = 0,
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE, show_rownames = TRUE,
         display_numbers = calculate_annotations(df_heatmap_teachers),
         number_color = RColorBrewer::brewer.pal(n = 11, name = "RdYlBu")[11])

pheatmap(df_heatmap_students,
         cluster_rows = FALSE, cluster_cols = FALSE,
         angle_col = 0,
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE, show_rownames = TRUE,
         display_numbers = calculate_annotations(df_heatmap_students),
         number_color = RColorBrewer::brewer.pal(n = 11, name = "RdYlBu")[11],
         fontsize = 15)

```

```{r export dimension reduction}
#| eval: false
# Importing results from Python
results_list <- py$results

# List of method names
# methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler")
methods <- c("Frobenius NNDSVD")
# Initialize df_components
df_components <- df_pca

# Loop over each method, extract the results at 3 components and bind to df_components
for (method in methods) {
  method_teacher <- paste0(method, "_nomin_teachers")
  method_student <- paste0(method, "_nomin_students")
  result_teacher <- results_list[[method_teacher]][["4"]]
  result_student <- results_list[[method_student]][["4"]]
  df_components <- df_components %>%
    bind_cols(result_teacher, result_student, .name_repair = )

  # Adjust column names
  new_cols <- paste0("Frobenius NNDSVD",
                     rep(c("_teacher", "_student"), each = 4),
                     rep(1:4, 2))
  names(df_components)[
    (ncol(df_components) -
       ncol(result_teacher) -
       ncol(result_student) + 1):ncol(df_components)
    ] <- new_cols
}

# Write to csv
write.csv(df_components, "./Data/df.csv")

```

### Feature Selection

```{r load dimension reduction}
#| include: false

minimum_weeks <- 12

df <- read.csv(file = "./Data/df.csv") %>%
  mutate(across(where(is.numeric),
                ~ ifelse(. < .Machine$double.eps, 0, .))) %>%
  arrange(Classroom.ID, week) %>%
  group_by(Classroom.ID) %>%
  # Filter out classrooms with low action rate
  filter(if_any(dplyr::starts_with("Frobenius.NNDSVD_teacher"),
                ~ sum(.>0) >= minimum_weeks/2)) %>%
  # Filter out classrooms with little data
  filter(n() >= minimum_weeks)

FR_cols <- grep("Frobenius.NNDSVD_teacher", names(df), value = TRUE)
df <- df %>%
  filter(sd(!!sym(FR_cols[1])) != 0) %>%
  filter(sd(!!sym(FR_cols[2])) != 0) %>%
  filter(sd(!!sym(FR_cols[3])) != 0) %>%
  filter(sd(!!sym(FR_cols[4])) != 0) %>%
  mutate(row_n = row_number()) %>%
  ungroup()

```

In order to pre-select the most appropriate action and reward variables, we estimated the Q-learning models alongside a) a baseline model (constant-only regression), b) a logistic regression model inspired by dynamic analysis [@lau2005], and c) a Q-learning model, and d) a simplified Q-learning model with no cost parameter and a starting Q-value of 0. This approach acted as a filter to capture the action-reward configurations displaying the best fit. More specifically, we applied reward structures extracted from classroom data via non-negative matrix factorization (NMF) with the Frobenius Non-negative Double Singular Value Decomposition (NNDSVD) and actions derived similarly from teacher data. Then, we selected one teacher component as the action and one student component as the reward, yielding 16 configurations (4 possible actions and 4 possible rewards).

To account for temporal dynamics of actions influenced by lagged rewards, the logistic regression models incorporated lagged variables. We tested lags ranging from one to six weeks, accounting for temporal autocorrelation and potential delayed effects. The results suggest a preference for a lag of two periods as optimal (based on the "elbow" in the Area Under the Receiver Operating Characteristic curve (AUC) and the minima in the Bayesian Information Criterion (BIC) curves, see @fig-panel-bic).

We evaluated the performance of each model configuration using log-likelihood values. @tbl-top-CBM provides the log-likelihood scores for the four models across all 16 action-reward configurations. Our analysis revealed that, across all model types, the "Pedagogical Knowledge" action consistently showed the best fit, as evidenced by the highest (least negative) log-likelihood values. Further, the full Q-learning model consistently outperformed the others. Within Q-learning, models incorporating "Badges" and "Activity" as rewards generally outperformed others, although differences were small.

## Model Performance and Behavioral Signatures

Given these findings, we focus our subsequent analyses on the models featuring "Pedagogical Knowledge" as the action and "Badges" as reward (see Appendix for similar analyses with "Activity" as reward). In the next stage of our analyses, we used the Akaike Information Criterion (AIC) to further compare the performance of our models, while adjusting for model complexity. @tbl-model-summary presents the number of parameters and mean AIC values for each model. The scores were computed using individual model-fitting with Matlab's cbm toolbox, with lower AIC values indicating better model fit. The baseline model, with its advantage in parsimony, achieved the lowest AIC, followed by the full Q-learning model. Note, however, that the baseline model does not provide insights into the decision-making process of teachers, which is a key goal of this study. As such, we proceed with a more detailed examination of the behavioral signatures in our data and investigate which models best capture them, excluding the simplified Q-learning model, which underperformed in both measures of model fit.

<!-- Load prediction data and prepare for analysis -->

```{r ppc-helper-functions}

remove_holiday_weeks <- function(data) {
  # Define holiday weeks (Thanksgiving and two weeks of Christmas)
  holiday_weeks <- c("2019-11-25",
                     "2019-12-23", "2019-12-30")

  # Filter out holiday weeks
  data %>%
    filter(!date %in% holiday_weeks)
}

# Function to delete months with no teacher activity
remove_inactive_periods <- function(data) {
  active_months <- data %>%
    mutate(month = floor_date(as.Date(date), "month")) %>%
    group_by(Classroom.ID, month) %>%
    summarise(monthly_activity = sum(rowSums(
      across(starts_with("Frobenius.NNDSVD_teacher"), ~ . > 0)
      )) > 0, .groups = "drop") %>%
    filter(monthly_activity)

  data %>%
    mutate(month = floor_date(as.Date(date), "month")) %>%
    semi_join(active_months, by = c("Classroom.ID", "month")) %>%
    select(-month)
}
df_cleaned <- df %>%
  mutate(date=(as.Date(date,format = "%Y-%m-%d") - 7)) %>%
  remove_inactive_periods() %>%
  remove_holiday_weeks()

calculate_ev_uncertainty <- function(data, action_col, reward_col) {
  data %>%
    arrange(Classroom.ID, week) %>%
    group_by(Classroom.ID) %>%
    mutate(
      v_0 = if_else(!!sym(action_col) == 0,
                    !!sym(reward_col), NA),
      v_1 = if_else(!!sym(action_col) > 0,
                    !!sym(reward_col), NA),
      ev_0 = cummean_ignore_na(v_0),
      ev_1 = cummean_ignore_na(v_1),
      ev = ev_1 - ev_0,  # EV is the difference of the means
      uncertainty_0 = cumsd(v_0),
      uncertainty_1 = cumsd(v_1),
      uncertainty = uncertainty_1 - uncertainty_0,
      most_uncertain = case_when(
        is.na(uncertainty_0) & is.na(uncertainty_1) ~ NA_real_,
        is.na(uncertainty_0) ~ 0,
        is.na(uncertainty_1) ~ 1,
        uncertainty_1 > uncertainty_0 ~ 1,
        TRUE ~ 0
      )
    ) %>%
    ungroup()
}

# Function to calculate cumulative mean ignoring NAs
cummean_ignore_na <- function(x) {
  cumsum(!is.na(x)) -> n
  ifelse(n == 0, NA, cumsum(replace(x, is.na(x), 0)) / n)
}

# Function to calculate cumulative standard deviation
cumsd <- function(x) {
  # Create a vector to store results, initially all NA
  result <- rep(NA_real_, length(x))
  # Identify non-NA positions
  valid_pos <- which(!is.na(x))
  if (length(valid_pos) > 1) {
    # Calculate cumsd for non-NA values
    valid_x <- x[valid_pos]
    n <- seq_along(valid_x)
    cumsd_values <-
      sqrt((cumsum(valid_x^2) - (cumsum(valid_x)^2) / n) / (n - 1))
    # Assign calculated values to their original positions
    result[valid_pos] <- cumsd_values
    # Forward fill NA positions
    for (i in seq_along(result)) {
      if (is.na(result[i]) && i > 1) {
        result[i] <- result[i-1]
      }
    }
  }
  return(result)
}

```

```{r load-Matlab-predictions}

load_predictions <- function(file_path) {
  if(grepl("_2", file_path)) student_var = 1
  if(grepl("_6", file_path)) student_var = 2
  if(grepl("_10", file_path)) student_var = 3
  if(grepl("_14", file_path)) student_var = 4
  if(!exists("student_var")) return(NULL)
  mat_data <- readMat(file_path)
  raw_data <- mat_data[["cbm"]][[5]][[5]]
  predictions <- mat_data[["cbm"]][[5]][[4]]
  if(grepl("q_model", file_path)) q_values <- mat_data[["cbm"]][[5]][[6]]

  # Create a data frame
  data <- list_rbind(
    map(1:length(predictions), function(i) {
      ID = raw_data[[i]][[1]][[9]][[3]]
      week_number <- raw_data[[i]][[1]][[9]][[1]]
      action <- raw_data[[i]][[1]][[2]]
      reward <- raw_data[[i]][[1]][[4 + student_var]]
      pred <- predictions[[i]][[1]]
      if(exists("q_values")) {
        q_val <- q_values[[i]][[1]][1,]
      } else {
        q_val <- NA
      }

      tibble(
        Classroom.ID = rep(ID, each = length(action)),
        week = as.vector(week_number),
        action = as.vector(action),
        reward = as.vector(reward),
        pred_0 = pred[,1],
        pred_1 = pred[,2],
        q_val = q_val
        )
      })
    )

  return(data)
}

# Load predictions for all wrappers and models
load_all_predictions <- function() {
  model_folder_path <- "CBM/zearn_results/aggr_results"
  prediction_files <- list.files(path = model_folder_path,
                                 pattern = "lap_aggr_[[:print:]]+\\.mat$",
                                 full.names = TRUE)

  predictions <- map(prediction_files, load_predictions)
  names(predictions) <- basename(prediction_files)

  return(predictions)
}

# Load all predictions
all_predictions <- load_all_predictions() %>%
  discard(is.null)

```

<!-- Load and process model comparison data -->

```{r}
#| cache: true
#| label: tbl-top-CBM
#| tbl-cap: "Model comparison based on log-likelihood. The table presents log-likelihood values for different models across various action-reward combinations. Less negative values indicate better fit. All models are compared against a baseline intercept-only regression model. The cost-free Q-learning models do not include a cost parameter and assume the starting q-value to be zero. Across all models, the 'Pedagogical Knowledge' action shows the best fit overall. Note that these estimates are not hierarchical; they represent the individual estimates of each teacher."

# Define paths and models
model_folder_path <- "CBM/zearn_results/aggr_results"
models <- c("q_model", "q_model_simple",
              "logit_model", "baseline_model")
model_names <- c("Q-learning","Q-learning (cost-free)",
                   "Lau & Glimcher", "Baseline")

# Create a mapping for action and reward
action_reward_mapping <- list(
  "1" = list(action = "Assessments", reward = "Badges"),
  "2" = list(action = "Pedagogical Knowledge", reward = "Badges"),
  "3" = list(action = "Group Instruction", reward = "Badges"),
  "4" = list(action = "Curriculum Planning", reward = "Badges"),
  "5" = list(action = "Assessments", reward = "Struggles"),
  "6" = list(action = "Pedagogical Knowledge", reward = "Struggles"),
  "7" = list(action = "Group Instruction", reward = "Struggles"),
  "8" = list(action = "Curriculum Planning", reward = "Struggles"),
  "9" = list(action = "Assessments", reward = "No. Students"),
  "10" = list(action = "Pedagogical Knowledge", reward = "No. Students"),
  "11" = list(action = "Group Instruction", reward = "No. Students"),
  "12" = list(action = "Curriculum Planning", reward = "No. Students"),
  "13" = list(action = "Assessments", reward = "Activity"),
  "14" = list(action = "Pedagogical Knowledge", reward = "Activity"),
  "15" = list(action = "Group Instruction", reward = "Activity"),
  "16" = list(action = "Curriculum Planning", reward = "Activity")
)

# Create a data frame for model comparison
model_comparison_df <- data.frame(
  Action = character(),
  Reward = character()
)

# Process models
for (wrapper in 1:16) {
  wrapper_str <- sprintf("%d", wrapper)

  row_data <- data.frame(
    Action = action_reward_mapping[[wrapper_str]]$action,
    Reward = action_reward_mapping[[wrapper_str]]$reward
  )

  for (model in models) {
    file_path <- file.path(model_folder_path, sprintf("lap_aggr_%s_%d.mat", model, wrapper))
    if (file.exists(file_path)) {
      mat_data <- readMat(file_path)
      log_lik <- sum(mat_data[["cbm"]][[5]][[2]])
      row_data[[model]] <- log_lik
    } else {
      row_data[[model]] <- NA
    }
  }

  model_comparison_df <- rbind(model_comparison_df, row_data)
}

# Rename columns
colnames(model_comparison_df)[3:(2+length(models))] <- model_names

# Display the table
model_comparison_df %>%
  gt(groupname_col = "Action", row_group_as_column = TRUE) %>%
  tab_stubhead("Action") %>%
  tab_header(title = "Model Comparison - Log Likelihood") %>%
  fmt_number(columns = model_names, decimals = 2)

```

<!-- tbl-model-summary -->

```{r}
#| cache: true
#| label: tbl-model-summary
#| tbl-cap: "Summary of model fits for Pedagogical Knowledge as actions and Badges as rewards. Each of the models (N~par~ = number of parameters) was fitted to participants data using Matlabs cbm toolbox. Using individual model-fitting, we computed the mean AIC (lower values indicate better model fit)."

# Define models and parameters
models <- c("baseline_model", "q_model", "q_model_simple", "logit_model")
model_names <- c("Baseline", "Q-learning", "Q-learning (cost-free)", "Lau & Glimcher")
n_par <- c(1, 5, 3, 5)

# Create a data frame for model summary
model_summary <- data.frame(
  Model = model_names,
  N_par = n_par,
  N_weeks = NA_real_,
  AIC = NA_real_,
  p_value = NA_real_
)

# Calculate AIC for each model (focusing on Badges reward, wrapper 2)
model_folder_path <- "CBM/zearn_results/aggr_results"
# model_folder_path <- "CBM/zearn_results/hbi_results"
wrapper <- 2  # Corresponds to Pedagogical Knowledge and Badges
baseline_aic <- NULL

for (i in seq_along(models)) {
  file_path <- file.path(model_folder_path,
                         sprintf("lap_aggr_%s_%d.mat", models[i], wrapper))
  # file_path <- file.path(model_folder_path,
  #                        sprintf("hbi_compare_wrapper_%d_model_%d.mat",
  #                                wrapper, i))
  if (file.exists(file_path)) {
    mat_data <- readMat(file_path)
    log_lik <- mat_data[["cbm"]][[5]][[2]]
    weeks <- mat_data[["cbm"]][[5]][[5]]
    n_weeks <- sapply(weeks, function(w) length(w[[1]][[9]][[1]]))
    # log_lik <- mat_data[["cbm"]][[5]][[8]]
    aic <- -2 * log_lik + 2 * n_par[i]
    model_summary$AIC[i] <- mean(aic)
    model_summary$SD_AIC[i] <- sd(aic)
    model_summary$N_weeks[i] <- mean(n_weeks)
    model_summary$SD_weeks[i] <- sd(n_weeks)
    if (models[i] == "baseline_model") {
      baseline_aic <- aic
    } else {
      # Perform t-test
      t_test_result <- t.test(aic, baseline_aic)
      model_summary$p_value[i] <- t_test_result$p.value
    }
  }
}

# Create the table
model_summary_table <- model_summary %>%
  # Join means with SD
  mutate(N_weeks = paste0(round(N_weeks, 1), " (", round(SD_weeks, 1), ")"),
         AIC = paste0(round(AIC, 1), " (", round(SD_AIC, 1), ")") ) %>%
  select(-SD_weeks, -SD_AIC) %>%
  gt() %>%
  cols_align(align = "center") %>%
  cols_label(
    N_par = "N~par~",
    N_weeks = "No. of Weeks (SD)",
    AIC = "AIC (SD)",
    p_value = "p-value"
  ) %>%
  fmt_markdown() %>%
  fmt_scientific(columns = p_value, decimals = 2) %>%
  fmt_number(columns = N_par, decimals = 0) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  )

# Display the table
model_summary_table

# Commented code for future implementation of OOS acc, Frequency, and N_best
# model_summary$OOS_acc <- NA  # Placeholder for out-of-sample accuracy
# model_summary$Frequency <- NA  # Placeholder for frequency
# model_summary$N_best <- NA  # Placeholder for N_best

```

### Reward Seeking

```{r}
#| label: fig-reward-seeking
#| fig-cap: "Reward-seeking behavior. The graphs compare three models (Q-learning, Lau & Glimcher, and Baseline) in their ability to capture teachers' reward-seeking behavior. The x-axis represents the percentile of the difference in Q-values between action and inaction. The y-axis shows the proportion of times teachers chose to act. Black points represent mean proportions observed across teachers, with error bars depicting the standard error of these means. The dashed black line represents the smoothed trend of observed teacher behavior. Colored lines and shaded areas show model predictions with 95% confidence intervals."

plot_reward_seeking <- function(sim_data, wrapper, 
                                lower_bound = 0.05, upper_bound = 0.95,
                                y_limits = c(0.2,0.5),
                                models = c("Q-learning",
                                           "Lau & Glimcher",
                                           "Baseline")) {
  # Calculate EV and Uncertainty
  sim_data <- sim_data %>%
    group_by(Classroom.ID) %>%
    mutate(
      q_val_percentile = percent_rank(q_val_ql)
    ) %>%
    ungroup()

  # Filter data for geom_smooth
  smooth_data <- sim_data %>%
    filter(q_val_percentile >= lower_bound,
           q_val_percentile <= upper_bound)

  # Create quantile-based bins for EV
  sim_data <- smooth_data %>%
    mutate(
      q_bin = cut(
        q_val_percentile,
        breaks = seq(lower_bound, upper_bound,
                     (upper_bound-lower_bound)/6),
        labels = FALSE,
        include.lowest = TRUE
        ))

  # Prepare plot data
  plot_data_qv <- sim_data %>%
    group_by(q_bin) %>%
    summarise(
      mean_choice = mean(action, na.rm = TRUE),
      se_choice = sd(action, na.rm = TRUE) / sqrt(n()),
      mean_q_diff = mean(q_val_ql, na.rm = TRUE),
      mean_q_diff_perc = mean(q_val_percentile, na.rm = TRUE),
      mean_pred_ql = mean(pred_1_ql, na.rm = TRUE),
      se_pred_ql = sd(pred_1_ql, na.rm = TRUE) / sqrt(n()),
      mean_pred_lg = mean(pred_1_lg, na.rm = TRUE),
      se_pred_lg = sd(pred_1_lg, na.rm = TRUE) / sqrt(n()),
      mean_pred_bl = mean(pred_1, na.rm = TRUE),
      se_pred_bl = sd(pred_1, na.rm = TRUE) / sqrt(n())
    ) %>%
    filter(!is.na(q_bin))

  # Calculate x-axis breaks and labels
  breaks_q <- plot_data_qv$mean_q_diff_perc
  labels <- c("5-20%", "20-35%", "35-50%", "50-65%", "65-80%", "80-95%")

  model_colors <- c(
    "Data" = "black", 
    "Q-learning" = brewer.pal(8, "Set2")[2],
    "Lau & Glimcher" = brewer.pal(8, "Set2")[1],
    "Baseline" = brewer.pal(8, "Set2")[8])

  p <- ggplot(smooth_data) +
    geom_smooth(aes(x = q_val_percentile,
                    y = action), color = model_colors["Data"],
                method = "glm", method.args = list(family = "binomial"),
                se = FALSE, linetype = "dashed") +
    geom_point(data = plot_data_qv,
               aes(x = mean_q_diff_perc,
                   y = mean_choice), color = model_colors["Data"], size = 1) +
    geom_errorbar(data = plot_data_qv,
                  aes(x = mean_q_diff_perc, y = mean_choice,
                      ymin = mean_choice - se_choice,
                      ymax = mean_choice + se_choice),
                  color = model_colors["Data"], width = 0)

  # Create a list to store the last points for each model
  last_points <- list()

  for (model in models) {
    if (model == "Baseline") {
      y_col <- "pred_1"
    } else if (model == "Lau & Glimcher") {
      y_col <- "pred_1_lg"
    } else if (model == "Q-learning") {
      y_col <- "pred_1_ql"
    }
    
    p <- p + geom_smooth(aes(x = q_val_percentile,
                             y = !!sym(y_col)), 
                         method = "glm",
                         method.args = list(family = "quasibinomial"),
                         se = TRUE, alpha = 0.2,
                         color = model_colors[model],
                         fill = model_colors[model])

    # Get the last point for this model
    last_point <- smooth_data %>%
      filter(q_val_percentile == max(q_val_percentile)) %>%
      summarise(x = max(q_val_percentile),
                y = mean(!!sym(y_col), na.rm = TRUE),
                model = model)
    
    last_points[[model]] <- last_point
  }

  # Combine all last points
  all_last_points <- bind_rows(last_points)
  
  p + labs(x = "Q-value difference (percentile)",
           y = "% Choice",
           title = paste("Reward Seeking -",
                         action_reward_mapping[[wrapper]]$reward)) +
    geom_label_repel(data = all_last_points, 
                     aes(x = x, y = y, 
                         label = model, color = model),
                     nudge_x = 0.1,
                     direction = "y",
                     segment.color = NA,
                     box.padding = 0.1,
                     label.padding = 0.1,
                     label.size = 0.1,
                     size = 3) +
    scale_x_continuous(breaks = breaks_q, labels = labels) +
    coord_cartesian(ylim = y_limits,
                    xlim = c(min(breaks_q), max(breaks_q) + 0.15)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    scale_color_manual(values = model_colors) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 10),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"  # Remove the legend
    )
}

all_reward_seeking_plots <- map(c(2, 14), function(wrapper) {
  full_data <- all_predictions[[
    paste0("lap_aggr_q_model_", wrapper, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("lap_aggr_logit_model_", wrapper, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_ql", "_lg")
    ) %>%
    full_join(all_predictions[[
      paste0("lap_aggr_baseline_model_", wrapper, ".mat")
    ]] %>% select(-reward), by = c("Classroom.ID", "week", "action"))
  plot_reward_seeking(full_data, wrapper)
})

all_reward_seeking_plots[[1]]

```

```{r reward-seeking-corr}

library(lmtest)
library(sandwich)

# Function to fit model and extract slope with clustered SE and CI
get_slope_ci <- function(data, y_var) {
  if(y_var != "action") {
    model_fam <- quasibinomial
  } else {
    model_fam <- binomial
  }
  model <- glm(as.formula(paste(y_var, "~ q_val_ql")),
               family = model_fam,
               data = data)
  
  # Calculate clustered standard errors
  vcov_cluster <- vcovCL(model, cluster = ~ Classroom.ID)
  
  # Extract coefficients and calculate CI
  coef_summary <- coeftest(model, vcov. = vcov_cluster)
  estimate <- coef_summary["q_val_ql", "Estimate"]
  se <- coef_summary["q_val_ql", "Std. Error"]
  ci_lower <- estimate - 1.96 * se
  ci_upper <- estimate + 1.96 * se
  
  return(tibble(estimate = estimate, 
                ci_lower = ci_lower, 
                ci_upper = ci_upper))
}

# Prepare data
full_data <- all_predictions[[
    paste0("lap_aggr_q_model_", wrapper, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("lap_aggr_logit_model_", wrapper, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_ql", "_lg")
    ) %>%
    full_join(all_predictions[[
      paste0("lap_aggr_baseline_model_", wrapper, ".mat")
    ]] %>% select(-reward), by = c("Classroom.ID", "week", "action")) %>%
  group_by(Classroom.ID) %>%
  mutate(q_val_ql = percent_rank(q_val_ql)) %>%
  ungroup()

# Calculate slopes for the entire dataset
slopes <- tibble(
  model = c("Actual", "Q-learning", "Lau & Glimcher", "Baseline"),
  slope_data = list(
    get_slope_ci(full_data, "action"),
    get_slope_ci(full_data, "pred_1_ql"),
    get_slope_ci(full_data, "pred_1_lg"),
    get_slope_ci(full_data, "pred_1")
  )
) %>%
  unnest(slope_data)


```

We first examine whether teachers display a preference for choices with higher relative expected values, as measured by the estimated Q-value difference. @fig-reward-seeking demonstrates this reward-seeking behavior in the data ($b = 1.02$, 95% CI $[0.947, 1.09]$). It also shows that the Q-learning model most closely fits the observed data ($b = 1.34$, 95% CI $[1.29, 1.39]$). The logit model underestimates the probability of choices with higher expected values and overestimates choices with lower ones ($b = 0.371$, 95% CI $[0.333,0.409]$). The baseline model, as expected, does not capture the reward-seeking behavior and estimates a fixed choice probability ($b = 0.005$, 95% CI $[0.002,0.007]$).

### Uncertainty Aversion

```{r}
#| label: fig-uncertainty-aversion
#| fig-cap: "Uncertainty aversion. The graphs compare three models (Q-learning, Lau & Glimcher, and Baseline) in their ability to capture teachers' uncertainty-related behavior. The x-axis represents the percentile of the difference in expected value (EV) between uncertain and certain options, calculated from the cumulative means and standard deviations of rewards associated with each action. The y-axis shows the proportion of times teachers chose the uncertain option. Black points represent mean proportions observed across teachers, with error bars depicting the standard error of these means. The dashed black line represents the smoothed trend of observed teacher behavior. Colored lines and shaded areas show model predictions with 95% confidence intervals. The horizontal dashed line at 50% represents indifference between certain and uncertain options."

# Function to calculate cumulative mean ignoring NAs
cummean_ignore_na <- function(x) {
  n <- cumsum(!is.na(x))
  result <- cumsum(replace(x, is.na(x), 0)) / n
  result[n == 0] <- 0
  return(result)
}

# Function to calculate cumulative standard deviation
cumsd <- function(x) {
  n <- cumsum(!is.na(x))
  cumsum_x <- cumsum(replace(x, is.na(x), 0))
  cumsum_x2 <- cumsum(replace(x^2, is.na(x), 0))

  variance <- (cumsum_x2 - (cumsum_x^2) / n) / (n - 1)
  variance[is.infinite(variance)] <- NA
  result <- sqrt(variance)

  # Forward fill NA values
  last_valid <- NA
  for (i in seq_along(result)) {
    if (!is.na(result[i])) {
      last_valid <- result[i]
    } else if (!is.na(last_valid)) {
      result[i] <- last_valid
    }
  }
  return(result)
}

# Function to calculate EV and Uncertainty
calculate_ev_uncertainty <- function(action, outcome) {
  v_0 <- outcome
  v_0[action > 0] <- NA
  if (is.na(v_0[1])) v_0[1] <- 0

  v_1 <- outcome
  v_1[action == 0] <- NA
  if (is.na(v_1[1])) v_1[1] <- 0

  ev_0 <- cummean_ignore_na(v_0)
  ev_1 <- cummean_ignore_na(v_1)
  ev <- ev_1 - ev_0

  uncertainty_0 <- cumsd(v_0)
  uncertainty_1 <- cumsd(v_1)
  uncertainty <- uncertainty_1 - uncertainty_0

  return(list(ev = ev, uncertainty = uncertainty))
}


plot_uncertainty_aversion <- function(sim_data, wrapper, 
                                      lower_bound = 0.05,
                                      upper_bound = 0.95,
                                      y_limits = c(0.35, 0.65),
                                      models = c("Q-learning",
                                                 "Lau & Glimcher",
                                                 "Baseline")) {
  # Calculate delta EV and choice_uncertain
  sim_data <- sim_data %>%
    group_by(Classroom.ID) %>%
    mutate(
      ev = calculate_ev_uncertainty(action, reward)[[1]],
      sd = calculate_ev_uncertainty(action, reward)[[2]],
      lag_ev = c(0, ev[-n()]),
      lag_sd = c(0, replace_na(sd[-n()], 0)),
      delta_ev = if_else(lag_sd > 0, lag_ev, -lag_ev),
      choice_uncertain = as.numeric(xor(action == 0, lag_sd > 0)),
      # choice_uncertain for each model
      choice_uncertain_pred_ql = if_else(lag_sd > 0,
                                        pred_1_ql,
                                        1 - pred_1_ql),
      choice_uncertain_pred_lg = if_else(lag_sd > 0,
                                        pred_1_lg,
                                        1 - pred_1_lg),
      choice_uncertain_pred_bl = if_else(lag_sd > 0,
                                        pred_1,
                                        1 - pred_1)
    ) %>%
    summarize(
      delta_ev = mean(delta_ev, na.rm = TRUE),
      choice_uncertain = mean(choice_uncertain, na.rm = TRUE),
      choice_uncertain_pred_ql = mean(choice_uncertain_pred_ql, na.rm = TRUE),
      choice_uncertain_pred_lg = mean(choice_uncertain_pred_lg, na.rm = TRUE),
      choice_uncertain_pred_bl = mean(choice_uncertain_pred_bl, na.rm = TRUE)
    ) %>%
    ungroup() %>%
    mutate(
      delta_ev_percentile = percent_rank(delta_ev)
    )

  # Filter data for geom_smooth
  smooth_data <- sim_data %>%
    filter(delta_ev_percentile >= lower_bound,
           delta_ev_percentile <= upper_bound)

  # Create quantile-based bins
  sim_data <- smooth_data %>%
    mutate(
      ev_bin = cut(
        delta_ev_percentile,
        breaks = seq(lower_bound, upper_bound,
                     (upper_bound-lower_bound)/6),
        labels = FALSE,
        include.lowest = TRUE
      ))

  # Prepare plot data
  plot_data <- sim_data %>%
    group_by(ev_bin) %>%
    summarise(
      mean_choice_uncertain = mean(choice_uncertain, na.rm = TRUE),
      se_choice_uncertain = sd(choice_uncertain, na.rm = TRUE) / sqrt(n()),
      mean_delta_ev = mean(delta_ev, na.rm = TRUE),
      mean_delta_ev_perc = mean(delta_ev_percentile, na.rm = TRUE),
      se_pred_ql = sd(choice_uncertain_pred_ql, na.rm = TRUE) / sqrt(n()),
      choice_uncertain_pred_ql = mean(
        choice_uncertain_pred_ql, na.rm = TRUE),
      se_pred_lg = sd(choice_uncertain_pred_lg, na.rm = TRUE) / sqrt(n()),
      choice_uncertain_pred_lg = mean(
        choice_uncertain_pred_lg, na.rm = TRUE),
      se_pred_bl = sd(choice_uncertain_pred_bl, na.rm = TRUE) / sqrt(n()),
      choice_uncertain_pred_bl = mean(
        choice_uncertain_pred_bl, na.rm = TRUE)
    ) %>%
    filter(!is.na(ev_bin))

  # Calculate x-axis breaks and labels
  breaks_q <- plot_data$mean_delta_ev_perc
  labels <- c("5-20%", "20-35%", "35-50%", "50-65%", "65-80%", "80-95%")

  model_colors <- c(
    "Data" = "black", 
    "Q-learning" = brewer.pal(8, "Set2")[2],
    "Lau & Glimcher" = brewer.pal(8, "Set2")[1],
    "Baseline" = brewer.pal(8, "Set2")[8])

  p <- ggplot(smooth_data) +
    geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray50") +
    geom_smooth(aes(x = delta_ev_percentile, y = choice_uncertain),
                color = model_colors["Data"],
                method = "glm", method.args = list(family = "quasibinomial"),
                se = FALSE, linetype = "dashed") +
    geom_point(data = plot_data,
               aes(x = mean_delta_ev_perc, y = mean_choice_uncertain),
               color = model_colors["Data"], size = 1) +
    geom_errorbar(data = plot_data,
                  aes(x = mean_delta_ev_perc, y = mean_choice_uncertain,
                      ymin = mean_choice_uncertain - se_choice_uncertain,
                      ymax = mean_choice_uncertain + se_choice_uncertain),
                  color = model_colors["Data"], width = 0)

  # Create a list to store the last points for each model
  last_points <- list()

  for (model in models) {
    if (model == "Baseline") {
      y_col <- "choice_uncertain_pred_bl"
    } else if (model == "Lau & Glimcher") {
      y_col <- "choice_uncertain_pred_lg"
    } else if (model == "Q-learning") {
      y_col <- "choice_uncertain_pred_ql"
    }
    
    p <- p + geom_smooth(aes(x = delta_ev_percentile, y = !!sym(y_col)), 
                         method = "glm",
                         method.args = list(family = "quasibinomial"),
                         se = TRUE, alpha = 0.2,
                         color = model_colors[model],
                         fill = model_colors[model])

    # Get the last point for this model
    last_point <- plot_data %>%
      filter(mean_delta_ev_perc == max(mean_delta_ev_perc)) %>%
      summarise(x = max(mean_delta_ev_perc),
                y = !!sym(y_col),
                model = model)
    
    last_points[[model]] <- last_point
  }

  # Combine all last points
  all_last_points <- bind_rows(last_points)
  
  p + labs(x = "Diff. EV (Uncertain - Certain) (percentile)",
           y = "% Choice Uncertain",
           title = paste("Uncertainty Aversion -",
                         action_reward_mapping[[wrapper]]$reward)) +
    geom_label_repel(data = all_last_points, 
                     aes(x = x, y = y, 
                         label = model, color = model),
                     nudge_x = 0.2,
                     direction = "y",
                     segment.color = NA,
                     box.padding = 0.1,
                     label.padding = 0.1,
                     label.size = 0.1,
                     size = 3) +
    scale_x_continuous(breaks = breaks_q, labels = labels) +
    coord_cartesian(ylim = y_limits,
                    xlim = c(min(breaks_q), max(breaks_q) + 0.25)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    scale_color_manual(values = model_colors) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 10),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"  # Remove the legend
    )
}

all_uncertainty_plots <- map(c(2, 14), function(wrapper) {
  full_data <- all_predictions[[
    paste0("lap_aggr_q_model_", wrapper, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("lap_aggr_logit_model_", wrapper, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_ql", "_lg")
    ) %>%
    full_join(all_predictions[[
      paste0("lap_aggr_baseline_model_", wrapper, ".mat")
    ]] %>% select(-reward), by = c("Classroom.ID", "week", "action"))
  plot_uncertainty_aversion(full_data, wrapper)
})

all_uncertainty_plots[[1]]

```

```{r uncertainty-corr}

# Function to fit model and extract indifference point with CI
get_indifference_point_ci <- function(data, y_var, x_var) {
  model <- glm(as.formula(paste(y_var, "~", x_var)),
               family = "quasibinomial",
               data = data)
  
  # Calculate clustered standard errors
  vcov_cluster <- vcovCL(model, cluster = ~ Classroom.ID)
  
  # Extract coefficients and calculate CI
  coef_summary <- coeftest(model, vcov. = vcov_cluster)
  intercept <- coef_summary["(Intercept)", "Estimate"]
  slope <- coef_summary[x_var, "Estimate"]
  
  # Calculate indifference point (where intercept + slope*x = 0)
  indifference_point <- -intercept / slope
  
  # Calculate standard error of indifference point
  se_intercept <- coef_summary["(Intercept)", "Std. Error"]
  se_slope <- coef_summary[x_var, "Std. Error"]
  se_indifference <- sqrt((se_intercept/slope)^2 + (se_slope*(0.5-intercept)/slope^2)^2)
  
  ci_lower <- indifference_point - 1.96 * se_indifference
  ci_upper <- indifference_point + 1.96 * se_indifference
  
  return(tibble(indifference_point = indifference_point, 
                ci_lower = ci_lower, 
                ci_upper = ci_upper))
}

# Prepare data
full_data <- full_data %>%
    group_by(Classroom.ID) %>%
    mutate(
      ev = calculate_ev_uncertainty(action, reward)[[1]],
      sd = calculate_ev_uncertainty(action, reward)[[2]],
      lag_ev = c(0, ev[-n()]),
      lag_sd = c(0, replace_na(sd[-n()], 0)),
      delta_ev = if_else(lag_sd > 0, lag_ev, -lag_ev),
      choice_uncertain = as.numeric(xor(action == 0, lag_sd > 0)),
      # choice_uncertain for each model
      choice_uncertain_pred_ql = if_else(lag_sd > 0,
                                        pred_1_ql,
                                        1 - pred_1_ql),
      choice_uncertain_pred_lg = if_else(lag_sd > 0,
                                        pred_1_lg,
                                        1 - pred_1_lg),
      choice_uncertain_pred_bl = if_else(lag_sd > 0,
                                        pred_1,
                                        1 - pred_1)
    ) %>%
    summarize(
      delta_ev = mean(delta_ev, na.rm = TRUE),
      choice_uncertain = mean(choice_uncertain, na.rm = TRUE),
      choice_uncertain_pred_ql = mean(choice_uncertain_pred_ql, na.rm = TRUE),
      choice_uncertain_pred_lg = mean(choice_uncertain_pred_lg, na.rm = TRUE),
      choice_uncertain_pred_bl = mean(choice_uncertain_pred_bl, na.rm = TRUE)
    ) %>%
    ungroup() %>%
    mutate(
      delta_ev = percent_rank(delta_ev)
    )

# Calculate indifference points
indifference_points <- tibble(
  model = c("Actual", "Q-learning", "Lau & Glimcher", "Baseline"),
  indifference_data = list(
    get_indifference_point_ci(full_data, "choice_uncertain", "delta_ev"),
    get_indifference_point_ci(full_data, "choice_uncertain_pred_ql", "delta_ev"),
    get_indifference_point_ci(full_data, "choice_uncertain_pred_lg", "delta_ev"),
    get_indifference_point_ci(full_data, "choice_uncertain_pred_bl", "delta_ev")
  )
) %>%
  unnest(indifference_data)

```

Next, we examine whether teachers are averse to choices with a more uncertain reward relationship. @fig-uncertainty-aversion provides insights into this behavioral signature. The x-axis represents the difference in expected value (EV) between uncertain and certain options, while the y-axis shows the proportion of times teachers chose the uncertain option. The key measure in this plot is the location at which the teacher is indifferent between uncertain and certain choices. The data show uncertainty happens at the $50.6$ percentile (95% CI $[40.7,60.6]$), and the models estimate similar indifference points (Q-learning: $56.5$, 95% CI $[44.9, 68.2]$; logit: $50.2$, 95% CI $[39.1, 61.3]$; baseline: $53.2$, 95% CI $[40.0, 66.5]$). Thus, this analysis cannot differentiate the three models in their ability to capture this signature.

### Evidence for Learning

```{r load_parameters}

load_parameters <- function(file_path) {
  mat_data <- readMat(file_path)
  if(grepl("_2_", file_path)) student_var = 1
  if(grepl("_6_", file_path)) student_var = 2
  if(grepl("_10_", file_path)) student_var = 3
  if(grepl("_14_", file_path)) student_var = 4
  
  parameters <- mat_data[["cbm"]][[5]][[1]]
  raw_data <- mat_data[["cbm"]][[5]][[5]]

  # Create a data frame
  data <- list_rbind(
    map(1:nrow(parameters), function(i) {
      tibble(Classroom.ID = raw_data[[i]][[1]][[9]][[3]][,1])
      })
    ) %>%
    bind_cols(
      tibble(
        alpha = 1 / (1 + exp(-parameters[, 1])),
        gamma = 1 / (1 + exp(-parameters[, 2])),
        tau = exp(parameters[, 3]),
        ev_init = exp(parameters[, 4]),
        cost = exp(parameters[, 5])
        )
    )

  return(data)
}

```

```{r}
#| label: fig-prediction-errors
#| fig-cap: "Learning curves. The plots show the mean reward prediction errors across teachers over time. The x-axis represents biweekly periods throughout the school year. The solid line represents mean values and the shaded area represents the standard error of the mean. A dashed horizontal line at y=0 serves as a reference for zero prediction error. The curve illustrates a general trend of increasing prediction accuracy (decreasing absolute prediction error) until March."

plot_prediction_errors <- function(sim_data, wrapper) {
  # Load parameters
  parameters <- load_parameters(
    paste0("CBM/zearn_results/aggr_results/lap_aggr_q_model_",
           wrapper, ".mat")
    )
  
  # Calculate prediction errors and prepare data
  plot_data <- sim_data %>%
    inner_join(parameters, by = "Classroom.ID") %>%
    group_by(Classroom.ID) %>%
    arrange(Classroom.ID, week) %>%
    mutate(
      time_diff = week - lag(week),
      q_pe = case_when(
        lag(action) == 1 ~  gamma^time_diff * reward - q_val,
        lag(action) == 0 ~ -gamma^time_diff * reward,
        TRUE ~ NA_real_
      ),
      biweek = ceiling(adjusted_week / 2)
    ) %>%
    ungroup() %>%
    group_by(biweek) %>%
    summarise(
      mean_q_pe = mean(q_pe, na.rm = TRUE),
      se_q_pe = sd(q_pe, na.rm = TRUE) / sqrt(n()),
      date = mean(date)
    )

  # Prepare x-axis breaks and labels
  month_breaks <- plot_data %>%
    mutate(month = floor_date(date, "month")) %>%
    group_by(month) %>%
    summarise(biweek = first(biweek)) %>%
    mutate(month_label = format(month, "%b")) %>%
    mutate(month_label = if_else(month_label == "Aug", "", month_label))

  # Define color scheme
  colors <- c("Prediction Error" = brewer.pal(8, "Set2")[1])

  # Create the plot
  ggplot(plot_data, aes(x = biweek)) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    geom_line(aes(y = mean_q_pe, color = "Prediction Error")) +
    geom_ribbon(aes(ymin = mean_q_pe - se_q_pe, 
                    ymax = mean_q_pe + se_q_pe, 
                    fill = "Prediction Error"), alpha = 0.2) +
    labs(x = "Biweekly Period", 
         y = "Value",
         title = paste("Learning Curves -", 
                       action_reward_mapping[[wrapper]]$reward),
         color = "Measure", fill = "Measure") +
    scale_color_manual(values = colors) +
    scale_fill_manual(values = colors) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "bottom"
    ) +
    scale_x_continuous(breaks = month_breaks$biweek,
                       labels = month_breaks$month_label)
    # coord_cartesian(ylim = y_limits)
}

# Generate plots
prediction_error_plots <- map(c(2, 14), function(wrapper) {
  full_data <- all_predictions[[
    paste0("lap_aggr_q_model_", wrapper, ".mat")
  ]]
  
  full_data <- full_data %>%
    group_by(Classroom.ID) %>%
    filter(week == 10) %>%
    pull(Classroom.ID) %>%
    {filter(full_data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - 8) %>%
    filter(adjusted_week >= 1) %>%
    mutate(date = as_date("2019-08-26") + weeks(adjusted_week - 1))
  plot_prediction_errors(full_data, wrapper)
})

prediction_error_plots[[1]]

```

```{r}
#| label: fig-q-value-reward-diff
#| fig-cap: "Q-value and reward differences over time. The x-axis represents biweekly periods throughout the school year. The y-axis shows the difference in Q-values or mean rewards between action and inaction. Shaded areas represent the standard error of the mean."

plot_q_value_reward_diff <- function(sim_data, wrapper, window_size = 4) {
  parameters <- load_parameters(
    paste0("CBM/zearn_results/aggr_results/lap_aggr_q_model_",
           wrapper, ".mat")
  )
  plot_data <- sim_data %>%
    inner_join(parameters, by = "Classroom.ID") %>%
    group_by(Classroom.ID) %>%
    arrange(Classroom.ID, week) %>%
    mutate(
      rolling_reward_action = rollmean(
        reward * lag(action, default = 0),
        k = window_size, fill = NA,
        align = "right"
        ),
      rolling_reward_inaction = rollmean(
        reward * lag(1 - action, default = 1),
        k = window_size, fill = NA,
        align = "right"
        ),
      date = as_date("2019-08-26") + weeks(week - 1)
    )

  # Check for NA or infinite values
  plot_data <- plot_data %>%
    mutate(across(where(is.numeric),
                  ~ifelse(is.infinite(.) | is.na(.), 0, .)))

  # Add rolling averages
  plot_data <- plot_data %>%
    mutate(
      biweek = ceiling(adjusted_week / 2)
    ) %>%
    ungroup() %>%
    group_by(biweek) %>%
    mutate(
      mean_q_val = mean(q_val, na.rm = TRUE),
      se_q_val = sd(q_val, na.rm = TRUE) / sqrt(n()),
      mean_reward_action = mean(rolling_reward_action, na.rm = TRUE),
      se_reward_action = sd(rolling_reward_action, na.rm = TRUE) / sqrt(n()),
      mean_reward_inaction = mean(rolling_reward_inaction, na.rm = TRUE),
      se_reward_inaction = sd(rolling_reward_inaction, na.rm = TRUE) / sqrt(n())
    ) %>%
    mutate(
      reward_diff = mean_reward_action - mean_reward_inaction,
      se_reward_diff = sqrt(se_reward_action^2 + se_reward_inaction^2)
    )
  
  # Prepare month breaks
  month_breaks <- plot_data %>%
    mutate(month = floor_date(date, "month")) %>%
    group_by(month) %>%
    summarise(biweek = first(biweek)) %>%
    mutate(month_label = format(month, "%b")) %>%
    mutate(month_label = if_else(month_label == "Aug", "", month_label))
  
   # Prepare label data
  last_points <- plot_data %>%
    ungroup() %>%
    filter(biweek == max(biweek)) %>%
    summarise(
      x = max(biweek),
      y_q = mean(mean_q_val),
      y_r = mean(reward_diff)
    )

  ggplot(plot_data, aes(x = biweek)) +
    geom_line(aes(y = mean_q_val, color = "Q-value difference")) +
    geom_ribbon(aes(ymin = mean_q_val - se_q_val, 
                    ymax = mean_q_val + se_q_val, 
                    fill = "Q-value difference"), alpha = 0.2) +
    geom_line(aes(y = reward_diff, color = "Difference in mean reward")) +
    geom_ribbon(aes(ymin = reward_diff - se_reward_diff, 
                    ymax = reward_diff + se_reward_diff, 
                    fill = "Difference in mean reward"), alpha = 0.2) +
    geom_label_repel(data = last_points, 
                     aes(x = x, y = y_q, 
                         label = "Q-value difference",
                         color = "Q-value difference"),
                     nudge_x = 1,
                     direction = "y",
                     segment.color = "gray50",
                     segment.size = 0.2,
                     box.padding = 0.1,
                     label.padding = 0.1,
                     label.size = 0.1,
                     size = 3) +
    geom_label_repel(data = last_points, 
                     aes(x = x, y = y_r, 
                         label = "Difference in mean reward",
                         color = "Difference in mean reward"),
                     nudge_x = 1,
                     direction = "y",
                     segment.color = "gray50",
                     segment.size = 0.2,
                     box.padding = 0.1,
                     label.padding = 0.1,
                     label.size = 0.1,
                     size = 3) +
    labs(x = "Biweekly Period", 
         y = "Difference",
         title = paste("Q-value and Reward Differences -", 
                       action_reward_mapping[[wrapper]]$reward)) +
    scale_color_manual(values = c("Q-value difference" = brewer.pal(8, "Set2")[2],
                                  "Difference in mean reward" = brewer.pal(8, "Set2")[1])) +
    scale_fill_manual(values = c("Q-value difference" = brewer.pal(8, "Set2")[2],
                                 "Difference in mean reward" = brewer.pal(8, "Set2")[1])) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    ) +
    scale_x_continuous(breaks = month_breaks$biweek,
                       labels = month_breaks$month_label,
                       minor_breaks = seq(min(plot_data$biweek),
                                          max(plot_data$biweek),
                                          by = 1)) +
    coord_cartesian(xlim = c(min(plot_data$biweek), max(plot_data$biweek) + 1.5))
}

# Create a list to store all plots
all_pe_plots <- list()

# Generate all 4 plots
for (wrapper in c(2, 14)) {
  full_data <- all_predictions[[
    paste0("lap_aggr_q_model_", wrapper, ".mat")
  ]]
  full_data <- full_data %>%
    # Filter data for teachers with week 9
    group_by(Classroom.ID) %>%
    filter(week == 9) %>%
    pull(Classroom.ID) %>%
    {filter(full_data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - 9 + 1) %>%
    filter(adjusted_week >= 1)

  plot <- plot_q_value_reward_diff(
    full_data, wrapper
    )
  all_pe_plots[[paste(wrapper, sep = "_")]] <- plot
}

all_pe_plots[[1]]

```

Given the Q-learning model best captures reward-seeking behavior, we also examine its performance in capturing learning processes in teachers' decision-making. @fig-prediction-errors illustrates the prediction error over the course of the academic year, averaged across classrooms. This measure is an indicator of how well teachers are able to anticipate the outcomes of their actions. We observe a consistent decrease in prediction errors, suggesting that teachers are progressively improving their ability to predict the rewards associated with their choices.

Furthermore, @fig-q-value-reward-diff provides additional insight into the learning process by showing the evolution of Q-values and reward differences over the academic year, averaged across classrooms. The graph demonstrates a gradual convergence between the Q-value difference and the empirical difference in mean rewards (from the previous four weeks). This convergence indicates that the model's value estimates (Q-values) are becoming increasingly accurate predictors of actual rewards.

### Top Model Selection

These findings collectively suggest that teachers exhibit important reinforcement learning characteristics: they seek to maximize rewards and learn from experience over time. The Q-learning model provides the best fit among the theoretically informative models while also offering interpretable parameters related to learning and decision-making processes.

## Heterogeneity and Optimality

```{r reg-helper-functions}

ordered_factor <- function(fact_var) {
  categories <- levels(fact_var)
  n_cat <- length(categories)
  cont <- matrix(data = 0, nrow = n_cat, ncol = (n_cat - 1))
  cont[col(cont) < row(cont)] <- 1
  rownames(cont) <- categories
  colnames(cont) <- paste(categories[2:n_cat],
                          categories[1:(n_cat - 1)],
                          sep = " vs. ")
  contrasts(fact_var) <- cont
  return(fact_var)
}

```

```{r, results='asis'}
#| label: tbl-optimality
#| tbl-cap: "Impact of Q-learning Model Parameters on Average Weekly Badges Earned per Student. Three linear regression models examine the correlations between a teacher's reinforcement learning (RL) parameters and student engagement, measured by average weekly badges earned per student. Model 1 includes only RL parameters. Model 2 adds controls for AIC, number of weeks, total students, and number of classes. Model 3 further incorporates controls for grade level, poverty level, charter school status, and whether the school has a paid Zearn account. Coefficients and standard errors (in parentheses) are provided for each parameter."

predictors <- c("alpha", "gamma", "tau", "ev_init", "cost")
model_path <- "CBM/zearn_results/aggr_results/lap_aggr_q_model_2.mat"
full_model <- readMat(model_path)

# Calculate AIC
AIC_full <- -2 * full_model[["cbm"]][[5]][[2]] + 2 * ncol(full_model[["cbm"]][[5]][[1]])
AIC_data <- tibble(
  Classroom.ID = map_int(full_model[["cbm"]][[5]][[5]], ~.[[1]][[9]][[3]][1,1]),
  AIC = as.vector(AIC_full)
)

full_data <- load_parameters(model_path) %>%
  inner_join(
    df_cleaned %>%
      group_by(Classroom.ID, Teacher.User.ID) %>%
      summarise(Income = first(income),
                Poverty = first(poverty),
                `Paid Account` = first(school.account),
                `No. of Weeks` = n_distinct(week),
                `Avg. Badges` = mean(Badges.per.Active.User),
                `Avg. Tower Alerts` = mean(Tower.Alerts.per.Tower.Completion),
                grade = first(Grade.Level),
                `Total Students` = mean(Students...Total),
                `No. of Classes` = mean(teacher_number_classes),
                `Charter School` = first(charter.school),
                .groups = "drop"),
    by = c("Classroom.ID")) %>%
  inner_join(AIC_data, by = "Classroom.ID") %>%
  mutate(across(all_of(predictors), scale),
         Income = factor(Income, ordered = TRUE), 
         grade = factor(grade, ordered = TRUE,
                        levels = c("Kindergarten",
                                   "1st", "2nd", "3rd", "4th", "5th")),
         Poverty = factor(Poverty, ordered = TRUE)) %>%
  arrange(Classroom.ID) %>%
  dplyr::mutate(
    Income = ordered_factor(Income), 
    grade = ordered_factor(grade),
    Poverty = ordered_factor(Poverty)
  )

# Run regression models
model1 <- lm(`Avg. Badges` ~ alpha + gamma + tau + cost + ev_init,
             data = full_data)
model2 <- lm(`Avg. Badges` ~ alpha + gamma + tau + cost + ev_init +
               AIC + `No. of Weeks` + `Total Students` + `No. of Classes`,
             data = full_data)
model3 <- lm(`Avg. Badges` ~ alpha + gamma + tau + cost + ev_init +
               AIC + `No. of Weeks` + `Total Students` + `No. of Classes` +
               grade +  Poverty + `Charter School` + `Paid Account`,
             data = full_data)
model4 <- lm(`Avg. Tower Alerts` ~ alpha + gamma + tau + cost + ev_init,
             data = full_data)
model5 <- lm(`Avg. Tower Alerts` ~ alpha + gamma + tau + cost + ev_init  +
               AIC + `No. of Weeks` + `Total Students` + `No. of Classes`,
             data = full_data)
model6 <- lm(`Avg. Tower Alerts` ~ alpha + gamma + tau + cost + ev_init +
               AIC + `No. of Weeks` + `Total Students` + `No. of Classes` +
               grade +  Poverty + `Charter School` + `Paid Account`,
             data = full_data)

# Create table with stargazer
stargazer(model1, model2, model3,
          type = "latex",
          header=FALSE,
          align = TRUE,
          covariate.labels = c("$\\alpha$", "$\\gamma$",
                               "$\\tau$", "Cost", "Starting Q-value",
                               "No. of Weeks", "No. of Students",
                               "No. of Classes",
                               "Charter School", "Paid Zearn Account"),
          omit = c("AIC", "grade", "Poverty"),
          dep.var.labels = "Badges",
          star.cutoffs = c(.05, .01, .001), 
          star.char = c("*", "**", "***"),
          add.lines = list(c("Control for AIC",
                             "", "Yes", "Yes", "", "Yes", "Yes"),
                           c("Control for Grade Level",
                             "", "", "Yes", "", "", "Yes"),
                           c("Control for Poverty Level",
                             "", "", "Yes", "", "", "Yes")))

```

We analyzed teacher-specific parameters to capture the individual differences in learning and the relationship between model parameters, classroom characteristics, and student performance metrics.

We found that the cost parameter showed significant negative association with income level ($b = -0.260$, $p < .001$) and positive association with poverty level ($b = 0.218$, $p < .001$). The learning rate ($\alpha$) also showed a negative association with income level ($b = -0.119$, $p = .0156$), whereas the initial Q-value difference showed a positive association ($b = 0.232$, $p < .001$). Inverse temperature ($\tau$) and the discount factor ($\gamma$) did not show significant association with classroom or school characteristics (see Table\~\ref{tbl-heterogeneity-reg}).

To understand how these parameter differences relate to student outcomes, we examined their relationship with average weekly badges earned (indicating lesson completion). Table\~\ref{tbl-optimality} shows that the learning rate ($\alpha$) is positively associated with badges ($b = 0.0609$, $p < .001$), along with the inverse temperate ($\tau$) ($b = 0.0459$, $p = .003$) and the cost parameter ($b = 0.0466$, $p = .029$). On the other hand, we found negative associations between badges and the discount factor ($\gamma$) ($b = -0.0829$, $p < .001$) and the starting Q-value difference ($b = -0.0568$, $p = .002$). We also found that the learning rate ($\alpha$) and the cost parameter were positively associated with average tower alerts (see @tbl-optimality-2).

# Materials and Methods

| **Step** | **Method** | **Software/Tools** |
|------------------|-----------------------------|-------------------------|
| Data Processing | Cleaning, normalization | R [@rcoreteam2024] (`tidyverse`, `data.table`). |
| Dimensionality Reduction | Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF) | Python (`scikit-learn` [@pedregosa2011]), R (`reticulate`) |
| Feature Selection, Analytical Methods | Q-learning Model Estimation | R (`R.matlab`), Matlab (`CBM` package for Laplace approximation) [@piray2019] |
| Model Evaluation | Behavioral Signatures | R (`lmtest`, `sandwich`) |
|  | Heterogeneity analyses of model performance across teachers | R (`sandwich`) |
| Visualization | Graphs and Tables | R (`ggplot2`, `gt`, `stargazer`) |

: Analytical steps employed in the study. {#tbl-methods}

## Data

Zearn provided administrative data for teachers and students, spanning across the 2020-2021 academic year. Teacher activity is time-stamped to the second and includes the time spent on the platform and specific actions taken. On the other hand, student data is aggregated at the classroom-week level due to data privacy considerations. As such, we aggregated the teacher data to the classroom-week unit of analysis. This level of granularity still enabled us to capture the temporal dynamics of teacher-student interactions and their subsequent influence on student achievement.

```{r}
#| label: tbl-summary
#| tbl-cap: "Summary statistics by school. The table presents the mean, median, standard deviation (SD), minimum, and maximum values for the number of teachers, total students, and average weeks of active engagement (across all classrooms within a school)."

gt_school_sum

```

The dataset includes `r N_class_pre` classrooms and `r N_teachers_pre` educators, with an average of `r avg_std_pre` students per classroom. Classrooms and teachers are also linked to a school, and @tbl-summary provides a summary of the number of students, teachers, and weeks per school (see also @fig-income-dist for the distributions of school median poverty and income levels).

### Preprocessing and Exclusion criteria

We focus our analysis on the teachers who most likely take advantage of a wide range of resources on the platform. Thus, we selected teachers who consistently use the platform and work in traditional school settings. First, we selected virtual classrooms with at least five active students weekly, filtering out parents or tutors who may use Zearn outside the classroom setting. We also removed teachers with more than four classrooms and those who logged in for less than 16 weeks (@fig-classroom-weeks reveals that a non-negligible number of classrooms has less than 3 to 4 months of data). Finally, we excluded classrooms in the 6th to 8th grades, as they represent only a small proportion of the data. @tbl-classroom-summary summarizes the refined dataset, providing a snapshot of the key variables of interest. Their means and standard deviations (SD) are computed for each grade level and overall (across all grades).

```{r}
#| label: fig-classroom-weeks
#| fig-cap: "Histogram of number of weeks of data per classroom. Most classrooms include a full year (52 weeks) of data. A smaller but significant subset of classrooms has less than 18 weeks of data. The dashed line acts as a threshold that excludes a notable segment of classrooms from further analysis. The lack of data between these two peaks suggests distinct patterns of usage. Some classrooms consistently use the platform throughout the academic year, while others show sporadic engagement, possibly reflecting trial periods or intermittent usage."

fig_classroom_weeks

```

```{r, results='asis'}
#| label: tbl-classroom-summary
#| tbl-cap: "Classroom engagement metrics by grade level. The table presents the means and standard deviations (in parentheses) of the following averages: minutes spent on the platform per student per week, badges earned per student per week (indicating lesson completion), Tower Alerts per lesson completion (indicating student struggle), and minutes spent on the platform per teacher per week."

gt_classroom_sum

```

## Operationalizing Actions and Rewards

### Teacher Actions

Teacher actions encompass a broad spectrum, from platform log-ins to resource downloads and specific instructional activities. @tbl-teacher-variables provides a list of the actions available in the data.

### Rewards (Student Actions)

In reinforcement learning (RL) models, reward variables capture the dynamics of the environment in which learning and decision-making occur. The Zearn platform provides a rich set of student activity and performance data that can be used to define these variables, offering a quantifiable snapshot of classroom engagement and learning challenges. Reward variables quantify the desirability of the outcomes resulting from the agent's actions, serving as feedback signals that guide the learning process.

In this study, we take an agnostic approach, allowing the following student variables to be treated as reward variables depending on our RL model specification:

1.  "Active Students": This variable represents the number of students actively logging in to complete digital lessons within a given week [@zearn2022]. A high number of active students could be considered a positive outcome, indicating successful student engagement.

2.  "Student Logins": This variable tallies the frequency of students entering the platform, potentially serving as an engagement metric [@zearn2024]. A high frequency of student logins could be viewed as a positive outcome, reflecting consistent student engagement.

3.  "Badges (on grade)" and "Badges": These metrics reflect the number of new lessons completed weekly at students' grade level and in general [@zearn2024a]. Accumulating badges can serve as a positive signal, indicating students' mastery of the curriculum.

4.  "Minutes per active student": This variable measures students' time on the platform, potentially correlating with their focus and learning progress [@zearn2022]. Achieving or exceeding the expected minutes can be considered a positive outcome, while falling short may be viewed as a negative outcome.

5.  "Tower Alerts": These alerts signal instances when students repeatedly encounter difficulties within the same lesson [@zearn2024b]. Tower Alerts could be viewed as a negative signal, indicating that the current teaching strategies may not be effective in addressing student difficulties.

### Dimensionality Reduction

First, we standardized the dataset by z-scoring the variables of interest at the school level (using school-wide means and standard deviations). We performed NMF and evaluated the data's reconstruction accuracy and cluster separation using, respectively, the sum of squared residuals (a measure of the difference between the original data and the reconstructed data) and silhouette scores (a measure of how similar an object is to its cluster compared to other clusters [@rousseeuw1987]).

We calculate the silhouette score with the formula $(b - a) / \max(a, b)$, where $a$ is the average distance within a cluster and $b$ is the average distance to the nearest neighboring cluster. This score ranges from -1 to 1, with higher values indicating a data point is well-matched to its cluster and poorly matched to neighboring clusters.

### Nonnegative Matrix Factorization (NMF) Methodology

Let the original matrix ($\mathbf{X}$) be a detailed description of all the teachers' (or students') behaviors. Each row in the matrix represents a unique teacher-week (or classroom-week), and each column represents a specific behavior or action. The entry in a specific row and column corresponds to the frequency of that behavior for that particular teacher-week (or classroom-week). We then estimate $\mathbf{X} \simeq \mathbf{W}\mathbf{H}$, such that we minimize the following:

$$
\left\| \mathbf{X} - \mathbf{W}\mathbf{H} \right\| , \mathbf{W} \geq 0, \mathbf{H} \geq 0.
$$

We used two different loss functions (Frobenius norm and Kullback-Leibler divergence) and two different initialization methods (nonnegative double singular value decomposition (NNDSVD) and NNDSVD with zeros filled with the average of the input matrix (NNDSVDA)). The resulting matrices are:

1.  Basis Matrix ($\mathbf{W}$): This matrix represents underlying behavior patterns. Each column is a "meta-behavior" or a group of behaviors occurring together.
2.  Mixture Matrix ($\mathbf{H}$): This matrix shows the extent to which each "meta-behavior" is present in each teacher-week (or classroom-week). Each entry in this matrix represents the contribution of a "meta-behavior" to a particular behavior present in the data.

These matrices can reveal underlying patterns of behaviors (from the basis matrix) and how these patterns are mixed and matched in different teachers (from the mixture matrix). It allows us to assess the method's performance under varying configurations, with the sum of squared residuals and silhouette scores for comparison.

## Model Performance and Feature Selection

To identify the most appropriate action and reward variables, we employed a multiple model types:

1.  Baseline Model:

$$
\text{Action}_t = \beta_0 + \epsilon_t
$$

where $\beta_0$ is the intercept and $\epsilon_t$ is the error term.

2.  Logistic Regression Model: Inspired by dynamic analysis [@lau2005], incorporating lagged variables to capture temporal dynamics:

$$
\text{logit}(P(\text{Action}_t = 1)) = \beta_0 + \sum_{i=1}^{L} (\beta_i R_{t-i} + \gamma_i \text{Action}_{t-i})
$$

where $L$ is the number of lags, $R_t$ is the reward at time $t$, and $\beta_i$, $\gamma_i$, and $\delta_i$ are coefficients.

3.  Q-learning Model: A reinforcement learning model capturing adaptive decision-making processes, as explained in the theory section.

4.  Simplified Q-learning Model: A version of the Q-learning model with no cost parameter and a starting Q-value of 0.

We applied reward structures extracted from classroom data via Non-negative Matrix Factorization (NMF) with the Frobenius Non-negative Double Singular Value Decomposition (NNDSVD). Actions were derived similarly from teacher data. We selected one teacher component as the action and one student component as the reward, resulting in 16 possible configurations (4 possible actions and 4 possible rewards).

### Model Estimation

We adopt the Hierarchical Bayesian Inference (HBI) framework, as described by @piray2019a, to assess the fitness of our RL models and estimate their respective parameters across subjects. This approach uses Laplace approximations for efficient computation of posteriors by approximating the integrals involved in Bayesian inference. Subsequently, it leverages population-level distributions to refine individual parameter variation. Within this framework, we assume that for any given model $m$ and subject $n$, the individual parameters ($h_{m,n}$) are normally distributed across the population with $h_{m,n} \sim N(\mu_m, \Sigma_m)$, where $\mu_m$ and $\Sigma_m$ represent the vector of means and the variance-covariance matrix of the distribution over $h_{m,n}$, respectively.

We use an expectation-maximization algorithm, iteratively performing the following two steps:

1.  Expectation Step: The algorithm calculates a posteriori estimates of the individual parameters ($h_{m,n}$) based on the existing group-level distributions.
2.  Maximization Step: The algorithm refines the group-level parameters ($\mu_m$ and $\Sigma_m$) using current individual parameter estimates. The updated mean group parameter $\mu_m$ is computed as the average of subject-level mean estimates across all subjects, $\mu_m = \frac{1}{N}\sum_{n}h_{m,n}$, where $N$ is the total number of subjects.

With this approach, we can estimate the log-likelihood for each subject's data, given the proposed models and parameter estimates. Recognizing the constraint of normality, we transform the initial estimates to generate constrained model parameters (e.g., the learning rate and discount factor in the Q-learning model). For parameters within a $(0,1)$ interval, we use the inverse logit function transform, $\text{Logit}^{-1}(x)=1/(1+e^{-x})$, and for intrinsically non-negative parameters, we use an exponential transformation. Consequently, HBI estimates the following unconstrained parameters:

1.  Q-learning:
    -   Learning Rate: $\text{Logit}(\alpha)=\log(\frac{\alpha}{1-\alpha})$
    -   Discount Rate: $\text{Logit}(\gamma)=\log(\frac{\gamma}{1-\gamma})$
    -   Inverse Temperature: $\log(\tau)$
    -   Cost: $\log(\text{cost})$
    -   Initial Q-value: $\Delta Q_{t=0}$
2.  Logistic and Baseline Regression Models:
    -   Parameters: $\beta$

### Top Model Selection

We determined the best-fit model from our set of candidates by considering each model's Akaike Information Criterion (AIC). To do so, we first computed the log-likelihood of the data given the estimated model parameters obtained from the Hierarchical Bayesian Inference (HBI) procedure. We then computed the average of the AICs across subjects for each model using the formula:

$$
\overline{\text{AIC}} = \frac{1}{N} \sum_{i=1}^{N} \left( 2p - 2\ln(\hat{L_i}) \right)
$$

where $N$ is the number of subjects, $p$ is the number of model parameters, and $\hat{L_i}$ is the maximum likelihood estimate for subject $i$ given the estimated model parameters.

### Behavioral Signatures

We examined three key behavioral signatures:

1.  Reward Seeking: We calculated the probability of choosing an action as a function of its Q-value difference (percentile rank of Q-values). For each model (Q-learning, Lau & Glimcher, and Baseline), we fitted a generalized linear model (GLM) with a quasibinomial family. The model took the form:

$$
\text{logit}(P(\text{Action})) = \beta_0 + \beta_1 Q_\text{percentile}
$$

where $Q_\text{percentile}$ is the percentile rank of Q-values. We extracted the slope ($\beta_1$) and its 95% confidence interval using clustered standard errors at the classroom level to account for within-classroom correlations.

2.  Uncertainty Aversion: We examined the probability of choosing the uncertain option as a function of the difference in expected value (EV) between uncertain and certain options. Uncertainty was defined based on the cumulative standard deviation of rewards associated with each action. We fitted a GLM with a quasibinomial family:

$$
\text{logit}(P(\text{Uncertain Choice})) = \beta_0 + \beta_1 \Delta EV_\text{percentile}
$$

where $\Delta EV_\text{percentile}$ is the percentile rank of the difference in expected value between uncertain and certain options. We calculated the indifference point (where $P(\text{Uncertain Choice}) = 0.5$) and its 95% confidence interval for each model.

3.  Learning Dynamics: We evaluated learning over time by examining two aspects. First, we computed mean prediction errors across teachers as:

$$
PE_t = \gamma R_t - Q_{t-1}(a)
$$ where $\gamma$ is the discount factor, $R_t$ is the reward at time $t$, and $Q_{t-1}(a)$ is the Q-value of the chosen action at the previous time step. We then evaluated the evolution of Q-value differences and the difference in mean rewards between action and inaction over time. The reward difference was calculated using a rolling mean with a window size of 4 weeks.

All statistical analyses were performed with clustered standard errors at the classroom level.

## Heterogeneity Analysis

After selecting the top-performing model (Q-learning), we explored heterogeneity through:

1.  Parameter-Classroom Characteristic Associations: For each Q-learning parameter (learning rate $\alpha$, discount factor $\gamma$, inverse temperature $\tau$, initial Q-value, and cost), we fitted separate models with the following specifications:

    a)  For ordinal outcomes (income and poverty levels), we used ordered logistic regression: $$
        \text{logit}(P(Y \leq j)) = \theta_j - (\beta_1X_1 + ... + \beta_pX_p)
        $$
    b)  For count outcomes (total students and number of classes), we employed Poisson regression: $$
        \log(E(Y)) = \beta_0 + \beta_1X_1 + ... + \beta_pX_p
        $$
    c)  For binary outcomes (paid account status), we used logistic regression: $$
        \text{logit}(P(Y=1)) = \beta_0 + \beta_1X_1 + ... + \beta_pX_p
        $$ where $X_1, ..., X_p$ represent the standardized Q-learning parameters.

2.  Parameter-Student Outcome Associations: We examined how model parameters relate to student outcomes using multiple linear regression models. Two key outcomes were analyzed:

    a)  Average weekly badges earned per student (indicating lesson completion)
    b)  Average weekly tower alerts per tower completion (indicating student struggles)

    The regression model took the form: $$
    Y = \beta_0 + \beta_1\alpha + \beta_2\gamma + \beta_3\tau + \beta_4Q_0 + \beta_5\text{cost} + C_i\Gamma + \epsilon
    $$ where $Y$ is the outcome variable, $\alpha$, $\gamma$, $\tau$, $Q_0$, and cost are the Q-learning parameters, and $C_i$ is a matrix of control variables including AIC, number of weeks, total students, number of classes, grade level, poverty level, charter school status, and paid account status.

# References

::: {#refs}
:::

{{< pagebreak >}}

# Supplemental Information {.appendix}

## Supplemental Methods {.appendix}

### PCA vs. NMF

Principal Component Analysis (PCA) was our first methodological choice. It is widely utilized but assumes data normality [@jolliffe2016] and maximizes variance explained, potentially overlooking subtle relationships between variables. Consequently, we also employed NMF, which, by contrast, imposes a non-negativity constraint and is more closely related to clustering algorithms, creating a more interpretable, sparse representation of behaviors [@ding2005; @lee1999]. This technique is particularly advantageous for data representing counts or frequencies. By trying different techniques, we can explore the reduced-dimension representation best suited to our specific dataset and research questions.

### Temporal Dynamics

Our investigation into temporal dynamics confirmed the impact of lagged rewards and actions on decision-making: shaping future decisions by past experiences. @fig-panel-bic illustrates this relationship, showcasing the predictive accuracy and model fit across fixed-effects models with different lags, with BIC and AUC scores for the models with one-week lags as the baseline. The results suggest a preference for a lag of two periods as optimal, based on the "elbow" in the AUC curves and the minima in the BIC curves.

```{r}
#| label: fig-panel-bic
#| fig-cap: "BIC and AUC variations across lags for fixed-effects panel logistic regression models. The plots show the percent change in model prediction accuracy (AUC) and fit (BIC) for different lag periods compared to the one-week lag baseline. The thin lines represent the percent change for each combination of reward functions and methods, while the dashed gray lines represent their average. The shaded bands around the average lines indicate the standard error. The optimal lag period can be determined based on the 'elbow' in the AUC curves (where increasing lags yields diminishing improvements in AUC) and the minima in the BIC curves (lower BIC indicates better model fit when penalizing for complexity)."
#| fig-subcap:
#|   - "BIC state-free"
#|   - "AUC state-free"
#|   - "BIC state-dependent"
#|   - "AUC state-dependent"
#| layout-ncol: 2

load("Regressions/fe-results.RData")

fe_results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = "None",
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
}))

teachers <- Reduce(intersect, sapply(results, function(x) {
  names(x$recoef$Teacher.User.ID)
}))

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    # State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         # bic = bic - bic_base,
         bic = bic/bic_base - 1,
         auc_base = auc[which(Lag == 1)],
         # auc = auc - auc_base,
         auc = auc/auc_base - 1) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "Percent Change in BIC",
                    "auc" = "Percent Change in AUC")

  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(Reward, Method),
                               # group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    scale_color_brewer(palette = "Set2") +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = brewer.pal(n = 8, name = "Set2")[8], alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, linetype = "dashed",
              color = brewer.pal(n = 8, name = "Dark2")[8]) +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    scale_y_continuous(labels = percent_format()) +
    theme(axis.text.x = element_text(hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#------------------

load("Regressions/fe-state-results.RData")

fe_results_df <- fe_results_df %>%
  rbind(
    do.call(rbind, lapply(results, function(x) {
      data.frame(
        Method = x$Method,
        Lag = x$Lag,
        State = x$State,
        Reward = x$Reward,
        auc = x$AUC,
        bic = x$bic,
        stringsAsFactors = FALSE
        )
      }))
    )

teachers <- intersect(
  teachers, Reduce(intersect, sapply(results, function(x) {
    names(x$recoef$Teacher.User.ID)
    }))
  )

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward, State) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         # bic = bic - bic_base,
         bic = bic/bic_base - 1,
         auc_base = auc[which(Lag == 1)],
         # auc = auc - auc_base,
         auc = auc/auc_base - 1) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "Percent Change in BIC",
                    "auc" = "Percent Change in AUC")

  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    scale_color_brewer(palette = "Set2") +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = brewer.pal(n = 8, name = "Set2")[8], alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, linetype = "dashed",
              color = brewer.pal(n = 8, name = "Dark2")[8]) +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    scale_y_continuous(labels = percent_format()) +
    theme(axis.text.x = element_text(hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#----------------

bic_plot_se
auc_plot_se
bic_plot_se_st
auc_plot_se_st

```

### Correlations Between Variables

We begin to unveil the intricate relationships among the variables under consideration through a comprehensive correlation analysis, as depicted in @fig-corr. This correlation matrix elucidates the magnitude and direction of associations among variables such as badges earned, minutes spent per student, tower alerts, the number of students, and teacher minutes. These interconnections inform the construction of our reinforcement learning models by suggesting the influence of teacher effort on student achievement. In this correlation matrix, each cell represents the Spearman correlation coefficient between a pair of variables. The color and size of the circles in each cell reflect the strength and direction of the correlation, with blue indicating positive correlations and red indicating negative correlations. The histograms along the diagonal provide a visual representation of the distribution of each variable.

```{r}
#| cache: true
#| label: fig-corr
#| fig-cap: "Correlation coefficients between variables after stardardization"
#| fig-format: png

df_corr <- df  %>%
  select(Badges.per.Active.User,
         Active.Users...Total,
         Minutes.per.Active.User,
         Tower.Alerts.per.Tower.Completion,
         Minutes.on.Zearn...Total) %>%
  rename("Badges" = Badges.per.Active.User,
         "Minutes per Student" = Minutes.per.Active.User,
         "Tower Alerts" = Tower.Alerts.per.Tower.Completion,
         "# of Students" = Active.Users...Total,
         "Teacher Minutes" = Minutes.on.Zearn...Total)

chart.Correlation(df_corr, histogram = TRUE, method = "spearman",
                  pch = 20, cex = 0.5, col = rgb(0, 0, 1, 0.5))

```

## Supplemental Tables {.appendix}

### Teacher Variables

| **Variable** | **Description** |
|--------------------|----------------------------------------------------|
| PD Course Guide Download [@zearn2023; @zearn2024c] | Detailed agenda for Professional Development (PD) courses focusing on classroom implementation, leadership, supporting diverse learners, using data to inform teaching practices, and accelerating student learning. |
| PD Course Notes Download [@zearn2023; @zearn2024c] | Professional development session notes offering insights into effectively using Zearn's curriculum. |
| Curriculum Map Download [@zearn2024d] | Detailed outline of learning objectives and content. Presents a sequence of interconnected math concepts across grades, aligning with states' instructional requirements. |
| Assessments Download [@zearn2024e] | Assessments to evaluate student understanding of the material, including ongoing formative assessments, digital daily checks, and paper-based unit assessments. |
| Assessments Answer Key Download [@zearn2024f] | Solutions for assessments to aid in grading and feedback. Provides detailed rubrics for mission-level assessments. |
| Elementary Schedule Download [@zearn2024g] | A recommended schedule for elementary school-level Zearn curriculum activities to guide daily and weekly instructional planning, ensuring comprehensive coverage of curriculum content. |
| Grade Level Overview Download [@zearn2024h] | Provides a summary of learning objectives, pacing guidance, key grade-level terminology, a list of required materials, and details on the standards covered by each lesson. |
| Kindergarten Schedule Download [@zearn2024i] | Recommended schedules for Kindergarten, supporting structured instruction planning. |
| Kindergarten Mission Download [@zearn2024j] | Details interactive activities focused on kindergarten-level concepts and their learning objectives. |
| Mission Overview Download [@zearn2024h] | Outlines a mission's (i.e., learning module) flow of topics, lessons, and assessments; highlights foundational concepts introduced earlier; lists recently introduced terms and required materials for teacher-led instruction. |
| Optional Homework Download [@zearn2024k] | Assignments for additional practice, enhancing student learning outside of class. |
| Optional Problem Sets Download [@zearn2024l] | Exercises for extra practice, tailored to reinforce lesson concepts. |
| Small Group Lesson Download [@zearn2024m] | Lessons designed for small-group engagement. |
| Student Notes and Exit Tickets Download [@zearn2024n; \@zearn2024o] | Student notes supplement digital lessons with paper-and-pencil activities. Exit tickets are lesson-level assessments for teachers to monitor daily learning. |
| Teaching and Learning Approach Download [@zearn2024p] | Resources outlining Zearn's pedagogical methods. |
| Whole Group Fluency Download [@zearn2024q] | Lesson-aligned practice activities to build math fluency through whole-class engagement. |
| Whole Group Word Problems Download [@zearn2024m] | Word problem-solving activities intended for collaborative, whole-class engagement. |
| Fluency Completed [@zearn2024q] | Indicates teacher completed a fluency activity, typically given to students before their daily digital lessons. |
| Guided Practice Completed [@zearn2024r] | Indicates teacher completed a guided practice segment, where students learn new concepts. These include videos with on-screen teachers, interactive activities, and paper-and-pencil Student Notes. |
| Kindergarten Activity Completed [@zearn2024s] | Indicates teacher completed an activity within the Kindergarten curriculum. |
| Number Gym Activity Completed [@zearn2024t] | Indicates teacher completed a Number Gym, an individually adaptive activity that builds number sense, reinforces previously learned skills, and addresses areas of unfinished learning. |
| Tower Completed [@zearn2024b] | Indicates teacher completed a Tower of Power, an activity that requires full mastery of lesson objectives and that students must complete independently. |
| Tower Struggled [@zearn2024u] | Indicates teacher committed a mistake when engaging with the Tower of Power activity in a student role, triggering a "boost" (scaffolding remediation). |
| Tower Stage Failed [@zearn2024b] | Indicates teacher received three consecutive "boosts" due to repeated errors when engaging with the Tower of Power in a student role. |

: Catalog of Teacher Activities. This table presents teachers' actions, including curriculum engagement, downloads of pedagogical materials, and completion of various interactive components within the Zearn educational platform. {#tbl-teacher-variables}

{{< pagebreak >}}

```{r}
#| label: tbl-heterogeneity-reg
#| tbl-cap: "Relationship between Q-learning Model Parameters and Classroom Characteristics. The table presents the results of five regression models examining how reinforcement learning (RL) parameters predict income and poverty levels, number of students, number of classes per teacher, and whether the school had a paid Zearn subscription. Coefficients and standard errors (in parentheses) are provided for each parameter. Income and Poverty are treated as ordinal variables, while Total Students and Number of Classes are count variables. Paid Account is a binary variable. All RL parameters are standardized (z-scored) before analysis."

predictors <- c("alpha", "gamma", "tau", "ev_init", "cost")
responses <- c("Income", "Poverty",
               "`Total Students`", "`No. of Classes`",
               "`Paid Account`")
model_path <- "CBM/zearn_results/aggr_results/lap_aggr_q_model_2.mat"
full_model <- readMat(model_path)

# Calculate AIC
AIC_full <- -2 * full_model[["cbm"]][[5]][[2]] + 2 * ncol(full_model[["cbm"]][[5]][[1]])
AIC_data <- tibble(
  Classroom.ID = map_int(full_model[["cbm"]][[5]][[5]], ~.[[1]][[9]][[3]][1,1]),
  AIC = as.vector(AIC_full)
)

full_data <- load_parameters(model_path) %>%
  inner_join(
    df_cleaned %>%
      group_by(Classroom.ID, Teacher.User.ID) %>%
      summarise(Income = first(income),
                Poverty = first(poverty),
                `Total Students` = mean(Students...Total),
                `No. of Classes` = mean(teacher_number_classes),
                `Paid Account` = first(school.account),
                .groups = "drop"),
    by = c("Classroom.ID")) %>%
  inner_join(AIC_data, by = "Classroom.ID") %>%
  mutate(across(all_of(predictors), scale),
         Income = factor(Income, ordered = TRUE),
         Poverty = factor(Poverty, ordered = TRUE)) %>%
  arrange(Classroom.ID)

# Run models
models <- list()
for(response in responses) {
  formula <- as.formula(paste(response, "~", paste(predictors, collapse = " + ")))
  if (response %in% c("Income", "Poverty")) {
    models[[response]] <- polr(formula, data = full_data, Hess = TRUE)
  } else if (response %in% c("`Total Students`", "`No. of Classes`")) {
    models[[response]] <- glm(formula, data = full_data, family = poisson(link = "log"))
  } else {
    models[[response]] <- glm(formula, data = full_data, family = binomial(link = "logit"))
  }
}

stargazer(models,
          type = "latex",
          title = "Relationship between RL Parameters and Classroom Characteristics",
          align = TRUE,
          dep.var.labels = c("Income", "Poverty", "Total Students",
                             "No. of Classes", "Paid Account"),
          covariate.labels = c("Learning Rate ($\\alpha$)", "Discount Factor ($\\gamma$)",
                               "Inverse Temperature ($\\tau$)", "Initial Q-value", "Cost"),
          model.names = TRUE,
          star.cutoffs = c(0.05, 0.01, 0.001),
          notes.append = FALSE)

```

```{r, results='asis'}
#| label: tbl-optimality-2
#| tbl-cap: "Impact of Q-learning Model Parameters on Average Weekly Tower Alerts per Tower Completion. Three linear regression models examine the correlations between a teacher's reinforcement learning (RL) parameters and student struggle, measured by average weekly Tower Alerts. Model 1 includes only RL parameters. Model 2 adds controls for AIC, number of weeks, total students, and number of classes. Model 3 further incorporates controls for grade level, poverty level, charter school status, and whether the school has a paid Zearn account. Coefficients and standard errors (in parentheses) are provided for each parameter. "

# Create table with stargazer
stargazer(model4, model5, model6,
          type = "latex",
          header=FALSE,
          align = TRUE,
          covariate.labels = c("$\\alpha$", "$\\gamma$",
                               "$\\tau$", "Cost", "Starting Q-value",
                               "No. of Weeks", "No. of Students",
                               "No. of Classes",
                               "Charter School", "Paid Zearn Account"),
          omit = c("AIC", "grade", "Poverty"),
          dep.var.labels = "Tower Alerts",
          star.cutoffs = c(.05, .01, .001), 
          star.char = c("*", "**", "***"),
          add.lines = list(c("Control for AIC",
                             "", "Yes", "Yes", "", "Yes", "Yes"),
                           c("Control for Grade Level",
                             "", "", "Yes", "", "", "Yes"),
                           c("Control for Poverty Level",
                             "", "", "Yes", "", "", "Yes")))

```

```{r}
#| cache: true
#| label: fig-corr-rl-params
#| fig-cap: "Correlation coefficients between estimated RL parameters and outcome variables"
#| fig-format: png

# Select the RL parameters and outcome variables
df_corr_rl <- full_data %>%
  select(alpha, gamma, tau, cost, ev_init, AIC) %>%
  mutate(across(c(alpha, gamma), function(x) 1/(1 + exp(-x))),
         tau = exp(tau),
         cost = exp(cost)) %>%
  # Remove tau and cost outliers
  filter(tau < quantile(full_data$tau, probs = .95),
         cost < quantile(full_data$cost, probs = .95)) %>%
  rename("Learning Rate" = alpha,
         "Discount Factor" = gamma,
         "Inverse Temperature" = tau,
         "Cost" = cost,
         "Initial Q-value" = ev_init,
         "AIC" = AIC)

# Generate the correlation chart
chart.Correlation(df_corr_rl,
                  histogram = TRUE,
                  method = "spearman",
                  pch="+")
```

## Supplemental Figures {#sec-supp-fig}

![Zearn Student Portal](images/student-feed.PNG){#fig-st-portal fig-align="center"}

![Professional Development Calendar](images/PD-calendar.jpg){#fig-prof-dev fig-align="center"}

```{r}
#| eval: false
#| echo: false
#| label: fig-raw-data
#| fig-cap: "The raw data from Zearn's platform, with each row corresponding to a different timestamp for a teacher action. The columns represent the different variables, including the number of minutes spent on the platform, the number of badges earned, and the number of boosts used. The data is organized by classroom, with each classroom having a different color. The data is also organized by week, with each week having a different shape. The data is organized by teacher, with each teacher having a different line type. The data is organized by school, with each school having a different line color."

```

{{< pagebreak >}}

```{r}
#| label: fig-income-dist
#| fig-cap: "Distributions of School Socioeconomic Profiles. The first graph categorizes schools into three groups based on the percentage of students eligible for free or reduced-price lunch (FRPL): low-poverty (0-40%), mid-poverty (40-75%), and high-poverty (over 75%). The second graph presents the distribution of median incomes for a school's associated region."
#| fig-subcap:
#|   - "School Poverty Distribution"
#|   - "Median Income Distribution"
#| layout-ncol: 2

poverty_plot
income_plot

```

```{r}
#| label: fig-teachers-map
#| fig-cap: "Geographic distribution of Zearn teachers across parishes in Louisiana. The color gradient represents the density of teachers, with darker hues indicating a higher concentration of educators using Zearn in each parish. The map also labels the top five cities where Zearn adoption is most prevalent."

map_LA

```

```{r}
#| label: fig-logins-week
#| fig-cap: "Total number of student logins over the 2019-2020 school year. The chart depicts the connection between academic schedules and platform engagement. Each bar represents a week, with peaks corresponding to active school weeks and troughs aligning with major holiday periods (e.g., Thanksgiving and Winter Break)."

logins_week
```

{{< pagebreak >}}

```{r}
#| label: fig-nmf-pca-comparison
#| fig-cap: "Comparison of dimensionality reduction techniques for teacher and student data. The figures compare the performance of Principal Component Analysis (PCA) against Nonnegative Matrix Factorization (NMF) in reducing the dimensionality of teacher and student data. The NMF variants include the Frobenius norm with two different initialization strategies: Nonnegative Double Singular Value Decomposition (Frobenius NNDSVD) and NNDSVD with the average of the input matrix X filled in place of zeros (Frobenius NNDSVDA). The third NMF variant uses the Kullback-Leibler divergence as the loss function. The left column assesses reconstruction quality using R-squared, where values closer to 1 indicate that the components can better recover the original data. The right column evaluates the interpretability of the low-dimensional representation using silhouette scores. Higher silhouette scores relate to better-defined clusters, values near 0 indicate overlapping clusters, and negative values generally suggest that a sample has been assigned to the wrong cluster."
#| fig-subcap: 
#| - "Teacher Data"
#| - "Student Data"
#| layout-ncol: 1

comparison_plot
comparison_plot2

```

{{< pagebreak >}}

```{r}
#| label: fig-aic-ecdf
#| fig-cap: "Empirical Cumulative Distribution Function (ECDF) of teacher-specific Akaike Information Criteria (AIC) for different models. Lower AIC values indicate better model fit."

# Load the full model data
full_model <- list(
  readMat("CBM/zearn_results/aggr_results/lap_aggr_baseline_model_2.mat"),
  readMat("CBM/zearn_results/aggr_results/lap_aggr_logit_model_2.mat"),
  readMat("CBM/zearn_results/aggr_results/lap_aggr_q_model_2.mat")
)
# # Extract log likelihoods
# model_freq_full <- full_model[["cbm"]][[5]][[5]]
# responsibility_full <- full_model[["cbm"]][[5]][[2]]
# model_classifier <- responsibility_full > 1/3

AIC_full <- sapply(full_model, FUN = function(x) {
  -2 * (x[["cbm"]][[5]][[2]])* 2 + 2 * ncol(x[["cbm"]][[5]][[1]])
}) %>% as.data.frame()
Model = c("Baseline", "Logit", "QL")
Action = c(rep("Pedagogical Knowledge", 3))
Reward = c(rep("Badges", 3))
names(AIC_full) <- paste0(Model,"_",Reward)

AIC_full <- AIC_full %>%
  pivot_longer(cols = everything(),
               names_to = "Model_Reward",
               values_to = "AIC") %>%
  separate(Model_Reward, c("Model", "Reward"), "_")

# Create the ECDF plot
ggplot(data = AIC_full, aes(x = AIC, color = Model)) +
  stat_ecdf() +
  scale_color_manual(values = c("Baseline" = brewer.pal(8, "Set2")[3],
                                "Logit" = brewer.pal(8, "Set2")[1],
                                "QL" = brewer.pal(8, "Set2")[2]),
                     labels = c("Baseline",
                                "Logit",
                                "QL")) +
  theme_minimal() +
  labs(
    x = "Individual Akaike Information Criterion (AIC)",
    y = "Cumulative Proportion",
    color = "Model"
  ) +
  theme(
    legend.position = "right",
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    axis.line = element_line(color = "black")
  )


```

<!-- # Plot action percentages -->

<!-- # This function visualizes how the percentage of chosen actions changes over time -->

<!-- # It compares actual behavior with model predictions -->

```{r}
#| label: fig-action-percentages
#| fig-cap: "Observed teacher behavior and model predictions over the academic year. The graph shows the percentage of times teachers chose to engage in Pedagogical Content activities. The x-axis represents biweekly periods throughout the school year. The black dashed line represents observed teacher behavior, while colored lines represent predictions from different models. Shaded areas represent the 95% confidence intervals for each model's predictions."

plot_action_percentages <- function(sim_data, wrapper, y_lim = c(0.13, 0.5)) {
  parameters <- load_parameters(
    paste0("CBM/zearn_results/aggr_results/lap_aggr_q_model_",
           wrapper, ".mat")
  )

  # Calculate action percentages
  plot_data <- sim_data %>%
    inner_join(parameters, by = "Classroom.ID") %>%
    group_by(Classroom.ID) %>%
    arrange(Classroom.ID, week) %>%
    mutate(
      biweek = ceiling(adjusted_week / 2),
      actual_action = action,
      q_action = pred_1_ql,  # Q-learning prediction
      lau_glimcher_action = pred_1_lg,  # Lau & Glimcher prediction
      baseline_action = pred_1  # Baseline model prediction
    ) %>%
    ungroup() %>%
    group_by(biweek) %>%
    summarise(
      actual_percent = mean(actual_action, na.rm = TRUE),
      se_actual = sd(actual_action, na.rm = TRUE) / sqrt(n()),
      q_percent = mean(q_action, na.rm = TRUE),
      se_q = sd(q_action, na.rm = TRUE) / sqrt(n()),
      lau_glimcher_percent = mean(lau_glimcher_action, na.rm = TRUE),
      se_lau_glimcher = sd(lau_glimcher_action, na.rm = TRUE) / sqrt(n()),
      baseline_percent = mean(baseline_action, na.rm = TRUE),
      se_baseline = sd(baseline_action, na.rm = TRUE) / sqrt(n()),
      date = mean(date)
    )
  
  # Prepare plot data
  month_breaks <- plot_data %>%
    mutate(month = floor_date(date, "month")) %>%
    group_by(month) %>%
    summarise(biweek = first(biweek)) %>%
    mutate(month_label = format(month, "%b")) %>%
    mutate(month_label = if_else(month_label == "Aug", "", month_label))

  # Prepare label data
  last_points <- plot_data %>%
    filter(biweek == max(biweek)) %>%
    summarise(
      x = max(biweek),
      y_actual = actual_percent,
      y_q = q_percent,
      y_lau_glimcher = lau_glimcher_percent,
      y_baseline = baseline_percent
    )
  
  # Create the plot
  ggplot(plot_data, aes(x = biweek)) +
    geom_line(aes(y = actual_percent, color = "Data"),
                  linetype = "dashed") +
    geom_ribbon(aes(ymin = actual_percent - se_actual, 
                    ymax = actual_percent + se_actual, 
                    fill = "Data"), alpha = 0.2) +
    geom_line(aes(y = q_percent, color = "Q-learning")) +
    geom_ribbon(aes(ymin = q_percent - se_q, 
                    ymax = q_percent + se_q, 
                    fill = "Q-learning"), alpha = 0.2) +
    geom_line(aes(y = lau_glimcher_percent, color = "Lau & Glimcher")) +
    geom_ribbon(aes(ymin = lau_glimcher_percent - se_lau_glimcher, 
                    ymax = lau_glimcher_percent + se_lau_glimcher, 
                    fill = "Lau & Glimcher"), alpha = 0.2) +
    geom_line(aes(y = baseline_percent, color = "Baseline")) +
    geom_ribbon(aes(ymin = baseline_percent - se_baseline, 
                    ymax = baseline_percent + se_baseline, 
                    fill = "Baseline"), alpha = 0.2) +
    geom_label_repel(data = last_points, 
                     aes(x = x, y = y_q, 
                         label = "Q-learning",
                         color = "Q-learning"),
                     nudge_x = 1.5,
                     segment.color = NA,
                     box.padding = 0.1,
                     label.padding = 0.1,
                     label.size = 0.1,
                     size = 3) +
    geom_label_repel(data = last_points, 
                     aes(x = x, y = y_actual, 
                         label = "Data",
                         color = "Data"),
                     nudge_x = 1.5,
                     segment.color = NA,
                     box.padding = 0.1,
                     label.padding = 0.1,
                     label.size = 0.1,
                     size = 3) +
    geom_label_repel(data = last_points, 
                     aes(x = x, y = y_baseline, 
                         label = "Baseline",
                         color = "Baseline"),
                     nudge_x = 1.5,
                     nudge_y = -.02,
                     segment.color = NA,
                     box.padding = 0.1,
                     label.padding = 0.1,
                     label.size = 0.1,
                     size = 3) +
    geom_label_repel(data = last_points, 
                     aes(x = x, y = y_lau_glimcher, 
                         label = "Lau & Glimcher",
                         color = "Lau & Glimcher"),
                     nudge_x = 1.5,
                     nudge_y = .01,
                     segment.color = NA,
                     box.padding = 0.1,
                     label.padding = 0.1,
                     label.size = 0.1,
                     size = 3) +
    labs(x = "Biweekly Period", 
         y = "Action Percentage",
         title = paste("Action Percentages -", 
                       action_reward_mapping[[wrapper]]$reward)) +
    scale_color_manual(values = c("Data" = "black",
                                  "Q-learning" = brewer.pal(8, "Set2")[2],
                                  "Lau & Glimcher" = brewer.pal(8, "Set2")[1],
                                  "Baseline" = brewer.pal(8, "Set2")[3])) +
    scale_fill_manual(values = c("Data" = "black",
                                 "Q-learning" = brewer.pal(8, "Set2")[2],
                                 "Lau & Glimcher" = brewer.pal(8, "Set2")[1],
                                 "Baseline" = brewer.pal(8, "Set2")[3])) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    ) +
    scale_x_continuous(breaks = month_breaks$biweek,
                       labels = month_breaks$month_label,
                       minor_breaks = seq(min(plot_data$biweek),
                                          max(plot_data$biweek),
                                          by = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                       limits = c(0, 1)) +
    coord_cartesian(xlim = c(min(plot_data$biweek),
                             max(plot_data$biweek) + 2),
                    ylim = y_lim)
}

# Create a list to store all plots
action_percentage_plots <- list()

# Generate all plots
for (wrapper in c(2, 14)) {
  full_data <- all_predictions[[
    paste0("lap_aggr_q_model_", wrapper, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("lap_aggr_logit_model_", wrapper, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_ql", "_lg")
    ) %>%
    full_join(all_predictions[[
      paste0("lap_aggr_baseline_model_", wrapper, ".mat")
    ]] %>% select(-reward), by = c("Classroom.ID", "week", "action"))
  full_data <- full_data %>%
    group_by(Classroom.ID) %>%
    filter(week == 10) %>%
    pull(Classroom.ID) %>%
    {filter(full_data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - 8) %>%
    filter(adjusted_week >= 1) %>%
    mutate(date = as_date("2019-08-26") + weeks(adjusted_week - 1))

  plot <- plot_action_percentages(
    full_data, wrapper
    )
  action_percentage_plots[[paste(wrapper, sep = "_")]] <- plot
}

action_percentage_plots

```

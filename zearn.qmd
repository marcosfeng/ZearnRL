---
title: "Predicting Repeated Behavior in Behavioral Sciences"
subtitle: "Applying Reinforcement Learning in Teacher Decision-Making"
abstract: "This paper aims to model the decision-making process of a teacher in a math-teaching platform named Zearn with an RL algorithm. Akin to a multi-armed bandit, teachers choose how much effort to put in per week (in minutes) based on a function of the cost of teachers’ time and the number of lessons their students earned as rewards. Every teacher is attempting to both “learn” (i.e., explore) and  “optimize” (i.e., exploit) their number of minutes spent on Zearn and learn over time how they should engage with the platform. We find that teachers who prefer to explore... , whereas teachers who prefer to exploit..."
keywords: "Reinforcement Learning, education, habits"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
format:
  elsevier-pdf:
    journal:
      keep-tex: true
      formatting: preprint
      model: 3p
      layout: onecolumn
      cite-style: number
      # graphical-abstract: "![](abstract.png)"
bibliography: zearnrefs.bib

execute:
  echo: false
  warning: false
  error: false
---

# Introduction

## Predicting Repeated Behavior in the Behavioral Sciences

Predicting repeated behavior has been a goal of the behavioral sciences, including economics, psychology, and neuroscience. One prominent way of quantifying this relationship is through reinforcement learning (RL) algorithms that assign a mathematical relationship between contextual cues (states), behavior (actions), and reward (CITE RL Neuroecon). In general, reinforcement learning is widely used in both neuroscience and computer science on extremely large data sets to help model agents in specific environments. Psychology and economics generally do not create dynamic models across time (at least in this way), but computer science and neuroscience generally do not work with such practical and applied data. This presents us with a novel opportunity to use methods from one set of disciplines on data that is traditionally used in another set of disciplines. <!--# Add advance organizer -->

## A Novel Approach

ADD: goals, research questions, hypotheses, overall contribution to the field.

This paper aims to model the decision-making process of a teacher in a math-teaching platform named Zearn (CITE Zearn) with an RL algorithm. Given these instructors' context, we argue that an RL model is most appropriate. Using both real and simulated data, we model the teachers as decision makers in a reinforcement learning context, akin to a multi-armed bandit, choosing how much effort to put in per week (in minutes) based on a function of cost of teachers' time and the number of badges their students earned in previous week's rewards. We assume that every teacher has an objective function where they want to balance how much time they spend using Zearn (e.g., assigning homework, checking student progress, and reviewing content) with the potential for students to receive badges. Every teacher is attempting to both "learn" and "optimize" their number of minutes spent on Zearn and learn over time how they should engage with the platform assuming that the relationship between teacher minutes and student badges is stochastic. There is a tradeoff between learning, also known as exploring (spending varying amounts of time on the platform to see how effort affects student achievement) and optimizing, also known as exploiting (spending amounts of time on the platform that teachers know already have good outcomes without wasting their time), that our model is able to explicitly quantify (learning rate and inverse temperature for the algorithmically informed). The RL algorithm allows for this type of flexibility for learning the best strategy given certain contextual information by modeling the tradeoff between teachers exploring unknown options while exploiting the information they have about the Zearn system. Inputs can, and perhaps should be flexible to temporary needs by students. For example, on a week in which students struggle more, the teacher should adjust their effort accordingly. As such, reinforcement learning offers a flexible and robust model for our available data.

## RL in teaching and education

1.  Markov decision processes
2.  Instructional actions, objectives, and costs

## The Zearn Platform

Zearn is a digital platform for mathematics education designed to facilitate the teaching and learning of mathematics, providing a rich environment for modeling the decision-making process of teachers. About 25% of elementary-school students and over 1 million middle-school students across the United States use Zearn [@post-weblog]. The platform's unique blend of hands-on teaching and immersive digital learning offers a fertile ground for understanding how teachers adapt their strategies to optimize student achievement.

Zearn's pedagogical approach includes interactive digital lessons using visual aids (see @fig-zearn-poster) and real-time student feedback. The platform's approach to mathematical concepts, such as fractions, is particularly noteworthy. Students go through a series of representations: concrete, pictorial, and abstract, each designed to scaffold (i.e., "breaking down" problems, see [@jumaat2014a; @reiser2014]) their understanding and prepare them for subsequent levels. This approach aligns with the Common Core State Standards, enhancing relevance in the contemporary educational landscape.

![Example of Teaching with Visual Models on Zearn](images/zearn-poster.jpg){#fig-zearn-poster fig-align="center"}

The platform's structure facilitates a personalized learning experience for students (see SI for a screenshot of the student portal), allowing teachers to track student progress and make informed decisions (see @fig-class-report for a sample class report). This structure includes self-paced online lessons and small group instruction, a rotational model that allows students to learn new grade-level content in two ways: independently through engaging digital lessons, and in small groups with their teacher and classmates. This dual approach enables students to learn at their own pace, fostering a sense of autonomy and self-directed learning.

![Sample Class Report](images/class-report.png){#fig-class-report fig-align="center"}

A key feature of Zearn is its badge system, which serves as a mechanism for tracking student progress and motivating continued learning (see @fig-badges-screen). Students earn badges upon mastery of specific skills, providing a tangible representation of their achievement. This system motivates students and provides teachers with valuable data on student performance, informing their decision-making process [@knudsen2020]. Zearn also incorporates notifications, known as Tower Alerts, sent to teachers when a student struggles with a specific concept. This feature allows teachers to provide timely support and address learning gaps, enhancing the platform's capacity for personalized learning.

![Badge System for Student Achievement](images/badges.PNG){#fig-badges-screen fig-align="center"}

Another noteworthy aspect is the platform's professional development component, which is available for schools with a paid account (see SI for a sample training schedule). Teachers explore each unit or mission through word problems, fluencies, and small group lessons, conducting collaborative analysis of student work and problem-solving strategies. This professional development revolves around each mission's big mathematical idea, visual representations to scaffold learning, and strategies to address unfinished learning from prior grades and preparation for future learning [@morrison2019].

In order to model interesting behavioral patterns, we focus our analysis on the teachers who most likely take advantage of a wide range of resources on the platform. Thus, we select teachers who consistently use the platform and work in traditional school settings.

The platform's integrated approach to math teaching and learning, connecting a print- and software-based curriculum with a rotational classroom model, professional development, and classroom- and school-level reports on student learning, provides a comprehensive data set for analysis. The variables of interest in this study have been made available by the Zearn team and include (1) teacher effort (as measured by a gamut of actions), (2) lesson completion (badges), (3) student performance (tower alerts), and the (4) time teachers and students spend on the platform.

<!--Lit review here -->

## Research Questions

<!--# ADD: How does it fill gaps in the lit? -->

<!-- Get some ideas from the mega-study prereg -->

# Theory

## Education production function

Previous research has explored how teacher effort affects student achievement. In particular, economists have studied the "education production function," in which educational outcomes are a function of various inputs, including teacher effort, student effort, school resources, and family background [@hedges1994]. This function serves as a theoretical framework for understanding how different factors contribute to educational achievement and how interventions can be designed to improve outcomes.

One of the key inputs in the education production function is teacher effort, as teachers play a crucial role in shaping students' learning experiences and outcomes. Their effort, which encompasses their time, energy, dedication, and instructional strategies, can significantly influence students' academic achievement [@rivkin2005]. However, measuring teacher effort and its impact on student outcomes can be challenging due to the complex and multifaceted nature of teaching.

To address this challenge, researchers have employed various strategies to identify the effects of teacher effort on student achievement. One common approach is to manipulate the conditions under which teachers operate, thereby changing the levels of teacher effort. For instance, Duflo, Dupas, and Kremer (2011) [@duflo2011] conducted a randomized controlled trial in Kenya to examine the effects of tracking, a practice of grouping students based on their ability levels. Tracking has been a prominent tool in the sociology of education and assumes that teacher inputs will depend on the ability level of the students. This tailoring of teaching strategies and content potentially enhances the instructor's effectiveness.

However, traditional social science approaches to studying the education production function often lack the flexibility to account for changes in context and experience and individual-level differences. Reinforcement learning (RL) offers a promising alternative approach. RL models incorporate a flexibility term (i.e., a learning rate) that allows for changes in behavior with experience, and an exploration versus exploitation term (e.g., inverse temperature) that captures individual differences in decision-making strategies. Furthermore, the RL framework inherently accounts for the process of learning about rewards, making it a flexible and dynamic tool for studying teacher behavior [@sutton2018].

## Reinforcement Learning to Capture Patterns in Repeated Behavior

In RL, an agent learns to make decisions over time to maximize a cumulative reward. At the heart of RL is the concept of a policy, which is a mapping from states to actions, or more commonly, a probability distribution over actions [@sutton2018]. The agent's goal is to learn an optimal policy, which maximizes the expected cumulative reward, even when the parameters of the environment are not known a priori.

The application of RL in the context of education and teaching is not new. One of the pioneers in the field of Markov decision processes, Ronald Howard, attempted to apply his mathematical framework to instruction theory as early as 1960 [@howard1960]. Later, in 1972, Richard Atkinson proposed a theory of instruction that encapsulates the key components of a Markov decision process, including states, actions, transition probabilities, reward functions, and a time horizon [@atkinson1972]. In Atkinson's framework, actions are instructional activities (e.g., assigning problem sets) that can change a given state (e.g., student learning level). These changes of states can yield rewards minus the associated cost of the action. For example, a teacher may be rewarded by an increase in the knowledge or skill of a student, but such reward must be balanced with its associated effort (e.g., labor cost). Atkinson and colleagues continued to test many parametrizations of this idea, contributing significantly to the development of RL theory in the context of education (see [@doroudi2019] for a full review).

More recently, RL models have been used in the psychology of habit to explain learning and reward association. One common approach in human studies is to apply the "multi-armed bandit" task. In this type of experiment, participants are presented with multiple actions, each with an unknown payoff. The subject's goal is to learn the best outcome through trial and error. In the beginning, the reward-action relationships are unknown, so the participant must explore or sample from each action [@sutton2018]. This exploration-exploitation trade-off is a central theme in RL and has the potential to provide valuable insights into how students learn and make decisions over time.

### Why Reinforcement Learning?

Before presenting these models, we argue for the usefulness of RL in our setting. Unlike traditional economic models that map teacher effort to student outcomes through a time-fixed education production function, RL allows us to model teacher behavior as a flexible, evolving process that can change week by week. The RL framework is inspired by the way animals learn from their experiences, with an agent representing a decision-maker that performs actions in an environment and receives observations and rewards in return. In the RL paradigm, teachers are modeled as agents that interact with their environment (the classroom), making decisions (actions) based on their current understanding of the environment (state) and the feedback they receive (rewards). This interaction can be represented as:

$$
\text{Agent} \xrightarrow[\text{Actions}]{\text{Performs}} \text{Environment} \xrightarrow[\text{Observations, Rewards}]{\text{Provides}} \text{Agent}
$$

In the context of predicting repeated behavior, we can use RL algorithms to model the decision-making process of individuals or groups, such as teachers, by learning from the patterns in their actions and the resulting outcomes. This approach allows us to model how individual teachers learn the relationships between actions, states, and rewards, thereby creating a typification of instructors. The RL framework's flexibility and robustness make it adaptable to changes in the learning environment and individual student needs. Furthermore, it allows for incorporating a diverse range of state, action, and reward variables and can be tailored to different educational contexts and objectives. RL models can also capture the inherent uncertainty in the teaching environment, providing a framework for making optimal decisions under uncertainty. This feature is particularly relevant in the context of teaching, where the effectiveness of a teaching strategy may not be immediately apparent, and the optimal strategy may evolve as students' understanding develops.

With RL, it then becomes possible to identify how teachers differ in their learning and behavior and how these characteristics relate to student outcomes. For example, teacher flexibility may be optimal in a learning environment, but without estimation of individual parameters, differentiating teachers would not be possible. Further, if a policy-maker can shift these individual-level parameters, they can affect student outcomes. These so-called "counterfactual analyses" can be powerful tools in creating innovative interventions or nudges to improve an outcome of interest. Finally, the RL model carries potential beyond the goals of this study. For instance, it can learn the associations between actions and rewards, allowing for automation of certain instructional inputs; if a teacher often assigns an activity under a certain state, the model could automate this action, freeing some of the teacher's time.

### Q-Learning Model

The first class of models we apply to our data is the so-called Q-learning algorithm. Q-learning is a model-free reinforcement learning algorithm designed to learn a policy, which tells an agent what action to take under what circumstances [@watkins1992]. It does not require a model of the environment (hence the connotation "model-free"), and it can handle problems with stochastic transitions and rewards without requiring adaptations. This model is inspired by the so-called "multi-armed bandit" problem. In this paradigm, an agent has a finite number of choices, each associate with a given reward. The agent must simply learn to choose which action yields the highest reward. Learning in this setting occurs by adjusting expectations and minimizing "surprises" (i.e., prediction errors). Notice that this setting does not require us to define a given state: in our simplest model, Q-learning assumes that the best action in one week is the best action at any other week. Thus, the model only prescribes an action-reward relationship. The teacher here learns the value of logging in regardless of the history of their classroom or students.

In the context of Q-learning, the agent interacts with the environment to gain experience and uses this experience to update its knowledge about the quality of particular actions, given the state of the environment. This knowledge is represented in the Q-values, a prediction of the future reward expected after taking an action in the current state (which in our context can be compared to subjective value, or a utility). The goal of Q-learning is to accurately learn these Q-values by updating them iteratively [@rummery].

The Q-learning model is based on the concept of a Q-function, which is a function of both state and action, and represents the expected future reward for taking a particular action in a particular state. The Q-function is updated iteratively using the Bellman equation, which expresses the value of a state-action pair in terms of the immediate reward plus the discounted value of the best future state-action pair.

The Q-function, denoted as $Q(s, a)$, is defined for all state-action pairs $(s, a)$, where $s$ is the state and $a$ is the action. The Q-function represents the expected return or future reward for taking action $a$ in state $s$ following a certain policy $\pi$. The Q-function is updated iteratively using the Bellman equation, as follows:

$$
Q(s, a) = Q(s, a) + \alpha \delta
$$ {#eq-q-learn}

where $\alpha$ is the learning rate, which determines how much the Q-value is updated based $\delta$, the reward prediction error. The reward prediction error is the difference between the estimated Q-value and the observed reward plus the discounted future Q-value. This error is used to update the Q-value in the direction of the observed reward, as follows:

$$
\delta = r + \gamma \max_{a'} Q(s', a') - Q(s, a)
$$ {#eq-RPE}

where:

-   $r$ is the immediate reward received after taking action $a$ in state $s$,

-   $\gamma$ is the discount factor,

-   $s'$ is the new state after taking action $a$,

-   $a'$ is the action to be taken in the new state $s'$,

-   $s$ is the current state,

-   $a$ is the action taken,

-   $\max_{a'} Q(s', a')$ is the maximum reward that can be obtained in the next state $s'$,

-   $Q(s, a)$ is the current estimate of the Q-value for action $a$ in state $s$.

The Q-learning algorithm uses this update rule to learn the Q-function and, hence, the optimal policy. The agent starts with an initial Q-function (which can be arbitrary) and then updates the Q-values based on the experiences it gathers from interactions with the environment. The update rule is applied every time the agent transitions from a state $s$ to a state $s'$ by taking an action $a$ and receiving a reward $r$. The agent selects actions based on a policy derived from the Q-values. A common choice is the softmax action selection method, which is a way to balance exploration and exploitation. The softmax method chooses actions probabilistically based on their Q-values. The softmax function determines the probability of choosing a particular action and is defined as follows:

$$
P(a) = \frac{e^{Q(s, a)/\tau}}{\sum_{a'} e^{Q(s, a')/\tau}}
$$ {#eq-softmax}

where:

-   $P(a)$ is the probability of choosing action $a$,

-   $Q(s, a)$ is the Q-value of action $a$ in state $s$,

-   $\tau$ is a parameter known as the temperature, which controls the level of exploration,

-   the denominator is the sum over all possible actions $a'$ of the exponential of their Q-values divided by the temperature.

The temperature parameter $\tau$ controls the trade-off between exploration and exploitation. When $\tau$ is high, the agent explores more because the action probabilities are more uniform. When $\tau$ is low, the agent exploits more because the action with the highest Q-value is more likely to be chosen than the others. As the agent learns, it can be beneficial to start with a high temperature to encourage exploration and then gradually decrease it to favor exploitation of the learned policy.

### State-Free vs. State-Based Models

So far, we have defined state-based models incorporating the environment into the Q-function. In state-free models, the Q-function, denoted as $Q(a)$, is solely a function of the action, $a$. The environmental state does not factor into the decision-making process. This simplifying assumption can be beneficial in scenarios where the environmental state exerts minimal influence on the outcome of the action. The agent learns a global policy that is independent of the specific state. This approach can be effective in environments with low state-action complexity or when the state is difficult to define or observe [@sutton2018a]. The Q-value update function, therefore, simplifies to:

$$
Q(a) = Q(a) + \alpha \left[ r(a) - Q(a) \right]
$$ {#eq-state-free}

where:

-   $Q(a)$ is the current estimate of the Q-value for action $a$,

-   $\alpha$ is the learning rate,

-   $r(a)$ is the immediate reward received after taking action $a$.

In this equation, the term in the brackets, $r(a) - Q(a)$, is the reward prediction error. It represents the difference between the observed reward and the current estimate of the Q-value. The Q-value is updated in the direction of this error, scaled by the learning rate $\alpha$.

### The Kernel Function

The kernel function is a mathematical tool used to measure the similarity between different states or actions [@ormoneit2002; @domingues; @liu]. In the context of the Q-learning model with states and a kernel, the kernel function is used to compute a weighted average of the rewards obtained in similar state-action pairs in the past. This allows the model to generalize from past experience and to make more informed decisions about future actions.

### The Actor-Critic Model

The next model we attempt to fit divides the action selection and action evaluation tasks into two components: the "actor" and the "critic" [@sutton2018b]. This division theoretically allows for more efficient learning, as the critic guides the actor's learning process.

The "actor" in this model selects actions based on a policy function, denoted as $\pi(a|s)$, which maps states to actions, determining the probability of taking each action in each state. The actor aims to learn an optimal policy that maximizes the expected cumulative reward. In our setting, we set the policy as a softmax function over action preferences:

$$
\pi(a|s) = \frac{e^{h(a, s)}}{\sum_{a'} e^{h(a', s)}}
$$

where $h(a, s)$ is the preference for action $a$ in state $s$.

The "critic," on the other hand, evaluates the actions taken by the actor by learning a value function, denoted as $V(s)$. Given the actor's current policy, the critic estimates the expected cumulative reward from each state. The critic's feedback, in the form of the value function, guides the actor's learning. We update the value function based on the Temporal Difference (TD) error, a measure of the difference between the estimated and actual return:

$$
\delta = r + \gamma V(s') - V(s)
$$

where $r$ is the reward, $\gamma$ is the discount factor, $s'$ is the new state, and $s$ is the current state.

This separation of action selection and evaluation distinguishes the Actor-Critic model. In Q-learning, a single Q-function selects and evaluates actions. Conversely, in the Actor-Critic case, the actor updates its policy to increase the probability of actions that lead to higher-than-expected returns and decrease the probability of actions that lead to lower-than-expected returns.

# Applying RL Models to the Zearn Context

In this section, we propose an application of the Reinforcement Learning (RL) algorithms for modeling teacher decision-making within the Zearn platform.

Consider a typical teaching scenario: the state is the students' current progress in the class, and the actions are a range of pedagogical strategies, such as assigning additional practice, providing personalized feedback, or adjusting lesson plans. Some relevant reward variables include improved student performance, increased student engagement, or reduced learning gaps. We represent this interaction as:

$$
State (S) \xrightarrow[\text{Action (A)}]{\text{Teacher Decides}} New State (S') \xrightarrow[\text{Reward (R)}]{\text{Resulting Outcome}} \text{Feedback}
$$ {#exm-zearn-RL}

In the Zearn context, we define the decision process as follows:

1.  Agents are the teachers.

2.  Actions include the time spent on the platform and the choice to apply specific pedagogical strategies.

3.  The environment is the Zearn platform with its students.

4.  The state is the week-over-week change in Tower Alerts.

5.  The reward is a linear function of the number of badges earned.

Mathematically mapping the agent-environment interaction is flexible, with many models potentially satisfying our initial assumptions. We approach this problem as a competition of models, selecting a set of models applicable to our setting, fitting them to the data, and comparing their performances.

## State-Free Q-Learning Model

We use the Q-learning model to predict the actions of teachers based on their past actions and the rewards they received.

The Q-value for a given state $s$ and action $a$ is updated based on the reward prediction error $\delta$:

$$
Q(a) = Q(a) + \alpha \left( \gamma (\text{Badges}_t - \text{cost}(a)) - Q(a) \right)
$$

where

-   $\alpha$ is the learning rate,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_t$ is the immediate reward received after taking action $a$ in state $s$,
-   $\text{cost}(a)$ is the cost associated with action $a$,
-   $Q(a)$ is the current estimate of the Q-value for action $a$.

The probability of choosing a particular action is determined by the softmax function and is defined as follows:

$$
P(a) = \frac{e^{Q(a)/\tau}}{\sum_{a'} e^{Q(a')/\tau}}
$$

where:

-   $P(a)$ is the probability of choosing action $a$,
-   $Q(a)$ is the Q-value of action $a$,
-   $\tau$ is a parameter known as the temperature, which controls the level of exploration.

Note that the reward is assumed to be a linear function of the number of badges earned, with the learning rate $\alpha$ as the multiplier and the cost as the constant subtracted from the reward. We interpret each of these parameters as follows:

-   Reward: A linear function of the number of badges earned, indicating that the more badges a teacher earns, the higher the reward. The learning rate determines how quickly the teacher updates their Q-values in response to new information, while the cost represents the perceived effort or inconvenience associated with the action.

-   Cost: The perceived effort or inconvenience associated with the action, such as the effort required to complete a particular task or the inconvenience of deviating from a preferred teaching method.

-   Learning rate ($\alpha$): The extent to which the newly acquired information will override the old information. A factor of 0 will make the agent not learn anything.

-   Discount rate ($\gamma$): The degree to which future rewards are discounted compared to immediate rewards. A high discount rate means that future rewards are considered almost as valuable as immediate rewards, which encourages long-term planning. A low discount rate means that immediate rewards are much more valuable than future rewards, which encourages short-term thinking.

-   Inverse temperature ($\tau$): The degree of randomness in the choice behavior. A high inverse temperature means that the agent is more likely to choose the action with the highest expected reward, while a low inverse temperature means that the agent is more likely to choose actions randomly. This parameter can be interpreted as a measure of the agent's confidence in its Q-values, reflecting the trade-off between exploration (trying out new actions) and exploitation (sticking to known beneficial actions).

## State-Based Q-Learning Model

In this model, the state $s$ represents the current situation or context in which the teacher decides. The state could include factors such as the students' current performance level. The state-dependent model is particularly relevant in teaching, where the effectiveness of a given strategy may depend on the specific circumstances of the class. Here, the Q-value for a given state $s$ and action $a$ is:

$$ Q(s, a) = Q(s, a) + \alpha \left( \gamma (\text{Badges}_t - \text{cost}(a)) - Q(s, a) \right) $$

where:

-   $\alpha$ is the learning rate,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_t$ is the immediate reward received after taking action $a$ in state $s$,
-   $\text{cost}(a)$ is the cost associated with action $a$,
-   $Q(s, a)$ is the current estimate of the Q-value for action $a$ in state $s$.

Like the state-free model, the softmax function determines the probability of choosing a particular action.

## The Kernel Function

We anticipate that the reward associated with a given action may be delayed, reflecting the time it takes for the effects of an action to manifest. For instance, teachers might spend the first week of each month planning activities on the platform, with the rewards of these actions becoming apparent in subsequent weeks. To capture this temporal aspect of teacher behavior, we employ a kernel function that incorporates a measure of similarity between the current and past state-action pairs, modulated by a discount factor that accounts for the delay in reward. The kernel function is defined as the Jaccard similarity, as follows:

$$
K(t, t') = 0.5 \cdot \text{I}(a_t = a_{t'}) \cdot \left[ 1 + \text{I}(s_t = s_{t'})\right]
$$

where:

-   $K(t, t')$ is the kernel function,
-   $t$ and $t'$ are the current and past time steps (weeks), respectively,
-   $\text{I}(\cdot)$ is the indicator function, which equals 1 if the condition inside the parentheses is true and 0 otherwise,
-   $a_t$ and $a_{t'}$ are the choices made at the current and past time steps, respectively,
-   $s_t$ and $s_{t'}$ are the states at the current and past time steps, respectively.

### Kernel-Based Reinforcement

We use the kernel function to compute a weighted average of past rewards (kernel reward), which updates the Q-value for the current state-action pair. The kernel reward is:

$$
R_K = \frac{\sum_{t' = 1}^{T} K(t, t') \cdot \gamma^{(t - t')}  \text{Badges}_{t'}}{\sum_{t' = 1}^{T} K(t, t')}
$$

where:

-   $R_K$ is the kernel reward,
-   $\gamma$ is the discount rate,
-   $\text{Badges}_{t'}$ is the number of badges earned at the past time step $t'$,
-   $T$ is the number of lags in the kernel.

Subsequently, the Q-value for the current state-action pair is updated:

$$
Q(s, a) = Q(s, a) + \alpha \left( R_K - \text{cost}(a) - Q(s, a) \right)
$$

## Dynamic Analysis (Lau & Glimcher, 2005)

In order to determine the number of lags in the kernel, we apply the Dynamic Analysis method, as proposed by [@lau2005]. The Dynamic Analysis method is based on the premise that both past reinforcers and past choices significantly influence an individual's choices on each trial. It is a powerful tool for understanding choice behavior in the context of reinforcement learning. This method is particularly useful in tasks where reinforcement contingencies vary within a session, as it allows for the analysis of behavior at a granular, response-by-response level, while also providing insights into behavior averaged over multiple trials. It also implies that the ratio of responses to different alternatives matches the ratio of reinforcements obtained from these alternatives. The method allows for the prediction of response rate using weighted sums of past reinforcers, a technique that has been applied to predict choice allocation in concurrent schedules on both short (response-by-response) and longer (within and between sessions) time scales.

Based on logistic regression, the model captures the linear combination of past reinforcers and choices on each trial, as defined by @eq-lauglimcher, where $p$ is the probability of choice $c$ ($R$, right or $L$, left) at time $i$ as a function of the rewards, $r$, and choices for every time point $i-j$ in the history.

$$
\begin{aligned}\log \left(\frac{p_{R, i}}{p_{L, i}}\right)= & \sum_{j=1} \alpha_{ j}( r_{R, i-j}-r_{L, i-j}) \\& +\sum_{j=1} \beta_{j} (c_{R, i-j}-c_{L, i-j})+\gamma,\end{aligned}
$$

A key aspect of Dynamic Analysis is its focus on local variations in behavior and how these relate to local variations in reinforcer history and behavioral history. This local focus allows for the identification of patterns and trends that may be obscured when looking at behavior on a larger scale. For example, the method can reveal a tendency to alternate between choices in the short term, while also showing a longer-term tendency to persist with the more frequently chosen alternative.

The Dynamic Analysis method also allows for the quantitative assessment of the relative effects of past reinforcers and choices. This is achieved by including covariates ($\alpha, \beta$) for past choices in the analysis, which can reveal patterns such as a fix-and-sample pattern of choice, where there is a tendency to persist on the rich alternative with brief visits to the lean alternative.

In terms of the weighting of past reinforcers and choices, the Dynamic Analysis method suggests that the effects of these factors decay over time. This is represented by a decay function, which can take the form of an exponential or hyperbolic function, depending on the specific characteristics of the task and the behavior of the individual.

### Dynamic Analysis in the Zearn context

In the context of the Zearn dataset, we implement the Dynamic Analysis method to understand the temporal dependencies and complex interactions between teacher actions, student outcomes, and the learning environment. The method is applied to a panel data model, which allows for the inclusion of both time-invariant and time-varying variables, and accounts for individual-specific effects.

(Based on theories A/B... these are the variables... show the relationship between the variables. Explain why one variable may be correlated to the other, etc. (why those relationships exist) -- Use diagram.) The key variables in the model are the rewards and choices. In this context, the rewards are represented by Badges per active student in a given week. The choices are represented by Teacher Minutes or the components from our dimensionality reduction process variable.

The model also includes lagged versions of these variables, which capture the effects of past rewards and choices on current choices. The lag period ranges from 1 week to 8 weeks, allowing for the examination of the effects of past rewards and choices over different time horizons.

The model can be represented by the following equation:

$$
y_{it} = \beta_0 + \sum_{j=1}^{L} \beta_{1j} y_{it-j} + \beta_2 x_{it} + \sum_{j=1}^{L} \beta_{3j} x_{it-j} + u_i + \epsilon_{it}
$$

where:

-   $y_{it}$ is the dependent variable (teacher minutes or method component) for individual $i$ at time $t$,
-   $y_{i t-j}$ is the dependent variable lagged by $j$ periods,
-   $x_{it}$ is the independent variable (badges per active user) for individual $i$ at time $t$,
-   $x_{i t-j}$ is the independent variable lagged by $j$ periods,
-   $u_i$ is the individual-specific effect,
-   $\epsilon_{it}$ is the error term, and
-   $L$ is the number of lag periods.

The coefficients $\beta_1j$ and $\beta_3j$ capture the effects of past choices and rewards, respectively, lagged by $j$ periods, on current choices. The sum over $j$ from $1$ to $L$ represents the inclusion of lagged variables for all lag periods from 1 week to 8 weeks.

The Dynamic Analysis method offers several advantages in the context of the Zearn dataset. First, it captures the temporal dependencies and complex interactions between teacher actions, student outcomes, and the learning environment. This allows for a more nuanced understanding of the factors that influence teacher behavior on the platform.

Second, the method allows for the identification of a kernel that best describes the temporal dynamics of the data. This kernel, represented by the lagged variables in the model, captures how far back rewards and past actions matter. This can provide valuable insights into the temporal scope of the effects of rewards and actions, and can inform strategies for influencing teacher behavior on the platform.

## The Actor-Critic Model

In this model, we estimate several variables: the cost in badge-units for each component, the discount rate ($\gamma$), and step-sizes ($\alpha_v$ and $\alpha_\pi$). These step-sizes are similar to learning rates, as they determine the extent to which the model updates the weights for the value and policy functions, respectively.

Our Actor-Critic model maintains two sets of parameters: the weights $w_a$ that parameterize the value function (the critic) and the parameters $\theta_a$ that define the policy (the actor). The weights and policy are updated based on the prediction error $\delta$. The equations for updating the weights and policy are as follows:

$$
\begin{align}
w_a &= w_a + \alpha_{v} \cdot \delta \cdot S \\
\theta_a &= \theta_a + \alpha_{\pi} \cdot \delta \cdot S
\end{align}
$$

where $w_a, \theta_a$ are the vectors of policy and value weights, respectively, for action $a$, and $S$ is a vector that characterizes the current state, defined as $S = \begin{bmatrix} 1 \\ \text{Tower Alerts}_t - \text{Tower Alerts}_{t-1} \end{bmatrix}$.

We define the parameterized policy as $\text{Prob}(a)=\text{Logit}^{-1}(\theta_a \cdot S)$, the values of each state as $v(S,a) = w_a \cdot S$, and prediction error $\delta$ as the difference between the actual outcome and the estimated value of the state-action pair:

$$
\delta = (\text{Badges}_{t} - \text{cost}(a)) - \left( \gamma v(S,a) - v(S',a) \right)
$$

# Data

Our data from the Zearn platform follows a time-series structure, spanning across an academic year, with the unit of analysis being the classroom-week. This level of granularity enables us to capture the temporal dynamics of teacher-student interactions and their subsequent influence on student achievement. In particular, we can model the decision-making process of teachers as they allocate their time and effort on the Zearn platform weekly and how these decisions translate into student outcomes, measured by the completion of lessons or "badges."

Geographically, the data encompasses a diverse range of schools in Louisiana. This geographical focus provides a fascinating context for our study, given the state's unique educational landscape and the widespread adoption of the Zearn platform across its schools. However, the insights derived from this study potentially have broader implications, extending beyond the confines of Louisiana, given the universal teaching and learning principles underpinning our analysis.

@fig-teachers-map depicts the geographical distribution of teachers using the Zearn platform across various parishes in Louisiana. This map visually represents the number of teachers per parish, revealing a heterogeneous distribution of Zearn usage across Louisiana, with certain parishes demonstrating a higher concentration of teachers. The map also highlights the top five cities with the highest number of teachers using Zearn.

```{r load packages}
library(tidyverse)
library(data.table)
library(ggrepel)
library(ggpubr)
library(RColorBrewer)
library(grid)
library(gridExtra)
library(scales)
library(gt)
library(gtsummary)
library(PerformanceAnalytics)
library(doParallel)
library(foreach)
library(reticulate)
use_condaenv(condaenv = "./py-zearn")

set.seed(832399554)
random_py <- import("random")
random_py$seed(832399554)
# https://www.random.org/integers/
# Timestamp: 2023-05-17 16:18:28 UTC
```

```{r data prep}

df <- read.csv(file = "Data/df_clean.csv")

# Convert columns to appropriate data types
dt <- as.data.table(df)
# Rename variable
dt[, `:=`(
  Usage.Week = as.Date(Usage.Week),
  week = week(Usage.Week),
  poverty = factor(poverty, ordered = TRUE, exclude = c("")),
  income = factor(income, ordered = TRUE, exclude = c("")),
  charter.school = ifelse(charter.school == "Yes",
                          1, ifelse(charter.school == "No",
                                    0, NA)),
  school.account = ifelse(school.account == "Yes",
                          1, ifelse(school.account == "No",
                                    0, NA)),
  # Log Transform
  Badges.per.Active.User = log(Badges.per.Active.User + 1),
  Tower.Alerts.per.Tower.Completion = log(Tower.Alerts.per.Tower.Completion + 1),
  tch_min = log(tch_min + 1)
)]

# Create new variables using data.table syntax
dt[, min_week := week(min(Usage.Week)),
   by = Teacher.User.ID]
dt[, `:=`(
  week = ifelse(week >= min_week, week - min_week + 1, week - min_week + 53),
  mean_act_st = mean(Active.Users...Total)
), by = .(Classroom.ID)]
dt[, Tsubj := max(week), by = .(Classroom.ID)]
dt[, `:=`(
  st_login = ifelse(Minutes.per.Active.User > 0, 1, 0),
  tch_login = ifelse(tch_min > 0, 1, 0)
), by = .(Classroom.ID, Teacher.User.ID, week)]
# Update the Grade.Level values and labels
dt <- dt[!(Grade.Level %in% c(-1, 11))] # Ignore -1 and 11
dt[, Grade.Level := factor(Grade.Level,
                           ordered = TRUE,
                           exclude = c(""))]
dt[, Grade.Level := factor(Grade.Level,
                           levels = c(0:8),
                           labels = c("Kindergarten", "1st", "2nd",
                                      "3rd", "4th", "5th",
                                      "6th", "7th", "8th"))]

# Remove duplicate classroom-week pairs
dt <- dt[order(-Active.Users...Total),
         .SD[1],
         by = .(Classroom.ID, week)]

df <- as.data.frame(dt) %>%
  ungroup()

```

```{r}
#| cache: true
#| label: fig-teachers-map
#| fig-cap: "Geographical distribution of teachers across various parishes in Louisiana, and the top 5 cities with the highest number of teachers."

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
unique_zipcodes <- unique(dt$zipcode) %>%
  as.list()
plan(strategy = "multisession", workers = availableCores())
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
dt <- merge(dt, address_geodata,
            by.x = "zipcode",
            by.y = "postalcode",
            all.x = TRUE)

# Aggregate the data to get the number of teachers in each county
dt_map <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID)
), by = .(county)]

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID),
  lat = mean(lat),
  long = mean(long)
), by = .(city, county)]
# Get the top 5 cities with the most teachers
top_cities <- top_cities[order(-num_teachers)][1:5,]

# Get the Louisiana county map data
df_map <- tigris::counties(cb = TRUE,
                           resolution = "20m",
                           class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    as.data.frame(dt_map),
    by = c("NAMELSAD" = "county")
  ) %>%
  sf::st_as_sf()

ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers", low = "white", high = "red", na.value = "gray90") +
  labs(
    title = "Number of Teachers by Parish in Louisiana"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city), size = 3, color = "black")
```

## Descriptive Collection and Preprocessing

Zearn provided administrative data for teachers and students at a granular level. Teacher data was time-stamped by the second and included the time spent on the platform and the specific actions taken. Note that, although extensive, the set of actions was not comprehensive. On the other hand, student data was aggregated at the classroom-week level due to data privacy considerations. This aggregation included our variables of interest: (1) teacher effort (weekly time spent on the Zearn platform, in minutes), (2) student achievement (student lesson completion or "badges"), and (3) level of student struggle (tower alerts). These variables form the basis of our reinforcement learning models, providing a comprehensive view of the dynamics of teacher-student interactions on the Zearn platform.

The dataset represents various aspects of Zearn schools including identifiers, usage data, and demographic information. It contains information for `r length(unique(df$Classroom.ID))` classrooms and `r length(unique(df$Teacher.User.ID))` teachers, with an average of `r mean(df$Students...Total, 1)` students per classroom.

The school summary statistics, as presented in Table @tbl-summary, provide a snapshot of the schools' characteristics. The table reveals the average number of teachers, students, and weeks of data per school. @tbl-proportions presents the proportions of schools across different poverty levels, income brackets, and other variables, such as the percentage of charter schools and schools with paid accounts. These proportions provide insights into the socio-economic context of the schools in the dataset, which is crucial for understanding the external factors that may influence teacher effort and student achievement.

```{r}
#| label: tbl-summary
#| tbl-cap: "Summary statistics for the schools"

df_summary <- df %>%
  group_by(MDR.School.ID) %>%
  summarise(
    Unique_Teacher_Count = n_distinct(Teacher.User.ID)
  ) %>%
  left_join(df %>%
              group_by(Classroom.ID, MDR.School.ID) %>%
              summarise(
                Students_Total = mean(Students...Total, na.rm = TRUE),
                Weeks_Total = n_distinct(Usage.Week)
                ) %>%
              group_by(MDR.School.ID) %>%
              summarize(
                Students_Total = sum(Students_Total),
                Weeks_Total = mean(Weeks_Total)
                ),
            by = "MDR.School.ID") %>%
  ungroup() %>%
  summarise(
    Mean_Teachers = mean(Unique_Teacher_Count),
    SD_Teachers = sd(Unique_Teacher_Count),
    Min_Teachers = min(Unique_Teacher_Count),
    Max_Teachers = max(Unique_Teacher_Count),
    Mean_Students_Total = mean(Students_Total),
    SD_Students_Total = sd(Students_Total, na.rm = TRUE),
    Min_Students_Total = min(Students_Total),
    Max_Students_Total = max(Students_Total),
    Mean_Weeks_Total = mean(Weeks_Total),
    SD_Weeks_Total = sd(Weeks_Total, na.rm = TRUE),
    Min_Weeks_Total = min(Weeks_Total),
    Max_Weeks_Total = max(Weeks_Total)
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Measure", "Variable"), sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)
  
df_proportions <- df %>%
  group_by(poverty) %>%
  summarise(n = n()) %>%
  mutate(Percentage = paste0(round(n / sum(n) * 100, digits = 2), "%")) %>%
  select(-"n") %>%
  na.omit() %>%
  rename(Variable = poverty) %>%
  mutate(Variable = as.character(Variable)) %>%
  bind_rows(df %>%
              group_by(income) %>%
              summarise(n = n()) %>%
              mutate(Percentage = paste0(round(n / sum(n) * 100, digits = 2), "%")) %>%
              select(-"n") %>%
              na.omit() %>%
              rename(Variable = income) %>%
              mutate(Variable = as.character(Variable))
            ) %>%
  bind_rows(df %>%
              ungroup() %>%
              summarise(
                Charter_Schools = mean(charter.school)*100,
                Schools_with_Paid_Account = mean(school.account)*100
                ) %>%
              mutate(
                Charter_Schools = paste0(round(Charter_Schools, digits = 2), "%"),
                Schools_with_Paid_Account = paste0(round(Schools_with_Paid_Account, digits = 2), "%")
              ) %>%
              transpose(keep.names = "Variable") %>%
              rename(Percentage = V1)) %>%
  add_row(Variable = "**Poverty Level**", Percentage = "", .before = 1) %>%
  add_row(Variable = "**Income**", Percentage = "", .before = 5) %>%
  add_row(Variable = "**Other**", Percentage = "", .before = 23)

# Summary statistics table
gt_summary <- df_summary %>%
  gt(rowname_col = "Variable") %>%
  cols_label(Mean = "Mean", SD = "Standard Deviation", Min = "Minimum", Max = "Maximum") %>%
  fmt_number(
    columns = c("Mean", "SD"),
    decimals = 2
  ) %>%
  fmt_number(
    columns = c("Min","Max"),
    decimals = 0
  )
gt_summary
```

```{r}
#| label: tbl-proportions
#| tbl-cap: "Proportions of different variables."

# Create the proportions table
gt_proportions <- df_proportions %>%
  gt(rowname_col = "Variable") %>%
  cols_label(Percentage = "Proportions") %>%
  fmt_markdown(columns = Variable)

# Print tables
gt_proportions

```

### Temporal Dynamics of Student Engagement

The total number of weeks of data per classroom, as shown in Figure @fig-classroom-weeks, reveals that the distribution of the number of weeks is bimodal, with one mode representing classrooms with less than 3 to 4 months of data and the other mode representing classrooms with four months or more. The classrooms with less than 16 weeks of data are color-coded in red and are excluded from the analysis.

@fig-logins-week illustrates the total number of student logins over time, offering a dynamic view of student engagement with the Zearn platform. Notably, there are marked decreases in logins during Thanksgiving and Christmas, as indicated by the text annotations on the plot.

```{r}
#| label: fig-classroom-weeks
#| fig-cap: "Total number of weeks of data per classroom."

# Create the histogram
df %>%
  group_by(Classroom.ID) %>%
  summarize(Tsubj = max(Tsubj)) %>%
  mutate(Tsubj_category = if_else(Tsubj < 16, "less than 16", "16 or more")) %>%
  ggplot(aes(x = Tsubj, fill = Tsubj_category)) +
  geom_histogram(color = "black", breaks = seq(min(df$Tsubj), max(df$Tsubj) + 1, by = 2)) +
  geom_vline(xintercept = 15, color = "darkgray", linetype = "dashed", size = 0.8) +
  annotate("text", x = 9, y = 3500, label = "Excluded\nClassrooms", vjust = 1, color = "red") +
  labs(title = "Histogram of Total Number of Weeks",
       x = "Total Number of Weeks",
       y = "Frequency") +
  scale_fill_manual(values = c("less than 16" = "red", "16 or more" = "steelblue")) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(1, seq(5, max(df$Tsubj), by = 5)))

```

```{r}
#| label: fig-logins-week
#| fig-cap: "Total number of student logins over time."

# Calculate the sum of login values by Usage.Week and Teacher.User.ID
login_data <- df %>%
  group_by(Usage.Week, Teacher.User.ID) %>%
  summarize(tch_login = max(tch_login),
            st_login  = max(st_login)) %>%
  group_by(Usage.Week) %>%
  summarize(tch_logins = sum(tch_login),
            st_logins  = sum(st_login))
# Create bar plot
bar_plot <- ggplot() +
  geom_bar(data = login_data, aes(x = Usage.Week, y = st_logins), stat = "identity") +
  # geom_point(data = login_data, aes(x = Usage.Week, y = tch_logins), color = "blue") +
  labs(
    title = "Mean of Logins Across Teachers' Classrooms",
    x = "Week",
    y = "Total Logins"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12)) +
  scale_x_date(date_breaks = "3 week", date_labels = "%Y-%m-%d")

# Add labels for Christmas and Thanksgiving
bar_plot +
  geom_text(aes(x = as.Date("2019-12-25"), y = 1250, label = "Christmas"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "red") +
  geom_text(aes(x = as.Date("2019-11-25"), y = 1250, label = "Thanksgiving"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "darkorange")

```

## Variables of interest

Our Zearn data can be viewed as time-series (over the course of the school year, \~ 40 weeks). We propose to analyze the relationship between teacher effort and student achievement through temporal dynamics in the data. For the implementation of RL models, we need both an input (action) and an output (reward). For this purpose, we first use teacher weekly log-ins (a binary variable, 0 = no log-ins and 1 = at least 1 log-in) as a quantifiable proxy for teacher effort. We assume that teachers on Zearn are in control of this input variable, and choosing how much time to allocate to the platform is central to this decision process. Although these are not perfect proxies (e.g., they do not account for time spent creating assignments and performing in person instruction to students), it is one of the few directly quantifiable information about teacher behavior available in the data. Further, we use student lesson completion (badges completed) as a measure of student effort level, given that Zearn emphasizes the importance of lesson completion (instead of a grade, for example).

### Teacher Actions

Another way of capturing teacher behavior is by analyzing their specific actions when they use Zearn. The platform provides a wide range of actions that teachers can take, including downloading resources, engaging in different teaching methods or activities, and choosing how much time they spend online. We can use these actions to create a more nuanced picture of teacher behavior on the platform. provides a list of the actions available in our data.

| Variable                                | Description                                                                               |
|--------------------------------|----------------------------------------|
| Assessments Answer Key Download         | Answer keys for assessments to facilitate grading.                                        |
| Assessments Download                    | Assessments to demonstrate understanding of the material.                                 |
| Course Guide Download                   | Overview of the content, objectives, and structure of Zearn's PD courses.                 |
| Course Notes Download                   | Notes from Zearn's professional development (PD) courses.                                 |
| Curriculum Map Download                 | Comprehensive overview of the learning objectives and content for each grade level.       |
| Elementary Schedule Download            | Schedule of elementary-level activities in the Zearn curriculum.                          |
| Fluency Completed                       | Tracks completion of a fluency activity.                                                  |
| Grade Level Overview Download           | Overview of the curriculum for a specific grade level.                                    |
| Guided Practice Completed               | Tracks completion of the guided practice portion of a Zearn lesson.                       |
| Kindergarten Activity Completed         | Tracks completion of a specific activity in the Kindergarten curriculum.                  |
| Kindergarten Mission Download           | A specific learning mission in the Kindergarten curriculum.                               |
| Kindergarten Schedule Download          | Detailed schedules for Kindergarten to help teachers plan their instruction.              |
| Mission Overview Download               | Overview of a specific learning mission in the Zearn curriculum.                          |
| Number Gym Activity Completed           | Tracks completion of a Number Gym activity.                                               |
| Optional Homework Download              | Optional homework assignments for extra practice.                                         |
| Optional Problem Sets Download          | Optional sets of problems for extra practice.                                             |
| Small Group Lesson Download             | Lessons designed for completion in small groups.                                          |
| Student Notes and Exit Tickets Download | Notes taken during a lesson and "exit tickets," brief assessments at the end of a lesson. |
| Teaching and Learning Approach Download | Resources related to Zearn's teaching and learning approach.                              |
| Tower Completed                         | Tracks completion of a "Tower of Power" activity in a Zearn lesson.                       |
| Tower Stage Failed                      | Tracks failure of a stage in a "Tower of Power" activity.                                 |
| Tower Struggled                         | Tracks struggles with a "Tower of Power" activity.                                        |
| Whole Group Fluency Download            | Activities for practicing fluency skills as a whole group.                                |
| Whole Group Word Problems Download      | Word problem activities completed as a whole group.                                       |

: List of Teacher Actions

### State Variables

For this paper's second class of Reinforcement Learning models, we require variables that can determine the state space of each week. Experienced teachers suggest that they strongly focus on measures of the level of difficulty encountered during the lessons. The Zearn platform provides a measure of this kind, namely, "Tower Alerts." If a student is struggling in a given lesson, the platform automatically provides scaffolded remediation (i.e., breaking the problems step by step), and if they struggle multiple times in that same lesson, a "Tower Alert" is generated for their teacher. We use the previous week's average Tower Alerts to measure the level of difficulty faced by the students. Further, we include the previous week's average student time usage (in minutes) as a state variable. Effectively, we assume that the value of a given state (i.e., the week at hand) is a function of the previous week's "tower alerts" and "student minutes."

## Exclusion criteria

The raw data underwent rigorous preprocessing to ensure its suitability for analysis. This process included performing log transformations on our variables of interest (i.e., minutes, badges, and tower alerts) to normalize their distributions. Given the diverse user base of Zearn, we applied specific exclusion criteria to select teachers who most likely come from traditional schools and classrooms that use the platform consistently. We selected virtual classrooms with at least five active students weekly, filtering out parents or tutors who may use Zearn outside the classroom setting. We removed teachers with more than four classrooms and those who logged in for less than 16 weeks. We excluded classrooms in the 6th to 8th grades, as those are a small proportion of our dataset. This deletion ensures a focus on traditional school settings and consistent platform usage, minimizing bias from teachers and schools that have not used Zearn consistently.

```{r preprocess data}

dt[, n_weeks := .N,
   by = Classroom.ID]

dt <- dt[
  n_weeks > 15 & # At least 4 months cumulative activity
    Tsubj < 2*n_weeks & # At least activity twice a month on average
    teacher_number_classes < 5 &
    Students...Total > 5 &
    mean_act_st > 3 &
    !(Grade.Level %in% c("6th","7th","8th")) &
    !(month(Usage.Week) %in% c(6, 7, 8)) &
    !is.na(District.Rollup.ID),
]

cols_to_select <- names(dt)[sapply(dt, function(x) !is.numeric(x) ||
                                     (is.numeric(x) && is.finite(sd(x)) && sd(x) != 0))]
dt <- dt[, ..cols_to_select]

df <- as.data.frame(dt) %>%
  ungroup() %>%
  arrange(Classroom.ID, week)
# Clean environment
rm(list = setdiff(ls(), c("df","random_py")))
gc(verbose = FALSE)

```

@tbl-summary-statistics summarizes the refined dataset, providing a snapshot of the key variables of interest. Their means and standard deviations (SD) are computed for each grade level and overall (across all grades).

```{r}
#| label: tbl-summary-statistics
#| caption: "Means (SD) of student variables by grade level."

create_summary <- function(var_name, var_label, type = "continuous") {
  summary_stat <- df %>%
    select(Grade.Level, var_name) %>%
    tbl_summary(
      by = Grade.Level,
      missing = "no",
      type = list(var_name ~ type),
      statistic = var_name ~ ifelse(type == "continuous", "{mean} ({sd})", "{n} ({p})")
    ) %>%
    add_overall() %>%
    as_tibble() 
  summary_stat[1] <- var_label
  return(summary_stat)
}

summaries_list <- list(
  # create_summary("Sessions.per.Active.User", "Sessions per Student"),
  create_summary("Minutes.per.Active.User", "Minutes per Student"),
  create_summary("Badges.per.Active.User", "Badges per Student"),
  create_summary("Tower.Alerts.per.Tower.Completion", "Tower Alerts per Lesson Completion"),
  # create_summary("tch_login", "Teacher Login (0/1)", type = "dichotomous"),
  create_summary("tch_min", "Minutes per Teacher")
)

summary_table <- bind_rows(summaries_list) %>% 
  transpose(keep.names = "Characteristic", make.names = 1)
names(summary_table)[1] <- "Grade Level"

gt(summary_table)

```

## Visualizing Relationships Between Variables

We begin to unveil the intricate relationships among the variables under consideration through a comprehensive correlation analysis, as depicted in @fig-corr. This correlation matrix elucidates the magnitude and direction of associations among variables such as badges earned, minutes spent per student, tower alerts, the number of students, and teacher minutes. These interconnections inform the construction of our reinforcement learning models by suggesting the influence of teacher effort on student achievement. In this correlation matrix, each cell represents the Spearman correlation coefficient between a pair of variables. The color and size of the circles in each cell reflect the strength and direction of the correlation, with blue indicating positive correlations and red indicating negative correlations. The histograms along the diagonal provide a visual representation of the distribution of each variable.

```{r}
#| cache: true
#| label: fig-corr
#| fig-cap: "This graph represents the correlation between variables after log transformation"
#| fig-format: png

df_corr <- df  %>%
  select(Badges.per.Active.User,
         Active.Users...Total,
         Minutes.per.Active.User,
         Tower.Alerts.per.Tower.Completion,
         tch_min) %>%
  rename("Badges" = Badges.per.Active.User,
         "Minutes per Student" = Minutes.per.Active.User,
         "Tower Alerts" = Tower.Alerts.per.Tower.Completion,
         "# of Students" = Active.Users...Total,
         "Teacher Minutes" = tch_min)

chart.Correlation(df_corr, histogram = TRUE, method = "spearman",
                  pch = 20, cex = 0.5, col = rgb(0, 0, 1, 0.5))

```

## Dimensionality Reduction

To capture the complex decision-making processes and trade-offs made by teachers, we employed a Principal Component Analysis (PCA), a Nonnegative Matrix Factorization (NMF), and an autoencoder (AE). We chose these methods as candidates to condense the multifaceted nature of our variables, including Teacher Minutes, Teacher Sessions, and the set of teacher actions.

While PCA is a widespread technique, it is only appropriate for some datasets or research questions. PCA assumes the data follows a Gaussian distribution, which may not always hold [@jolliffe2016]. Further, PCA is an unsupervised method that only considers the input data structure and not the associated target variables or labels. On the other hand, methods like NMF and autoencoders offer different advantages. NMF, for instance, imposes a non-negativity constraint, which makes its components easier to interpret in many contexts, especially when the data elements represent quantities that cannot be negative, such as counts or frequencies. NMF also tends to produce a more parts-based, sparse representation of the data, which can be more interpretable in some cases [@lee1999]. Autoencoders are a type of neural network that can model more complex, non-linear relationships in the data. They can also be trained in a supervised manner to learn a reduced-dimension representation specifically optimized for predicting a target variable, leading to improved performance in subsequent predictive modeling tasks [@goodfellow2016].

By trying different techniques, we can explore these different trade-offs and potentially discover a reduced-dimension representation better suited to our specific dataset and research question than PCA alone would provide. This approach aligns with the principle of methodological triangulation, which suggests that using multiple methods can help to overcome the limitations of any single method and provide a more robust and comprehensive understanding of the data.

The Nonnegative Matrix Factorization (NMF) operates as follows:

The original matrix is a detailed description of all the teachers' behaviors. Each row in the matrix represents a unique teacher, and each column represents a specific behavior or action the teacher might take. The entry in a specific row and column corresponds to the frequency of that behavior for that particular teacher.

After the NMF, we have two matrices:

1.  Basis Matrix (W): This matrix represents underlying behavior patterns. Each column is a "meta-behavior" or a group of behaviors occurring together.
2.  Mixture Matrix (H): This matrix shows the extent to which each "meta-behavior" is present in each teacher. Each entry in this matrix represents the contribution of a "meta-behavior" to a particular teacher's behaviors.

By looking at these matrices, we can identify underlying patterns of behaviors (from the basis matrix) and see how these patterns are mixed and matched in different teachers (from the mixture matrix).

We first scale the data such that all features are in the range \[0, 1\], then we create training (80%) and testing (20%) sets. We then perform a PCA and calculate the sum of squared residuals (a measure of the difference between the original data and the data reconstructed from the PCA) and silhouette scores (a measure of how similar an object is to its cluster compared to other clusters). The silhouette score ranges from -1 to 1, with a high value indicating that the object is well-matched to its cluster and poorly matched to neighboring clusters.

Next, we perform an NMF using two different loss functions (Frobenius norm and Kullback-Leibler divergence) and two different initialization methods (nonnegative double singular value decomposition (NNDSVD) and NNDSVD with zeros filled with the average of the input matrix (NNDSVDA)). We also calculate the sum of squared residuals and silhouette scores for each model.

Finally, we implement an autoencoder to perform dimensionality reduction. The autoencoder is a neural network designed to learn an encoded, compressed representation of the input data and then reconstruct the original data from this encoded representation. The architecture of the autoencoder is optimized using a hyperparameter search, which involves training multiple versions of the model with different hyperparameters and selecting the one that performs best on the validation data. These hyperparameters include (1) the number of layers in the autoencoder, (2) the number of units in each layer, (3) the regularization strength, and (4) the number of components in the encoded representation. The target score for the training period is an average of mean squared error losses for the input reconstruction and the target variable prediction (Badges per Student), weighted by the standard deviations of the input data and the target variable.

The PCA, NMF, and AE results are then compared by plotting the sum of squared residuals and silhouette scores for each method and the number of components used. @fig-nmf-pca-comparison allows us to visually compare the performance of the different methods and choose the one that provides the best balance between reconstruction accuracy (as measured by the sum of squared residuals) and cluster separation (as measured by the silhouette score). As such, we select the NMF with three components as our preferred method.

```{r pca nmf data-prep}
#| include: false
# Choose which Teacher.User.IDs will be train vs test
df$set <- ifelse(df$Teacher.User.ID %in%
                 sample(unique(df$Teacher.User.ID),
                        size = floor(0.8 * length(unique(df$Teacher.User.ID)))),
                 "train", "test")

# Create base data.table for models (faster than data.frame)
df_comp <- as.data.table(df %>% 
                           select(Classroom.ID, week, Badges.per.Active.User,
                                  set, tch_min,
                                  RD.elementary_schedule:RD.pd_course_guide))
# Arrange
setorder(df_comp, Classroom.ID, week)

## Prep data for PCA
df_pca <- as.data.frame(df_comp) %>%
  arrange(Classroom.ID, week) %>%
  ungroup() %>%
  mutate(across(everything(), ~ifelse(is.na(.), 0, .))) %>%
  mutate(across(RD.elementary_schedule:RD.pd_course_guide, ~log1p(.)))
# Calculate standard deviations
std_devs <- apply(df_pca %>% select(-c("Classroom.ID", "week", "set")), 2, sd)
# Identify columns with defined standard deviations (not NaN or Inf)
invalid_cols <- names(std_devs[is.na(std_devs) | is.infinite(std_devs)])
df_pca <- df_pca %>% select(-all_of(invalid_cols))

# Clean environment
rm(list = setdiff(ls(), c("df", "df_pca", "df_comp", "random_py")))
gc(verbose = FALSE)

```

```{python load data}
import numpy as np
import pandas as pd
from sklearnex import patch_sklearn
patch_sklearn()
from sklearn.decomposition import PCA, NMF
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import silhouette_score

## Basic Variables
# Import data from R
dfpca_py = pd.DataFrame(r.df_pca)
dfpca_py.sort_values(['Classroom.ID', 'week'], inplace=True)

# Initialize scaler
scaler = MinMaxScaler()

# Drop unnecessary columns
X = dfpca_py.drop(['Badges.per.Active.User', 'set', 'Classroom.ID', 'week'], axis=1)
X_cols  = X.columns
# Scale the data
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X_cols)

# Dictionaries for export
components = {}
results = {}
residuals = {}
silhouette = {}

# Number of components for NMF
n_comp = min(X.shape) // 3
```

```{python pca-nmf}

################ PCA
for n in range(2, n_comp):
  pca = PCA(n_components=n)
  X_pca = pca.fit_transform(X_scaled)
  pca_comp = pca.components_
  X_hat = pca.inverse_transform(X_pca)
  labels = np.argmax(pca_comp, axis=0)

  results.setdefault("PCA", {})[n] = X_pca
  components.setdefault("PCA", {})[n] = pca_comp
  residuals.setdefault("PCA", {})[n] =((X_scaled - X_hat)**2).sum().sum()  # RSS
  silhouette.setdefault("PCA", {})[n] = silhouette_score(pca_comp.transpose(), labels)

################ Non-negative Matrix Factorization
# Function for NMF
def nmf_method(n, method, initial, X_scaled, solv = 'mu'):
    method_name = f"{method.title()} {initial.upper()}"
    if method == 'frobenius' and initial == 'nndsvd': solv = 'cd'
    if method != 'frobenius' and initial == 'nndsvd': return
    if method != 'frobenius': method_name = f"{method.title()}"
    
    nmf = NMF(
      n_components=n,
      init=initial,
      beta_loss=method,
      solver=solv,
      max_iter=4_000
    )
    X_nmf, nmf_comp = nmf.fit_transform(X_scaled), nmf.components_
    X_hat = nmf.inverse_transform(X_nmf)
    labels = np.argmax(nmf_comp, axis=0)
    
    results.setdefault(method_name, {})[n] = X_nmf
    components.setdefault(method_name, {})[n] = nmf_comp
    residuals.setdefault(method_name, {})[n] = ((X_scaled - X_hat)**2).sum().sum()  # RSS
    silhouette.setdefault(method_name, {})[n] = silhouette_score(nmf_comp.transpose(), labels)

# Call the function for NMF
for n in range(2, n_comp):
  for method in {'frobenius', 'kullback-leibler'}:
    for initial in {'nndsvd', 'nndsvda'}:
      try:
        nmf_method(n, method, initial, X_scaled)
      except:
        continue
```

```{python train autoencoder}
#| eval: false
import os # To fix: https://github.com/tensorflow/tensorflow/issues/59779
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from keras.models import Model
from keras.layers import Input, Dense
from kerastuner.tuners import Hyperband
from kerastuner.engine.hyperparameters import HyperParameters
from keras.constraints import NonNeg
from keras.callbacks import EarlyStopping
from keras.regularizers import L1

# Create training and testing data frames
train_df = dfpca_py[dfpca_py['set'] == 'train']
test_df = dfpca_py[dfpca_py['set'] == 'test']

# Predictors
X_train = train_df.drop(['Badges.per.Active.User', 'set', 'Classroom.ID', 'week'], axis=1)
X_test = test_df.drop(['Badges.per.Active.User', 'set', 'Classroom.ID', 'week'], axis=1)
# Scale the data
X_train = scaler.transform(X_train)
X_train = pd.DataFrame(X_train, columns=X_cols)
X_test = scaler.transform(X_test)
X_test = pd.DataFrame(X_test, columns=X_cols)

# Get the target variable
Y = dfpca_py[['Badges.per.Active.User']]
Y_train = train_df[['Badges.per.Active.User']]
Y_test = test_df[['Badges.per.Active.User']]
Y = scaler.fit_transform(Y)
Y_train = scaler.transform(Y_train)
Y_test = scaler.transform(Y_test)

# Define the number of components and features
n_features = X_scaled.shape[1]
n_labels = 1  # For regression, we usually have just one output node

# Determine loss weights according to the data structure:
decoding_weight = Y.std() / (Y.std() + X_scaled.std(numeric_only=True).mean())
prediction_weight = 1 - decoding_weight
def build_model(hp):
    # Define the input layer
    input_data = Input(shape=(n_features,))
    
    # Define the encoding layer(s)
    n_layers = hp.Int('n_layers', min_value=1, max_value=4, step=1)
    n_units = [
      hp.Choice('units_' + str(i), values=[8, 16, 32, 64, 128, 256, 512])
      for i in range(n_layers)
    ]
    encoded = input_data
    for i in range(n_layers):
        encoded = Dense(
          units=n_units[i],
          activation='relu')(encoded)
          
    # Generate the latent vector
    latent_dim = hp.Int('encoding_units', min_value=2, max_value=n_comp, step=1)
    l1_value = hp.Float('l1_value', min_value=0.0001, max_value=0.001, default=0.0005, step=0.0001)
    latent = Dense(
      units=latent_dim,
      activation='linear',
      activity_regularizer= L1(l1=l1_value),
      kernel_constraint=NonNeg(),
      name='latent')(encoded)      
    
    # Decoder
    decoded = latent
    for i in range(n_layers):
        decoded = Dense(
          units=n_units[n_layers - i - 1],
          activation='relu')(decoded)
    decoded = Dense(n_features, activation='sigmoid', name='decoded')(decoded)
    
    # Define the label output layer
    label_output = Dense(n_labels, activation='linear', name='label_output')(latent)
    
    # Define the autoencoder model
    autoencoder = Model(inputs=input_data, outputs=[decoded, label_output])
    # Compile the model
    autoencoder.compile(optimizer='adadelta',
                    loss={
                      'decoded': 'mean_squared_error',
                      'label_output': 'mean_squared_error'
                    },
                    loss_weights={
                      'decoded': decoding_weight,
                      'label_output': prediction_weight
                    })
    
    return autoencoder

# Set up hyperparameter tuner
tuner = Hyperband(build_model,
                  objective='val_loss',
                  max_epochs=20,
                  directory='autoencoder_tuning',
                  project_name='autoencoder_2nd_tuning')

# Perform hyperparameter search
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)
tuner.search(x=X_train,
            y=[X_train, Y_train],
            epochs=50,
            validation_data=(X_test, [X_test, Y_test]),
            callbacks=[early_stopping_callback])
# tuner.results_summary()
# Get the optimal hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=2)[1]
model = tuner.hypermodel.build(best_hyperparameters)
history = model.fit(x=X_train, 
                    y=[X_train, Y_train],
                    epochs=2_000,
                    validation_data=(X_test, [X_test, Y_test]),
                    callbacks=[early_stopping_callback])
best_model = model


# Function to get encoded representation and components
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

# Get encoded representation and components
get_encoded_representation_and_components(best_model, X_scaled)

# save the model
model.save('./autoencoder_tuning/final_model.h5')

```

```{python load autoencoder}
#| include: false
from tensorflow.keras.models import load_model
from keras.models import Model
from keras.layers import Input

# Function to get encoded representation and components
loaded_model = load_model('./autoencoder_tuning/final_model.h5')
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

n_features = X_scaled.shape[1]
# Get encoded representation and components
get_encoded_representation_and_components(loaded_model, X_scaled)
```

```{python clean environment}
#| include: false
# get the names of all variables in the global namespace
all_vars = list(globals().keys())

# list of variables to keep
keep_vars = ['residuals', 'silhouette', 'components', 'results', 'r']

# delete all variables that are not in keep_vars
for var in all_vars:
    if var not in keep_vars:
        del globals()[var]
del keep_vars
del all_vars
del var

# manually call the garbage collector
import gc
gc.collect()

```

```{r}
#| cache: true
#| label: fig-nmf-pca-comparison
#| fig-cap: "Comparison of residuals and silhouette scores for PCA, Frobenius, and Kullback-Leibler methods."
# Importing from Python
residuals_list <- py$residuals
silhouette_list <- py$silhouette

# Creating dataframes
df_residuals <- do.call(rbind, lapply(names(residuals_list), function(method) {
  do.call(rbind, lapply(names(residuals_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Residuals = residuals_list[[method]][[n]]
    )
  }))
}))
# Creating separate dataframes for 'Autoencoder' and the rest of the methods
df_residuals_autoencoder <- df_residuals[df_residuals$Method == "Autoencoder", ]
df_residuals_others <- df_residuals[df_residuals$Method != "Autoencoder", ]

df_silhouette <- do.call(rbind, lapply(names(silhouette_list), function(method) {
  do.call(rbind, lapply(names(silhouette_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Silhouette = silhouette_list[[method]][[n]]
    )
  }))
}))
# Add nuisance row for Autoencoder to unify legends
df_silhouette <- rbind(df_silhouette,
                       data.frame(Method = "Autoencoder",
                                  Components = 4,
                                  Silhouette = mean(df_silhouette$Silhouette)))

# Plotting residuals
p1 <- ggplot() +
  geom_line(data = df_residuals_others,
            aes(x = Components, y = Residuals, color = Method)) +
  geom_point(data = df_residuals_autoencoder,
             aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Sum of Square Residuals",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals$Components),
                                  max(df_residuals$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())
# Plotting silhouette scores
p2 <- ggplot(df_silhouette, aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette$Components),
                                  max(df_silhouette$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette$Silhouette) +
                                  2*sd(df_silhouette$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Combine the plots and place the legend at the bottom
comparison_plot <- ggarrange(p1, p2,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
comparison_plot

```

#### Interpreting Components

@fig-nmf-heatmap shows the loadings of the NMF components using a heatmap, depicting how each original feature contributes to each component. In other words, each component combines the original features to explain a substantial portion of the variance, potentially improving the efficiency and interpretability of the reinforcement learning models. Given the loadings, we interpret the components as follows:

1.  **Component 1 (Teacher Engagement)**: This component seems to be heavily influenced by the variable "Teacher Minutes," suggesting teacher engagement with the Zearn platform. The higher the value in this component, the more time teachers are spending on the platform, which could indicate a higher level of engagement with the curriculum and resources. High values in this component suggest that teachers are actively using the platform, spending time reviewing reports and possibly engaging with other features [@morrison2019a].

2.  **Component 2 (Resource Utilization)**: This component has high weights for variables related to different resources available on the Zearn platform, such as "Optional Problem Sets," "Student Notes and Exit Tickets," and "Mission Overview". High values in this component suggest that teachers are downloading and possibly using a variety of resources in their teaching. This pattern is consistent with the findings from [@knudsen2020a] that teachers reported learning from a variety of Zearn Math resources and that the curriculum materials and their implementation are important sources of learning.

3.  **Component 3 (Pedagogical Content Knowledge)**: This component has high weights for variables related to student activities, such as "Guided Practice Completed," "Tower Completed," and "Fluency Completed," suggesting that teachers may be engaged in acquiring subject-matter-specific pedagogy, learning to scaffold and to explain concepts in a variety of ways. This finding aligns with [@morrison2019a], where teachers were most likely to report using Independent Digital Lessons, student notes and workbooks, small-group lessons, and paper Exit Tickets frequently or very frequently.

```{r}
#| cache: true
#| label: fig-nmf-heatmap
#| fig-cap: "Heatmap of Non-negative Matrix Factorization (NMF) components for Zearn data. Each row represents a variable, and each column represents a component. The color intensity indicates the weight of each variable in each component, with darker colors indicating higher weights. Component 1 represents Teacher Engagement, Component 2 represents Resource Utilization, and Component 3 represents Pedagogical Content Knowledge."

library(pheatmap)

components_list <- py$components
df_heatmap <- components_list[["Kullback-Leibler"]][["3"]] %>%
  t() %>% as.data.frame()

# Define a named vector to map old variable names to new ones
variable_names <- c(
  "tch_min" = "Teacher Minutes",
  "RD.optional_problem_sets" = "Optional Problem Sets Download",
  "RD.student_notes_and_exit_tickets" = "Student Notes and Exit Tickets Download",
  "RD.mission_overview" = "Mission Overview Download",
  "RD.pd_course_notes" = "Course Notes Download",
  "RD.elementary_schedule" = "Elementary Schedule Download",
  "RD.whole_group_fluency" = "Whole Group Fluency Download",
  "Guided.Practice.Completed" = "Guided Practice Completed",
  "RD.small_group_lessons" = "Small Group Lesson Download",
  "Tower.Completed" = "Tower Completed",
  "Fluency.Completed" = "Fluency Completed",
  "Number.Gym.Activity.Completed" = "Number Gym Activity Completed",
  "RD.grade_level_overview" = "Grade Level Overview Download",
  "Tower.Stage.Failed" = "Tower Stage Failed",
  "Kindergarten.Activity.Completed" = "Kindergarten Activity Completed",
  "Tower.Struggled" = "Tower Struggled",
  "RD.k_mission" = "Kindergarten Mission Download",
  "RD.whole_group_word_problems" = "Whole Group Word Problems Download",
  "RD.assessments" = "Assessments Download",
  "RD.teaching_and_learning_approach" = "Teaching and Learning Approach Download",
  "RD.optional_homework" = "Optional Homework Download",
  "RD.k_schedule" = "Kindergarten Schedule Download",
  "RD.curriculum_map" = "Curriculum Map Download",
  "RD.assessments_answer_key" = "Assessments Answer Key Download",
  "RD.pd_course_guide" = "Course Guide Download"
)
# Rename the rows of the dataframe
row.names(df_heatmap) <- variable_names[names(df_pca)[-c(1:4)]]

names(df_heatmap) <- paste0("Component ", 1:3)
df_heatmap <- df_heatmap %>% arrange(-`Component 1`)

color_scheme <- colorRampPalette(
  c("#F7F7F7",brewer.pal(n = 9, name = "YlOrRd"))
  )(100)
breaks <- seq(0, 1, by = 0.01)
minmax <- function(x) ((x - min(x))/(max(x) - min(x)))
pheatmap(df_heatmap %>%
           mutate_all(minmax),
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         angle_col = 0,
         main = "Loadings of NMF components",
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE,
         show_rownames = TRUE)

```

```{r export dimension reduction}
#| eval: false
# Importing results from Python
results_list <- py$results

# List of method names
methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler", "Autoencoder")
names(results_list) <- methods
# Initialize df_components
df_components <- df

# Loop over each method, extract the results at 3 components and bind to df_components
for (method in methods) {
  result_3 <- results_list[[method]][["3"]]
  df_components <- df_components %>%
    bind_cols(result_3)
  
  # Adjust column names
  new_cols <- paste0(method, 1:3)
  names(df_components)[(ncol(df_components) - 2):ncol(df_components)] <- new_cols
}

# Write to csv
write.csv(df_components, "./Bayesian/df.csv")

```

```{r load dimension reduction}
#| include: false
df <- read.csv(file = "./Bayesian/df.csv")
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
gc(verbose = FALSE)
```

# Methods

## Dynamic Analysis

The model is estimated using the `plm` function from the `plm` package, which allows for the estimation of linear models for panel data. We estimate both within and random effects models, and the Hausman test is used to select the most appropriate model based on its p-value.

## Variable Selection

The selection of variables in the model is a crucial step in the Dynamic Analysis method. The goal is to identify the variables that best explain the dependent variable, while also considering the complexity of the model and the risk of overfitting.

The selection process involves comparing models with different combinations of variables and lags, and selecting the model that provides the best fit to the data. The fit of the models is evaluated using two criteria: the Bayesian Information Criterion (BIC) and the out-of-sample negative log-likelihood (NLL). The BIC is a criterion for model selection that balances the goodness of fit of the model against its complexity (the number of parameters in the model). The NLL is a measure of the predictive accuracy of the model, with lower values indicating better predictive performance.

## Reinforcement Learning Model Fit

The Q-learning model is implemented in the Stan programming language, which is a probabilistic programming language used for statistical inference. The model is trained using the data on the number of weeks, choices made, and outcomes (log badges) for each subject. The model parameters, including the cost, discount rate, learning rate, and inverse temperature, are estimated from the data. The model uses a Bernoulli logit model to compute the action probabilities and updates the expected values of the actions based on the prediction errors. The model also generates posterior predictions and computes the log likelihood for each subject.

The reinforcement learning models were estimated using the Bayesian statistical framework implemented in Stan, a probabilistic programming language for statistical inference. The Stan models were interfaced using the CmdStanR package in R. Stan uses the Hamiltonian Monte Carlo (HMC) algorithm, which is a state-of-the-art Markov Chain Monte Carlo (MCMC) method that is particularly well-suited for high-dimensional and complex posterior distributions. We specify 3 independent MCMC chains to check for convergence of the MCMC algorithm by comparing them. Each chain had 2,500 warmup (or "burn-in") iterations and 2,500 sampling iterations. During the warmup phase, the HMC algorithm adapts its parameters to the shape of the posterior distribution. The samples drawn during the warmup phase are discarded. The models were run until convergence was achieved, as assessed by the R-hat statistic, which compares the within-chain and between-chain variance of the MCMC samples; values close to 1 indicate that the chains have converged to the same distribution.

## Model Performance

### Base Models: Random Effects Panel Logit

### Hierarchical Bayesian Method

Our data contains 400 teacher-classroom pairs spanned across approximately 40 weeks. Estimating individual level with maximum likelihood estimation would yield noisy results as each teacher has an insufficient amount of data. A group-level estimation does yield reliable estimates (see Results) but ultimately ignores individual differences, which are important to our analysis .

Our estimation method of choice is a hierarchical Bayesian analysis, which allows us to pool information across individuals, while allowing for individual differences. Individual-level parameters are merely a function of group-level hyperparameters, and this anchoring improves our power by assuming commonalities among individuals (CITE Ahn et al., 2011; Huys et al., 2011). One particular feature of this estimation technique is that pooling is evident in the hyperparameter variance. Strong pooling means low hierarchical variance, and vice verse for weak pooling.

For Bayesian updating, we use the Stan software package (http://mc-stan.org/), which implements a Hamiltonian Monte Carlo algorithm (See <http://mc-stan.org/documentation/>). In the case of Zearn data, fast convergence of the algorithm required us to use priors that are weakly informed by parameter values found by searching the whole grid of parameter space (i.e., grid search).

## Heterogeneity

### Across Teachers

### Across Schools

### Across Demographics

# Results

## Component Selection

The results of the model selection process are summarized in the following plots, which show the BIC and NLL for different combinations of variables and lags:

Based on these results, the selected model includes 4 lags of the `FrobeniusNNDSVD` variable. This variable represents one of the components of the Frobenius Non-negative Double Singular Value Decomposition (NNDSVD) method, which is used to decompose the matrix of teacher actions into a set of basis vectors. The inclusion of 4 lags of this variable suggests that the past 4 weeks of teacher actions, as captured by this component of the NNDSVD method, have a significant influence on the current week's teacher minutes.

The selection of 4 lags is justified by the trade-off between model complexity and predictive accuracy. Including more lags would increase the complexity of the model, potentially leading to overfitting and poorer predictive performance. On the other hand, including fewer lags might result in a model that fails to capture important temporal dependencies in the data. The choice of 4 lags represents a balance between these considerations, providing a model that is both parsimonious and accurate in its predictions.

In summary, the variable selection process in the Dynamic Analysis method involves a careful balance between model complexity and predictive accuracy. The selected model, which includes 4 lags of the `FrobeniusNNDSVD` variable, provides a parsimonious and accurate representation of the temporal dependencies in the Zearn dataset. This model allows for a detailed and nuanced analysis of teacher behavior on the platform, and can inform strategies for influencing teacher behavior and improving student outcomes.

```{r}
bic_plm <- function(object) {
  # object is "plm", "panelmodel"
  sp = summary(object)
  if (class(object)[1] == "plm") {
    u.hat <- residuals(sp) # extract residuals
    model_data <- cbind(as.vector(u.hat), attr(u.hat, "index"))
    names(model_data)[1] <- "resid"
    c = length(unique(model_data[, "Classroom.ID"])) # extract classroom dimensions
    t = length(unique(model_data[, "week"])) # extract time dimension
    np = length(sp$coefficients[, 1]) # number of parameters
    n.N = nrow(sp$model) # number of data
    s.sq  <- log((sum(u.hat ^ 2) / (n.N))) # log sum of squares
    
    # effect = c("individual", "time", "twoways", "nested"),
    # model = c("within", "random", "ht", "between", "pooling", "fd")
    
    if (sp$args$model == "within" & sp$args$effect == "individual") {
      np = np + c + 1 # update number of parameters
    }
    
    if (sp$args$model == "within" & sp$args$effect == "time") {
      np = np + t + 1 # update number of parameters
    }
    
    if (sp$args$model == "within" & sp$args$effect == "twoways") {
      np = np + c + t # update number of parameters
    }
    
    if (sp$args$model == "random" & sp$args$effect == "twoways") {
      np = np + length(sp$ercomp$sigma2) # update number of parameters
    }
    
    bic <- round(log(n.N) * np  +  n.N * (log(2 * pi) + s.sq  + 1), 1)
    names(bic) = "BIC"
    return(bic)
  }
}

compute_nloglik <- function(residuals) {
  n <- length(residuals)
  sigma2 <- sum(residuals^2) / n
  nll <- n/2 * ( log(2 * pi) + log(sigma2) + 1 ) # Negative log likelihood
  return(as.numeric(nll))
}

```

```{r panel model comparison}
#| cache: true
library(plm)

# Helper functions
get_lag_value <- function(datatable, col, lag_period, n_comp = NULL) {
  # Add a column for week_lag
  datatable[, week_lag := c(0, diff(week)), by = Classroom.ID]

  if (is.null(n_comp)) {
    # Update the lag column with shift function
    datatable[, (paste0(col, "_", lag_period)) :=
                shift(get(col), lag_period, fill = 0, type = "lag"),
              by = Classroom.ID]
  } else {
    for (comp in 1:n_comp) {
      # Update the lag column with shift function
      datatable <- datatable[, (paste0(col, comp, "_", lag_period)) :=
                              shift(get(paste0(col, comp)), lag_period, fill = 0, type = "lag"),
                            by = Classroom.ID]
    }
  }
  
  return(datatable)
}

create_formula <- function(method, lag, comp) {
  if (lag == 1) {
    return(as.formula(
    paste0(method, comp, " ~ ",
           paste0(method, comp, "_", 1), " + ",
           "Badges.per.Active.User",
           " + ", "week_lag")))
  }
  return(as.formula(
    paste0(method, comp, " ~ ",
           paste0(method, comp, "_", 1:lag, collapse = " + "), " + ",
           "Badges.per.Active.User + ",
           paste0("Badges.per.Active.User", "_", 1:(lag - 1), collapse = " + "),
           " + ", "week_lag")))
}

model_selection <- function(wi, re) {
  if (phtest(wi, re)$p.value < 0.05) return(wi) else return(re)
}

create_model <- function(formula, data) {
  # Models
  wi <- plm(formula, data = data, effect = "twoway", model = "within")
  re <- plm(formula, data = data, effect = "twoway", model = "random")
  
  return(model_selection(wi, re))
}

# Data and Variables
df <- setDT(df)
results_df_list <- list()
lags <- c(1:8)
n_comp = 3
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler", "Autoencoder")
columns <- c("tch_min", "Badges.per.Active.User", methods)

# Create lags
for (col in columns) {
  for (lag_period in 1:n_lags) {
    if (!col %in% methods) {
      df <- get_lag_value(df, col, lag_period)
      next
    }
    df <- get_lag_value(df, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- pdata.frame(df[set == "train"], index = c("Classroom.ID", "week", "Teacher.User.ID"))
test_data <- pdata.frame(df[set == "test"], index = c("Classroom.ID", "week", "Teacher.User.ID"))

# Estimation
cl <- makeCluster(detectCores())
registerDoParallel(cl)
params <- expand.grid(col = c("tch_min", methods),
                      lag = lags,
                      comp = 1:n_comp) %>%
  mutate(comp = as.character(comp),
         comp = case_when(col == "tch_min" ~ "",
                          .default = comp)) %>%
  unique()
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .export = ls(),
                   .packages = "plm") %dopar% {
  col <- as.character(params$col[i])
  lag <- params$lag[i]
  comp <- params$comp[i]
  formula <- create_formula(col, lag, comp)
  model <- create_model(formula, train_data)

  # Out of Sample Log Likelihood
  predictions <- predict(model, newdata = test_data, na.fill = TRUE)
  residuals <- test_data[,paste0(col, comp)] - predictions

  # Return the results as a list
  list(Method = col,
       Component = comp,
       Lag = lag,
       nloglik = compute_nloglik(residuals),
       bic = as.numeric(bic_plm(model)),
       coef = summary(model)$coefficients)
}
# Stop the cluster
stopCluster(cl)
```

```{r}
#| label: fig-panel-bic
#| fig-cap: ""
# Wrangling the results data frame for plotting
results_df <- map_dfr(results, ~as.data.frame(t(unlist(.))))
# Convert variables to appropriate data types
results_df <- results_df %>%
  mutate(Method = as.factor(Method),
         Component = as.numeric(Component),
         Lag = as.numeric(Lag),
         nloglik = as.numeric(nloglik),
         bic = as.numeric(bic)) %>%
  group_by(Method, Lag) %>%
  summarise(avg_bic = mean(bic, na.rm = TRUE),
            avg_nloglik = mean(nloglik, na.rm = TRUE),
            .groups = "drop")
# Filter methods for the plots
methods_for_plots <- c("FrobeniusNNDSVD", "KullbackLeibler", "FrobeniusNNDSVDA")
plot_data <- filter(results_df, Method %in% methods_for_plots)
# Shared aesthetics for the plots
aes <- ggplot2::aes(x = Lag, y = value, color = Method)

# Summarize methods not included in the plots and prepare tables
methods_for_summary <- setdiff(unique(results_df$Method), methods_for_plots)
summary_data <- filter(results_df, Method %in% methods_for_summary) %>%
  group_by(Method) %>%
  summarise(BIC = mean(avg_bic, na.rm = TRUE),
            NLL = mean(avg_nloglik, na.rm = TRUE),
            .groups = "drop")
# Rename method
levels(summary_data$Method)[levels(summary_data$Method)=="tch_min"] <- "Minutes"
# Convert to scientific notation
summary_data$BIC <- format(summary_data$BIC, scientific = TRUE, digits = 2)
summary_data$NLL <- format(summary_data$NLL, scientific = TRUE, digits = 2)

# Create tables and remove headers
bic_table <- summary_data %>% select(Method, BIC) %>%
  tableGrob(theme = ttheme_minimal(base_size = 8, 
                                   core=list(bg_params = list(fill = "white", col=NA),
                                             fg_params=list(fontface=3))),
             rows = NULL)
bic_table <- bic_table[-1, ]  # Remove header
bic_data <- filter(plot_data, !is.na(avg_bic)) %>% 
  mutate(value = avg_bic, metric = "BIC")
# BIC Plot
bic_plot <- ggplot(bic_data, aes) + 
  geom_line() + 
  labs(title = "BIC", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none") +
  annotation_custom(bic_table, xmin = max(bic_data$Lag) - 3.5,
                    xmax = max(bic_data$Lag) - 0.5,
                    ymin = 1.01*max(bic_data$value),
                    ymax = Inf)

nll_table <- summary_data %>% select(Method, NLL) %>%
  tableGrob(theme = ttheme_minimal(base_size = 8, 
                                   core=list(bg_params = list(fill = "white", col=NA),
                                             fg_params=list(fontface=3))), 
             rows = NULL)
nll_table <- nll_table[-1, ]  # Remove header
nll_data <- filter(plot_data, !is.na(avg_nloglik)) %>% 
  mutate(value = avg_nloglik, metric = "Negative Log-Likelihood")
nll_plot <- ggplot(nll_data, aes) + 
  geom_line() + 
  labs(title = "Out-of-sample NLL", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none") +
  annotation_custom(nll_table, xmin = max(nll_data$Lag) - 3.5,
                    xmax = max(nll_data$Lag) - 0.5,
                    ymin = 1.01*max(nll_data$value),
                    ymax = Inf)

# Combine the plots and add a legend
combined_plot <- ggarrange(bic_plot, nll_plot,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
combined_plot

```

## Base Models

In order to get a baseline understanding of the influence of our key variables on the number of badges per active user, we employed a series of panel data models, with control variables: 1) the number of classes each teacher is responsible for, 2) the grade level of the classes, and 3) the total number of students. The 'Minutes Model' considers the number of minutes each teacher spends on the platform. The 'PCA Models' incorporate the three principal components we derived earlier.

Both of these models use a random effects approach, which is suitable for our panel data structure and accounts for unobserved heterogeneity.

## Models with Lags

Subsequently, we accounted for the temporal dynamics of our dataset by applying the Lau & Glimcher (2005) method. We introduced lagged variables into the models, thereby allowing us to account for temporal autocorrelation and potential delayed effects. We included lagged versions of the variables 'Teacher Minutes', 'PC1', 'PC2', 'PC3', and 'Badges per Students', with eight lags for each.

We then ran models with these lagged variables using the same random effects approach as in the base models.

In order to better understand and interpret the output of our models, we created a plot of the coefficients associated with each of the lagged variables. This plot allows us to see how the influence of each variable changes as the lag increases, and to compare these dynamics across variables.

```{r}
#| eval: false
#| label: fig-lags
#| fig-cap: "The estimated coefficients of the lagged variables in the random effects models. The lines represent different variables, and the shaded areas indicate the standard errors of the coefficients. The grey line and shaded area represent the coefficients for the lagged Badges per Student."

# Extract coefficients from models
lag_period = 4
col = "FrobeniusNNDSVD"
select_model <- function(model) {
  model[["Method"]] == col & model[["Lag"]] == lag_period
}
model_coeffs <- Filter(select_model, results)

extract_coeffs <- function(model) {
  coef_df <- data.frame(coeff_value_estimate = model$coef[,1],
                        coeff_value_se = model$coef[,2])
  coef_df$Coefficient_Name <- rownames(model$coef)
  # Add other model properties
  coef_df$Method <- model$Method
  coef_df$Component <- model$Component
  return(coef_df)
}
df_estimates <- do.call(rbind, lapply(model_coeffs, extract_coeffs)) %>%
  filter(Coefficient_Name != "(Intercept)" &
           Coefficient_Name != "week_lag") %>%
  mutate(Lag = as.numeric(str_extract(Coefficient_Name, "\\d+$")),
         Coefficient_Name = case_when(
           grepl("FrobeniusNNDSVD1", Coefficient_Name) ~ "Frobenius 1",
           grepl("FrobeniusNNDSVD2", Coefficient_Name) ~ "Frobenius 2",
           grepl("FrobeniusNNDSVD3", Coefficient_Name) ~ "Frobenius 3",
           grepl("Badges.per.Active.User", Coefficient_Name) ~ "Badges",
           TRUE ~ "Other"),
         Lag = case_when(is.na(Lag) ~ 1,
                         Coefficient_Name == "Badges" ~ Lag + 1,
                         .default = Lag),
         Coefficient_Name = case_when(Coefficient_Name == "Badges" ~ 
                                 paste0(Coefficient_Name, " ", Component),
                               .default = Coefficient_Name))

plot_comp <- ggplot(df_estimates %>% filter(!grepl("Badges", Coefficient_Name)),
       aes(x = Lag,
           y = coeff_value_estimate,
           color = Component,
           group = Component)) +
  geom_line() +
  geom_ribbon(aes(ymin = coeff_value_estimate - coeff_value_se,
                  ymax = coeff_value_estimate + coeff_value_se),
              alpha = 0.1) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 0.2) +
  facet_wrap(~Coefficient_Name, scales = "free_y") +
  labs(x = "Lag", y = "Coefficient", linetype = "") +
  theme_light() +
  theme(legend.position = "none")
plot_badge <- ggplot(data = df_estimates %>% filter(grepl("Badges", Coefficient_Name)),
            aes(x = Lag,
                y = coeff_value_estimate,
                group = Component,
                linetype = "Badges")) +
  geom_line(color = "black") +
  geom_ribbon(aes(ymin = coeff_value_estimate - coeff_value_se,
                  ymax = coeff_value_estimate + coeff_value_se),
              alpha = 0.1,
              fill = "grey",
              color = "grey") +  # set color to gray
  geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 0.2) +
  facet_wrap(~Coefficient_Name, scales = "free_y") +
  labs(x = "Lag", y = "Coefficient", linetype = "") +
  theme_light() +
  theme(legend.position = "none")

grid.arrange(plot_comp, plot_badge)

```

## Variable selection

To ensure that our models are parsimonious and to help determine which set of variables provides the best fit for the data, we compared the Bayesian Information Criterion (BIC) of the different models: the Minutes Model, PCA Model, and Lag Models. @tbl-choose-RL-model displays the BICs, with each row corresponding to a different model.

BIC penalizes models based on their complexity (number of parameters used) and the number of observations, favoring simpler models and models that fit the data better. In @fig-panel-bic, the 8-lag PC1 Model has the lowest BIC value, suggesting that it provides the best fit to the data when considering both complexity and fit.

## Q-Learning Analysis

```{r}
#| label: tbl-choose-RL-model
#| tbl-cap: "Comparison of Negative Log likelihood values of posteriors across different models. Lower values indicate better model fit."
results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]

# Function to extract median of lp__ from a model
extract_lp_median <- function(model_file) {
  fit <- readRDS(model_file)
  lp_median <- fit$summary("lp__", "median")  # Get the median of lp__
  data.frame(Model = gsub("Bayesian/Results/||.RDS", "", model_file),
             NLL_Median = -1 * as.numeric(lp_median))
}

# Apply function to all models and combine into one dataframe
lp_df <- map_df(results_files, extract_lp_median) %>%
  na.omit()
lp_df$Method <- gsub(".*-", "", lp_df$Model)
lp_df$ModelType <- str_extract(lp_df$Model, ".*(?=-)")

# Create the table with models as rows and NMF methods as columns
table_df <- lp_df %>%
  dplyr::select(-Model) %>%
  pivot_wider(names_from = Method, values_from = NLL_Median) %>%
  arrange(desc(ModelType)) %>%
  na.omit()
# To highlight the best value in the table
table_df %>%
  gt() %>%
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      columns = names(table_df)[colSums(table_df == min(table_df[,-1])) >= 1],
      rows = rowSums(table_df == min(table_df[,-1])) >= 1
    )
  )


```

<!-- Find a graph that displays results better (rather than the one user to fit) -->

We first present the following tables with the 25th, 50th, and 75th percentile of the learning rate (), inverse temperature (), weights, and cost parameters of the Q-learning models (non-hierarchical and hierarchical).

These are the group-level parameters that reflect the distribution of the subject-level parameters.

## Actor-Critic Analysis

Table. Fitted Parameters for Non-Hierarchical Actor-Critic learning model with Eligibility

## Model Comparison

We compare these four models by calculating the associated Bayesian Information Criterion (BIC) of each. Models with lags will be penalized for including two extra parameters.

```{r Bayesian LOOIC prep}
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py", "get_lag_value")))
gc(verbose = FALSE)
```

```{r}
#| cache: true
#| label: tbl-RL-logit-comp
#| tbl-cap: "LOOIC (Leave-One-Out Information Criterion) values were calculated for four different models: Q-learning, Logit, Q-learning Hierarchical, and Logit Hierarchical. The LOOIC provides a measure of model quality, with lower values indicating better model performance. Q-learning models were built using a kernel-based approach. Logit models were fit using logistic regression, with the Hierarchical versions incorporating a hierarchical structure to account for classroom-level variations. The LOOIC was calculated using the 'loo' function from the 'loo' package in R, which estimates the expected log predictive density for a held-out data point, based on the rest of the data."
library(brms)
library(loo)

# Non-hierarchical models
## Q-learning model
post <- read_rds("Bayesian/Results/Q-learning-kernel-FR.RDS")
loo_qlearn <- post$loo()$estimates["looic", ]
## Logit
models_nh <- readRDS("Bayesian/Results/logit.RDS")
loo_nh <- sapply(models_nh, loo)
looic_nh <- sum(unlist(loo_nh["looic",]))

# Hierarchical models
## Q-learning
post_hierarchical <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
loo_qhierarchical <- post_hierarchical$loo()$estimates["looic", ]
## Logit
models_h <- readRDS("Bayesian/Results/logit-hierarchical.RDS")
loo_h <- sapply(models_h, loo)
looic_h <- sum(unlist(loo_h["looic",]))

# Collect LOOIC values
looic_values <- c(loo_qlearn[1],looic_nh, 
                  loo_qhierarchical[1], looic_h)
df_looic <- data.frame(Model = c("Q-learning", "Logit", "Q-learning Hierarchical", "Logit Hierarchical"),
                       LOOIC = looic_values)

# Create gt table
gt(df_looic)

```

```{r}
#| eval: false

prediction <- post$summary()
prediction_hierarchical <- post_hierarchical$summary()
# write.csv(prediction, "./Bayesian/Results/prediction.csv")
# write.csv(prediction_hierarchical, "./Bayesian/Results/prediction_hierarchical.csv")

## Prediction data:
# y_pred[teacher, week, choice]
stan_data <- read_rds("Bayesian/Results/stan_data.RDS")
## Choice data:
# stan_data$choice


## TO-DO
# Here I'd like to have a visual representation of the model fit.
# I'm thinking of Having the number of weeks in the x-axis
# and the probability of each y=1 on the y-axis.
# Then graph two lines: one for the model fit (y_pred from the bayesian models)
#                   and one for the real data.
# Probably need to do this for the average across a number of teachers.


```

## **Optimality**

If the goal of a teacher is to maximize lesson completion, analyzing the performance of teachers across parameter levels is interesting. The following correlation plot shows the relationship between parameters and average weekly badges per teacher.

## Heterogeneity

```{r Heterogeneity analysis}
library(tidybayes)

hierarchical_model <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
classroom_data <- read.csv("Bayesian/df_subset.csv") %>%
  group_by(Teacher.User.ID) %>%
  summarise(n_active_students = mean(Active.Users...Total),
            n_students = mean(Students...Total),
            minutes_students = mean(Minutes.per.Active.User),
            badges = mean(Badges.per.Active.User),
            boosts = mean(Boosts.per.Tower.Completion),
            tower_alers = mean(Tower.Alerts.per.Tower.Completion),
            n_classes_by_teacher = median(teacher_number_classes),
            grade = first(Grade.Level),
            n_weeks = mean(n_weeks),
            poverty = first(poverty),
            income = first(income),
            charter_school = first(charter.school),
            school_account = first(school.account))

posterior_samples <- hierarchical_model$draws() %>%
  spread_draws(cost[207,3],
               gamma[207],
               alpha[207], 
               tau[207])

summary_cost <- posterior_samples %>%
  unnest_wider(cost, names_sep = "_")
list_of_dfs <- lapply(seq_along(summary_cost)[grepl("cost", names(summary_cost))], function(x){
  temp_df <- as.data.frame(summary_cost[[x]])
  names(temp_df) <- paste0("C", 1:ncol(temp_df))
  temp_df$teacher <- paste0("teacher_", (x - 3))
  return(temp_df)
})
cost_df <- do.call(rbind, list_of_dfs)
cost_df$teacher <- as.numeric(gsub("teacher_", "", cost_df$teacher)) 
summary_cost <- cost_df %>% 
  group_by(teacher) %>% 
  summarise(across(starts_with("C"), mean, .names = "mean_{.col}"))

summary_gamma_alpha_tau <- posterior_samples %>%
  unnest_wider(gamma, names_sep = "_") %>%
  unnest_wider(alpha, names_sep = "_") %>%
  unnest_wider(tau, names_sep = "_") %>%
  summarise(across(c(starts_with("gamma"),
                     starts_with("alpha"),
                     starts_with("tau")),
                   mean, .names = "{.col}")) %>%
  pivot_longer(cols = c(starts_with("gamma"),
                        starts_with("alpha"),
                        starts_with("tau")),
               names_to = "variable",
               values_to = "value") %>%
  separate(variable, into = c("type", "teacher"), sep = "_", convert = TRUE) %>%
  pivot_wider(names_from = type, values_from = value)

# merge the two dataframes
summary_all <- merge(summary_cost, summary_gamma_alpha_tau, by = "teacher")

classroom_data <- classroom_data %>%
  arrange(Teacher.User.ID) %>%
  bind_cols(summary_all)

lm1 <- lm(badges ~ mean_C1 + mean_C2 + mean_C3 +
            gamma + alpha + tau,
            # n_active_students +
            # n_classes_by_teacher +
            # grade +
            # n_weeks +
            # poverty +
            # income +
            # charter_school +
            # school_account,
          classroom_data)

test <- classroom_data %>%
  select(badges,
         mean_C1, mean_C2, mean_C3,
         gamma, alpha, tau)

chart.Correlation(test, histogram = TRUE, method = "spearman")


## TO-DO

# Here I would like to correlate individual parameters from the hierarchical
# model with specific classroom level variables.
# Including:
## Student Variables (mean across classroom):
# "Active.Users...Total", "Minutes.per.Active.User",
# "Badges.per.Active.User", "Boosts.per.Tower.Completion",
# "Tower.Alerts.per.Tower.Completion",
## Classroom and Teacher Variables
# "teacher_number_classes", "Grade.Level",
# "Students...Total", "n_weeks",
## School Variables
# "poverty", "income", "charter.school",
# "school.account

# That is, do the parameters from the Bayesian model change with these variables?
# If so, how?


```

# Discussion

6.  Comparing the performance of the models
7.  Advantages and limitations of each model
8.  Insights from each model

## Implications for Teachers and Schools

3.  Decision-making patterns
4.  Optimal strategies
5.  Implications for the education field
6.  Potential impact on teaching practices
7.  Policy recommendations

## Limitations

3.  Data limitations and biases
4.  Model assumptions and simplifications
5.  Generalizability of results

## Challenges

## Future research

6.  Application to other educational contexts
7.  Integration with other models and approaches
8.  Expanding the scope of variables and data sources

# References

::: {#refs}
:::

# Supplemental Information {.appendix}

## Figures {#sec-supp-fig}

![Zearn Student Portal](images/student-feed.PNG){#fig-st-portal fig-align="center"}

![Professional Development Calendar](images/PD-calendar.jpg){#fig-prof-dev fig-align="center"}

## Zearn's Eye View of the Data

```{r}
#| eval: false
#| label: fig-raw-data
#| fig-cap: "The raw data from Zearn's platform, with each row corresponding to a different timestamp for a teacher action. The columns represent the different variables, including the number of minutes spent on the platform, the number of badges earned, and the number of boosts used. The data is organized by classroom, with each classroom having a different color. The data is also organized by week, with each week having a different shape. The data is organized by teacher, with each teacher having a different line type. The data is organized by school, with each school having a different line color."

```

## Meta-Analysis of Correlations between Components and Student Outcomes

Subsequently, we performed a meta-analysis of these correlations to reveal the pooled effect of our variables. In this case, we conducted a multivariate meta-analysis, offering the advantage of modeling multiple, potentially correlated, outcomes. We transformed the correlations using Fisher's z-transformation (to ensure a normal distribution of the correlations) and ran a random effects model with each unique combination of "Teacher" and "School".

The resulting multivariate meta-analysis provides a comprehensive estimate of the correlations for each outcome, considering the hierarchical structure of the data. Thus, we can understand the overarching relationships between the different outcomes and the Badges across diverse schools and teachers. This robust conclusion, therefore, provides a more resilient analysis than a simple correlation analysis.

@fig-meta-analysis presents the results of a meta-analysis on the correlation.

```{r Meta-correlation prep}
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
# Importing results from Python
results_list <- py$results
gc(verbose = FALSE)
```

```{r}
#| cache: true
#| label: fig-meta-analysis
#| fig-cap: "Results of the correlation meta-analysis."
library(metafor)
library(ppcor)

n_comp <- 3
method <- "FrobeniusNNDSVD"
selected_cols <- c("Classroom.ID", "Teacher.User.ID",
                   "MDR.School.ID", "District.Rollup.ID",
                   "week", "Usage.Week",
                   # Main loadings of Components:
                   "tch_min", "tch_min_1",
                   paste0("FrobeniusNNDSVD", seq_len(n_comp)),
                   paste0("FrobeniusNNDSVD", seq_len(n_comp), "_1"),
                   "RD.optional_problem_sets",
                   "Guided.Practice.Completed", "Tower.Completed",
                   # Student Variables
                   "Active.Users...Total", "Minutes.per.Active.User",
                   "Badges.per.Active.User", "Boosts.per.Tower.Completion",
                   "Tower.Alerts.per.Tower.Completion",
                   # Classroom and Teacher Variables
                   "teacher_number_classes", "Grade.Level",
                   "Students...Total", "n_weeks",
                   # School Variables
                   "poverty", "income", "charter.school",
                   "school.account", "zipcode")  # Column to select

df_corr <- df %>%
  ungroup() %>%
  arrange(Classroom.ID, week) %>%
  dplyr::select(all_of(selected_cols)) %>%
  rename_with(~paste0("Frobenius", seq_len(n_comp)),
              paste0("FrobeniusNNDSVD", seq_len(n_comp))) %>%
  rename_with(~paste0("Frobenius", seq_len(n_comp), "_1"),
              paste0("FrobeniusNNDSVD", seq_len(n_comp), "_1"))

# Define a safe version of pcor.test that returns NA when there's an error
safe_pcor <- possibly(~pcor.test(..1, ..2, ..3, method = "spearman")$estimate,
                      otherwise = NA)
df_corr <- df_corr %>%
  group_by(Classroom.ID,Teacher.User.ID,MDR.School.ID,District.Rollup.ID) %>%
  summarise(
    n = n(),
    Frobenius1 = safe_pcor(Frobenius1, Badges.per.Active.User, Frobenius1_1),
    Frobenius2 = safe_pcor(Frobenius2, Badges.per.Active.User, Frobenius2_1),
    Frobenius3 = safe_pcor(Frobenius3, Badges.per.Active.User, Frobenius3_1),
    Minutes    = safe_pcor(tch_min, Badges.per.Active.User, tch_min_1),
    n_weeks = mean(n_weeks),
    teacher_number_classes = mean(teacher_number_classes),
    poverty = first(poverty),
    income = first(income),
    school.account = mean(school.account)
  ) %>%
  filter(!is.na(Frobenius1) &
           !is.na(Frobenius2) &
           !is.na(Frobenius3))

df_corr_sub <- df_corr %>%
  as_tibble() %>%
  slice_sample(prop = 0.1) %>%
  mutate_at(vars(paste0("Frobenius", seq_len(n_comp)), "Minutes"),
            list(~atanh(.))) %>%  # Fisher's z transformation
  mutate_at(vars(paste0("Frobenius", seq_len(n_comp)), "Minutes"),
            list(se = ~sqrt(1/(n - 2 - 2)))) %>%  # standard error sqrt(1/N−2−g)
  gather(key = "outcome", value = "correlation",
         c(paste0("Frobenius", seq_len(n_comp)), "Minutes")) %>%
  gather(key = "outcome_se", value = "se",
         c(paste0("Frobenius", seq_len(n_comp), "_se"), "Minutes_se")) %>%
  filter(str_replace(outcome, "_se", "") == str_replace(outcome_se, "_se", "")) %>%
  dplyr::select(-"outcome_se") %>%
  filter(!is.na(se))

# Run multivariate meta-analysis
res <- rma.mv(yi = correlation,
              V = se^2,
              random = ~ 1 | Classroom.ID/Teacher.User.ID/MDR.School.ID/District.Rollup.ID,
              mods = ~ -1 + outcome,
              data = df_corr_sub)

# Add columns for back-transformed effect sizes and their standard errors
res_df <- data.frame(
  estimate = coef(res),
  outcome = str_replace(names(coef(res)), "outcome", ""),
  # se = sqrt(diag(vcov(res)))
  ci.lb = res$ci.lb,
  ci.ub = res$ci.ub
  ) %>%
  mutate(
    estimate_r = tanh(estimate),
    ci.lb = tanh(ci.lb),
    ci.ub = tanh(ci.ub)
    # se_r = se / (1 - estimate^2)  # delta method for SE
  )
# Reverse the Fisher's Z transformation
df_corr_sub <- df_corr_sub %>%
  mutate(correlation = tanh(correlation))
# Create the plot
ggplot(df_corr_sub, aes(x = correlation, y = after_stat(density))) +
  geom_histogram(binwidth = 0.1, fill = "lightblue") +
  geom_density(aes(weight = weights(res)),
               alpha = 0.5) +
  geom_vline(data = res_df,
             aes(xintercept = estimate_r,
                 color = outcome),
             linetype = "dashed") +
  geom_segment(data = res_df,
               aes(x = ci.lb,
                   y = 0,
                   xend = ci.ub,
                   yend = 0,
                   color = outcome),
               linewidth = 1.5) +
  geom_text(data = res_df,
            aes(x = estimate_r,
                y = 0.15,
                label = sub('0\\.', '.', round(estimate_r, 2))),
            color = "black",
            size = 3.5,
            check_overlap = TRUE) +
  facet_wrap(~outcome, scales = "free",  ncol = 2) +
  labs(x = "Correlation", y = "Density") +
  theme_light() +
  scale_color_discrete(name = "Pooled effects \n (with 95% C.I.)")
  theme(legend.position = c(0.85, 0.24),
        legend.direction = "vertical")  # Set legend position and direction

```

```{r meta-analysis summary}
#| eval: false
summary(res)
```

## Bayesian Model Diagnostics

```{r}
#| eval: false
#| cache: true
#| label: tbl-diagnostics
#| tbl-cap: "Diagnostics for the Bayesian Models used in Q-learning and Q-learning with states. The tables show diagnostics for three Non-negative Matrix Factorization (NMF) models: Frobenius (initialized with a Nonnegative Double Singular Value Decomposition), Frobenius Average (initialized with a Nonnegative Double Singular Value Decomposition with zeros filled with the average of each variable), and Kullback-Leibler. The rows correspond to 'alpha,' the learning rate, 'gamma,' the discount factor, 'tau,' the temperature parameter, 'cost' for each of the components from the NMF, 'sensi,' the reward sensitivity parameter, and 'Log Posterior,' the logarithm of the posterior probability. 'Rhat' indicates the potential scale reduction factor on split chains. Values close to 1.0 imply model convergence. 'ESS Bulk' and 'ESS Tail' represent the effective sample size for the bulk of the posterior distribution and its tail, respectively. An ESS of at least a few hundred is often considered sufficient for estimation purposes."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]
results_files <- results_files[!grepl("hierarchical", results_files)]
diag_df <- data.frame()

for (result_file in results_files) {
  fit <- readRDS(result_file)
  
  # Get summary
  fit_summary <- fit$summary()
  
  # Create a data frame of the Rhat values and ESS
  result_df <- data.frame(
    Model = gsub("Bayesian/Results/||.RDS", "", result_file),
    Variable = fit_summary$variable,
    Rhat = fit_summary$rhat,
    ESS_Bulk = fit_summary$ess_bulk,
    ESS_Tail = fit_summary$ess_tail
  )
  
  # Add result to the overall diagnostics data frame
  diag_df <- rbind(diag_df, result_df)
}
# Reshape the data frame
diag_df <- diag_df %>%
  pivot_longer(cols = c(Rhat, ESS_Bulk, ESS_Tail),
               names_to = "Statistic") %>%
  pivot_wider(names_from = c(Model, Statistic),
              values_from = value) %>%
  mutate(Variable = if_else(Variable == "lp__", "Log Posterior", Variable))

# Split the data into two separate data frames
diag_df_actor_critic <- diag_df[,c(TRUE, grepl("Actor-Critic-", names(diag_df))[-1])]
diag_df_q_learning_states <- diag_df[,c(TRUE, grepl("Q-learning-states-", names(diag_df))[-1])]
diag_df_q_learning <- diag_df[,c(TRUE, (!names(diag_df) %in% names(diag_df_q_learning_states) &
                                          !names(diag_df) %in% names(diag_df_actor_critic))[-1])]

# Define the spanner labels for each model
diag_df_q_learning %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_q_learning_states %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_actor_critic %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

```

# Alternative models {.appendix}

```{python LSTM autoencoder}
#| eval: false
import os # To fix: https://github.com/tensorflow/tensorflow/issues/59779
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from keras.models import Model, load_model
from keras.layers import Input, Dense, LSTM, GRU, RepeatVector, TimeDistributed, Reshape, Flatten
from keras.constraints import NonNeg
from keras.callbacks import EarlyStopping
from keras.regularizers import L1
from keras_tuner import Hyperband
from keras_tuner.engine.hyperparameters import HyperParameters

# Function to reshape a 2D DataFrame into a 3D array
def reshape_data(df, num_samples, num_features):
    num_timesteps = dfpca_py['week'].nunique()
    X_reshaped = np.zeros((num_samples, num_timesteps, num_features))
    grouped = df.groupby('Classroom.ID')
    for i, (classroom_id, group) in enumerate(grouped):
        group = group.sort_values('week')
        group_features = group.drop(['Classroom.ID', 'week'], axis=1)
        X_reshaped[i, :len(group), :] = group_features.values
    X_reshaped = np.nan_to_num(X_reshaped)
    return X_reshaped
# Function to prepare the data
def prepare_data(df):
    df_id_week = df[['Classroom.ID', 'week']]
    df = df.drop(['Classroom.ID', 'week'], axis=1)
    df = scaler.transform(df)
    df = pd.DataFrame(df, columns=X_cols)
    df = pd.concat([df_id_week, df], axis=1)
    df_reshaped = reshape_data(
      df,
      df['Classroom.ID'].nunique(),
      len(df.columns) - 2
    )
    return df_reshaped

# Separate predictors and target variable for both training and testing data
X_train = train_df.drop(['Badges.per.Active.User', 'set'], axis=1)
Y_train = train_df[['Badges.per.Active.User', 'Classroom.ID', 'week']]
X_test = test_df.drop(['Badges.per.Active.User', 'set'], axis=1)
Y_test = test_df[['Badges.per.Active.User', 'Classroom.ID', 'week']]

# Prepare the data
X_train_reshaped = prepare_data(X_train)
X_test_reshaped = prepare_data(X_test)
Y_train_reshaped = reshape_data(
  Y_train,
  Y_train['Classroom.ID'].nunique(), 1)
Y_test_reshaped = reshape_data(
  Y_test,
  Y_test['Classroom.ID'].nunique(), 1)

################ Neural Net
# Determine loss weights according to the data structure:
decoding_weight = Y_train_reshaped.std() / (Y_train_reshaped.std() + X_train_reshaped.std())
prediction_weight = 1 - decoding_weight
# Function to build model
def build_model(hp):
    n_timesteps = X_train_reshaped.shape[1]
    n_features = X_train_reshaped.shape[2]
    n_labels = 1 # Regression: just one output node
    
    input_data = Input(shape=(n_timesteps, n_features))
    x = input_data
    
    # Add a variable number of hidden LSTM layers for encoder
    num_layers = hp.Int('num_layers', 2, 4)
    lstm_units = [
      hp.Choice('units_' + str(i), values=[2, 4, 8, 16, 32, 64, 128])
      for i in range(num_layers)
    ]
    for i in range(num_layers):
        x = LSTM(
          units=lstm_units[i],
          activation='tanh',
          return_sequences=True)(x)
    
    # Generate the latent vector
    latent_dim = hp.Int('encoding_units', min_value=2, max_value=n_comp, step=1)
    l1_value = hp.Float('l1_value', min_value=0.005, max_value=0.05, default=0.01, step=0.001)
    latent = TimeDistributed(Dense(
      units=latent_dim,
      activation='linear',
      activity_regularizer= L1(l1=l1_value),
      kernel_constraint=NonNeg(),
      name='encoded-vector'),
      name='latent')(x)
    
    # Decoder
    x = latent
    for i in range(num_layers):
        x = LSTM(
          units=lstm_units[i],
          activation='tanh',
          return_sequences=True)(x)
          
    decoded = TimeDistributed(
      Dense(units=n_features,
            activation='linear'),
      name='decoding')(x)
    
    label_output = TimeDistributed(
      Dense(1, activation='linear'),
      name='prediction')(latent)
    
    # Instantiate Autoencoder Model using Input and Output
    autoencoder = Model(inputs=input_data, outputs=[decoded, label_output])
    autoencoder.compile(optimizer='adadelta',
                        loss={
                          'decoding': 'mean_squared_error',
                          'prediction': 'mean_squared_error'
                        },
                        loss_weights={
                          'decoding': decoding_weight,
                          'prediction': prediction_weight
                        })
    return autoencoder

# Set up hyperparameter tuner
tuner = Hyperband(build_model,
                  objective='val_loss',
                  max_epochs=50,
                  factor=3,
                  directory='autoencoder_tuning',
                  project_name='LSTM_2nd_tuning')

# Perform hyperparameter search
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)
tuner.search(x=X_train_reshaped, 
            y=[X_train_reshaped, Y_train_reshaped],
            epochs=500,
            validation_data=(X_test_reshaped, [X_test_reshaped, Y_test_reshaped]),
            callbacks=[early_stopping_callback])
# tuner.results_summary()
# Get the optimal hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters()[0]
model = tuner.hypermodel.build(best_hyperparameters)
history = model.fit(x=X_train_reshaped, 
                    y=[X_train_reshaped, Y_train_reshaped],
                    epochs=5_000,
                    validation_data=(X_test_reshaped, [X_test_reshaped, Y_test_reshaped]))
best_model = history

# Function to get encoded representation and components
def get_encoded_representation_and_components(best_model, X):
    X_reshaped = reshape_data(
      X,
      X['Classroom.ID'].nunique(),
      len(X.columns) - 2
    )
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X_reshaped)
    X_hat = best_model.predict(X_reshaped)[0]
    res = ((X_reshaped - X_hat)**2).sum(axis=(1,2)).sum()
    n_components = X_encoded.shape[2]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

# Get encoded representation and components
X = pd.concat([dfpca_py[['Classroom.ID', 'week']], X_scaled], axis=1)
get_encoded_representation_and_components(best_model, X)

```
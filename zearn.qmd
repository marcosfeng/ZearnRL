---
title: "Unveiling Adaptive Pedagogy: Reinforcement Learning Models Illuminate Teacher Decision-Making in an Online Math-Teaching Platform"
abstract: "This study introduces a new use of reinforcement learning (RL) models to gain insight into teachers' decision-making processes on the Zearn online math-teaching platform. Analyzing data from 3029 classrooms, our approach treats pedagogical decision-making as a complex adaptive system, taking into account educators' reward history and the unique contexts of their instructional environments. Using Q-learning and Actor-Critic models, we identify distinct patterns of teaching behavior. Our comparison of RL models with a traditional logistic regression baseline demonstrates the superiority of the Actor-Critic model in capturing teacher behavior across most classrooms. Importantly, our findings reveal a strong positive correlation between the learning rates of state value weights and improved student achievement within the preferred Actor-Critic model. This finding highlights educators' adaptability in meeting their students' evolving needs in the digital learning landscape, ultimately leading to better learning outcomes. By demonstrating the relevance of RL models in the educational domain, our study introduces a new approach to exploring teacher decision-making and showcases their potential to inform the development of more effective, adaptive teaching strategies."
keywords: "Reinforcement Learning, Pedagogical Decision-Making, Digital Education Platforms, Instructional Adaptation, Q-learning, Actor-Critic"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
format:
  elsevier-pdf:
    keep-tex: true
    include-in-header:
      - text: |
          \usepackage{dcolumn}
          \usepackage{typearea}
          \usepackage{longtable}
    journal:
      formatting: preprint
      model: 3p
      layout: onecolumn
      cite-style: number
      # graphical-abstract: "![](abstract.png)"
bibliography: zearnrefs.bib

execute:
  echo: false
  warning: false
  error: false
  
knitr:
  opts_chunk:
    cache.extra: set.seed(832399554)
---

```{r load packages}
# Figures
library(ggforce)
library(pheatmap)
library(scales)
library(RColorBrewer)
library(ggrepel)
library(ggpubr)

# Tables
library(stargazer)
library(gtsummary)
library(gtExtras)
library(kableExtra)

# Other platforms
library(R.matlab)
library(reticulate)

# Statistics
library(broom)
library(PerformanceAnalytics)
library(ppcor)
library(fixest)
library(pROC)

# Basic packages
library(doParallel)
library(data.table)
library(tidyverse)

# library(cmdstanr)
# library(brms)
# library(bayesplot)
```

```{r}
set.seed(832399554)
random_py <- reticulate::import("random")
random_py$seed(832399554)
# https://www.random.org/integers/
# Timestamp: 2023-05-17 16:18:28 UTC
```

# Introduction

Predicting repeated behavior has been a long-standing goal of the behavioral sciences, including economics, psychology, and neuroscience. Much of human behavior results from stimulus-response associations, which are context-sensitive and not always consciously deliberated <!-- CITE: general RL paper -->. Reinforcement learning (RL) algorithms have emerged as a prominent way of quantifying these relationships, assigning a mathematical relationship between contextual cues (states), behavior (actions), and reward [@sutton2018]. These algorithms have found wide application in neuroscience and cognitive psychology, where they are used in data sets to model agents in specific environments. <!-- CITE: general RL paper --> However, these disciplines generally do not work with the practical and applied data typically used in social psychology and economics [@buyalskaya2023]. <!-- CITE source -->

This gap presents a novel opportunity to use methods from one set of disciplines on data traditionally used in another <!-- CITE: include some other applications! -->. In this paper, we aim to further this integration by applying RL algorithms to model the decision-making process of teachers in the math-teaching platform Zearn. RL provides a system of rewards and punishments where the agent (in this case, the teacher) learns to make optimal decisions by maximizing the rewards and minimizing the punishments.<!-- CITE source --> By assuming every teacher has an objective function to balance with their potential rewards, we model the sequential behavior of a teacher throughout a school year. For instance, the teacher chooses which pedagogical actions to employ, such as assigning homework, checking student progress, or reviewing content, in anticipation of enhancing student achievement. Applying RL algorithms allows for flexibility in learning the best strategy given certain contextual information. We provide a model of how teachers adapt their strategies in response to student performance and other contextual factors. This approach offers a flexible model for our available data and opens new avenues for understanding and enhancing human behavior in field settings.

<!-- -   Discuss existing models and where they fall short, justifying your approach. -->

## The Zearn Platform

Zearn is a digital platform for mathematics education designed to facilitate the teaching and learning of mathematics. About 25% of elementary school students and over 1 million middle school students across the United States use Zearn [@post-weblog]. Its unique blend of hands-on teaching and immersive digital learning, paired with its widespread adoption, provides the perfect setting for understanding how teachers adapt their strategies to optimize student achievement.

Zearn's pedagogical approach includes interactive digital lessons using visual aids (see @fig-zearn-poster) and real-time student feedback. The platform's approach to mathematical concepts, such as fractions, is particularly noteworthy. Students go through a series of representations — concrete, pictorial, and abstract — each designed to scaffold their understanding (i.e., "breaking down" problems, see [@jumaat2014; @reiser2014]) and prepare them for subsequent levels.

![Screenshot of student lesson on Zearn. The image is an example of teaching with visual models on the platform.](images/zearn-poster.jpg){#fig-zearn-poster fig-align="center"}

The platform's structure provides students with a personalized learning experience (see Appendix for a screenshot of the student portal) and teachers with resources to track student progress and make informed decisions (see @fig-class-report for a sample class report). Zearn follows a rotation model of learning — that is, a blend of traditional face-to-face learning (i.e., small group instruction) with online learning (i.e., self-paced online lessons). With this approach, students can learn new grade-level content in two distinct ways: independently, by engaging in digital lessons, and in small groups with their teacher and peers.

![Screenshot of sample classroom report. The image displays a summary dashboard where teachers can follow their students' progress as indicated by the number of lessons completed.](images/class-report.png){#fig-class-report fig-align="center"}

A key feature of Zearn is its badge system, which tracks student progress and motivates continued learning (see @fig-badges-screen). Students earn badges upon mastery of specific skills, providing a tangible representation of their achievement. This system motivates students and provides teachers with valuable data on student performance, informing their decision-making process [@knudsen2020]. Zearn also incorporates notifications, known as Tower Alerts, sent to teachers when a student struggles with a specific concept. This feature allows teachers to provide timely support and address learning gaps, enhancing the platform's capacity for personalized learning.

![Screenshot of student badges page. The image displays a summary dashboard where students can see their total badges earned (i.e., lessons completed) for a given mission (i.e., course module). Faded badges on the image signify open lessons to be completed, unfaded badges represent earned badges, and the locked icons correspond to future digital lessons that will open once all activities in the current lesson are completed.](images/badges.PNG){#fig-badges-screen fig-align="center"}

Another noteworthy feature is the platform's comprehensive professional development component, which is accessible to schools with a paid account (see @fig-prof-dev for a sample training schedule). In this program, teachers within a school collaborate to explore each unit or mission through word problems, fluencies, and small group lessons. They also analyze student work and problem-solving strategies. This professional development prioritizes (1) each mission's primary mathematical concept, (2) visual representations to scaffold learning, and (3) strategies to address unfinished learning from prior grades while preparing for future learning [@morrison2019].

Zearn's integrated framework provides a rich repository of data for our analysis. The variables delineated for investigation by Zearn encompass: (1) teacher engagement, quantified through a diverse set of actions; (2) student achievement, denoted by variables such as lesson completion (i.e., "badges" earned after each lesson is finished with full proficiency); and (3) student struggles, monitored through variables such as "tower alerts" (see @tbl-teacher-variables for a full glossary of available variables).

## Research Questions

We propose the following research questions:

1\. Characterizing Teacher Behavior: How can we best explain teachers' action choices? How does the explanatory power of reinforcement learning compare to simpler baseline models? Which specific reinforcement learning model best captures the empirical data on teacher behavior?

2\. Impact of Estimated RL Parameters: How do individual differences in teachers' decision-making patterns, as inferred from the parameters of the best-fitting reinforcement learning model, relate to heterogeneity in student achievement gains? Can we identify specific teacher behavioral profiles that predict better student learning outcomes?

3\. Influence of Teacher and School Background: To what extent do school contextual factors (e.g., socioeconomic status) account for variation in teachers' instructional choices, as quantified by the parameters of the reinforcement learning model?

# Theory

## Reinforcement Learning to Capture Patterns in Repeated Behavior

In RL, an agent learns to make decisions over time. Formally, an RL task is a tuple $\langle S,A,R,P,\gamma \rangle$, where $S$ is a set of states, $A$ is a set of actions, $R = \mathbb{E}[R_{t+1}|S_t = s, A_t = a]$ is a reward function, $P= \Pr[S_{t+1} = s′ |S_t = s,A_t = a]$ is a state transition probability, and $\gamma \in [0,1]$ is a discount factor. We define the agent's decisions as a probability distribution over actions, namely, the policy $\pi(a|s) = \Pr[A_t = a|S_t = s]$ [@sutton2018]. <!-- CITE: include early Thorndike 1931 and Rescola Wagner 1972 -->

RL models have been used in the psychology of habit to explain learning and reward association. One common approach in human studies is to apply the "multi-armed bandit" task. <!-- CITE --> In this type of experiment, participants are presented with multiple actions, each with an unknown payoff. The subject's goal is to learn the best outcome through trial and error. In the beginning, the reward-action relationships are unknown, so the participant must explore or sample each action [@sutton2018] <!-- CITE page# -->. This exploration-exploitation trade-off is a central theme in RL and has the potential to provide valuable insights into how individuals learn and make decisions over time.

In the context of education and teaching, RL has been present as early as 1960, with Ronald Howard applying this mathematical framework to instruction theory [@howard1960]. Later, in 1972, Richard Atkinson proposed a theory of instruction that encapsulates the key components of a Markov decision process, including states, actions, transition probabilities, reward functions, and a time horizon [@atkinson1972]. In Atkinson's framework, actions are instructional activities (e.g., assigning problem sets) that can change a given state (e.g., student learning level). These changes in states can yield rewards minus the associated cost of the action. For example, a teacher may be rewarded with an increase in the knowledge or skill of a student, but such reward must be balanced with its associated effort (e.g., labor cost). Atkinson and colleagues continued to test many parametrizations of this idea, contributing significantly to the development of RL theory in the context of education (see @doroudi2019 for a full review).

In the Zearn context, we define the decision process as follows:

1.  Agents are the teachers.

2.  Actions include the teachers' choices of specific pedagogical strategies.

3.  The reward is a function of the average student performance or activity within a classroom.

4.  The state is a function of other classroom variables not used in the reward function.

Note that we use $S$ in only one of our models. Further, we only consider model-free RL algorithms, which are so called because they do not require the agent to learn $P$ (i.e., state transition probabilities) to approximate expected rewards [@watkins1992]. In the following sections, we describe the two models we use to capture teacher behavior in the Zearn platform: Q-learning and Actor-Critic.

## Q-Learning Model

Consider a teacher using the Zearn platform. Each week, they must decide between two actions: assigning additional homework (action 1) or spending more time reviewing the material in class (action 2). At first, the teacher is uncertain about the best action to take. They start with initial beliefs about each action's long-term value (Q-value). However, they know that these beliefs may not be accurate and that they need to learn from experience.

Each week, the teacher chooses an action based on the current Q-value estimates. For example, in week 1, the teacher believes that assigning homework (action 1) has a slightly higher Q-value than reviewing in class (action 2). So, they assign homework and observe the outcome.

Then, the teacher receives a reward signal (e.g., the students' performance after the homework assignment). They use this reward to update their estimate of the Q-value for assigning homework, following the Q-learning update rule. This rule adjusts the Q-value estimate based on the difference between the observed reward and the previous estimate multiplied by a learning rate parameter.

Over the following weeks, the teacher continues to make decisions and update their estimates based on the outcomes observed. Sometimes, they explore new actions to gather more information, even if these actions do not seem optimal based on the current estimates. Other times, the teacher exploits their experience by choosing the action with the highest estimated Q-value.

As the teacher learns from experience, the Q-value estimates gradually converge toward the true values for each action.

| Week | Q-value Difference | Policy               | Choice | Reward | Prediction Error              | Updated Q-value                 |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| $t$  | $Q_t$              | $\text{Pr}_t(a = 1)$ | $a$    | $R_t$  | $\delta_t = \gamma R_t-c-Q_t$ | $Q_{t+1}=Q_{t}+\alpha \delta_t$ |
| 1    | 0.5                |                      | 1      | 0.8    |                               | 0.62                            |
| 2    | 0.2                |                      | 2      | 0.7    |                               | 0.67                            |
| 3    | 0.62               |                      | 1      | 0.6    |                               | 0.61                            |
| 4    | 0.3                |                      | 2      | 0.9    |                               | 0.75                            |
| 5    | 0.61               |                      | 1      | 0.7    |                               | 0.64                            |
| ...  |                    |                      |        |        |                               |                                 |

: Example of a Q-learning algorithm for a teacher on the Zearn platform. Each week ($t$), the teacher decides between action 1 and action 2 based on the difference in Q-values ($Q_t$) for each action. The teacher's choice ($a$) is determined by the policy ($\text{Pr}_t(a = 1)=1/(1+e^{-\tau Q_t})$). After observing the reward ($R_t$) associated with the chosen action, the teacher computes the prediction error ($\delta_t$) using the discount factor ($\gamma$) and the cost ($c$) of the action. The Q-value is then updated using the learning rate ($\alpha$) and the prediction error. As the teacher learns from experience, the Q-value converges toward the value that yields the highest reward. {#tbl-qvalue-example}

This model frames decision-making as a result of accumulated experience and the anticipation of future rewards. In other words, Q-learning involves the iterative refinement of Q-value functions, which map an agent's actions to evolving expectations of future rewards (analogous to subjective value or utility). This methodological approach is closely related to the classic "multi-armed bandit" problem, wherein the agent faces a finite set of choices (e.g., slot machines), each linked to a specific reward schedule, and aims to learn the action that yields the highest returns. Learning in this model depends on adjusting expectations to reduce the impact of prediction errors (the "surprise level," or the difference between expected and realized outcomes), using the Bellman equation to update Q-values iteratively [@rummery].

In this study, we opt for a state-independent version of Q-learning. That is, the Q-values do not vary with a contextual variable but are learned for each action only. This assumption is useful in scenarios where the state exerts minimal influence on the outcome of the action or when the state is difficult to define or observe [@sutton2018]. As such, the Q-function represents the expected return or future reward for taking action $a \in A = \{a_1,a_2,...\}$ following a certain policy $\pi = \Pr(a)$. The updating rule uses the Bellman equation as follows:

$$
Q_{t}(a) = Q_{t-1}(a) + \alpha \delta_t
$$ {#eq-q-learn}

where $\alpha$ is the learning rate, which determines how much the Q-value is updated based on $\delta$, the reward prediction error. The reward prediction error is the difference between the estimated Q-value and the observed reward. This error is used to update the Q-value of the chosen action in the direction of the observed reward, scaled by the learning rate $\alpha$, as follows:

$$
\delta_t = \gamma R_t - \text{cost}(a) - Q_{t-1}(a)
$$ {#eq-RPE}

where:

-   $a$ is the chosen action,

-   $R_t$ is the immediate reward received after taking action $a$,

-   $\text{cost}(a)$ is the perceived effort or inconvenience associated with action $a$,

-   $\gamma$ is the discount factor[^1],

-   $Q_{t-1}(a)$ is the estimate of the Q-value for action $a$ in the previous period.

[^1]: Commonly, this parameter captures the degree to which future rewards are discounted compared to immediate rewards. In this example, it could also act as a scaling factor of net reward.

In other words, $\alpha$ is the extent to which the newly acquired information will override the old information. A value of 0 means the agent does not learn anything. The agent starts with an initial Q-value (which can be arbitrary) and then updates the Q-values based on the experiences it gathers from interactions with the environment. The update rule is applied every time the agent takes action $a$ and receives a reward $R$. The agent selects actions based on a policy function of the Q-values. A common choice is the softmax action selection method, which chooses actions probabilistically based on their Q-values, as follows:

$$
\text{Pr}_t(a) = \frac{e^{\tau Q_t(a)}}{\sum_{a'} e^{\tau Q_t(a')}}
$$ {#eq-softmax}

where:

-   $\Pr_t(a)$ is the probability of choosing action $a$ at time $t$,

-   $Q_t(a)$ is the Q-value of action $a$ at time $t$,

-   $\tau$ is a parameter known as the inverse temperature, or the degree of randomness in the choice behavior,[^2]

-   The denominator is the sum over all possible actions $a' \in A$ of the exponential of their Q-values multiplied by the inverse temperature, and it functions as a normalizing value.

[^2]: One possible interpretation of the inverse temperature parameter $\tau$ is the agent's confidence in its Q-values, which controls the trade-off between exploration and exploitation. When $\tau$ is high, the agent explores more because the action probabilities are more uniform. When $\tau$ is low, the agent exploits more because the action with the highest Q-value is more likely to be chosen than the others. Some models may also allow for agents to start with a high inverse temperature to encourage exploration and then gradually decrease it to favor the exploitation of the learned policy.

### Binary Actions

For cases in which actions are binary (i.e., only two options $a_1$ and $a_2$), we set one of the actions as an outside option with a Q-value and cost of zero (i.e., base value $Q(a_2)=0$ and $\text{cost}(a_2)=0$). In this case, we only update the Q-value of the other variable ($Q(a_1)$), using the same Bellman equation but a modified prediction error:

$$
\delta_t = 
\begin{cases}
\gamma R_t - \text{cost}(a_1) - Q_{t-1}(a) & \text{if } a_1 \text{ is chosen} \\
-\gamma R_t & \text{if } a_2 \text{ is chosen}
\end{cases}
$$ {#eq-state-free}

where:

-   $Q_{t}(a)=Q_{t}(a_1)=Q_{t}(a_1)-Q_{t}(a_2)$ is the estimate of the difference in Q-values for the two actions,

-   $\alpha$ is the learning rate,

-   $R_t$ is the immediate reward received after taking the action.

Thus, the probability of choosing a particular action is determined by the logistic function as follows:

$$
\text{Pr}_t(a_1) = \frac{1}{1+e^{-\tau Q_{t}(a)}}
\\
\text{Pr}_t(a_2) = 1 - \text{Pr}_t(a_1)
$$

## The Actor-Critic Model

In the previous example, we considered a state-independent model in which the teacher's actions were based solely on their Q-values. However, in many real-world scenarios, the best action may depend on the current state of the environment. For example, if the teacher uses the Zearn platform, the effectiveness of assigning homework or reviewing material may vary depending on the students' current level of understanding. To model this type of behavior, we use the Actor-Critic model, which divides the decision-making process into two components: the "actor" and the "critic" [@sutton2018b].

Consider a teacher who, each week, observes how much the students understand and chooses an action based on this observation. For example, in week 4, the teacher sees that students struggled (e.g., more "tower alerts") with the week 3 material and believes that reviewing in class (action 2) helps students progress more than assigning extra homework (action 1). So, they review in class and observe the outcome. After taking this action, the teacher checks the students' performance and updates the action selection rule (the actor) and the state value estimate (the critic) based on the observed outcome.

Over time, as the teacher continues to interact with the environment and receive feedback, the actor learns to select actions that lead to higher-than-expected returns, while the critic provides increasingly accurate estimates of state values.

| Week | State           | Value         | Policy                               | Action | Reward | TD Error   | Updated Policy Weights | Updated Value Weights |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| $t$  | $S_t$           | $V_t=w_t S_t$ | $\pi_t=1/(1+e^{-\tau \theta_t S_t})$ | $a$    | $R_t$  | $\delta_t$ | $\theta_{t+1}$         | $w_{t+1}$             |
| 1    | \[1, 0.2, 0.8\] |               |                                      | 1      | 0.8    | 0.3        | \[0.1, 0.2, 0.3\]      | \[0.2, 0.4, 0.6\]     |
| 2    | \[1, 0.7, 0.3\] |               |                                      | 2      | 0.7    | 0.2        | \[0.2, 0.3, 0.4\]      | \[0.3, 0.5, 0.7\]     |
| 3    | \[1, 0.3, 0.7\] |               |                                      | 1      | 0.6    | -0.1       | \[0.1, 0.2, 0.3\]      | \[0.2, 0.4, 0.6\]     |
| ...  |                 |               |                                      |        |        |            |                        |                       |

: Example of an Actor-Critic model for a teacher on the Zearn platform. Each week (\$t\$), the teacher observes the state (\$S_t\$), which is a vector representing the students' struggles. The critic estimates the value (\$V_t\$) of the current state using the value weights (\$w_t\$). The actor selects an action (\$a\$) based on the policy (\$\\pi_t\$), which is determined by the policy weights (\$\\theta_t\$) and the state. After observing the reward (\$R_t\$) associated with the chosen action, the teacher computes the temporal difference (TD) error (\$\\delta_t\$). The policy weights and value weights are then updated using the TD error. As the teacher learns from experience, the actor learns to select actions that lead to higher-than-expected returns, while the critic provides increasingly accurate estimates of state values. {#tbl-ac-example}

More formally, the actor selects action $a \in A$ based on a policy function, denoted as $\pi_t(a|S_t)$, which maps states to actions, determining the probability of taking each action in each state.

In our setting, we set the policy as a softmax function:

$$
\pi_t(a|S_t,\theta^a_t) = \frac{e^{\tau \theta^a_t S_t}}{\sum_{a'} e^{\tau \theta^{a'}_t S_t}}
$$

where $S_{t}$ is a vector that characterizes the current state, with the first element, $1$, allowing for a bias term, followed by the remaining state variables. $\theta^a_t S_t$ is a linear combination of the state variables as determined by the policy weights $\theta^a_t$ for action $a$ at time $t$.

The "critic," on the other hand, evaluates the actions taken by the actor by learning a value function, denoted as $V_t$. Given the actor's current policy, the critic estimates the expected reward for each state, and the critic's feedback (i.e., value function) guides the actor's learning. In our case:

$$
V_t = w_t S_t
$$

where $w_t$ is the vector of value weights at time $t$.

We update the value function based on the difference between the estimated and actual return, analogous to the Temporal Difference (TD) error:

$$
\delta_t = (R_{t} - \text{cost}(a)) + \gamma V_t - V_{t-1}
$$

where $a$ is the chosen action at $t-1$.

The agent aims to learn a policy that maximizes the expected reward by taking the gradient ascent with the partial derivatives of the value and policy functions with respect to the parameters $w_t$ and $\theta^a_t$. The update rules for the critic and actor are as follows:

$$
w_{t+1} = w_{t} + \alpha^w \delta_t \nabla V_t
$$

$$
\theta_{t+1} = \theta_{t} + \alpha^{\theta} \gamma^{t-1} \delta_t \nabla \ln \pi_t
$$

where $\alpha^w$ and $\alpha^{\theta}$ are the learning rates for the critic and actor, respectively.

### Binary Actions

For cases in which actions are binary (i.e., only two options $a_1$ and $a_2$), we set one of the actions as an outside option with $\pi_t(a_2) = 1 - \pi_t(a_1|S_t,\theta^{a_1}_t)$. In this case, we have the policy function:

$$
\pi_t(a_1|S_t,\theta_t) = \frac{1}{1 + e^{-\tau \theta_t S_t}}
$$

where we call $\theta_t = \theta^{a_1}_t - \theta^{a_2}_t$ for simplicity. Therefore, we update the value and policy weights as follows:

$$
\delta_t = 
\begin{cases}
R_{t} - \text{cost}(a_1) + \gamma V_t - V_{t-1} & \text{if } a_1 \text{ is chosen} \\
R_{t} + \gamma V_t - V_{t-1} & \text{if } a_2 \text{ is chosen}
\end{cases}
$$

$$
w_{t+1} = w_{t} + \alpha^w \delta_t S_t
$$

$$
\theta_{t+1} = \theta_{t} + \alpha^{\theta} \gamma^{t-1} \delta_t \frac{\tau S_t}{1 + e^{\tau \theta_t S_t}}
$$

## Why Reinforcement Learning?

Reinforcement Learning (RL) presents a few advantages over models that employ a static approach to link teacher efforts with student outcomes. It embodies the flexibility to adapt and evolve strategies over time. This dynamic framework reflects the continuous learning process seen in biological systems and naturally aligns with the evolving nature of teacher-student interactions in the classroom. Beyond our immediate study goals, RL models hold the potential for automating instructional decisions based on identified patterns, potentially alleviating the workload on teachers and optimizing the educational process.

In this study, we position teachers as agents who navigate their environment (i.e., the classroom) by taking actions based on their observations and the feedback they receive. RL algorithms can characterize individual profiles for teachers, providing insights into how they adapt and respond to various states and rewards within the educational setting. By estimating individual teacher parameters, RL provides insights into valuable aspects for policymakers to design targeted interventions aimed at enhancing educational outcomes.

Further, the flexibility of RL makes it an ideal tool to model how teachers address changing classroom needs. By incorporating a wide range of variables (i.e., states, actions, and rewards), RL models are customizable to diverse educational contexts and objectives. Given this flexibility in mathematically mapping the agent-environment interaction (i.e., many models potentially satisfy our initial assumptions), our first step is a competition of models, selecting a set of models applicable to our setting, fitting them to the data, and comparing their performances.

<!-- -   Introduce the concept of data-driven exploration in RL models. -->

# Results

## Selecting a model specification

To analyze Zearn data spanning an entire academic year, we first establish a framework for actions, rewards, and, optionally, state variables for state-based RL models. In this framework, teacher activities drive the educational process, student achievements result from these efforts, and the constantly changing educational environment represents the states. Instead of relying solely on one analytical approach, our strategy involves a large set of candidate models, progressively escalating in complexity, as shown in @tbl-methods. Our overarching goal was to strengthen the reliability of our findings and offer a detailed understanding of the underlying behavioral patterns.

```{r data prep}

dt <- read.csv(file = "Data/df_clean.csv")

# Convert columns to appropriate data types
setDT(dt)
# Rename variable
dt[, `:=`(
  poverty = factor(poverty, ordered = TRUE, exclude = c("", NA)),
  income = factor(income, ordered = TRUE, exclude = c("", NA)),
  st_login = fifelse(Minutes.per.Active.User > 0, 1, 0, na=0),
  tch_login = fifelse(User.Session > 0, 1, 0, na=0)
  # Log Transform
  # Badges.per.Active.User = log(Badges.per.Active.User + 1),
  # Tower.Alerts.per.Tower.Completion = log(Tower.Alerts.per.Tower.Completion + 1),
  # tch_min = log(tch_min + 1)
)]

# Code datetime variables and compute additional metrics
setorder(dt, Teacher.User.ID, year, week, Classroom.ID)
dt[, isoweek := week]
dt[, week := week + 52*(year - 2019)]
# Fixing week == 1 (last week of 2019 counts as week 1 of 2020)
dt[week == 1, week := week + 52]
dt[, first_week := min(week), by = .(Teacher.User.ID)]
dt[, week := week - first_week + 1]
dt[, Tsubj := max(week), by = .(Classroom.ID)]

# Update the Grade.Level values and labels
dt <- dt[!(Grade.Level %in% c(-1, 11))] # Ignore -1 and 11
dt[, Grade.Level := factor(Grade.Level,
                           ordered = TRUE,
                           exclude = c(""))]
dt[, Grade.Level := factor(Grade.Level,
                           levels = c(0:8),
                           labels = c("Kindergarten", "1st", "2nd",
                                      "3rd", "4th", "5th",
                                      "6th", "7th", "8th"))]

df <- as.data.frame(dt) %>%
  ungroup()

# Convert year and isoweek to a date (Monday of that week)
df <- df %>%
  mutate(date = as.Date(paste(year, isoweek, 1, sep="-"), format="%Y-%U-%u"))

```

```{r table-summary-code}

df_summary <- df %>%
  group_by(MDR.School.ID) %>%
  summarise(
    Unique_Teacher_Count = n_distinct(Teacher.User.ID)
  ) %>%
  na.omit() %>%
  left_join(df %>%
              group_by(Classroom.ID, MDR.School.ID) %>%
              summarise(
                Students_Total = mean(Students...Total, na.rm = TRUE),
                Weeks_Total = max(week)
                ) %>%
              group_by(MDR.School.ID) %>%
              summarize(
                Students_Total = sum(Students_Total, na.rm = TRUE),
                Weeks_Total = mean(Weeks_Total)
                ),
            by = "MDR.School.ID") %>%
  ungroup()

# Summary statistics table
gt_school_sum <- df_summary %>%
  summarise(
    Mean_Teachers = mean(Unique_Teacher_Count),
    Median_Teachers = median(Unique_Teacher_Count),
    SD_Teachers = sd(Unique_Teacher_Count),
    Min_Teachers = min(Unique_Teacher_Count),
    Max_Teachers = max(Unique_Teacher_Count),
    Mean_Students_Total = mean(Students_Total),
    Median_Students_Total = median(Students_Total),
    SD_Students_Total = sd(Students_Total, na.rm = TRUE),
    Min_Students_Total = min(Students_Total),
    Max_Students_Total = max(Students_Total),
    Mean_Weeks_Total = mean(Weeks_Total),
    Median_Weeks_Total = median(Weeks_Total),
    SD_Weeks_Total = sd(Weeks_Total, na.rm = TRUE),
    Min_Weeks_Total = min(Weeks_Total),
    Max_Weeks_Total = max(Weeks_Total)
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Measure", "Variable"),
           sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value) %>%
  mutate(Variable = gsub("_Total", "", Variable)) %>%
  gt(rowname_col = "Variable") %>%
  # cols_label(Mean = "Mean", SD = "Standard Deviation",
  #            Min = "Minimum", Max = "Maximum") %>%
  fmt_number(
    columns = c("Mean", "SD"),
    decimals = 2
  ) %>%
  fmt_number(
    columns = c("Median", "Min","Max"),
    decimals = 0
  )

# Pre-exclusion variables
N_class_pre = length(unique(df$Classroom.ID))
N_teachers_pre = length(unique(df$Teacher.User.ID))
avg_std_pre = round(mean(df$Students...Total, na.rm = T), 1)

```

```{r draw-classroom-weeks}

# Create the histogram
fig_classroom_weeks <- df %>%
  group_by(Classroom.ID) %>%
  summarize(Tsubj = n()) %>%
  mutate(Tsubj_category = if_else(Tsubj < 18, "less than 18", "18 or more")) %>%
  ggplot(aes(x = Tsubj, fill = Tsubj_category)) +
  geom_histogram(color = "black", breaks = seq(min(df$Tsubj, na.rm = T),
                                               max(df$Tsubj, na.rm = T) + 1,
                                               by = 2)) +
  geom_vline(xintercept = 17, color = "darkgray",
             linetype = "dashed", linewidth = 0.8) +
  annotate("text", x = 10, y = 4000, label = "Excluded\nClassrooms",
           vjust = 1, color = "red") +
  labs(x = "Total Number of Weeks",
       y = "Frequency (Classrooms)") +
  scale_fill_manual(values = c("less than 18" = "red",
                               "18 or more" = "steelblue")) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(1, seq(5, max(df$Tsubj, na.rm = T), by = 5)))

```

```{r draw-logins-week}

# Calculate the sum of login values by date and Teacher.User.ID
login_data <- df %>%
  group_by(date, Teacher.User.ID) %>%
  summarize(tch_login = max(tch_login),
            st_login  = max(st_login)) %>%
  group_by(date) %>%
  summarize(tch_logins = sum(tch_login),
            st_logins  = sum(st_login))
# Create bar plot
bar_plot <- ggplot() +
  geom_bar(data = login_data, aes(x = date, y = st_logins), stat = "identity") +
  # geom_point(data = login_data, aes(x = date, y = tch_logins), color = "blue") +
  labs(
    x = "Week",
    y = "Total Logins"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12)) +
  scale_x_date(date_breaks = "3 week", date_labels = "%Y-%m-%d")

# Add labels for Christmas and Thanksgiving
logins_week <- bar_plot +
  geom_text(aes(x = as.Date("2019-12-25"), y = 1250, label = "Christmas"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "red") +
  geom_text(aes(x = as.Date("2019-11-25"), y = 1250, label = "Thanksgiving"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "darkorange")

```

```{r draw-map}
#| cache: true

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
# Set the library path to where proj.db is located
# Sys.setenv(PROJ_LIB = "")
unique_zipcodes <- unique(dt$zipcode) %>%
  na.omit() %>%
  as.list()
# plan(strategy = "multisession", workers = availableCores())
plan(strategy = "multisession", workers = 1)
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
dt <- merge(dt, address_geodata,
            by.x = "zipcode",
            by.y = "postalcode",
            all.x = TRUE)
dt <- dt[!is.na(zipcode)]

# Aggregate the data to get the number of teachers in each county
dt_map <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID)
), by = .(county)]

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID),
  lat = mean(lat),
  long = mean(long)
), by = .(city, county)]
# Get the top 5 cities with the most teachers
top_cities <- top_cities[order(-num_teachers)][1:5,]

# Get the Louisiana county map data
df_map <- tigris::counties(cb = TRUE,
                           resolution = "20m",
                           class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    as.data.frame(dt_map),
    by = c("NAMELSAD" = "county")
  ) %>%
  sf::st_as_sf() %>%
  mutate(num_teachers = ifelse(is.na(num_teachers), 0, num_teachers))

map_LA <- ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers", low = "white", high = "red", na.value = "gray90") +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city), size = 3, color = "black")
```

```{r draw-income-dist}

df_proportions <- df %>%
  filter(!is.na(poverty)) %>%
  group_by(poverty) %>%
  summarise(n = n()) %>%
  mutate(Percentage = paste0(
    round(n / sum(n) * 100, digits = 2), "%"
    )) %>%
  select(-"n") %>%
  na.omit() %>%
  rename(Variable = poverty) %>%
  mutate(Variable = as.character(Variable)) %>%
  bind_rows(df %>%
              filter(!is.na(income)) %>%
              group_by(income) %>%
              summarise(n = n()) %>%
              mutate(Percentage = paste0(
                round(n / sum(n) * 100, digits = 2), "%"
                )) %>%
              select(-"n") %>%
              na.omit() %>%
              rename(Variable = income) %>%
              mutate(Variable = as.character(Variable))
            ) %>%
  bind_rows(df %>%
              ungroup() %>%
              summarise(
                Charter_Schools = mean(charter.school, na.rm = T)*100,
                Schools_with_Paid_Account = mean(school.account, na.rm = T)*100
                ) %>%
              mutate(
                Charter_Schools = paste0(round(Charter_Schools, digits = 2), "%"),
                Schools_with_Paid_Account = paste0(round(Schools_with_Paid_Account,
                                                         digits = 2), "%")) %>%
              t() %>% as.data.frame() %>%
              rename(Percentage = V1) %>%
              mutate(Variable = row.names(.))) %>%
  add_row(Variable = "Poverty Level", Percentage = "", .before = 1) %>%
  add_row(Variable = "Income", Percentage = "", .before = 5) %>%
  add_row(Variable = "Other", Percentage = "", .before = 23)

# Splitting df_proportions into different categories for pie charts
df_poverty <- df_proportions[2:4,]
df_income <- df_proportions[6:22,]

# Convert Percentage to numeric
df_poverty$Percentage <- as.numeric(gsub("%", "", df_poverty$Percentage))/100

# Create Bar Graph for Poverty Level
poverty_plot <- ggplot(df_poverty, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Poverty Level",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Convert Percentage to numeric
df_income$Percentage <- as.numeric(gsub("%", "", df_income$Percentage))/100

# Create Bar Graph for Income Distribution
income_plot <- ggplot(df_income, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Income Range",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r preprocess data}
#| include: false

dt <- setDT(df)
dt[, `:=`(
  n_weeks = .N,
  mean_act_st = mean(Active.Users...Total, na.rm = TRUE)
  ), by = Classroom.ID]

dt <- dt[
  n_weeks > 18 & # At least 4.5 months cumulative activity
    Tsubj < 2*n_weeks & # At least activity twice a month on average
    teacher_number_classes < 5 &
    Students...Total > 5 &
    mean_act_st > 3 &
    !(Grade.Level %in% c("6th","7th","8th")) &
    !(month(date) %in% c(6, 7, 8)) &
    !is.na(District.Rollup.ID),
]

df <- as.data.frame(dt) %>%
  select(-X) %>%
  mutate(Minutes.per.Active.User = as.numeric(Minutes.per.Active.User)) %>%
  mutate(across(Active.Users...Total:Tower.Alerts.per.Tower.Completion,
         ~ ifelse(is.na(.), 0, .))) %>%
  arrange(Classroom.ID, week)

```

```{r table-classroom-summary-code}

create_summary <- function(var_name, var_label, type = "continuous") {
  summary_stat <- df %>%
    select(Grade.Level, var_name) %>%
    tbl_summary(
      by = Grade.Level,
      missing = "no",
      type = list(var_name ~ type),
      statistic = var_name ~ ifelse(type == "continuous",
                                    "{mean} ({sd})",
                                    "{n} ({p})")
    ) %>%
    add_overall() %>%
    as_tibble() 
  summary_stat[1] <- var_label
  return(summary_stat)
}

summaries_list <- list(
  # create_summary("Sessions.per.Active.User", "Sessions per Student"),
  create_summary("Minutes.per.Active.User", "Minutes per Student"),
  create_summary("Badges.per.Active.User", "Badges per Student"),
  create_summary("Tower.Alerts.per.Tower.Completion",
                 "Tower Alerts per Lesson Completion"),
  # create_summary("tch_login", "Teacher Login (0/1)", type = "dichotomous"),
  create_summary("Minutes.on.Zearn...Total", "Minutes per Teacher")
)

school_summary <- bind_rows(summaries_list) %>%
  select(-c(9:11)) %>% t() %>% as.data.frame()
colnames(school_summary) <- school_summary[1,]
# Remove the first row and set row names
school_summary <- school_summary[-1, ]
school_summary$`Grade Level` <- rownames(school_summary)

# Create a gt table
gt_classroom_sum <- school_summary %>%
  mutate(`Grade Level` = gsub("\\*\\*", "", `Grade Level`)) %>%
  gt(rowname_col = "Grade Level") %>%
  cols_label(
    `Minutes per Student` = "Minutes",
    `Badges per Student` = "Badges",
    `Tower Alerts per Lesson Completion` = "Tower Alerts",
    `Minutes per Teacher` = "Teacher Minutes"
  )


```

## Dimensionality Reduction

We first conducted a dimensionality reduction with Non-negative Matrix Factorization (NMF) and four components (see @fig-nmf-pca-comparison for a comparison of different methods by balancing reconstruction accuracy, i.e., R-squared, with clustering clarity, i.e., Silhouette Scores). While our dataset offers many potential action and reward variables, the direct use of these variables presents significant challenges:

1.  **Complexity**: The sheer number of available variables complicates the identification of meaningful patterns and relationships.
2.  **Dimensionality**: The high-dimensional nature of the data risks diluting important signals due to the "curse of dimensionality."[^3]
3.  **Interpretability**: Directly interpreting the impact of specific actions or behaviors on outcomes can be obscured by the intertwined nature of the data.

[^3]: Richard Bellman coined this phrase to describe the challenge of optimizing a control process by searching over a discrete multidimensional grid, where the number of grid points increases exponentially with the number of dimensions. He wrote: “In view of all that we have said in the foregoing sections, the many obstacles we appear to have surmounted, what casts the pall over our victory celebration? It is the curse of dimensionality, a malediction that has plagued the scientist from the earliest days” [@bellman2015adaptive].

By reducing the data to a manageable number of components, we can more readily identify underlying patterns of behavior and interaction. To achieve this, we applied Nonnegative Matrix Factorization (NMF) and used the results to define action, reward, and state variables rather than using individual metrics. One desirable feature of NMF is that it produces sparse components, providing a distilled representation of the data, where each one reflects a combination of behaviors or activities with a potential thematic linkage. Given that our chosen RL models require discreet action variables, we choose to split teacher actions into binary variables, following a median split. In our case, a median split is equivalent to giving a value of 1 to any positive value. 

```{r pca nmf data-prep}
#| include: false

df <- df %>%
  filter(!is.na(Minutes.on.Zearn...Total)) %>%
  group_by(Classroom.ID) %>%
  # train/test split
  mutate(set = sample(c("train", "test"), size = n(),
                      prob = c(0.8, 0.2), replace = TRUE)) %>%
  ungroup() %>% arrange(Classroom.ID, week)

columns <- names(
  df %>% select(RD.elementary_schedule:Minutes.on.Zearn...Total,
                Active.Users...Total:Tower.Alerts.per.Tower.Completion)
  )

# Create base data.table for models (faster than data.frame)
df_pca <- as.data.table(df)
# Convert columns to double to prevent precision loss
df_pca[, (columns) := lapply(.SD, as.numeric),
        .SDcols = columns]

# Apply the scaling operation
df_pca[, (columns) := lapply(.SD, function(x) {
  x[is.na(x)] <- 0
  sd_x <- sd(x, na.rm = TRUE)
  if(sd_x == 0 | is.na(sd_x)) return(rep(0, .N))
  x / sd_x
}), by = MDR.School.ID, .SDcols = columns]
setorder(df_pca, Classroom.ID, week)

# Calculate standard deviations
std_devs <- apply(df_pca %>% select(all_of(columns)), 2, sd, na.rm = T)
# Identify columns with defined standard deviations (not NaN or Inf)
invalid_cols <- names(std_devs[is.na(std_devs) |
                                 is.infinite(std_devs) |
                                 std_devs == 0])
df_pca <- df_pca %>% select(-all_of(invalid_cols))

```

```{python load data}
#| include: false
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA, NMF
from sklearn.metrics import silhouette_score

## Basic Variables
# Import data from R
dfpca_py = pd.DataFrame(r.df_pca)
dfpca_py.sort_values(['Classroom.ID', 'week'], inplace=True)

# Split the data into teacher and student subsets
teacher_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "RD.elementary_schedule"
    ):dfpca_py.columns.get_loc(
      "RD.grade_level_teacher_materials"
      # "Minutes.on.Zearn...Total"
      )+1
  ]
student_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "Active.Users...Total"
    ):dfpca_py.columns.get_loc(
      "Tower.Alerts.per.Tower.Completion"
      )+1
  ]

X_teachers = dfpca_py[teacher_variables]
X_students = dfpca_py[student_variables]

# Dictionaries for export
components = {}
results = {}
residuals = {}
silhouette = {}

```

```{python pca-nmf}
#| cache: true
#| include: false

# Function for NMF
def nmf_method(n, method, initial, X_scaled, data_label, solv = 'mu', nomin = False):
  method_name = f"{method.title()} {initial.upper()}"
  
  if method == 'frobenius' and initial == 'nndsvd': solv = 'cd'
  if method != 'frobenius' and initial == 'nndsvd': return
  if method != 'frobenius': method_name = f"{method.title()}"
  if nomin: method_name = method_name + "_nomin"
  
  nmf = NMF(
    n_components=n,
    init=initial,
    beta_loss=method,
    solver=solv,
    max_iter=4_000
  )
  X_nmf, nmf_comp = nmf.fit_transform(X_scaled), nmf.components_
  X_hat = nmf.inverse_transform(X_nmf)
  labels = np.argmax(nmf_comp, axis=0)
  
  results.setdefault(f"{method_name}_{data_label}", {})[n] = X_nmf
  components.setdefault(f"{method_name}_{data_label}", {})[n] = nmf_comp
  residuals.setdefault(f"{method_name}_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
  silhouette.setdefault(f"{method_name}_{data_label}", {})[n] = silhouette_score(nmf_comp.transpose(), labels)

def perform_pca_nmf(X_scaled, data_label, n_comp):
  for n in range(2, n_comp):
    ## PCA
    pca = PCA(n_components=n)
    X_pca = pca.fit_transform(X_scaled)
    pca_comp = pca.components_
    X_hat = pca.inverse_transform(X_pca)
    labels = np.argmax(pca_comp, axis=0)
    results.setdefault(f"PCA_{data_label}", {})[n] = X_pca
    components.setdefault(f"PCA_{data_label}", {})[n] = pca_comp
    residuals.setdefault(f"PCA_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
    silhouette.setdefault(f"PCA_{data_label}", {})[n] = silhouette_score(pca_comp.transpose(), labels)
    
    ## Non-negative Matrix Factorization
    for method in {'frobenius', 'kullback-leibler'}:
      for initial in {'nndsvd', 'nndsvda'}:
        nmf_method(
          n, method, initial, X_scaled,
          data_label=data_label,
          nomin = True    # No teacher minutes included
          )

# Run PCA and NMF for teachers
perform_pca_nmf(X_teachers, "teachers", min(X_teachers.shape) // 3)
# Run PCA and NMF for students
perform_pca_nmf(X_students, "students", min(X_students.shape))

```

```{python clean environment}

# get the names of all variables in the global namespace
all_vars = list(globals().keys())

# list of variables to keep
keep_vars = ['residuals', 'silhouette', 'components', 'results', 'r']

# delete all variables that are not in keep_vars
for var in all_vars:
    if var not in keep_vars:
        del globals()[var]
del keep_vars
del all_vars
del var

# manually call the garbage collector
import gc
gc.collect()

```

```{r nmf-pca-comparison-code}
#| cache: true

# Importing from Python
residuals_list <- py$residuals
silhouette_list <- py$silhouette

teacher_variables <- names(
  df_pca[,RD.elementary_schedule:RD.grade_level_teacher_materials]
)
student_variables <- names(
  df_pca[,Active.Users...Total:Tower.Alerts.per.Tower.Completion]
)
TSS_teacher <- df_pca %>%
  select(all_of(teacher_variables)) %>%
  mutate(across(all_of(teacher_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(teacher_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()
TSS_student <- df_pca %>%
  select(all_of(student_variables)) %>%
  mutate(across(all_of(student_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(student_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()

# Creating dataframes
df_residuals <- do.call(rbind, lapply(names(residuals_list), function(method) {
  do.call(rbind, lapply(names(residuals_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Residuals = residuals_list[[method]][[n]]
    )
  }))
}))
df_residuals_teachers <- df_residuals[
  grepl("_teachers", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_teacher)
df_residuals_students <- df_residuals[
  grepl("_students", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_student)

df_silhouette <- do.call(rbind, lapply(names(silhouette_list), function(method) {
  do.call(rbind, lapply(names(silhouette_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Silhouette = silhouette_list[[method]][[n]]
    )
  }))
}))
df_silhouette_teachers <- df_silhouette[
  grepl("_teachers", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method))
df_silhouette_students <- df_silhouette[
  grepl("_students", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method))

# Plotting residuals
p1 <- ggplot() +
  geom_line(data = df_residuals_teachers,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R-squared",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_teachers$Components),
                                  max(df_residuals_teachers$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p2 <- ggplot(df_silhouette_teachers,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_teachers$Components),
                                  max(df_silhouette_teachers$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_teachers$Silhouette) +
                                  3*sd(df_silhouette_teachers$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting residuals
p3 <- ggplot() +
  geom_line(data = df_residuals_students,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R-squared",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_students$Components),
                                  max(df_residuals_students$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p4 <- ggplot(df_silhouette_students,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_students$Components),
                                  max(df_silhouette_students$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_students$Silhouette) +
                                  3*sd(df_silhouette_students$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Combine the plots and place the legend at the bottom
comparison_plot <- ggarrange(p1, p2,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
comparison_plot2 <- ggarrange(p3, p4,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")

```

#### Interpreting Components

After analyzing the NMF data, we identified four significant components for teachers and students. @fig-nmf-heatmap displays these components as heatmaps, offering insight into the underlying behavioral structures. Given the loadings, we interpret the components as follows:

##### Teachers Components

**Component 1 (Assessments)**: This component has substantial weights on supplemental assessment materials, such as "Optional Problem Sets Download," "Optional Homework Download," and "Student Notes and Exit Tickets Download," indicating a proactive approach to evaluating and supporting student learning progress. It could also reflect a proactive approach to monitoring student understanding and providing feedback.

**Component 2 (Pedagogical Knowledge)**: The high weights on "Guided Practice Completed," "Tower Completed," "Tower Stage Failed," and "Fluency Completed" suggest that this component reflects when teachers are engaged in acquiring subject-matter-specific pedagogy, learning to scaffold and explain concepts in various ways.

**Component 3 (Group Instruction)**: This component, with prominent weights on "Small Group Lesson Download," "Whole Group Word Problems Download," and "Whole Group Fluency Download," suggests a pedagogical approach focused on fostering interactive and comprehensive classroom instruction. It implies engagement in activities that promote group learning dynamics and collective problem-solving skills.

**Component 4 (Curriculum Planning)**: The dominance of "Mission Overview Download" and "Grade Level Overview Download" in this component suggests that teachers are highly involved in strategic planning and curriculum mapping. It involves organizing the curriculum content and structuring lesson plans to align with grade-level objectives and mission overviews.

##### Student Components

**Component 1 (Badges)**: This component emphasizes "On-grade Badges" and "Badges," indicating that it measures students' overall engagement and advancement through the curriculum.

**Component 2 (Struggles)**: This component, which heavily weights "Boosts" and "Tower Alerts," seems to capture the frequency of occasions when students require additional scaffolding and assistance.

**Component 3 (Number of Students)**: This component mainly consists of "Active Students," which provides insight into what proportion of students regularly log in to complete Digital Lessons.

**Component 4 (Activity)**: Dominated by "Student Minutes" and "Student Logins," this component highlights the amount of time students invest in and the frequency of their interactions with Zearn.

```{r}
#| cache: true
#| label: fig-nmf-heatmap
#| fig-cap: "Heatmap of Non-negative Matrix Factorization (NMF) components for teacher and student data. The rows represent the original variables, and the columns correspond to the components. The color gradient indicates the relative importance of each variable within a component based on the proportion of the component's total weight attributed to that variable. These proportions were calculated by normalizing each variable weight within a component so that they all sum to 1. To provide context, the heatmaps label examples of low, moderate, and high proportion values."
#| fig-subcap:
#| - "Teacher Data. Component 1 (Assessments) focuses on using supplemental materials for student evaluation; Component 2 (Pedagogical Knowledge) emphasizes developing subject-specific teaching strategies; Component 3 (Group Instruction) centers on collaborative and whole-class teaching methods; Component 4 (Curriculum Planning) highlights planning and lesson preparation."
#| - "Student Data. Component 1 (Badges) measures curriculum engagement and progression; Component 2 (Struggles) indicates the need for additional academic support; Component 3 (Number of Students) tracks student participation within the platform; Component 4 (Activity) reflects the overall time spent and frequency of platform usage."
#| layout-ncol: 1

components_list <- py$components
df_heatmap_teachers <- 
  components_list[["Frobenius NNDSVD_nomin_teachers"]][["4"]] %>%
  t() %>% as.data.frame()
df_heatmap_students <- 
  components_list[["Frobenius NNDSVD_nomin_students"]][["4"]] %>%
  t() %>% as.data.frame()

# Define a named vector to map old variable names to new ones
variable_names <- c(
  "Minutes.on.Zearn...Total" = "Teacher Minutes",
  "RD.optional_problem_sets" = "Optional Problem Sets Download",
  "RD.student_notes_and_exit_tickets" = "Student Notes and Exit Tickets Download",
  "RD.mission_overview" = "Mission Overview Download",
  "RD.pd_course_notes" = "Course Notes Download",
  "RD.elementary_schedule" = "Elementary Schedule Download",
  "RD.whole_group_fluency" = "Whole Group Fluency Download",
  "Guided.Practice.Completed" = "Guided Practice Completed",
  "RD.small_group_lessons" = "Small Group Lesson Download",
  "Tower.Completed" = "Tower Completed",
  "Fluency.Completed" = "Fluency Completed",
  "Number.Gym.Activity.Completed" = "Number Gym Activity Completed",
  "RD.grade_level_overview" = "Grade Level Overview Download",
  "Tower.Stage.Failed" = "Tower Stage Failed",
  "Kindergarten.Activity.Completed" = "Kindergarten Activity Completed",
  "Tower.Struggled" = "Tower Struggled",
  "RD.k_mission" = "Kindergarten Mission Download",
  "RD.whole_group_word_problems" = "Whole Group Word Problems Download",
  "RD.assessments" = "Assessments Download",
  "RD.teaching_and_learning_approach" = "Teaching and Learning Approach Download",
  "RD.optional_homework" = "Optional Homework Download",
  "RD.k_schedule" = "Kindergarten Schedule Download",
  "RD.curriculum_map" = "Curriculum Map Download",
  "RD.assessments_answer_key" = "Assessments Answer Key Download",
  "RD.pd_course_guide" = "Course Guide Download",
  "RD.grade_level_teacher_materials" = "Teacher Materials Download",
  "Active.Users...Total" = "Active Students",
  "Sessions.per.Active.User" = "Student Logins",
  "Badges.per.Active.User" = "Badges",
  "Badges..on.grade..per.Active.Student" = "On-grade Badges",
  "Minutes.per.Active.User" = "Student Minutes",
  "Tower.Alerts.per.Tower.Completion" = "Tower Alerts",
  "Boosts.per.Tower.Completion" = "Boosts"
)
# Rename the rows of the dataframe
row.names(df_heatmap_teachers) <- variable_names[teacher_variables]
row.names(df_heatmap_students) <- variable_names[student_variables]

names(df_heatmap_teachers) <- paste0("Comp ", 1:4)
names(df_heatmap_students) <- paste0("Comp ", 1:4)
df_heatmap_teachers <- df_heatmap_teachers %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)
df_heatmap_students <- df_heatmap_students %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)

# Normalize by column sums to show proportion of contribution from each variable
df_heatmap_teachers <- df_heatmap_teachers %>%
  mutate(across(everything(), ~ round(.x / sum(.x), 3)))
df_heatmap_students <- df_heatmap_students %>%
  mutate(across(everything(), ~ round(.x / sum(.x), 3)))

# Define color scheme and breaks
breaks <- seq(0, max(df_heatmap_students,df_heatmap_teachers),
              by = round(max(df_heatmap_students,df_heatmap_teachers)/50,2))
color_scheme <- colorRampPalette(
  c("#F7F7F7", brewer.pal(n = 11, name = "RdYlBu")[6:1])
  )(length(breaks))

# Calculate annotations for both dataframes
calculate_annotations <- function(df) {
  data <- unlist(df)
  data <- data[data > breaks[4]]
  high <- max(data)
  low <- min(data)
  median <- ifelse(any(data == median(data)), median(data),
                   data[which.min(abs(data - median(data)))])
  return(df %>%
           mutate(across(everything(),
                         ~ if_else(. == low | . == high | . == median,
                                   ., 0))) %>%
           mutate(across(everything(), ~ as.character(.))) %>%
           mutate(across(everything(), ~ if_else(. == "0", "", .))) %>%
           as.matrix())
}

pheatmap(df_heatmap_teachers,
         cluster_rows = FALSE, cluster_cols = FALSE,
         angle_col = 0,
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE, show_rownames = TRUE,
         display_numbers = calculate_annotations(df_heatmap_teachers),
         number_color = RColorBrewer::brewer.pal(n = 11, name = "RdYlBu")[11])

pheatmap(df_heatmap_students,
         cluster_rows = FALSE, cluster_cols = FALSE,
         angle_col = 0,
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE, show_rownames = TRUE,
         display_numbers = calculate_annotations(df_heatmap_students),
         number_color = RColorBrewer::brewer.pal(n = 11, name = "RdYlBu")[11],
         fontsize = 15)

```

```{r export dimension reduction}
#| eval: false
# Importing results from Python
results_list <- py$results

# List of method names
# methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler")
methods <- c("Frobenius NNDSVD")
# Initialize df_components
df_components <- df_pca

# Loop over each method, extract the results at 3 components and bind to df_components
for (method in methods) {
  method_teacher <- paste0(method, "_nomin_teachers")
  method_student <- paste0(method, "_nomin_students")
  result_teacher <- results_list[[method_teacher]][["4"]]
  result_student <- results_list[[method_student]][["4"]]
  df_components <- df_components %>%
    bind_cols(result_teacher, result_student, .name_repair = )
  
  # Adjust column names
  new_cols <- paste0("Frobenius NNDSVD",
                     rep(c("_teacher", "_student"), each = 4),
                     rep(1:4, 2))
  names(df_components)[
    (ncol(df_components) -
       ncol(result_teacher) -
       ncol(result_student) + 1):ncol(df_components)
    ] <- new_cols
}

# Write to csv
write.csv(df_components, "./Data/df.csv")

```

## Feature Selection

```{r load dimension reduction}
#| include: false

minimum_weeks <- 12

df <- read.csv(file = "./Data/df.csv") %>%
  mutate(across(where(is.numeric),
                ~ ifelse(. < .Machine$double.eps, 0, .))) %>%
  arrange(Classroom.ID, week) %>%
  group_by(Classroom.ID) %>%
  # Filter out classrooms with low action rate
  filter(if_any(dplyr::starts_with("Frobenius.NNDSVD_teacher"),
                ~ sum(.>0) >= minimum_weeks/2)) %>%
  # Filter out classrooms with little data
  filter(n() >= minimum_weeks)

FR_cols <- grep("Frobenius.NNDSVD_teacher", names(df), value = TRUE)
df <- df %>%
  filter(sd(!!sym(FR_cols[1])) != 0) %>%
  filter(sd(!!sym(FR_cols[2])) != 0) %>%
  filter(sd(!!sym(FR_cols[3])) != 0) %>%
  filter(sd(!!sym(FR_cols[4])) != 0) %>%
  mutate(row_n = row_number()) %>%
  ungroup()
    
```

```{r helper-functions}

get_lag_value <- function(datatable, col, lag_period, n_comp = NULL) {
  # Add a column for week_lag
  datatable[, week_lag := c(0, diff(week)), by = Classroom.ID]

  if (is.null(n_comp)) {
    # Update the lag column with shift function
    datatable[, (paste0(col, "_", lag_period)) :=
                shift(get(col), lag_period, fill = 0, type = "lag"),
              by = Classroom.ID]
  } else {
    for (comp in 1:n_comp) {
      # Update the lag column with shift function
      datatable <- datatable[, (paste0(col, comp, "_", lag_period)) :=
                              shift(get(paste0(col, comp)), lag_period, fill = 0, type = "lag"),
                            by = Classroom.ID]
    }
  }
  
  return(datatable)
}

# Function to identify high-density regions for a given vector of values
find_hdr <- function(values) {
  IQR <- quantile(values, 0.75) - quantile(values, 0.25)
  return(c(quantile(values, 0.25) - 1.5*IQR,
           quantile(values, 0.75) + 1.5*IQR))
}

in_hdr <- function(values) {
  hdr <- find_hdr(values)
  return(hdr[1] <= values & values <= hdr[2])
}

```

```{r panel-model}
#| eval: false

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial)
  return(model)
}

create_formula <- function(action, reward, state = NULL, lag = 1) {
  terms <- c()

  for (i in 1:lag) {
    terms <- c(terms, paste0(reward, "_", i))
    terms <- c(terms, paste0(action, "_", i))
    # Interaction term for reward_i * action_i
    terms <- c(terms, paste0(reward, "_", i, ":", action, "_", i))
    if (i != lag) {
      for (j in (i + 1):lag) {
        # Interaction for reward_i * action_j when i < lag
        terms <- c(terms, paste0(reward, "_", i, ":", action, "_", j))
        
      }
    }
  }

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + week_lag | ", # Adding fixed effects part
                             "Teacher.User.ID + week")
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + ", paste0(state, "_1"),
                             " + ", paste0(state, "_1", ":",
                                           action, "_", (2:max(lag,2)),
                                           collapse = " + "),
                             " + week_lag | ",
                             "Teacher.User.ID + week")
  }
  return(formula_string)
}
#-------------
lags <- c(1:6)
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  # st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  # filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     # st <- as.character(params$st[i])
                     st <- NULL
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          # State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-results.RData")

```

```{r panel-model-states}
#| eval: false

#-------------
lags <- c(1:6)
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     st <- as.character(params$st[i])
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-state-results.RData")

```

In order to pre-select the most appropriate action, reward, and state variables, we used a panel logistic regression model inspired by dynamic analysis [@lau2005]. This approach acted as a filter to capture the action-reward (or action-reward-state) configurations displaying characteristics reminiscent of reinforcement learning (RL). We applied four criteria: a) the influence of consistent rewards on the propensity of actions being repeated, b) the immediate impact of states on action selection, c) the strategic role of actions in navigating towards desirable states, and d) the identification of action auto-correlation as an indicator of incremental learning processes.

Herein, the interaction between lagged rewards and actions aimed to capture the reinforcement aspect (a), where prior rewards enhance the likelihood of repeating specific actions. Including current state variables addressed (b) and examining how present educational contexts inform action choices. The interaction between current states and lagged actions encapsulated (c) that actions are deliberately chosen to navigate towards or sustain preferable educational states. Lastly, considering lagged rewards alone, we sought to elucidate (d) the phenomenon where past successes influence future endeavors, indicative of a learning trajectory.

We first explore reinforcement learning (RL)-like characteristics within the teacher and classroom usage data. We aimed to uncover patterns indicative of RL, where actors (teachers) select actions (teaching strategies) that historically yield higher rewards (improved student outcomes) and use states (classroom contexts) as signals for action selection. Further, we sought to understand how actions contribute to achieving or maintaining desired states and the extent to which actions exhibit auto-correlation due to incremental learning processes.

In order to capture the temporal dynamics of actions influenced by lagged rewards and states, we employed panel logistic regression models across different combinations of variables and lags. We incorporate lagged variables (ranging from one to six weeks) into the models using the Dynamic Analysis approach proposed to account for temporal autocorrelation and potential delayed effects. We applied reward and state structures extracted from classroom data via non-negative matrix factorization (NMF) with the Frobenius Non-negative Double Singular Value Decomposition (NNDSVD) and actions derived similarly from teacher data. We evaluate these models using the Bayesian Information Criterion (BIC) for model complexity and fit and the Area Under the Receiver Operating Characteristic curve (AUC) for predictive accuracy.

More specifically, we select one teacher component as the action and one student component as the reward. When constructing state-based models, we incorporate an additional student component as the state variable. This choice yields 16 state-free models (4 possible actions and 4 possible rewards) and 48 state-based models (4 possible actions, 4 possible rewards, and 3 possible state variables).

### Temporal Dynamics

Our investigation into temporal dynamics confirmed the impact of lagged rewards and actions on decision-making: shaping future decisions by past experiences. @fig-panel-bic illustrates this relationship, showcasing the predictive accuracy and model fit across fixed-effects models with different lags, with BIC and AUC scores for the models with one-week lags as the baseline. The results suggest a preference for a lag of two periods as optimal, based on the "elbow" in the AUC curves and the minima in the BIC curves.

```{r}
#| label: fig-panel-bic
#| fig-cap: "BIC and AUC variations across lags for fixed-effects panel logistic regression models. The plots show the percent change in model prediction accuracy (AUC) and fit (BIC) for different lag periods compared to the one-week lag baseline. The thin lines represent the percent change for each combination of reward functions and methods, while the dashed gray lines represent their average. The shaded bands around the average lines indicate the standard error. The optimal lag period can be determined based on the 'elbow' in the AUC curves (where increasing lags yields diminishing improvements in AUC) and the minima in the BIC curves (lower BIC indicates better model fit when penalizing for complexity)."
#| fig-subcap: 
#|   - "BIC state-free"
#|   - "AUC state-free"
#|   - "BIC state-dependent"
#|   - "AUC state-dependent"
#| layout-ncol: 2

load("Regressions/fe-results.RData")

fe_results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = "None",
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
}))

teachers <- Reduce(intersect, sapply(results, function(x) {
  names(x$recoef$Teacher.User.ID)
}))

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    # State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         # bic = bic - bic_base,
         bic = bic/bic_base - 1,
         auc_base = auc[which(Lag == 1)],
         # auc = auc - auc_base,
         auc = auc/auc_base - 1) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "Percent Change in BIC",
                    "auc" = "Percent Change in AUC")
  
  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(Reward, Method),
                               # group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    scale_color_brewer(palette = "Set2") +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = brewer.pal(n = 8, name = "Set2")[8], alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, linetype = "dashed",
              color = brewer.pal(n = 8, name = "Dark2")[8]) +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    scale_y_continuous(labels = percent_format()) +
    theme(axis.text.x = element_text(hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#------------------

load("Regressions/fe-state-results.RData")

fe_results_df <- fe_results_df %>%
  rbind(
    do.call(rbind, lapply(results, function(x) {
      data.frame(
        Method = x$Method,
        Lag = x$Lag,
        State = x$State,
        Reward = x$Reward,
        auc = x$AUC,
        bic = x$bic,
        stringsAsFactors = FALSE
        )
      }))
    )

teachers <- intersect(
  teachers, Reduce(intersect, sapply(results, function(x) {
    names(x$recoef$Teacher.User.ID)
    }))
  )

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward, State) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         # bic = bic - bic_base,
         bic = bic/bic_base - 1,
         auc_base = auc[which(Lag == 1)],
         # auc = auc - auc_base,
         auc = auc/auc_base - 1) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "Percent Change in BIC",
                    "auc" = "Percent Change in AUC")
  
  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    scale_color_brewer(palette = "Set2") +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = brewer.pal(n = 8, name = "Set2")[8], alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, linetype = "dashed",
              color = brewer.pal(n = 8, name = "Dark2")[8]) +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    scale_y_continuous(labels = percent_format()) +
    theme(axis.text.x = element_text(hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#----------------

bic_plot_se
auc_plot_se
bic_plot_se_st
auc_plot_se_st

```

### Model Selection and Interpretability

```{r panel-model-subset}
#| eval: false

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial)
  return(model)
}

create_formula <- function(action, reward, state = NULL) {
  terms <- c()

  # Interaction term for reward_i * action_i
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_1"))
  terms <- c(terms, paste0(reward, "_1:", action, "_1"))
  
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(reward, "_2"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_2:", action, "_2"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + week_lag | ", # Adding fixed effects part
                             "Teacher.User.ID + week")
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + ", paste0(state, "_1"),
                             " + ", paste0(state, "_1", ":", action, "_2"),
                             " + week_lag | ",
                             "Teacher.User.ID + week")
  }
  return(formula_string)
}
#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  # st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  # filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     # st <- as.character(params$st[i])
                     st <- NULL
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          # State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-subset-results.RData")

```

```{r panel-model-states-subset}
#| eval: false

#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     st <- as.character(params$st[i])
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-state-subset-results.RData")

```

```{r panel-model-restricted}
#| eval: false

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial)
  return(model)
}

create_formula <- function(action, reward, state = NULL) {
  terms <- c()

  # Interaction term for reward_i * action_i
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(action, "_1"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + week_lag | ", # Adding fixed effects part
                             "Teacher.User.ID + week")
  } else {
    formula_string <- paste0(action, " ~ ",
                             paste(terms, collapse = " + "),
                             " + ", paste0(state, "_1"),
                             " + ", paste0(state, "_1", ":", action, "_2"),
                             " + week_lag | ",
                             "Teacher.User.ID + week")
  }
  return(formula_string)
}
#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  # st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  # filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     # st <- as.character(params$st[i])
                     st <- NULL
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          # State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-restricted-results.RData")

```

```{r panel-model-states-restricted}
#| eval: false

#-------------
lags <- 2
n_comp = 4
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
# Estimation
params <- crossing(
  act = colnames(df)[str_detect(colnames(df), paste0(action, collapse = "|"))],
  lag = lags,
  st  = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))],
  rwd = colnames(df)[str_detect(colnames(df), paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    # Filter out Classroom.IDs where any relevant variable has sd = 0
    group_by(Classroom.ID) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

# Estimation
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC")) %dopar% {
                     act <- as.character(params$act[i])
                     lag <- params$lag[i]
                     st <- as.character(params$st[i])
                     rwd <- as.character(params$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out of Sample Log Likelihood
                     predictions <- predict(model, newdata = test_data)

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions)$auc
                          ),
                          bic = BIC(model),
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/fe-state-restricted-results.RData")

```

```{r RL-mixed-effects}
#| eval: false

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}
create_formula <- function(action, reward, state = NULL, lag = 1) {
  terms <- c()

  for (i in 1:lag) {
    # Base terms for action and reward
    terms <- c(terms, paste0(reward, "_", i), paste0(action, "_", i))

    # Interaction term for reward_i * action_i with varying slopes
    terms <- c(terms, paste0(reward, "_", i, ":", action, "_", i))

    if (i != lag) {
      for (j in (i + 1):lag) {
        # Add the interaction for reward_i * action_j when i < lag with varying slopes
        terms <- c(terms, paste0(reward, "_", i, ":", action, "_", j))
      }
    }
  }

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/me-results.RData")

```

```{r RL-mixed-effects-subset}
#| eval: false

load("Regressions/me-results.RData")
teachers <- intersect(
  Reduce(union, sapply(results, function(x) {
    x$perform_df$Teacher.User.ID
    })),
  Reduce(intersect, sapply(results, function(x) {
    names(x$recoef$Teacher.User.ID)
    }))
  )

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}
create_formula <- function(action, reward, state = NULL, lag = 1) {
  terms <- c()

  for (i in 1:lag) {
    # Base terms for action and reward
    terms <- c(terms, paste0(reward, "_", i), paste0(action, "_", i))

    # Interaction term for reward_i * action_i with varying slopes
    terms <- c(terms, paste0(reward, "_", i, ":", action, "_", i))

    if (i != lag) {
      for (j in (i + 1):lag) {
        # Add the interaction for reward_i * action_j when i < lag with varying slopes
        terms <- c(terms, paste0(reward, "_", i, ":", action, "_", j))
      }
    }
  }

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st, lag = lag)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/me-subset-results.RData")

```

```{r RL-mixed-effects-restricted}
#| eval: false

create_model <- function(formula, data) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = 50, mem.clean = TRUE)
  return(model)
}
create_formula <- function(action, reward, state = NULL) {
  terms <- c()
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(action, "_1"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique()

# Data and Variables
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    arrange(Classroom.ID, week) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr")) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)

                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model))
                   }
# Stop the cluster
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/me-results-restricted.RData")

```

```{r, results='asis'}
#| label: tbl-RL-exploration
#| tbl-cap: "Comparison of performance metrics for fixed-effects (FE) and varying-coefficient (VC) models predicting student outcomes. The FE models estimate a single set of coefficients, while the VC models allow coefficients to vary across teachers. FE AUC represents the area under the receiver operating characteristic (ROC) curve for the fixed-effects models. Higher values (up to 1) indicate better predictive power. FE BIC represents the Bayesian Information Criterion (in thousands) for the fixed effects models, balancing model fit and complexity. Lower values indicate a preferred model. Avg. Individual AUC is the mean of the predictive power scores across all classrooms in each VC model. Avg. Individual Log is the mean of model fit scores across all classrooms in each VC model. Lower (less negative) values indicate better fit. The table shows the distributions of each metric across different model specifications, along with their mean, median, and standard deviation (SD)."

load("Regressions/me-subset-results.RData")
# load("Regressions/me-results-restricted.RData")

top_fits_df <- do.call(rbind, lapply(results, function(x) {
  # Assuming 'perform_df' is correctly structured as shown in your summary
  top_fits <- x$perform_df %>%
    mutate(keep = case_when(!in_hdr(n_train) ~ F,
                            !in_hdr(n_test) ~ F,
                            !in_hdr(logLik_ind) ~ F,
                            auc_in < 0.5 ~ F,
                            auc_out < 0.5 ~ F,
                            auc_in  == 1 ~ F,
                            auc_out == 1 ~ F,
                            .default = T)) %>%
    filter(keep) %>%
    summarize(
      # across(c(auc_out, auc_in, logLik_ind),
      #        ~ sd(., na.rm = T)/sqrt(n()), .names = "{.col}_se"),
      across(c(auc_out, auc_in, logLik_ind),
             ~ list(mean(., na.rm = T))),
      total = n()) %>%
    mutate(
      Action = x$Method,
      Reward = x$Reward,
      State = coalesce(x$State, "None")
    )
  # top_fits$AUC <- x$AUC
  # top_fits$BIC <- x$bic
  return(top_fits)
})) %>%
  mutate(across(c(auc_out, auc_in, logLik_ind), ~ unlist(.))) %>%
  na.omit()

top_fits_df <- top_fits_df %>%
  full_join(fe_results_df %>%
              filter(Lag == 2),
            by = c("Action" = "Method", "Reward", "State")) %>%
  mutate(across(c(Action, Reward, State),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Badges",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggles",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "Activity",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Pedagogical Knowledge",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Group Instruction",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Curriculum Planning",
                            .default = .)))
  
# top_statefree <- rbind(
#   top_fits_df %>% filter(State == "None") %>%
#     arrange(desc(auc)) %>% slice_head(n = 1) %>% select(Action, Reward),
#   top_fits_df %>% filter(State == "None") %>%
#     arrange(bic) %>% slice_head(n = 1) %>% select(Action, Reward),
#   top_fits_df %>% filter(State == "None") %>%
#     arrange(desc(auc_out)) %>% slice_head(n = 1) %>% select(Action, Reward),
#   top_fits_df %>% filter(State == "None") %>%
#     arrange(desc(logLik_ind)) %>% slice_head(n = 1) %>% select(Action, Reward)
# )
# top_statebased <- rbind(
#   top_fits_df %>% filter(State != "None") %>%
#     arrange(desc(auc)) %>% slice_head(n = 1) %>% select(Action, Reward, State),
#   top_fits_df %>% filter(State != "None") %>%
#     arrange(bic) %>% slice_head(n = 1) %>% select(Action, Reward, State),
#   top_fits_df %>% filter(State != "None") %>%
#     arrange(desc(auc_out)) %>% slice_head(n = 1) %>% select(Action, Reward, State),
#   top_fits_df %>% filter(State != "None") %>%
#     arrange(desc(logLik_ind)) %>% slice_head(n = 1) %>% select(Action, Reward, State)
# )

top_fits_df %>%
  mutate(bic = bic/1000) %>%
  dplyr::select(auc, bic, auc_out, logLik_ind) %>%
  rename("FE AUC" = auc,
         "FE BIC (x10<sup>3</sup>)" = bic,
         "Avg. Individual AUC" = auc_out,
         "Avg. Individual Log Likelihood" = logLik_ind) %>%
  gt_plt_summary() %>%
  # Remove "Missing" column
  cols_hide(n_missing) %>%
  tab_header(
    title = "",
    subtitle = ""
  ) %>%
  cols_label(
    name = "Performance Score",
    value = "Histogram"
  ) %>%
  fmt_markdown() %>%
  fmt_number(
    columns = c(Mean, Median, SD),
    decimals = 2
  # ) %>% as_word()
  ) %>% gt::gtsave(filename = "images/tbl-RL-exploration.png")
knitr::include_graphics("images/tbl-RL-exploration.png")

  # # Add columns for top models
  # cols_add(
  #   top_action = top_statefree$Action,
  #   top_reward = top_statefree$Reward,
  #   top_action_st = top_statebased$Action,
  #   top_reward_st = top_statebased$Reward,
  #   top_state = top_statebased$State) %>%
  # cols_label(
  #   top_action = "Top Action",
  #   top_reward = "Top Reward",
  #   top_action_st = "Top Action",
  #   top_reward_st = "Top Reward",
  #   top_state = "Top State"
  # ) %>%
  # tab_spanner(
  #   label = "State-Free",
  #   columns = c(top_action, top_reward)
  # ) %>%
  # tab_spanner(
  #   label = "State-Based",
  #   columns = c(top_action_st, top_reward_st, top_state)
  # ) %>%
  # gt::gtsave(filename = "images/tbl-RL-exploration.png")

```

<!-- Our analysis of fixed effects (FE) and varying coefficients (VC) models reveals the complexities and variations in the Zearn data on predicting teacher behavior based on previous student performances. @tbl-RL-exploration synthesizes our model performance scores, revealing that the fixed-effect area under the receiver operating characteristic curve (FE AUC) averages 0.81. We found that "Assessments" emerged as the most influential action and "Number of Students" as the leading reward in the state-free context. The Fixed Effects Bayesian Information Criterion (FE BIC) also reflects a mean of approximately 93.06 x 10\^3. The lowest BIC values correspond with "Scaffolding" as a pivotal action and "Struggle" as a salient reward variable in the state-free approach. -->

<!-- Furthermore, the average teacher-specific AUC, a metric derived from the varying coefficient models, is 0.70. Notably, "Scaffolding" continues to be a consistent influencing factor for both state-free and state-based models, with "Number of Sessions" as the corresponding reward. The average out-of-sample teacher-specific log-likelihood, also derived from varying coefficients, shows an average of -262.63. In the state-based scenario, "Planning Guides" and "Activities" are the top-performing actions, aligning with "Struggle" and "Completion" as state variables. -->

#### Model Performance

Drawing on the resulting metrics, we strategically narrowed our focus to the action-reward-state configurations that most closely align with Reinforcement Learning (RL) principles. @fig-RL-exploration delineates the top models based on their high fixed-effects Area Under the Curve (AUC) and mean teacher-specific AUC, as well as their Fixed Effects Bayesian Information Criterion (BIC) and higher mean teacher-specific log-likelihood. Panels A and B of the figure show the state-free and state-dependent models, respectively. Each point on the plot represents a different model, identified by its BIC and average teacher-specific AUC. The actions "Pedagogical Knowledge" and "Group Instruction" stand out for both models. However, the choice of associated rewards and states is unclear, suggesting the need to fit RL models using all configurations featuring these two actions.

```{r}
#| label: fig-RL-exploration
#| fig-cap: "Selection of Actions, Rewards, and States for Reinforcement Learning (RL) Analysis. The plots compare the performance of various logistic regression models designed to capture RL-like teacher behaviors. Area Under the ROC Curve (AUC) measures discrimination accuracy, with higher values indicating better predictive power. Bayesian Information Criterion (BIC) assesses model fit penalizing by model complexity, with lower values preferred. Each point represents a model specification, with different combinations of actions (pedagogical practices) and rewards (student outcomes). State-dependent models also incorporate variables describing the classroom state, selected from the pool of student variables not already chosen as the reward variable for that model. Models closest to the top left corner achieve the best trade-off between predictive power (high AUC) and model fit (low BIC)."
#| fig-subcap: 
#|   - "State-free"
#|   - "State-dependent"
#| layout-ncol: 2

# Select top models by BIC (lower is better) and AUC (higher is better)

# Keep models for later analysis
selected_models <- top_fits_df %>%
  filter(State == "None") %>%
  filter(auc_out > 0.7, bic < 61000)

## State-free
plot1 <- top_fits_df %>%
  filter(State == "None") %>%
  dplyr::select(Action, Reward, bic, auc_out) %>%
  ggplot(aes(x = bic, y = auc_out, color = Reward)) +
  geom_point() +
  geom_mark_ellipse(
    aes(group = Action, label = Action, color = NULL),
    con.type = "straight",
    con.cap = 0,
    expand = .02,
    label.buffer = unit(0.1, "mm")
  ) +
  # # Draw a segment for the x-line up to y = 90000
  # geom_segment(x = 84000, y = 0.7, xend = 90000, yend = 0.7,
  #              linetype = "dashed", color = "darkgray") +
  # # Draw a segment for the y-line starting from x = 0.7
  # geom_segment(x = 90000, y = 0.7, xend = 90000, yend = 0.72,
  #              linetype = "dashed", color = "darkgray") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "BIC F.E.", y = "Avg. Teacher AUC")
       # title = paste(nrow(selected_models), "Models Selected"))

# Join with selected_models
selected_models <- bind_rows(
  selected_models,
  top_fits_df %>%
    filter(State != "None") %>%
    filter(auc_out > 0.7, bic < 61000)
  )

## State-based
plot2 <- top_fits_df %>%
  filter(State != "None") %>%
  dplyr::select(Action, Reward, State, bic, auc_out) %>%
  ggplot(aes(x = bic, y = auc_out, color = Reward, shape = State)) +
  geom_point() +
  theme(legend.position = "bottom") +
  geom_mark_ellipse(
    aes(group = Action, label = Action, color = NULL),
    con.type = "straight",
    con.cap = 0,
    expand = .02,
    label.buffer = unit(0.1, "mm")
  ) +
  # # Draw a segment for the x-line up to y = 90000
  # geom_segment(x = 84000, y = 0.7, xend = 90000, yend = 0.7,
  #              linetype = "dashed", color = "darkgray") +
  # # Draw a segment for the y-line starting from x = 0.7
  # geom_segment(x = 90000, y = 0.7, xend = 90000, yend = 0.72,
  #              linetype = "dashed", color = "darkgray") +
  theme_minimal() +
  labs(x = "BIC F.E.", y = "Avg. Teacher AUC")
       # title = paste(nrow(selected_models %>% filter(State != "None")),
       #               "Models Selected"))

plot1
plot2

```

```{r RL-me-interpretation}
#| eval: false

reate_model <- function(formula, data, iter = 25) {
  # Fit a logistic regression model with fixed effects
  model <- feglm(formula, data = data, family = binomial, 
                 panel.id = c("Teacher.User.ID", "week"),
                 glm.iter = iter, mem.clean = TRUE)
  return(model)
}

# Use only reward_1:action_2 given the previous results
create_formula <- function(action, reward, state = NULL) {
  terms <- c()
  
  # Interaction term for reward_i * action_i
  terms <- c(terms, paste0(reward, "_1"))
  terms <- c(terms, paste0(action, "_1"))
  terms <- c(terms, paste0(reward, "_1:", action, "_1"))
  
  terms <- c(terms, paste0(reward, "_1:", action, "_2"))
  terms <- c(terms, paste0(reward, "_2"))
  terms <- c(terms, paste0(action, "_2"))
  terms <- c(terms, paste0(reward, "_2:", action, "_2"))

  if (is.null(state)) {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "),
                             "] + week")
  } else {
    formula_string <- paste0(action, " ~ week_lag ",
                             " | Teacher.User.ID[",
                             paste(terms, collapse = ", "), ", ",
                             state, "_1, ", state, "_1:", action, "_2",
                             "] + week")
  }

  return(formula_string)
}

action <- c("Frobenius.NNDSVD_teacher")
reward <- c("Frobenius.NNDSVD_student")
lags = 2
n_lags = 2
n_comp = 4
params2 <- crossing(
  act = colnames(df)[str_detect(colnames(df),
                                paste0(action, collapse = "|"))],
  lag = lags,
  st  = c(colnames(df)[str_detect(colnames(df),
                                  paste0(reward, collapse = "|"))],
          "NA"),
  rwd = colnames(df)[str_detect(colnames(df),
                                paste0(reward, collapse = "|"))]
) %>%
  filter(rwd != st) %>%
  unique() %>%
  semi_join(
    selected_models %>%
      select(Action, Reward, State) %>%
      mutate(across(c(Action, Reward, State),
                    ~ case_when(. == "Badges" ~ "Frobenius.NNDSVD_student1",
                                . == "Struggles" ~ "Frobenius.NNDSVD_student2",
                                . == "No. Students" ~ "Frobenius.NNDSVD_student3",
                                . == "Activity" ~ "Frobenius.NNDSVD_student4",
                                . == "Assessments" ~ "Frobenius.NNDSVD_teacher1",
                                . == "Pedagogical Knowledge" ~ "Frobenius.NNDSVD_teacher2",
                                . == "Group Instruction" ~ "Frobenius.NNDSVD_teacher3",
                                . == "Curriculum Planning" ~ "Frobenius.NNDSVD_teacher4",
                                . == "None" ~ "NA",
                                .default = .))),
    by = c("act" = "Action", "rwd" = "Reward", "st" = "State"))

# Data and Variables
load("Regressions/me-results.RData")
teachers <- intersect(
  Reduce(union, sapply(results, function(x) {
    x$perform_df$Teacher.User.ID
    })),
  Reduce(intersect, sapply(results, function(x) {
    names(x$recoef$Teacher.User.ID)
    }))
  )
df_bin <- as.data.table(
  df %>%
    mutate(across(dplyr::starts_with(action),
                  ~ if_else( . > 0, 1, 0))) %>%
    mutate(across(dplyr::starts_with(reward), ~ ./sd(., na.rm = TRUE))) %>%
    arrange(Classroom.ID, week) %>%
    filter(Teacher.User.ID %in% teachers) %>%
    ungroup()
)

# Create lags
for (col in c(action, reward)) {
  for (lag_period in 1:n_lags) {
    df_bin <- get_lag_value(df_bin, col, lag_period, n_comp)
  }
}
# Panel data
train_data <- as.data.frame(df_bin[set == "train"])
test_data <- as.data.frame(df_bin[set == "test"])

library(doSNOW)
cl <- makeCluster(min(nrow(params2), detectCores()-1))
registerDoSNOW(cl)
pb <- txtProgressBar(max = nrow(params2), style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)
results <- foreach(i = 1:nrow(params2),
                   .multicombine = TRUE,
                   .errorhandling = "remove",
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .packages = c("fixest", "pROC", "dplyr"),
                   .options.snow = opts) %dopar% {
                     act <- as.character(params2$act[i])
                     lag <- params2$lag[i]
                     st <- NULL
                     if (as.character(params2$st[i]) != "NA") {
                       st <- as.character(params2$st[i])
                     }
                     rwd <- as.character(params2$rwd[i])
                     fmla <- create_formula(action = act, reward = rwd,
                                            state = st)
                     model <- create_model(as.formula(fmla), train_data)
                     mult <- 1
                     while (!model$convStatus & mult < 5) {
                       model <- create_model(as.formula(fmla), train_data,
                                             2^(mult)*25)
                       mult <- mult + 1
                     }
                     
                     # Out-of-Sample Predictions
                     predictions <- predict(model, newdata = test_data)
                     perform_df <- cbind(test_data, predictions) %>%
                       filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                       group_by(Teacher.User.ID) %>%
                       filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                       summarise(n_test = n(),
                                 auc_out = as.numeric(
                                   roc(response = !!sym(act),
                                       predictor = predictions,
                                       quiet = TRUE)$auc)) %>%
                       # In-Sample Predictions
                       inner_join(cbind(train_data,
                                        predictions = predict(
                                          model, newdata = train_data)) %>%
                           filter(!is.na(predictions), !is.na(!!sym(act))) %>%
                           mutate(logLik_ind = if_else(!!sym(act) == 1,
                                                       log(predictions),
                                                       log(1 - predictions))) %>%
                           group_by(Teacher.User.ID) %>%
                           filter(sd(!!sym(act)) > 0, sd(predictions) > 0) %>%
                           summarise(logLik_ind = sum(logLik_ind, na.rm = TRUE),
                                     n_train = n(),
                                     auc_in = as.numeric(
                                       roc(response = !!sym(act),
                                           predictor = predictions,
                                           quiet = TRUE)$auc)),
                           by = "Teacher.User.ID")

                     # Return the results as a list
                     list(Method = act,
                          Lag = lag,
                          State = st,
                          Reward = rwd,
                          formula = fmla,
                          AUC = as.numeric(
                            roc(response = test_data[,act],
                                predictor = predictions,
                                quiet = TRUE)$auc),
                          bic = BIC(model),
                          perform_df = perform_df,
                          fecoef = summary(model)$coeftable,
                          recoef = fixef(model),
                          convergence = model$convStatus)
                   }
# Stop the cluster
close(pb)
stopCluster(cl)
rm(cl)

save(results, file = "Regressions/me-standardized-coef.RData")

```

## Estimating RL Models

The next step is to identify and evaluate reinforcement learning (RL) models that best represent the teaching strategies derived from our data. The performance of each model is quantified by computing the Bayesian Information Criterion (BIC). Our computational analysis identified a distinct group of models that excelled in their low BIC scores. @tbl-top-CBM provides a breakdown of scores for different combinations of rewards and states under this action, offering insight into the comparative performance of various model setups. Notably, all top-performing models featured "Pedagogical Knowledge" as their primary action. Models with "Badges" and "Activity" as their rewards outperform others, as do state-free models, likely due to their higher simplicity.

```{r}
#| label: tbl-top-CBM
#| tbl-cap: "Best-fit models. The table presents the best-fit models, organized by their reward and state components, based on Bayesian Information Criterion (BIC) scores obtained from Laplace approximations. All models feature the 'Pedagogical Knowledge' action component. The BIC scores serve as a metric for model fit, penalizing model complexity, with lower scores indicating superior performance. Missing values in the State column denote Q-learning models, while non-empty State values indicate Actor-Critic models. Note that these estimates are not hierarchical; they represent the individual estimates of each teacher."

ql_folder_path <- "CBM/zearn_results/ql_refine"
ac_folder_path <- "CBM/zearn_results/ac_refine"

# Create a data frame for ggplot
evidence_df <- data.frame(
  Action = as.character(NA),
  Reward = as.character(NA),
  State = as.character(NA),
  BIC = as.numeric(NA)
)
files <- list.files(path = ql_folder_path,
                    pattern = "refine_ql", full.names = TRUE)
for (file in files) {
  mat_data <- readMat(file)
  evidence_df <- rbind(evidence_df, data.frame(
    Action = file,
    Reward = as.character(NA),
    State = as.character(NA),
    BIC = -2*sum(mat_data[["cbm"]][[5]][[2]]) +
      dim(mat_data[["cbm"]][[5]][[1]])[2] * 
      log(dim(mat_data[["cbm"]][[5]][[1]])[1])
    ))
}
files <- list.files(path = ac_folder_path,
                    pattern = "refine_ac", full.names = TRUE)
for (file in files) {
  mat_data <- readMat(file)
  evidence_df <- rbind(evidence_df, data.frame(
    Action = file,
    Reward = as.character(NA),
    State = as.character(NA),
    BIC = -2*sum(mat_data[["cbm"]][[5]][[2]]) + 
      dim(mat_data[["cbm"]][[5]][[1]])[2] * 
      log(dim(mat_data[["cbm"]][[5]][[1]])[1])
    ))
}

evidence_df <- evidence_df[-1,]
evidence_df[1,1:3] <- c("Pedagogical Knowledge","Badges", NA) # QL 1
evidence_df[2,1:3] <- c("Pedagogical Knowledge","Struggles", NA) # QL 3
evidence_df[3,1:3] <- c("Pedagogical Knowledge","No. Students", NA) # QL 5
evidence_df[4,1:3] <- c("Pedagogical Knowledge","Activity", NA) # QL 7
evidence_df[5,1:3] <- c("Pedagogical Knowledge","Badges",
                        "Struggles, No. Students, Activity") # AC 13
evidence_df[6,1:3] <- c("Pedagogical Knowledge","Struggles",
                        "No. Students") # AC 16
evidence_df[7,1:3] <- c("Pedagogical Knowledge","Struggles",
                        "Badges, Activity") # AC 22
evidence_df[8,1:3] <- c("Pedagogical Knowledge","Struggles",
                        "No. Students, Activity") # AC 23
evidence_df[9,1:3] <- c("Pedagogical Knowledge","Struggles",
                        "Badges, No. Students, Activity") # AC 27
evidence_df[10,1:3] <- c("Pedagogical Knowledge","No. Students",
                         "Badges, Activity") # AC 36
evidence_df[11,1:3] <- c("Pedagogical Knowledge","Activity",
                         "Badges, No. Students") # AC 50
evidence_df[12,1:3] <- c("Pedagogical Knowledge","Activity",
                         "Struggles, No. Students") # AC 51
evidence_df[13,1:3] <- c("Pedagogical Knowledge","Badges",
                         "Struggles, No. Students") # AC 7
evidence_df[14,1:3] <- c("Pedagogical Knowledge", "Badges",
                         "No. Students, Activity") # AC 9

# Display the table
evidence_df %>%
  dplyr::select(-Action) %>%
  group_by(Reward) %>%
  arrange(BIC,Reward,State) %>%
  gt(row_group_as_column = TRUE) %>%
  tab_stubhead(label = "Reward") %>%
  sub_missing() %>%
  fmt_number(columns = c(BIC), decimals = 1)

```

#### Hierarchical Bayesian Inference (HBI) Results and Model Comparison

To further refine our selection, we employ Hierarchical Bayesian Inference (HBI) to perform a more detailed comparison of the selected models. This statistical approach allows us to compare models not just by their individual fits but also by their ability to explain data across different subjects. In interpreting the results from HBI, we observed the differential performances of Q-learning (QL) and Actor-Critic (AC) models. @tbl-CBM-HBI provides quantifiable insights into each model's fit (BIC) and predictive power (AUC, the area under the receiver operating characteristic curve) across teachers. Generally, Q-learning models fit the data better than Actor-Critic models (i.e., lower BIC scores), and AC models outperform in terms of predictive power (higher AUC scores).

```{r}
#| label: tbl-CBM-HBI
#| tbl-cap: "Top Q-learning and Actor-Critic models under Hierarchical Bayesian Inference (HBI). The table presents the best-performing models, all featuring the 'Pedagogical Knowledge' action component, ranked by their Bayesian Information Criterion (BIC) scores. BIC quantifies the models' fit to the data, penalizing for model complexity, with lower values indicating better performance. The model frequency represents the proportion of teachers best described by each model. Q-learning models (denoted by missing values in the State column) assume teachers learn directly from rewards, while Actor-Critic models incorporate state information to guide decision-making."

# Load the MATLAB files for the top QL and AC models
top_model_ql <- list(
  readMat('CBM/zearn_results/ql_refine/hbi_ql3.mat'),
  readMat('CBM/zearn_results/ql_refine/hbi_ql5.mat'),
  readMat('CBM/zearn_results/ql_refine/hbi_ql7.mat'),
  readMat('CBM/zearn_results/ql_refine/hbi_ql1.mat')
)
top_model_ac <- list(
  readMat('CBM/zearn_results/ac_refine/hbi_ac23.mat'),
  readMat('CBM/zearn_results/ac_refine/hbi_ac22.mat'),
  readMat('CBM/zearn_results/ac_refine/hbi_ac16.mat'),
  readMat('CBM/zearn_results/ac_refine/hbi_ac51.mat'),
  readMat('CBM/zearn_results/ac_refine/hbi_ac13.mat')
  )

# Extract the relevant data from each model
# model_freq_ql <- top_model_ql[["cbm"]][[5]][[5]]
AUC_ql <- sapply(top_model_ql, FUN = function(x) {
  apply(x[["cbm"]][[5]][[8]], 2, mean, na.rm = TRUE)
})
AUC_sd_ql <- sapply(top_model_ql, FUN = function(x) {
  apply(x[["cbm"]][[5]][[8]], 2, sd, na.rm = TRUE)
})
BIC_ql <- sapply(top_model_ql, FUN = function(x) {
  apply(x[["cbm"]][[5]][[11]], 2, mean, na.rm = TRUE)
})
BIC_sd_ql <- sapply(top_model_ql, FUN = function(x) {
  apply(x[["cbm"]][[5]][[11]], 2, sd, na.rm = TRUE)
})

# model_freq_ac <- top_model_ac[["cbm"]][[5]][[5]]
AUC_ac <- sapply(top_model_ac, FUN = function(x) {
  apply(x[["cbm"]][[5]][[8]], 2, mean, na.rm = TRUE)
})
AUC_sd_ac <- sapply(top_model_ac, FUN = function(x) {
  apply(x[["cbm"]][[5]][[8]], 2, sd, na.rm = TRUE)
})
BIC_ac <- sapply(top_model_ac, FUN = function(x) {
  apply(x[["cbm"]][[5]][[11]], 2, mean, na.rm = TRUE)
})
BIC_sd_ac <- sapply(top_model_ac, FUN = function(x) {
  apply(x[["cbm"]][[5]][[11]], 2, sd, na.rm = TRUE)
})

# Create a data frame with each model
top_models_table <- data.frame(
  Model = c(rep("Q-learning",4), rep("Actor-Critic",5)),
  Action = rep("Pedagogical Knowledge", 9),
  Reward = c("Struggles", "No. Students", "Activity", "Badges",
             "Struggles", "Struggles", "Struggles", "Activity", "Badges"),
  State = c(rep(NA,4),
            "No. Students, Activity",
            "Badges, Activity",
            "No. Students",
            "Struggles, No. Students",
            "Struggles, No. Students, Activity"),
  # ModelFrequency = c(model_freq_ql, model_freq_ac),
  BIC_mean = c(BIC_ql, BIC_ac),
  BIC_sd = c(BIC_sd_ql, BIC_sd_ac),
  AUC_mean = c(AUC_ql, AUC_ac),
  AUC_sd = c(AUC_sd_ql, AUC_sd_ac)
)

# Display the table
top_models_table %>%
  dplyr::select(-Action) %>%
  group_by(Model) %>%
  arrange(BIC_mean, desc(State), Reward) %>%
  gt() %>%
  tab_stubhead(label = "Reward") %>%
  cols_label(
    BIC_mean = "BIC",
    AUC_mean = "AUC",
    # `ModelFrequency` = "Model Frequency"
  ) %>%
  sub_missing() %>%
  fmt_number(columns = c(BIC_mean, BIC_sd), decimals = 1) %>%
  fmt_number(columns = c(AUC_mean, AUC_sd), decimals = 3) %>%
  cols_merge(
    columns = c(BIC_mean, BIC_sd),
    pattern = "{1} ({2})"
  ) %>%
  cols_merge(
    columns = c(AUC_mean, AUC_sd),
    pattern = "{1} ({2})"
  ) %>%
  tab_spanner(
    label = "Mean (SD)",
    columns = c(BIC_mean, AUC_mean)
  )
  # fmt_percent(columns = c(ModelFrequency), decimals = 1)
```

#### Hybrid Models Comparison

In pursuit of a more comprehensive understanding of our model space, we turn to the investigation of hybrid models that blend elements from both the Logit and RL frameworks. These hybrids offer a potentially more nuanced approach to representing the complexities of learning behavior, aiming to combine the strengths of logistic regression with the dynamic adaptability inherent in reinforcement learning techniques.

The comparative analysis of these models is conducted via Hierarchical Bayesian Inference. @tbl-CBM-second shows the performance for different reward and state contexts across the logit model, the corresponding reinforcement learning model (Q-learning model in state-free cases and Actor-Critic in state-based cases), and two hybrid configurations (i.e., Logit and RL hybrids, Q-learning and Actor-Critic hybrids). Our evaluation suggests that any potential performance gain from hybridizing do not outweigh the cost of increased model complexity.

```{r}
#| label: tbl-CBM-second
#| tbl-cap: "Comparative fit of hybrid models. The columns Logit, RL, Logit-RL Hybrid, and QL-AC Hybrid present the proportion of data best explained by each model configuration when the four models are compared. The top model (highest percentage) from each row is then included in a final comparison, with the results displayed in the 'Top Model Frequency' column. All models are based on the action 'Pedagogical Knowledge.' The QL-AC Hybrid model combines predictions from an Actor-Critic model with a Q-learning model that shares the same action and reward components."

# Load the MATLAB files for the comparative models
comp_models <- list(
  readMat('CBM/zearn_results/comp_results/hbi/hbi_compare_3.mat'),
  readMat('CBM/zearn_results/comp_results/hbi/hbi_compare_7.mat'),
  readMat('CBM/zearn_results/comp_results/hbi/hbi_compare_51s.mat'),
  readMat('CBM/zearn_results/comp_results/hbi/hbi_compare_13s.mat')
)
BIC_comp <- sapply(comp_models, FUN = function(x) {
  apply(x[["cbm"]][[5]][[11]], 2, mean, na.rm = TRUE)
}) %>% t()
AUC_comp <- sapply(comp_models, FUN = function(x) {
  apply(x[["cbm"]][[5]][[8]], 2, mean, na.rm = TRUE)
}) %>% t()
model_freq <- data.frame(cbind(BIC_comp,AUC_comp))
names(model_freq) <- c(paste0("BIC_",c("Logit", "RL", "Hybrid")),
                       paste0("AUC_",c("Logit", "RL", "Hybrid")))

model_freq$Reward <- c(
  "Struggles", # QL 3
  "Activity", # QL 7
  "Activity", # AC 51
  "Badges" # AC 13
)
model_freq$State <- c(
  NA, # QL
  NA,
  "Struggles, No. Students", # AC 51
  "Struggles, No. Students, Activity" # AC 13
)

# Find the maximum value in each row (excluding the "Top Model Frequency" column)
min_bics <- apply(model_freq[, 1:3], 1, min)
max_aucs <- apply(model_freq[, 4:6], 1, max)

model_freq %>%
  gt(groupname_col = "Reward", 
     row_group_as_column = T) %>%
  tab_spanner_delim(delim = "_") %>%
  tab_stubhead(label = "Reward") %>%
  cols_move_to_start(State) %>%
  sub_missing() %>%
  fmt_number(columns = gt::starts_with("BIC"), 
              decimals = 1) %>%
  fmt_number(columns = gt::starts_with("AUC"), 
              decimals = 3) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = BIC_Logit,
                                   rows = BIC_Logit == min_bics)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = BIC_RL,
                                   rows = BIC_RL == min_bics)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = BIC_Hybrid,
                                   rows = BIC_Hybrid == min_bics)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = AUC_Logit,
                                   rows = AUC_Logit == max_aucs)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = AUC_RL,
                                   rows = AUC_RL == max_aucs)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = AUC_Hybrid,
                                   rows = AUC_Hybrid == max_aucs)) %>%
  tab_footnote(
    footnote = "Q-learning model in state-free case and Actor-Critic in state-dependent case.",
    locations = cells_column_labels(columns = gt::ends_with("RL"))
  ) %>%
  tab_footnote(
    footnote = "Combines predictions of Logit and RL models through a linear combination.",
    locations = cells_column_labels(columns = gt::ends_with("Hybrid"))
  )
```

#### Top Model Selection

@tbl-CBM-second also shows that Q-learning consistently outperforms logistic regression and actor-critic models in BIC scores. As such, we further analyze the fit from the top models from this table to the entire dataset. That is, we compare both Q-learning models and the top actor-critic model with their corresponding logistic regression. The histogram of teacher-specific log-likelihoods in @fig-loglik-histogram reveals that the Q-learning model with "Struggles" as the reward has the lowest BIC scores.

```{r}
#| label: fig-loglik-histogram
#| fig-cap: "Distribution of teacher-specific Bayesian Information Criteria (BIC). The histogram displays the spread of individual teachers' log-likelihoods, with higher values indicating better fit. The dashed line represents the median log-likelihood across all teachers."

# Load the full model data
full_model <- list(
  readMat("CBM/zearn_results/top_results/hbi_compare_3.mat"),
  readMat("CBM/zearn_results/top_results/hbi_compare_7.mat"),
  readMat("CBM/zearn_results/top_results/hbi_compare_51.mat")
)
BIC_full <- sapply(full_model, FUN = function(x) {
  apply(x[["cbm"]][[5]][[11]], 2, mean, na.rm = TRUE)
}) %>% t()
AUC_full <- sapply(full_model, FUN = function(x) {
  apply(x[["cbm"]][[5]][[8]], 2, mean, na.rm = TRUE)
})
# # Extract log likelihoods
# model_freq_full <- full_model[["cbm"]][[5]][[5]]
# responsibility_full <- full_model[["cbm"]][[5]][[2]]
# model_classifier <- responsibility_full > 1/3

loglik_df <- data.frame(
  cbind(full_model[[1]][["cbm"]][[5]][[11]],
        full_model[[2]][["cbm"]][[5]][[11]],
        full_model[[3]][["cbm"]][[5]][[11]])
)
Model = c("Logit", "QL",
          "Logit", "QL",
          "Logit", "AC")
Action = c(rep("Pedagogical Knowledge", 6))
Reward = c("Struggles", "Struggles",
           "Activity", "Activity",
           "Activity", "Activity")
State = c(NA, NA, NA, NA,
          "Struggles, No. Students", "Struggles, No. Students")
names(loglik_df) <- paste0(Model,"_",Reward,"_",State)

loglik_df <- loglik_df %>%
  pivot_longer(cols = everything(),
               names_to = "Model_Reward_State",
               values_to = "BIC") %>%
  separate(Model_Reward_State, c("Model", "Reward", "State"), "_")

# Create a histogram of log likelihoods
ggplot(data = loglik_df, aes(x = BIC, fill = interaction(Model, Reward),
                             color = interaction(Model, Reward))) +
  geom_histogram(alpha = 0.8) +
  scale_fill_manual(values = c("Logit.Struggles" = brewer.pal(8, "Set2")[1],
                               "QL.Struggles" = brewer.pal(8, "Set2")[2],
                               "Logit.Activity" = brewer.pal(8, "Set2")[3],
                               "QL.Activity" = brewer.pal(8, "Set2")[4],
                               "AC.Activity" = brewer.pal(8, "Set2")[5]),
                    labels = c("AC, Activity",
                               "Logit, Activity",
                               "QL, Activity",
                               "Logit, Struggles",
                               "QL, Struggles")) +
  scale_color_manual(values = c("Logit.Struggles" = brewer.pal(8, "Dark2")[1],
                                "QL.Struggles" = brewer.pal(8, "Dark2")[2],
                                "Logit.Activity" = brewer.pal(8, "Dark2")[3],
                                "QL.Activity" = brewer.pal(8, "Dark2")[4],
                                "AC.Activity" = brewer.pal(8, "Dark2")[5]),
                    labels = c("AC, Activity",
                               "Logit, Activity",
                               "QL, Activity",
                               "Logit, Struggles",
                               "QL, Struggles")) +
  theme_minimal() +
  labs(x = "Individual BIC",
       y = "Frequency",
       fill = "Model, Reward",
       color = "Model, Reward") +
  guides(fill = guide_legend(title = "Model, Reward", nrows = 2),
         color = guide_legend(title = "Model, Reward", nrows = 2))

```

## Heterogeneity and Optimality

The spread of BIC scores in @fig-loglik-histogram also implies a diversity in model fit, which warrants further investigation into heterogeneity. We analyzed teacher-specific parameters to capture these individual differences. @fig-params-histogram provides a set of visualizations depicting relationships between teacher-specific parameters and various contextual factors. We observed differences in the cost parameter across poverty levels, with high-poverty schools showing lower cost estimates (a). The learning rate parameter (alpha) also varied across poverty levels (b), with high-poverty schools exhibiting lower learning rates. The number of weeks a teacher used the platform showed a small negative correlation with the estimated cost (r = -0.11, p < .001) (d), while it positively correlated with the inverse temperature parameter (tau) (r = 0.14, p = 1.5e-09) (e). When examining the relationship between the learning rate, inverse temperature, and student outcomes (c, f), we found a positive correlation between inverse temperature and both average badges (r = 0.17, p < .001) and average tower alerts (r = 0.1, p < .001). However, the learning rate did not significantly correlate with average badges earned (r = 0.03, p = 0.23) or average tower alerts (r = 0.0, p = .85).

```{r}
#| label: fig-params-histogram
#| fig-cap: "Histogram of Teacher-Specific Parameters"
#| fig-subcap:
#| - "Poverty x Cost"
#| - "Poverty x Alpha"
#| - "Student Outcomes x Gamma"
#| - "No. of Weeks x Cost"
#| - "No. of Weeks x Tau"
#| - "Student Outcomes x Tau"
#| layout-ncol: 3

# Load the full model data
validsubj <- as.logical(
  unlist(readMat("CBM/data/full_data.mat"))
  )
# Extract log likelihoods
BIC_ind_full <- full_model[[1]][["cbm"]][[5]][[11]]

# Classify each teacher according to the best-fit model
model_classifier <- BIC_ind_full[,1] - BIC_ind_full[,2] > 
  median(BIC_ind_full[,1] - BIC_ind_full[,2])
teacher_classification <- ifelse(model_classifier, 2, 1)
logit_params <- full_model[[1]][["cbm"]][[5]][[1]][[1]][[1]]
ql_params <- full_model[[1]][["cbm"]][[5]][[1]][[2]][[1]]
# Make min(teacher_classification) the baseline
load("CBM/data/classrooms_full.RData")
heterogeneity <- classrooms %>%
  cbind(
    data.frame(
      ModelType = factor(teacher_classification - min(teacher_classification), 
                         labels = c("Logit", "Q-learning"))
      )) %>%
  cbind(logit_params) %>%
  cbind(ql_params)
names(heterogeneity) <- c(
  "Classroom.ID", "Model",
  paste0("beta", 1:8),
  "alpha", "gamma", "tau", "ev_init", "cost"
  # "alpha_w", "alpha_theta", "gamma", "tau", "theta_init", "w_init", "cost"
)
heterogeneity <- heterogeneity %>%
  # mutate(across(c(alpha, gamma), ~ 1/(1+exp(-.)))) %>%
  # mutate(across(c(alpha_w, alpha_theta, gamma), ~ 1/(1+exp(-.)))) %>%
  # mutate(across(c(tau, cost), ~ exp(.))) %>%
  left_join(
    df %>%
      group_by(Classroom.ID, Teacher.User.ID) %>%
      summarize(
        `Charter School` = mean(charter.school, na.rm = TRUE),
        `Paid Account` = mean(school.account, na.rm = TRUE),
        `Students...Total` = mean(Students...Total, na.rm = TRUE),
        Action = mean(Frobenius.NNDSVD_teacher2>0, na.rm = TRUE),
        Reward = sum(Frobenius.NNDSVD_student1, na.rm = TRUE),
        State = sum(Frobenius.NNDSVD_student3, na.rm = TRUE),
        `Active Students` = sum(`Active.Users...Total`, na.rm = TRUE),
        `Avg. Badges` = sum(`Badges.per.Active.User`, na.rm = TRUE),
        `Avg. Tower Alerts` = sum(`Tower.Alerts.per.Tower.Completion`, na.rm = TRUE),
        Income = unique(income),
        Poverty = unique(poverty),
        `No. of Weeks` = n(),
        n_classes_by_teacher = median(teacher_number_classes),
        grade = first(Grade.Level)
        ), by = "Classroom.ID"
    ) %>%
  mutate(across(c(Income, Poverty), as.ordered)) %>%
  rename(`Total Students` = `Students...Total`,
         `No. of Classes` = n_classes_by_teacher)

# Extract BIC and classify teachers based on median split
full_data <- heterogeneity %>%
  mutate(
    BIC = BIC_ind_full[,2]
  ) %>%
  filter(!is.na(Teacher.User.ID)) %>%
  mutate(
    FitGroup = factor(BIC > median(BIC),
                      levels = c(FALSE, TRUE),
                      labels = c("Bad Fit", "Good Fit"))
  ) %>%
  mutate(
    `Paid Account` = factor(`Paid Account`, levels = c(0, 1),
                            labels = c("No", "Yes"))
  )

# Update color palette
color_palette <- brewer.pal(4, "Set2")
color_palette_dark <- brewer.pal(4, "Dark2")

# Create a transformation for the inverse logit function
transform_inv_logit <- new_transform(
  name = "inv_logit",
  transform = function(x) 1/(1+exp(-x)),
  inverse = function(x) log(x/(1-x))
)


# Create a data frame for correlation tests
cor_tests <- data.frame(
  Variable = c("Avg. Badges", "Avg. Tower Alerts"),
  Alpha_cor = c(
    cor.test(full_data$alpha, full_data$`Avg. Badges`)$estimate,
    cor.test(full_data$alpha, full_data$`Avg. Tower Alerts`)$estimate
  ),
  Alpha_p = c(
    cor.test(full_data$alpha, full_data$`Avg. Badges`)$p.value,
    cor.test(full_data$alpha, full_data$`Avg. Tower Alerts`)$p.value
  ),
  Tau_cor = c(
    cor.test(full_data$tau, full_data$`Avg. Badges`)$estimate,
    cor.test(full_data$tau, full_data$`Avg. Tower Alerts`)$estimate
  ),
  Tau_p = c(
    cor.test(full_data$tau, full_data$`Avg. Badges`)$p.value,
    cor.test(full_data$tau, full_data$`Avg. Tower Alerts`)$p.value
  ),
  Gamma_cor = c(
    cor.test(full_data$gamma, full_data$`Avg. Badges`)$estimate,
    cor.test(full_data$gamma, full_data$`Avg. Tower Alerts`)$estimate
  ),
  Gamma_p = c(
    cor.test(full_data$gamma, full_data$`Avg. Badges`)$p.value,
    cor.test(full_data$gamma, full_data$`Avg. Tower Alerts`)$p.value
  )
)

## Grid

## [1,1:2]

ggplot(data = full_data %>%  
         filter(!is.na(Poverty)) %>%
         mutate(cost = exp(cost)),
       aes(x = Poverty, y = cost)) +
  geom_violin(alpha = 0.6, scale = "count", fill = color_palette[3]) +
  geom_boxplot(width = 0.025, outlier.shape = NA) +
  theme_minimal() +
  labs(x = "Poverty", y = "Cost") +
  stat_compare_means(label = "p.signif",
                     comparisons = list(
                       c("0-40% (Low)", "40-75% (Mid-High)"),
                       c("0-40% (Low)", "75%+ (High)"),
                       c("40-75% (Mid-High)", "75%+ (High)")))

ggplot(data = full_data %>%  
         filter(!is.na(Poverty)) %>%
         mutate(alpha = 1/(1+exp(-alpha))),
       aes(x = Poverty, y = alpha)) +
  geom_violin(alpha = 0.6, scale = "count", fill = color_palette[4]) +
  geom_boxplot(width = 0.025, outlier.shape = NA) +
  # coord_trans(y = "log") +
  theme_minimal() +
  labs(x = "Poverty", y = "Alpha") + 
  stat_compare_means(label = "p.signif",
                     comparisons = list(
                       c("0-40% (Low)", "40-75% (Mid-High)"),
                       c("0-40% (Low)", "75%+ (High)"),
                       c("40-75% (Mid-High)", "75%+ (High)")))

## [1,3]

# ggplot(data = full_data, aes(x = 1/(1+exp(-alpha)))) +
#   geom_point(aes(y = `Avg. Badges`),
#              color = color_palette[2], alpha = 0.1) +
#   geom_smooth(aes(y = `Avg. Badges`), method = "lm",
#               color = color_palette[2], fill = color_palette[2]) +
#   geom_point(aes(y = `Avg. Tower Alerts`),
#              color = color_palette[1], alpha = 0.1) +
#   geom_smooth(aes(y = `Avg. Tower Alerts`), method = "lm",
#               color = color_palette[1], fill = color_palette[1]) +
#   scale_x_continuous(name = "Alpha", transform = "logit") +
#   scale_y_continuous(name = "Avg. Badges / Tower Alerts") +
#   theme_minimal() +
#   labs(color = "Variable") +
#   geom_text(data = cor_tests[1,], 
#             aes(x = min(1/(1+exp(-full_data$alpha))),
#                 y = mean(full_data$`Avg. Badges`), 
#                 label = paste0("r = ", round(Alpha_cor, 2),
#                                ", p = ", signif(Alpha_p, 2))),
#             color = color_palette[2], vjust = -1.5, hjust = 0) +
#   geom_text(data = cor_tests[2,],
#             aes(x = min(1/(1+exp(-full_data$alpha))),
#                 y = mean(full_data$`Avg. Tower Alerts`),
#                 label = paste0("r = ", round(Alpha_cor, 2),
#                                ", p = ", signif(Alpha_p, 2))),
#             color = color_palette[1], vjust = -1.5, hjust = 0)

ggplot(data = full_data, aes(x = exp(gamma))) +
  geom_point(aes(y = `Avg. Badges`), size = 0.4,
             color = color_palette[2], alpha = 0.19,
             position = position_jitter(width = 0.1, height = 0)) +
  geom_smooth(aes(y = `Avg. Badges`), method = "lm",
              color = color_palette[2], fill = color_palette[2]) +
  geom_point(aes(y = `Avg. Tower Alerts`), size = 0.4,
             color = color_palette[1], alpha = 0.19,
             position = position_jitter(width = 0.1, height = 0)) +
  geom_smooth(aes(y = `Avg. Tower Alerts`), method = "lm",
              color = color_palette[1], fill = color_palette[1]) +
  scale_x_continuous(name = "Gamma", transform = "log") +
  scale_y_continuous(name = "Avg. Badges / Tower Alerts") +
  theme_minimal() +
  labs(color = "Variable") +
  geom_text(data = cor_tests[1,], 
            aes(x = min(exp(full_data$gamma)),
                y = mean(full_data$`Avg. Badges`), 
                label = paste0("r = ", round(Gamma_cor, 2),
                               ", p ", ifelse(Gamma_p > 0.001,
                                                paste0("= ", signif(Gamma_p, 2)),
                                                " < 0.001"))),
            color = color_palette_dark[2], vjust = 1.5, hjust = 0) +
  geom_text(data = cor_tests[2,],
            aes(x = min(exp(full_data$gamma)),
                y = mean(full_data$`Avg. Tower Alerts`),
                label = paste0("r = ", round(Gamma_cor, 2),
                               ", p ", ifelse(Gamma_p > 0.001,
                                                paste0("= ", signif(Gamma_p, 2)),
                                                " < 0.001"))),
            color = color_palette_dark[1], vjust = 1.5, hjust = 0)


# [2,1:2]

ggplot(data = full_data, aes(x = `No. of Weeks`, y = exp(cost))) +
  geom_point(alpha = 0.2, position = position_jitter(width = 0, height = 0.01)) +
  geom_smooth(method = "lm", color = color_palette[3], fill = color_palette[3]) +
  theme_minimal() +
  labs(x = "No. of Weeks", y = "Cost") +
  geom_text(aes(x = max(`No. of Weeks`), y = min(exp(cost)),
                label = paste0("r = ",
                               round(cor.test(full_data$`No. of Weeks`,
                                              full_data$cost)$estimate, 2),
                               ", p ",
                               ifelse(cor.test(full_data$`No. of Weeks`,
                                               full_data$cost)$p.value > 0.001,
                                                paste0("= ", signif(
                                                  cor.test(full_data$`No. of Weeks`,
                                                           full_data$cost)$p.value, 2)),
                                                " < 0.001"))),
                color = color_palette_dark[3],
            vjust = -1.5, hjust = 1)

ggplot(data = full_data, aes(x = `No. of Weeks`, y = exp(tau))) +
  geom_point(alpha = 0.2, position = position_jitter(width = 0, height = 0.1)) +
  geom_smooth(method = "lm", color = color_palette[4], fill = color_palette[4]) +
  scale_y_continuous(trans = "log", breaks = c(1, 2, 4, 8, 16, 32)) +
  theme_minimal() +
  labs(x = "No. of Weeks", y = "Tau") +
  geom_text(aes(x = max(`No. of Weeks`), y = min(exp(tau)),
                label = paste0("r = ", round(cor.test(full_data$`No. of Weeks`,
                                                      full_data$tau)$estimate, 2),
                               ", p ",
                               ifelse(cor.test(full_data$`No. of Weeks`,
                                               full_data$tau)$p.value > 0.001,
                                                paste0("= ", signif(
                                                  cor.test(full_data$`No. of Weeks`,
                                                           full_data$tau)$p.value, 2)),
                                                " < 0.001"))),
                color = color_palette_dark[4],
            vjust = -1.5, hjust = 1)

# ggplot(data = full_data %>% mutate(gamma = 1/(1+exp(-gamma))), 
#        aes(x = `Paid Account`, y = gamma)) +
#   geom_violin(alpha = 0.6, scale = "count", fill = color_palette[3]) + 
#   geom_boxplot(width = 0.05, outlier.shape = NA) +
#   theme_minimal() +
#   labs(x = "Paid Account", y = "Gamma") +
#   stat_compare_means(comparisons = list(c("No", "Yes")), label = "p.signif")
# 
# ggplot(data = full_data %>% mutate(cost = exp(cost)),
#        aes(x = `Paid Account`, y = cost)) +
#   geom_violin(alpha = 0.6, scale = "count", fill = color_palette[3]) +
#   geom_boxplot(width = 0.05, outlier.shape = NA) + 
#   theme_minimal() +
#   labs(x = "Paid Account", y = "Cost") +
#   stat_compare_means(comparisons = list(c("No", "Yes")), label = "p.signif")

# [2,3]

ggplot(data = full_data, aes(x = exp(tau))) +
  geom_point(aes(y = `Avg. Badges`), size = 0.4,
             color = color_palette[2], alpha = 0.19,
             position = position_jitter(width = 0.1, height = 0)) +
  geom_smooth(aes(y = `Avg. Badges`), method = "lm",
              color = color_palette[2], fill = color_palette[2]) +
    geom_point(aes(y = `Avg. Tower Alerts`), size = 0.4,
             color = color_palette[1], alpha = 0.19,
             position = position_jitter(width = 0.1, height = 0)) +  
  geom_smooth(aes(y = `Avg. Tower Alerts`), method = "lm",
              color = color_palette[1], fill = color_palette[1]) +
  scale_x_continuous(name = "Tau", transform = "log",
                     breaks = c(1, 2, 4, 8, 16, 32)) +
  scale_y_continuous(name = "Avg. Badges / Tower Alerts") +
  theme_minimal() +
  labs(color = "Variable") +
  geom_text(data = cor_tests[1,],
            aes(x = min(exp(full_data$tau)),
                y = mean(full_data$`Avg. Badges`),
                label = paste0("r = ", round(Tau_cor, 2),
                               ", p ", ifelse(Tau_p > 0.001,
                                                paste0("= ", signif(Tau_p, 2)),
                                                " < 0.001"))),
            color = color_palette_dark[2], vjust = 0.5, hjust = 0) +
  geom_text(data = cor_tests[2,],
            aes(x = min(exp(full_data$tau)),
                y = mean(full_data$`Avg. Tower Alerts`),
                label = paste0("r = ", round(Tau_cor, 2),
                               ", p ", ifelse(Tau_p > 0.001,
                                                paste0("= ", signif(Tau_p, 2)),
                                                " < 0.001"))),
            color = color_palette_dark[1], vjust = 0.5, hjust = 0)

```

We further investigated the heterogeneity in teacher behavior by comparing classrooms based on the relative fit of the logit and Q-learning models. Classrooms were classified into two groups using a median split of the distribution of the differences in Bayesian Information Criterion (BIC) scores between the two models, i.e., BIC(logit) - BIC(Q-learning). Q-learning demonstrated a greater advantage over the logistic regression model in classrooms with a higher number of weeks and average weekly student activity (see @tbl-CBM-teachers). Additionally, classrooms were divided into 'Good Fit' and 'Bad Fit' groups based on a median split of their Q-learning BIC scores. Classrooms in the 'Good Fit' group exhibited higher student activity and overall weekly usage by teachers (see @tbl-CBM-teachers-full).

To understand what makes a teacher effective in helping students complete their lessons, we examined teachers' performance across the parameters from the previous hierarchical model. We were especially interested in the learning rate ("Alpha") and the inverse temperature ("Tau") because of their potential impact on behavior adaptation and decision consistency. We found that tau, but not alpha, correlated positively to the number of weekly Badges earned by the average student, i.e., average lesson completion, and Tower Alerts, i.e., students struggling with the material. Further, the starting Q-values negatively correlated with Badges, and the discount factor ("Gamma") was positively correlated with Tower Alerts (see @tbl-optimality and @tbl-optimality-2 for the models controlling the number of active students, the number of classes taught by the teacher, the grade level, the number of weeks, the poverty level, the income level, whether the school is a charter school, and whether the school has a paid account).

We then regressed classroom characteristics on the estimated model parameters and found several notable relationships (see @fig-heterogeneity-reg). The cost parameter was negatively associated with income level (b = -0.13, p < .05) and the number of weeks (b = -0.021, p < .001), and positively associated with poverty level (b = 0.18, p < .01). The learning rate (alpha) was positively correlated with income level (b = 0.094, p < .05) and negatively correlated with poverty level (b = -0.1, p < .05). The inverse temperature (tau) was negatively related to income level (b = -0.18, p < .001) and positively related to having a paid account (b = 0.24, p < .001) and the number of weeks (b = 0.016, p < .01). The discount factor (gamma) was negatively associated with the number of weeks (b = -0.016, p < .01). Lastly, the initial Q-value was positively associated with income level (b = 0.17, p < .001) and negatively associated with having a paid account (b = -0.16, p < .05).

<!-- ## Posterior Predictive Checks -->

```{r}
#| eval: false
#| label: fig-timelines
#| fig-cap: ""

# Load the data
hbi_ac_data <- readMat('CBM/zearn_results/top_results/hbi_ac_data.mat')
model_fit <- readMat('CBM/zearn_results/top_results/hbi_ac.mat')
teacher_specific_params <- model_fit[["cbm"]][[5]][[1]][[1]][[1]]

simulate_predictions <- function(parameters, subj) {
  alpha_w <- 1 / (1 + exp(-parameters[1]))  # Learning rate for w (critic)
  alpha_theta <- 1 / (1 + exp(-parameters[2]))  # Learning rate for theta (actor)
  gamma <- 1 / (1 + exp(-parameters[3]))  # Discount factor
  tau <- exp(parameters[4])
  theta_init <- parameters[5]
  w_init <- parameters[6]
  cost <- exp(parameters[7])
  
  # Unpack data
  Tsubj <- length(subj[[1]][[2]])
  choice <- subj[[1]][[2]]
  outcome <- subj[[1]][[5]]
  state <- cbind(1, as.matrix(subj[[1]][[7]]))
  week <- subj[[1]][[9]][[1]]
  
  # Initialize
  w <- w_init
  theta <- theta_init
  log_probabilities <- rep(0, Tsubj)
  
  # Loop through trials
  for (t in 1:Tsubj) {
    if (week[t] == 0) break  # End loop if week is zero
    s <- state[t, ]
    a <- choice[t]
    o <- outcome[t]
    
    # Actor: Compute policy (log probability of taking the action)
    product <- sum(s * theta * tau)
    # Handle numerical issues for large values of theta
    if (product < -8) {
      log_probabilities[t] <- log_probabilities[t] - product * a
    } else if (product > 8) {
      log_probabilities[t] <- log_probabilities[t] + (1 - product) * (1 - a)
    } else {
      log_probabilities[t] <- log_probabilities[t] - log(1 + exp(-product)) * a - log(1 + exp(product)) * (1 - a)
    }
    
    # Critic: Compute TD error (delta)
    delta <- (o - cost)  # Basic TD error, without considering future states
    
    # Update weights
    theta <- theta + alpha_theta * gamma ^ (week[t] - week[1]) * (tau * s) / (1 + exp(product)) * delta
    w <- w + alpha_w * s * delta
  }
  
  # Sum log probabilities to get log-likelihood
  log_likelihood <- sum(log_probabilities)
  
  return(list(predictions = choice, log_likelihood = log_likelihood))
}

# Prepare the data for plotting (replace with actual variables and data)
long_df <- as.data.frame(hbi_ac_data$Variable) %>%
  mutate(Week = rep(1:nrow(.), each = ncol(.)),
         Value = as.vector(t(.)))

# Simulate predictions using group mean parameters
predictions <- simulate_predictions(group_mean_transformed, long_df)

# Variables to plot
variables_to_plot <- names(hbi_ac_data$Variable)

# Create individual plots
plots <- lapply(variables_to_plot, plot_variable, data = long_df)

# Reshape the data to long format
long_df <- df %>%
  select(MDR.School.ID, week, Active.Users...Total, Sessions.per.Active.User, 
         Minutes.per.Active.User, Badges.per.Active.User, 
         Boosts.per.Tower.Completion, Tower.Alerts.per.Tower.Completion, 
         FrobeniusNNDSVD1, FrobeniusNNDSVD2, FrobeniusNNDSVD3,
         poverty, school.account) %>%
  group_by(MDR.School.ID, week, poverty, school.account) %>%
  summarize(across(where(is.numeric), ~ mean(., na.rm = TRUE))) %>%
  gather(key = "Variable", value = "Value",
         -MDR.School.ID, -week, -poverty, -school.account)

# Function to create a plot for each variable with error ribbons
plot_variable <- function(variable_name, var) {
  # Calculate overall average and standard error
  avg_data <- long_df %>%
    filter(Variable == variable_name) %>%
    filter(!is.na({{var}})) %>%
    group_by({{var}}, week) %>%
    summarize(
      Avg = mean(Value, na.rm = TRUE),
      SE = sd(Value, na.rm = TRUE)/sqrt(n()),
      CI_low = Avg - SE,
      CI_high = Avg + SE
    )
  
  # Calculate 2 standard deviations from the overall data
  y_high <- quantile(avg_data$Value[avg_data$Variable == variable_name],
                     0.85, na.rm = T)
  y_low <- quantile(avg_data$Value[avg_data$Variable == variable_name],
                    0.15, na.rm = T)

  ggplot(avg_data, aes(x = week, y = Avg, group = {{var}})) +
    geom_line(aes(color = {{var}})) +
    geom_ribbon(data = avg_data,
                aes(x = week, y = Avg, group = {{var}}, alpha = 0.2,
                    fill = {{var}}, ymin = CI_low, ymax = CI_high)) +
    theme_minimal() +
    labs(title = variable_name, x = "Time", y = "Value") +
    theme(legend.position = "none") +
    coord_cartesian(ylim = c(y_low, y_high))  # Adjust y-axis range
}

# Variables to plot
variables_to_plot <- c(
  "Active.Users...Total", "Sessions.per.Active.User",
  "Minutes.per.Active.User", "Badges.per.Active.User",
  "Boosts.per.Tower.Completion", "Tower.Alerts.per.Tower.Completion", 
  "FrobeniusNNDSVD1", "FrobeniusNNDSVD2", "FrobeniusNNDSVD3"
  )

# Create individual plots
plots <- lapply(variables_to_plot, plot_variable, var = school.account)

# Combine the plots into a panel
do.call(grid.arrange, c(plots, ncol = 3))

```

# Discussion

Our study aimed to unravel the complex and adaptive nature of teacher behavior within the Zearn Math online platform. By leveraging the power of reinforcement learning (RL) models, particularly the hierarchical Q-learning approach, we uncover compelling evidence that teachers are not merely adhering to a fixed set of pedagogical strategies but are actively learning and adapting based on feedback from student performance.

## Characterizing Teacher Behavior

The superior fit of the hierarchical Q-learning model underscores the importance of accounting for individual teacher differences in learning rates and decision-making processes. The model's parameters serve as a window into the diverse ways teachers navigate the digital learning environment, offering a more nuanced understanding than traditional, static educational practice models.

## Impact of Estimated RL Parameters

Crucially, we find that teachers with higher inverse temperature values had students with superior outcomes, indicating more consistent decision-making. This trade-off suggests a teacher's ability to balance using new teaching strategies (exploration) and sticking with known effective strategies (exploitation). @morrison2019 noted that teachers exhibited varied levels of preparedness for implementing the Zearn Math curriculum, with just under half of the teachers reporting feeling adequately prepared for implementation. This lack of preparation could influence how effectively teachers navigate the exploration-exploitation trade-off, impacting student outcomes. In contrast, the starting Q-value is negatively associated with Badges earned, implying that teachers who initially overestimate the value of certain actions may struggle to adapt their strategies to optimize student outcomes.

Interestingly, while the discount factor shows a significant negative association with Badges, this effect disappears when controlling for additional variables. This result highlights the importance of considering the broader context in which teachers operate, such as school characteristics and student demographics. When examining the impact of RL parameters on student struggles, as measured by average weekly Tower Alerts per student, we find that the discount factor is consistently negatively associated with Tower Alerts. This association suggests that teachers who place greater value on future rewards are more effective in preventing student struggles.

These findings align with qualitative evidence from @knudsen2020, where teachers expressed the value of Zearn Math's multifaceted instructional approaches. A 3rd-grade Zearn teacher stated, "I like that Zearn provides several strategies to get to the answer...you see the problems; you see what you need to hit on and stress the first time around." The ability to adapt teaching strategies based on feedback, akin to higher tau values in our RL models, is a valuable attribute in promoting student achievement. However, the relationship between teacher characteristics and student outcomes is complex, with veteran teachers relying more heavily on traditional methods while novice teachers blend innovative and traditional practices [@knudsen2020].

## Influence of Teacher and School Background

Our investigation also reveals the profound influence of socioeconomic factors on teachers' interactions with Zearn Math. Teachers in lower-income and high-poverty schools may perceive higher costs associated with implementing new teaching strategies, possibly due to resource constraints or lack of adequate training, underscoring the challenges in adopting new educational technologies in disadvantaged contexts.

In contrast, teachers in more affluent regions exhibit greater adaptability (learning rate) in adjusting their pedagogical approaches based on student feedback. This finding suggests that access to resources and support plays a crucial role in teachers' ability to effectively use online learning platforms and adapt their teaching methods to meet students' needs.

We also observe that teachers with paid Zearn Math accounts demonstrate more consistent decision-making patterns (higher inverse temperatures). This may be attributed to the structured support and training provided by the platform, which helps teachers navigate the challenges of implementing new technologies and pedagogies.

These findings underscore the complex interplay between socioeconomic factors, school characteristics, and teachers' decision-making processes in the context of online learning platforms. They highlight the need for targeted interventions and support to ensure that all teachers, regardless of their school's socioeconomic status, have the resources and training necessary to effectively adapt their teaching strategies and promote student success.

## Implications for Teachers and Schools

Our Reinforcement Learning model highlights the dynamic, adaptive nature of teaching and the need for ongoing professional development and supportive learning environments. The heterogeneity in optimal teaching strategies across educators underscores the importance of personalized training that builds on individual strengths. Policymakers and educational leaders should prioritize allocating resources and providing professional development opportunities to teachers, especially in disadvantaged communities.

Moreover, our results shed light on the systemic educational disparities across different school contexts and emphasize the importance of developing targeted policies and interventions to bridge these gaps. By modeling the factors that influence teachers' decision-making and adaptability, RL can inform the design of interventions to enhance student achievement. For instance, interventions could help teachers improve their learning rates or better balance exploration and exploitation in their teaching strategies.

## Future Directions

While our findings contribute significantly to educational technology and teacher behavior analysis, they are not without limitations. While the focus on Zearn Math provides a rich dataset for analysis, it limits the generalizability of our conclusions. Future research should explore the applicability of our findings across different populations, educational platforms, and learning environments. Additionally, our study opens new avenues for integrating RL models with other analytical approaches, such as machine learning and large language models, to further enhance our understanding of effective teaching strategies in digital contexts.

Future research holds immense potential to explore the applicability of advanced RL models across diverse educational contexts and populations. They should also integrate a broader spectrum of variables, including teacher background and training, to enrich the understanding of teaching and learning's multifaceted nature. Additionally, exploring novel RL models and methodologies promises to uncover more profound insights into the dynamics of educational technologies, guiding the development of more effective and equitable learning environments.

In sum, our study marks a significant stride in applying RL to elucidate the complex, dynamic behaviors of teachers in online learning environments. The insights gleaned from this work pave the way for more personalized, equitable, and evidence-based approaches to digital education, ultimately fostering better outcomes for all students.

# Materials and Methods

| **Step**                 | **Method**                                                                  | **Software/Tools**                                               |
|------------------|-----------------------------|-------------------------|
| Data Preprocessing       | Cleaning, normalization                                                     | R [@rcoreteam2024]                                               |
| Dimensionality Reduction | Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF) | Python (`scikit-learn`) [@pedregosa2011]                         |
| Feature Selection        | Regression analysis                                                         | R (`fixest` package) [@berge2018]                                |
| Analytical Methods       | Q-learning, Actor-Critic Model Estimation                                   | R, Matlab (`CBM` package for Laplace approximation) [@piray2019] |
| Statistical Analysis     | Hierarchical Bayesian Inference                                             | Matlab (`CBM` package for expectation-maximization algorithm)    |
| Model Evaluation         | Heterogeneity analyses of model performance across teachers                | R                                                                |

: Analytical steps employed in the study. {#tbl-methods}

## Data

Zearn provided administrative data for teachers and students, spanning across the 2020-2021 academic year. Teacher activity is time-stamped to the second and includes the time spent on the platform and specific actions taken. On the other hand, student data is aggregated at the classroom-week level due to data privacy considerations. As such, we aggregated the teacher data to the classroom-week unit of analysis. This level of granularity still enables us to capture the temporal dynamics of teacher-student interactions and their subsequent influence on student achievement.

```{r}
#| label: tbl-summary
#| tbl-cap: "Summary statistics by school. The table presents the mean, median, standard deviation (SD), minimum, and maximum values for the number of teachers, total students, and average weeks of active engagement (across all classrooms within a school)."

gt_school_sum

```

The dataset includes `r N_class_pre` classrooms and `r N_teachers_pre` educators, with an average of `r avg_std_pre` students per classroom. Classrooms and teachers are also linked to a school, and @tbl-summary provides a summary of the number of students, teachers, and weeks per school (see also @fig-income-dist for the distributions of school median poverty and income levels).

### Preprocessing and Exclusion criteria

We focus our analysis on the teachers who most likely take advantage of a wide range of resources on the platform. Thus, we selected teachers who consistently use the platform and work in traditional school settings. First, we selected virtual classrooms with at least five active students weekly, filtering out parents or tutors who may use Zearn outside the classroom setting. We also removed teachers with more than four classrooms and those who logged in for less than 16 weeks (@fig-classroom-weeks reveals that a non-negligible number of classrooms has less than 3 to 4 months of data). Finally, we excluded classrooms in the 6th to 8th grades, as they represent only a small proportion of the data. @tbl-classroom-summary summarizes the refined dataset, providing a snapshot of the key variables of interest. Their means and standard deviations (SD) are computed for each grade level and overall (across all grades).

```{r}
#| label: fig-classroom-weeks
#| fig-cap: "Histogram of number of weeks of data per classroom. Most classrooms include a full year (52 weeks) of data. A smaller but significant subset of classrooms has less than 18 weeks of data. The dashed line acts as a threshold that excludes a notable segment of classrooms from further analysis. The lack of data between these two peaks suggests distinct patterns of usage. Some classrooms consistently use the platform throughout the academic year, while others show sporadic engagement, possibly reflecting trial periods or intermittent usage."

fig_classroom_weeks

```

```{r, results='asis'}
#| label: tbl-classroom-summary
#| tbl-cap: "Classroom engagement metrics by grade level. The table presents the means and standard deviations (in parentheses) of the following averages: minutes spent on the platform per student per week, badges earned per student per week (indicating lesson completion), Tower Alerts per lesson completion (indicating student struggle), and minutes spent on the platform per teacher per week."

gt_classroom_sum

```

## Operationalizing Actions, Rewards, and States

### Teacher Actions

Teacher actions encompass a broad spectrum, from platform log-ins to resource downloads and specific instructional activities. @tbl-teacher-variables provides a list of the actions available in the data.

### Reward and State Variables
<!-- Reconsider the use of the term "State Variables" -- if you're going to keep it in, make sure you define it-->

Reward and state variables in Reinforcement Learning (RL) models capture the dynamics of the environment in which learning and actions occur. The Zearn platform captures these variables from student activity and performance data, providing a quantifiable snapshot of classroom engagement and learning challenges. The student variables in our data are:

1.  "Active Students" directly measures classroom engagement, representing the number of students actively logging in to complete digital lessons within a given week [@zearn2022].

2.  "Student Logins" functions as an attendance roster, tallying the frequency of students entering the platform, potentially serving as an engagement metric [@zearnaf].

3.  "Badges (on grade)" and "Badges" per active user metrics offer insights into the level of curriculum mastery. They reflect the number of new lessons completed weekly at students' grade level and in general. Accumulating badges can serve as a reward signal in an RL model, indicating students' progress and pace through the curriculum [@zearnae].

4.  "Minutes per active student" variable measures students' time on the platform, potentially correlating with their focus and learning progress. Once routines are established, teachers can use this metric to monitor whether students meet their expected weekly minutes [@zearn2022].

5.  "Tower Alerts" signal instances when students repeatedly encounter difficulties within the same lesson, prompting the platform to notify teachers. In the context of RL models, Tower Alerts could be viewed as a negative reinforcement signal, highlighting areas where students may require additional support or intervention to improve learning outcomes [@zearnad].
<!-- Can you more concretely explain how the terms "state" and "reward" correspond to your list of variables above?-->

### Dimensionality Reduction

First, we standardized the dataset by z-scoring the variables of interest at the school level (using school-wide means and standard deviations). We performed NMF and evaluated the data's reconstruction accuracy and cluster separation using, respectively, the sum of squared residuals (a measure of the difference between the original data and the reconstructed data) and silhouette scores (a measure of how similar an object is to its cluster compared to other clusters [@rousseeuw1987]).

We calculate the silhouette score with the formula $(b - a) / \max(a, b)$, where $a$ is the average distance within a cluster and $b$ is the average distance to the nearest neighboring cluster. This score ranges from -1 to 1, with higher values indicating a data point is well-matched to its cluster and poorly matched to neighboring clusters.

### Nonnegative Matrix Factorization (NMF) Methodology

Let the original matrix ($\mathbf{X}$) be a detailed description of all the teachers' (or students') behaviors. Each row in the matrix represents a unique teacher-week (or classroom-week), and each column represents a specific behavior or action. The entry in a specific row and column corresponds to the frequency of that behavior for that particular teacher-week (or classroom-week). We then estimate $\mathbf{X} \simeq \mathbf{W}\mathbf{H}$, such that we minimize the following:

$$
\left\| \mathbf{X} - \mathbf{W}\mathbf{H} \right\| , \mathbf{W} \geq 0, \mathbf{H} \geq 0.
$$

We used two different loss functions (Frobenius norm and Kullback-Leibler divergence) and two different initialization methods (nonnegative double singular value decomposition (NNDSVD) and NNDSVD with zeros filled with the average of the input matrix (NNDSVDA)). The resulting matrices are:

1.  Basis Matrix ($\mathbf{W}$): This matrix represents underlying behavior patterns. Each column is a "meta-behavior" or a group of behaviors occurring together.
2.  Mixture Matrix ($\mathbf{H}$): This matrix shows the extent to which each "meta-behavior" is present in each teacher-week (or classroom-week). Each entry in this matrix represents the contribution of a "meta-behavior" to a particular behavior present in the data.

These matrices can reveal underlying patterns of behaviors (from the basis matrix) and how these patterns are mixed and matched in different teachers (from the mixture matrix). It allows us to assess the method's performance under varying configurations, with the sum of squared residuals and silhouette scores for comparison.

### Feature Selection

The general model formulation for state-free and state-based scenarios is as follows:

#### State-Free Model

```{=tex}
\begin{align*}
\text{Action}_t =& \ \sum_{i=1}^{L} \left( \beta_{i} R_{t-i} + \gamma_i \text{Action}_{t-i} + \sum_{j=i}^{L} \delta_{ij} (R_{t-i} \times \text{Action}_{t-j}) \right) \\
& + \mu_{\text{Teacher}} + \lambda_{\text{Week}} + \epsilon_t 
\end{align*}
```
<!-- Why should Rewardt-1 influence Action_t independent of the R*A term?-->
#### State-Based Model

```{=tex}
\begin{align*}
\text{Action}_t =& \ \sum_{i=1}^{L} \left( \beta_{i} R_{t-i} + \gamma_i \text{Action}_{t-i} + \sum_{j=i}^{L} \delta_{ij} (R_{t-i} \times \text{Action}_{t-j}) \right) \\
& + \phi \text{State}_{t-1} + \psi (\text{State}_{t-1} \times \text{Action}_{t-2}) + \mu_{\text{Teacher}} + \lambda_{\text{Week}} + \epsilon_t
\end{align*}
```
<!-- Should action be conditioned on state?-->

where $\text{Action}_t$ denotes the binary outcome at time $t$, $R_{t-i}$ and $\text{Action}_{t-i}$ represent the reward and action variables lagged by $i$ periods, and $L$ is the maximum lag considered. $\mu_{\text{Teacher}}$ and $\lambda_{\text{Week}}$ represent fixed effects for teachers and weeks, respectively.


#### Varying Coefficients Model

We used a varying coefficients model to capture individual teacher effects on the dynamics between rewards, actions, and states. The model is specified as follows:

```{=tex}
\begin{align*}
\text{Action}_{kt} =& \ \sum_{i=1}^{L} \left( \beta_{ki} R_{k, t-i} + \gamma_{ki} \text{Action}_{k, t-i} + \sum_{j=i}^{L} \delta_{kij} (R_{k, t-i} \times \text{Action}_{k, t-j}) \right) \\
& + \phi_k \text{State}_{kt} + \psi_k (\text{State}_{kt} \times \text{Action}_{k, t-1}) + \mu_k + \lambda_{\text{Week}} + \epsilon_{kt}
\end{align*}
```
where $\beta_{ki}$, $\gamma_{ki}$, and $\delta_{kij}$ represent the reward, action, and their interaction coefficients that vary by teacher; $\phi_k$ and $\psi_k$ are the effects for the state and the interaction between state and lagged action, respectively; and $\mu_k$ and $\lambda_{\text{Week}}$ are fixed effects for teachers and weeks, respectively.

To operationalize these models, we constructed lagged versions of the variables, extending up to six weeks to determine the optimal lag. This lag value captured how far back rewards and past actions influenced teacher behavior.

Two types of metrics determined model selection. First, we split the data into training (80%) and testing (20%) subsets. Then, we evaluated model fit and predictive accuracy using the Bayesian Information Criterion (BIC) and the Out-of-Sample fit, gauged through the Area Under the Receiver Operating Characteristic curve (AUC). <!-- Can you put more detail about AUC here?-->  As such, we balanced model parsimony with predictive power. Second, we aimed to delineate which regression coefficients most closely embodied RL-like dynamics. We focused on a) variables that consistently enhanced action probability in response to rewards (i.e., positive coefficient for the state-reward interaction), b) actions that changed according to a given state, c) actions chosen to achieve desired states (i.e., significant coefficient for the action-state interaction), <!-- Write out exactly what coefficients these are--> and d) demonstrate action auto-correlation. <!-- There doesn't seem to be any state-reward interaction in the notation above-->
<!-- Do you need to use all three of these types?  What if they differ?-->

### Hybrid Models

The hybrid models maintain parallel estimations of two models (Logistic regression, Q-learning, or Actor-Critic), combining them to estimate action selection. This integration is achieved through a weighting scheme that balances the contributions of each model. Specifically, the probability of selecting action ($a$) at time ($t$), denoted ($\text{Pr}_t(a)$), is calculated through a linear combination of the softmax outputs of the selected models. This integration uses a weighting parameter ($\lambda$), which adjusts the relative influence of each model and is formulated as:

$$
\text{Pr}_t(a) = \lambda \text{Pr}_t(a)_{\text{Model 1}} + (1-\lambda) \text{Pr}_t(a)_{\text{Model 2}}
$$

### Reinforcement Learning Model Estimation

For the initial model selection, we fit the data from approximately 10 percent (190) classrooms due to computational constraints. After selecting the optimal model, we scale up the analysis to include all classrooms.

We adopt the Hierarchical Bayesian Inference (HBI) framework, as described by @piray2019a, to assess the fitness of our RL models and estimate their respective parameters across subjects. This approach uses Laplace approximations for efficient computation of posteriors by approximating the integrals involved in Bayesian inference. Subsequently, it leverages population-level distributions to refine individual parameter variation. Within this framework, we assume that for any given model $k$ <!-- Use a different letter to denote your model instead of k, since you're already using that for your teacher's index--> and subject $n$, <!-- should this be k?--> the individual parameters ($h_{k,n}$) <!-- is this a vector?--> are normally distributed across the population with $h_{k,n} \sim N(\mu_k, \Sigma_k)$, where $\mu_k$ and $\Sigma_k$ represent the mean and variance of the distribution over $h_{k,n}$, respectively. 


We use an expectation-maximization algorithm, iteratively performing the following two steps:

1.  Expectation Step: The algorithm calculates a posteriori estimates of the individual parameters ($h_{k,n}$) based on the existing group-level distributions.
2.  Maximization Step: The algorithm refines the group-level parameters ($\mu_k$ and $\Sigma_k$) using current individual parameter estimates. The updated mean group parameter $\mu_k$ is computed as the average of subject-level mean estimates across all subjects, $\mu_k = \frac{1}{N}\sum_{n}h_{k,n}$, where $N$ is the total number of subjects.

With this approach, we can estimate the log-likelihood for each subject's data, given the proposed models and parameter estimates. Recognizing the constraint of normality, we transform the initial estimates to generate constrained model parameters (e.g., the learning rate and discount factor in the Q-learning model). For parameters within a $(0,1)$ interval, we use the inverse logit function transform, $\text{Logit}^{-1}(x)=1/(1+e^{-x})$, and for intrinsically non-negative parameters, we use an exponential transformation. Consequently, HBI estimates the following unconstrained parameters:

1.  Q-learning:
    -   Learning Rate: $\text{Logit}(\alpha)=\log(\frac{\alpha}{1-\alpha})$
    -   Discount Rate: $\text{Logit}(\gamma)=\log(\frac{\gamma}{1-\gamma})$
    -   Inverse Temperature: $\log(\tau)$
    -   Cost: $\log(\text{cost})$
2.  Actor-Critic:
    -   Weights $w$ and $\theta$: $\log(w)$, $\log(\theta)$
    -   Learning rates ($\alpha_w$, $\alpha_\theta$): $\text{Logit}(\alpha_w)=\log(\frac{\alpha_w}{1-\alpha_w})$, $\text{Logit}(\alpha_\theta)=\log(\frac{\alpha_\theta}{1-\alpha_\theta})$
    -   Discount Factor: $\text{Logit}(\gamma)=\log(\frac{\gamma}{1-\gamma})$
    -   Inverse Temperature: $\log(\tau)$
    -   Initial Values: $\theta_{\text{init}}$, $w_{\text{init}}$
    -   Costs: $\log(\text{cost})$
3.  Logistic Regression Model:
    -   Parameters: $\beta$

#### Top Model Selection

We used a Bayesian model comparison technique from @piray2019a to determine the best-fit model from our set of candidates by considering the exceedance probability (i.e., the likelihood that one model is more probable than others in explaining the data) and the protected exceedance probability (i.e., the exceedance probability adjusted for the chance that observed differences in model evidence are due to random fluctuations).

The model evidence $\Pr(D | M_k)$ for model $k$ represents the likelihood of the observed data $D$ given the model $M_k$, and is given by $\Pr(D|M_k) = \int_{H} \Pr(D|\theta, M_k) \Pr(\theta|M_k) dh$ for all possible parameters $h \in H$. This value allows us to assess the models' overall fit across the entire parameter space.

We then calculated the likelihood of each model being the best fit for the data. We do so by first computing the posterior probabilities

$$
\Pr(M_k|D) = \frac{\Pr(D|M_k)\Pr(M_k)}{\sum_{k=1}^K \Pr(D|M_k)\Pr(M_k)}.
$$

We then proceeded to compute the exceedance probabilities, $EP_k = \Pr(\Pr(M_k|D) > \Pr(M_j|D) \text{ for all } j \neq k)$. In simple terms, it is the likelihood of model $k$ having a posterior probability higher than any other model in our comparison.

Finally, to account for false positives, we estimated the protected exceedance probability $PEP_k = EP_k(1 - P_0) + P_0/K$, <!-- Doesn't seem like the capital K variable is mentioned before this--> where $P_0 = 1/(1 + \exp(L - L_0))$, <!-- Isn't L max time lag?--> and $L$ and $L_0$ represent the log-likelihoods under the alternative and null hypotheses (i.e., the models are equally likely). The $PEP$ helps prevent overconfident selection of a model when the evidence is not sufficiently robust to distinguish model performance reliably.

### Heterogeneity Analysis

After selecting the top-performing model, we re-estimated the parameters using the entire dataset of 3029 classrooms. We explore the heterogeneity across schools and teachers by analyzing individual and group-level parameters as follows:

-   Classification of Individual Responses: Using the hierarchical Bayesian framework, we assign each teacher to the model that best captures their behavior, recognizing the individual differences that emerge from our population-level analysis.

-   Parameter Estimation Across Models: We estimate individual-specific parameters for each teacher and use them as a behavioral profile.

-   Analysis of Group-Level Trends: By aggregating the teacher data at the school level, we identify patterns and trends beyond individual variation. This aggregation allows us to investigate the influence of collective attributes (e.g., school income levels and classroom size) on educational outcomes and teacher performance.

-   Investigation of Influential Variables: We are particularly interested in how socioeconomic factors, such as zipcode median income, may impact the models' ability to describe teacher behavior. These variables provide insights into the heterogeneity in teacher classification and parameter estimates, offering a deeper understanding of the complex interplay between educational resources and pedagogical success.

# References

::: {#refs}
:::

{{< pagebreak >}}

# Supplemental Information {.appendix}

## Supplemental Methods {.appendix}

### PCA vs. NMF

Principal Component Analysis (PCA) was our first methodological choice. It is widely utilized but assumes data normality [@jolliffe2016] and maximizes variance explained, potentially overlooking subtle relationships between variables. Consequently, we also employed NMF, which, by contrast, imposes a non-negativity constraint and is more closely related to clustering algorithms, creating a more interpretable, sparse representation of behaviors [@ding2005; @lee1999]. This technique is particularly advantageous for data representing counts or frequencies. By trying different techniques, we can explore the reduced-dimension representation best suited to our specific dataset and research questions.

### Regression Models Coefficient Analyses

Prior to fitting the reinforcement learning (RL) models, we analyzed the regression coefficients in our best-fitting logit models to identify patterns that align with RL principles. Our primary focus was on the relationship between rewards and actions. Their interaction should positively influence future actions for desired outcomes (e.g., lesson completion) and negatively for undesired outcomes (e.g., struggles). Additionally, we examined the effect of current states on strategic action selection and the interaction between states and lagged actions to indicate how actions attain and maintain desired states.

We re-estimated the models with scaled independent variables, allowing for a direct coefficient comparison. In this context, a unit increase in these variables equates to a one standard deviation increase. @tbl-re-estimation-statefree and @tbl-re-estimation summarize these results, presenting a coherent overview of the standardized coefficients and highlighting the significance of interactions between rewards, states, and lagged actions.

The results convey that no singular approach fits all educational contexts. Instead, a spectrum of pedagogical strategies exists, with specific teaching methods aligning more closely with the principles of RL. By focusing on coefficients that display RL-like effects, particularly the interaction term R(t-1) x A(t-2), we identified dominant strategies that include 1) Action: Group Instruction with Reward: Struggles, and 2) Action: Group Instruction with Reward: Activity and State: Badges.

In contrast, @fig-RL-exploration favors models that occupy the upper-left quadrant, indicative of an optimized balance between model complexity and predictive accuracy. Noteworthy configurations include 1) Action: Pedagogical Knowledge with Reward: Activity, and 2) Action: Pedagogical Knowledge with Reward: Activity, State: Number of Students.

These configurations highlight teacher individual differences and support the idea that using a hybrid modeling approach could improve the process of fitting RL.

### Correlations Between Variables

We begin to unveil the intricate relationships among the variables under consideration through a comprehensive correlation analysis, as depicted in @fig-corr. This correlation matrix elucidates the magnitude and direction of associations among variables such as badges earned, minutes spent per student, tower alerts, the number of students, and teacher minutes. These interconnections inform the construction of our reinforcement learning models by suggesting the influence of teacher effort on student achievement. In this correlation matrix, each cell represents the Spearman correlation coefficient between a pair of variables. The color and size of the circles in each cell reflect the strength and direction of the correlation, with blue indicating positive correlations and red indicating negative correlations. The histograms along the diagonal provide a visual representation of the distribution of each variable.

```{r}
#| cache: true
#| label: fig-corr
#| fig-cap: "Correlation coefficients between variables after stardardization"
#| fig-format: png

df_corr <- df  %>%
  select(Badges.per.Active.User,
         Active.Users...Total,
         Minutes.per.Active.User,
         Tower.Alerts.per.Tower.Completion,
         Minutes.on.Zearn...Total) %>%
  rename("Badges" = Badges.per.Active.User,
         "Minutes per Student" = Minutes.per.Active.User,
         "Tower Alerts" = Tower.Alerts.per.Tower.Completion,
         "# of Students" = Active.Users...Total,
         "Teacher Minutes" = Minutes.on.Zearn...Total)

chart.Correlation(df_corr, histogram = TRUE, method = "spearman",
                  pch = 20, cex = 0.5, col = rgb(0, 0, 1, 0.5))

```

## Supplemental Tables {.appendix}

### Teacher Variables

| **Variable**                                               | **Description**                                                                                                                                                                                                                 |
|---------------------------|---------------------------------------------|
| PD Course Guide Download [@zearnaa; @zearnab]              | Detailed agenda for Professional Development (PD) courses focusing on classroom implementation, leadership, supporting diverse learners, using data to inform teaching practices, and accelerating student learning.             |
| PD Course Notes Download [@zearnaa; @zearnab]              | Professional development session notes offering insights into effectively using Zearn's curriculum.                                                                                                                             |
| Curriculum Map Download [@zearna]                          | Detailed outline of learning objectives and content. Presents a sequence of interconnected math concepts across grades, aligning with states' instructional requirements.                                                       |
| Assessments Download [@zearnb]                             | Assessments to evaluate student understanding of the material, including ongoing formative assessments, digital daily checks, and paper-based unit assessments.                                                                 |
| Assessments Answer Key Download [@zearnc]                  | Solutions for assessments to aid in grading and feedback. Provides detailed rubrics for mission-level assessments.                                                                                                              |
| Elementary Schedule Download [@zearne]                     | A recommended schedule for elementary school-level Zearn curriculum activities to guide daily and weekly instructional planning, ensuring comprehensive coverage of curriculum content.                                         |
| Grade Level Overview Download [@zearnf]                    | Provides a summary of learning objectives, pacing guidance, key grade-level terminology, a list of required materials, and details on the standards covered by each lesson.                                                     |
| Kindergarten Schedule Download [@zearng]                   | Recommended schedules for Kindergarten, supporting structured instruction planning.                                                                                                                                             |
| Kindergarten Mission Download [@zearnh]                    | Details interactive activities focused on kindergarten-level concepts and their learning objectives.                                                                                                                            |
| Mission Overview Download [@zearnf]                        | Outlines a mission's (i.e., learning module) flow of topics, lessons, and assessments; highlights foundational concepts introduced earlier; lists recently introduced terms and required materials for teacher-led instruction. |
| Optional Homework Download [@zearnj]                       | Assignments for additional practice, enhancing student learning outside of class.                                                                                                                                               |
| Optional Problem Sets Download [@zearnk]                   | Exercises for extra practice, tailored to reinforce lesson concepts.                                                                                                                                                            |
| Small Group Lesson Download [@zearnl]                      | Lessons designed for small-group engagement.                                                                                                                                                                                    |
| Student Notes and Exit Tickets Download [@zearnz; @zearny] | Student notes supplement digital lessons with paper-and-pencil activities. Exit tickets are lesson-level assessments for teachers to monitor daily learning.                                                                    |
| Teaching and Learning Approach Download [@zearnn]          | Resources outlining Zearn's pedagogical methods.                                                                                                                                                                                |
| Whole Group Fluency Download [@zearno]                     | Lesson-aligned practice activities to build math fluency through whole-class engagement.                                                                                                                                        |
| Whole Group Word Problems Download [@zearnl]               | Word problem-solving activities intended for collaborative, whole-class engagement.                                                                                                                                             |
| Fluency Completed [@lesson-a]                              | Indicates teacher completed a fluency activity, typically given to students before their daily digital lessons.                                                                                                                 |
| Guided Practice Completed [@zearnr]                        | Indicates teacher completed a guided practice segment, where students learn new concepts. These include videos with on-screen teachers, interactive activities, and paper-and-pencil Student Notes.                             |
| Kindergarten Activity Completed [@zearns]                  | Indicates teacher completed an activity within the Kindergarten curriculum.                                                                                                                                                     |
| Number Gym Activity Completed [@zearnt]                    | Indicates teacher completed a Number Gym, an individually adaptive activity that builds number sense, reinforces previously learned skills, and addresses areas of unfinished learning.                                         |
| Tower Completed [@zearnu]                                  | Indicates teacher completed a Tower of Power, an activity that requires full mastery of lesson objectives and that students must complete independently.                                                                        |
| Tower Struggled [@zearnac]                                 | Indicates teacher committed a mistake when engaging with the Tower of Power activity in a student role, triggering a "boost" (scaffolding remediation).                                                                         |
| Tower Stage Failed [@zearnad]                              | Indicates teacher received three consecutive "boosts" due to repeated errors when engaging with the Tower of Power in a student role.                                                                                           |

: Catalog of Teacher Activities. This table presents teachers' actions, including curriculum engagement, downloads of pedagogical materials, and completion of various interactive components within the Zearn educational platform. {#tbl-teacher-variables}

{{< pagebreak >}}

<!-- FE Logit Model Summary -->

<!-- FE Logit State-free -->

```{r, results='asis'}
#| label: tbl-fe-results-statefree-full
#| tbl-cap: "State-Free Panel Logistic Regression Results"

# Load the results from both subset and restricted analyses
load("Regressions/fe-subset-results.RData")
original_results <- results
load("Regressions/fe-restricted-results.RData")
restricted_results <- results

# Combine the model stats from original and restricted results for comparison
original_models <- do.call(rbind, lapply(original_results, function(x) {
  if (x$Lag != 2) return(NULL)
  data.frame(
    Action = x$Method,
    Reward = x$Reward,
    AUC = x$AUC,
    BIC = x$bic,
    Type = "Original"
  )
}))

# Select top 2 AUC models and top 2 BIC models for both original and restricted
top_models <- original_models %>%
  arrange(desc(AUC)) %>%
  slice_head(n = 2)
top_models <- original_models %>%
  arrange(BIC) %>%
  slice_head(n = 2) %>%
  bind_rows(top_models)

# Prepare the results from selected top models for the table
tidy_fe_results <- lapply(original_results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (!(paste(x$Method, x$Reward) %in%
        paste(top_models$Action, top_models$Reward))) return(NULL)
  temp <- as.data.frame(x$fecoef) %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  as.numeric)) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")
  
  return(list(
    Model = paste(x$Method, x$Reward,"Full"),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results <- tidy_fe_results[!sapply(tidy_fe_results, is.null)]
tidy_fe_results_restrict <- lapply(restricted_results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (!(paste(x$Method, x$Reward) %in%
        paste(top_models$Action, top_models$Reward))) return(NULL)
  temp <- as.data.frame(x$fecoef) %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  as.numeric)) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")
  
  return(list(
    Model = paste(x$Method, x$Reward,"Restricted"),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results_restrict <- 
  tidy_fe_results_restrict[!sapply(tidy_fe_results_restrict, is.null)]
tidy_fe_results <- c(tidy_fe_results, tidy_fe_results_restrict)

# Combine all model summaries into one dataframe
model_summary <- do.call(rbind, lapply(tidy_fe_results, function(x) {
  x_name <- unlist(strsplit(x[[1]], " "))
  x_data <- x$Data %>%
    rename(Coefficient = Estimate,
           Std_Error = `Std. Error`) %>%
    mutate(
      Significance = case_when(
        `Pr(>|z|)` < .001 ~ "***",
        `Pr(>|z|)` < .01 ~ "**",
        `Pr(>|z|)` < .05 ~ "*",
        TRUE ~ ""),
      Term = gsub(x_name[1], "lag", Term),
      Term = gsub(x_name[2], "rwd", Term),
      Term = gsub("_", "",  Term),
      Term = gsub(":", "_",  Term)) %>%
    add_row(Term = "BIC", Coefficient = x[[3]]) %>%
    add_row(Term = "N", Coefficient = x[[4]])
  x_data$Model <- x[[1]]

  x_data
})) %>% 
  dplyr::select(Model, Term, Coefficient, Std_Error, Significance) %>%
  mutate(Model = gsub(" ", "&", Model),
         Model = gsub("Frobenius.NNDSVD_student1", "Badges", Model),
         Model = gsub("Frobenius.NNDSVD_student2", "Struggles", Model),
         Model = gsub("Frobenius.NNDSVD_student3", "No. Students", Model),
         Model = gsub("Frobenius.NNDSVD_student4", "Activity", Model),
         Model = gsub("Frobenius.NNDSVD_teacher1", "Assessments", Model),
         Model = gsub("Frobenius.NNDSVD_teacher2", "Pedagogical Knowledge", Model),
         Model = gsub("Frobenius.NNDSVD_teacher3", "Group Instruction", Model),
         Model = gsub("Frobenius.NNDSVD_teacher4", "Curriculum Planning", Model))

# Create the table using the 'gt' package
gt_table <- model_summary %>%
  filter(!Term %in% c("rwd1","rwd2","lag1","lag2")) %>%
  mutate(Estimate = ifelse(Term %in% c("BIC", "N"),
                           sprintf("%d", as.integer(Coefficient)),
                           sprintf("%1.3f%s\n(%1.3f)",
                                   Coefficient, Significance, Std_Error)),
         Term = factor(Term, levels = c("rwd1_lag1", "rwd1_lag2", "lag2_rwd2",
                                        # "lag1", "lag2",
                                        "st", "lag1_st",
                                        "BIC", "N"),
                       labels = c("R(t-1) x \n A(t-1)",
                                  "R(t-1) x \n A(t-2)",
                                  "R(t-2) x \n A(t-2)",
                                  # "Action(t-1)", "Action(t-2)",
                                  "S(t)", "S(t) x \n A(t-1)",
                                  "BIC", "N"))) %>%
  dplyr::select(-Coefficient, -Std_Error, -Significance) %>%
  # Pivot wider to have one column per model
  pivot_wider(names_from = Model,
              values_from = Estimate,
              names_sep = "_") %>%
  dplyr::select(order(names(.))) %>%
  arrange(Term) %>%
  gt(rowname_col = "Term") %>%
  sub_missing() %>%
  tab_spanner_delim("&") %>%
  tab_header(
    title = "Fixed Effects Logistic Regression Results"
  )

# Print the table
gt_table %>% as_latex()
```

<!-- FE Logit State-based -->

```{r}
#| label: tbl-fe-results-full
#| tbl-cap: "State-Based Panel Logistic Regression Results"

load("Regressions/fe-state-subset-results.RData")

# Combine the model stats from original and restricted results for comparison
results_df <- do.call(rbind, lapply(results, function(x) {
  if (x$Lag != 2) return(NULL)
  data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = x$State,
    AUC = x$AUC,
    BIC = x$bic,
    Type = "Original"
  )
}))
# Select top 2 AUC models and top 2 BIC models for both original and restricted
top_models <- results_df %>%
  arrange(desc(AUC)) %>%
  slice_head(n = 2)
top_models <- results_df %>%
  arrange(BIC) %>%
  slice_head(n = 2) %>%
  bind_rows(top_models)

# Filter and tidy the results
tidy_fe_results <- lapply(results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (top_models %>%
      filter(Action == x$Method &
             Reward == x$Reward &
             State  == x$State) %>%
      nrow() == 0) return(NULL)
  temp <- x$fecoef %>%
    as.data.frame() %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  ~ as.numeric(.))) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")
  
  return(list(
    Model = paste(x$Method, x$Reward, x$State),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results <- tidy_fe_results[!sapply(tidy_fe_results, is.null)]
	
# Combine all model summaries into one dataframe
model_summary <- do.call(rbind, lapply(tidy_fe_results, function(x) {
  x_name <- unlist(strsplit(x$Model, " "))
  x_data <- x$Data %>%
    rename(Coefficient = Estimate,
           Std_Error = `Std. Error`) %>%
    mutate(
      Significance = case_when(
        `Pr(>|z|)` < .001 ~ "***",
        `Pr(>|z|)` < .01 ~ "**",
        `Pr(>|z|)` < .05 ~ "*",
        TRUE ~ ""),
      Term = gsub(x_name[1], "lag", Term),
      Term = gsub(x_name[2], "rwd", Term),
      Term = gsub(x_name[3], "st",  Term),
      Term = gsub("_", "",  Term),
      Term = gsub(":", "_",  Term)) %>%
    add_row(Term = "BIC", Coefficient = x[[3]]) %>%
    add_row(Term = "N", Coefficient = x[[4]])
  x_data$Model <- x$Model

  x_data
})) %>% dplyr::select(Model, Term, Coefficient, Std_Error, Significance) %>%
  mutate(Model = gsub(" ", "&", Model),
         Model = gsub("Frobenius.NNDSVD_student1", "Badges", Model),
         Model = gsub("Frobenius.NNDSVD_student2", "Struggles", Model),
         Model = gsub("Frobenius.NNDSVD_student3", "No. Students", Model),
         Model = gsub("Frobenius.NNDSVD_student4", "Activity", Model),
         Model = gsub("Frobenius.NNDSVD_teacher1", "Assessments", Model),
         Model = gsub("Frobenius.NNDSVD_teacher2", "Pedagogical Knowledge", Model),
         Model = gsub("Frobenius.NNDSVD_teacher3", "Group Instruction", Model),
         Model = gsub("Frobenius.NNDSVD_teacher4", "Curriculum Planning", Model))

# Create the table using the 'gt' package
gt_table <- model_summary %>%
  filter(!Term %in% c("rwd1","rwd2","lag1","lag2")) %>%
  mutate(Estimate = ifelse(Term %in% c("BIC", "N"),
                           sprintf("%d", as.integer(Coefficient)),
                           sprintf("%1.3f%s\n(%1.3f)",
                                   Coefficient, Significance, Std_Error)),
         Term = factor(Term, levels = c("rwd1_lag1", "rwd1_lag2", "lag2_rwd2",
                                        # "lag1", "lag2",
                                        "st1", "lag2_st1",
                                         "BIC", "N"),
                       labels = c("R(t-1) x \n A(t-1)",
                                  "R(t-1) x \n A(t-2)",
                                  "R(t-2) x \n A(t-2)",
                                  # "Action(t-1)", "Action(t-2)",
                                  "S(t-1)", "S(t-1) x \n A(t-2)",
                                  "BIC", "N"))) %>%
  dplyr::select(-Coefficient, -Std_Error, -Significance) %>%
  # Pivot wider to have one column per model
  pivot_wider(names_from = Model,
              values_from = Estimate,
              names_sep = "_") %>%
  dplyr::select(order(names(.))) %>%
  arrange(Term) %>%
  gt(rowname_col = "Term") %>%
  sub_missing() %>%
  tab_spanner_delim("&") %>%
  tab_header(
    title = "Fixed Effects Logistic Regression Results"
    )

# Print the table
gt_table

```

<!-- VC Logit Model Summary -->

```{r, results='asis'}
#| label: tbl-re-estimation-statefree
#| tbl-cap: "Summary statistics of coefficients from state-free mixed effects logistic regression models predicting teacher actions. Each row group represents a predictor variable: R(t-1) x A(t-1) is the interaction between the reward at time t-1 and the action taken at time t-1, R(t-1) x A(t-2) is the interaction between the reward at t-1 and action at t-2, and R(t-2) x A(t-2) is the interaction between the reward and action at time t-2. Columns represent different combinations of action, reward, and state, where the actions are teacher NMF components, and the rewards are student NMF components. Cell values show the mean, standard deviation, quartiles, and proportion of teachers that show RL-like coefficients (i.e., those in the direction predicted by reinforcement learning: positive for positive rewards and negative for negative rewards)."

load("Regressions/me-standardized-coef.RData")

large_coef_threshold <- 10

filtered_results <- lapply(results, function(x) {
  if (!is.null(x$State)) return(NULL)
  if (!x$convergence) return(NULL)
  temp <- data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = "None"
  ) %>%
  mutate(across(c(Action, Reward),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Badges",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggles",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "Activity",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Pedagogical Knowledge",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Group Instruction",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Curriculum Planning",
                            .default = .)))
  # if(nrow(
  #   temp %>%
  #   semi_join(selected_models, by = c("Action", "Reward", "State"))
  #   ) == 0) return(NULL)
  if(x$AUC < 0.5) return(NULL)
  x$recoef <- do.call(cbind, x$recoef[-c(1, length(x$recoef))]) %>%
    as.data.frame() %>%
    filter(if_all(everything(),
                  ~ . >= find_hdr(.)[1] &
                    . <= find_hdr(.)[2])) %>%
    rownames_to_column(var = "Teacher.User.ID")
  if (any(abs(colMeans(x$recoef[,-1])) > large_coef_threshold)) return(NULL)

  return(list(
    Action = temp$Action,
    Reward = temp$Reward,
    AUCmodel = x$AUC,
    BICmodel = x$bic,
    coef = data.frame(
      Teacher.User.ID = as.integer(x$recoef[,1]),
      lag1 = x$recoef[,3],
      lag2 = x$recoef[,7],
      rwd1 = x$recoef[,2],
      rwd2 = x$recoef[,6],
      rwd1_lag1 = x$recoef[,4],
      rwd1_lag2 = x$recoef[,5],
      rwd2_lag2 = x$recoef[,8]
    ) %>%
      inner_join(x$perform_df %>%
                   select(Teacher.User.ID, auc_out, logLik_ind) %>%
                   filter(if_all(everything(), ~ . >= find_hdr(.)[1] &
                                                 . <= find_hdr(.)[2])) %>%
                   filter(0.5 < auc_out & auc_out < 1),
                 by = "Teacher.User.ID")
  ))
})
filtered_results <- filtered_results[!sapply(filtered_results, is.null)]

# Gather AUC and BIC and calculate thresholds
all_auc_bic <- do.call(
  rbind,
  lapply(filtered_results, function(x) {
    return(c(x$AUCmodel, x$BICmodel))
    }))
auc75 <- quantile(all_auc_bic[,1], 0.75, na.rm = TRUE)
bic25 <- quantile(all_auc_bic[,2], 0.25, na.rm = TRUE)

model_summary <- do.call(rbind, lapply(filtered_results, function(x) {
  summary <- data.frame(
    Action = x$Action,
    Reward = x$Reward,
    AUC = x$AUCmodel,
    BIC = x$BICmodel
  )
  summary$lag1 = list(x$coef$lag1)
  summary$lag2 = list(x$coef$lag2)
  summary$rwd1_lag1 = list(x$coef$rwd1_lag1)
  summary$rwd1_lag2 = list(x$coef$rwd1_lag2)
  summary$rwd2_lag2 = list(x$coef$rwd2_lag2)
  summary$auc_ind = list(x$coef$auc_out)
  summary$logLik_ind = list(x$coef$logLik_ind)
  
  summary <- summary %>%
    # # Standardize the coefficients by dividing by the standard deviation
    # mutate(across(c(rwd1_lag1, rwd1_lag2, rwd2_lag2, st, st_lag1),
    #               ~ lapply(., function(x) x / sd(x, na.rm = T)))) %>%
    # # Fill in x$State == "None" with 0s
    # mutate(across(c(st, st_lag1), ~ if_else(is.na(.), list(0), .))) %>%
    mutate(n_teachers = lapply(summary$lag1, length)) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2, 
                    rwd2_lag2,
                    auc_ind, logLik_ind),
                  ~ lapply(., mean, na.rm = T),
                  .names = "{.col}.mean")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., sd, na.rm = T),
                  .names = "{.col}.sd")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., quantile, 0.25, names = F),
                  .names = "{.col}.q1")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., median, na.rm = T),
                  .names = "{.col}.median")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., quantile, 0.75, names = F),
                  .names = "{.col}.q3")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., function(x) mean(x > 0, na.rm = T)),
                  .names = "{.col}.pos")) %>%
    select(-c(lag1, lag2,
              rwd1_lag1,
              rwd1_lag2,
              rwd2_lag2,
              auc_ind, logLik_ind, auc_ind.mean, logLik_ind.mean))
  
  return(summary)
}))

# Transform the model_summary dataframe to a long format for plotting
model_summary_long <- model_summary %>%
  mutate(across(!c(Action, Reward), as.numeric)) %>%
  mutate(across(dplyr::ends_with(".pos") & dplyr::starts_with("rwd"),
                ~ case_when(Reward == "Struggle" ~ 1 - ., .default = .))) %>%
  filter(AUC >= auc75 | BIC <= bic25) %>%
  select(!c(AUC, BIC)) %>%
  pivot_longer(cols = -c(Action, Reward), 
               names_to = c("variable", "statistic"), values_to = "value",
               names_sep = "\\.") %>%
  arrange(Action, Reward) %>%
  pivot_wider(names_from = c(Action, Reward), values_from = c(value))
  
max <- model_summary_long %>%
  filter(statistic == "pos" & !variable %in% c("lag1","lag2")) %>%
  rowwise() %>%
  # Remove columns that do not have the max of at least 1 row
  mutate(max_val = max(c_across(is.numeric), na.rm = T)) %>%
  ungroup() %>%
  mutate(across(is.numeric, ~ if_else(. == max_val, ., NA))) %>%
  select(where(~ !all(is.na(.)))) %>%
  select(-c("max_val","statistic")) %>%
  mutate(across(is.numeric, ~ !is.na(.))) %>%
  pivot_longer(cols = -variable, names_to = "model", values_to = "keep") %>%
  filter(keep) %>% select(-keep)

model_summary_long %>%
  filter(!variable %in% c("lag1", "lag2")) %>%
  mutate(statistic = factor(statistic,
                            levels = c("mean", "sd", "q1", "median", "q3", "pos"),
                            labels = c("Mean", "SD", "Q1", "Median", "Q3", "RL-like")),
         variable = factor(variable,
                           levels = c(
                             # "lag1", "lag2",
                             "rwd1_lag1",
                             "rwd1_lag2",
                             "rwd2_lag2",
                             "n_teachers"),
                           labels = c(
                             # "Action(t-1)",
                             # "Action(t-2)",
                             "R(t-1) x \n A(t-1)",
                             "R(t-1) x \n A(t-2)",
                             "R(t-2) x \n A(t-2)",
                             "N"
                             ))) %>%
  select(order(names(model_summary_long))) %>%
  group_by(variable) %>%
  gt(rowname_col = "statistic", row_group_as_column = T) %>%
  row_group_order(groups = c(
    # "Action(t-1)",
    # "Action(t-2)",
    "R(t-1) x \n A(t-1)",
    "R(t-1) x \n A(t-2)",
    "R(t-2) x \n A(t-2)",
    "N")) %>%
  tab_stubhead(label = "Action \n Reward") %>%
  fmt_markdown(columns = "variable") %>%
  fmt_number(decimals = 2) %>%
  fmt_integer(rows = variable == "N") %>%
  fmt_percent(rows = statistic == "RL-like") %>%
  # cols_hide(starts_with("Assessments_No. Students")) %>%
  sub_missing() %>%
  tab_spanner_delim("_") %>%
  tab_header(
    title = "Summary of Model Coefficients"
  ) %>%
  tab_options(
    table.font.size = "small"
  ) %>%
  as_latex()

```

{{< pagebreak >}}

```{r, results='asis'}
#| label: tbl-re-estimation
#| tbl-cap: "Summary statistics of coefficients from state-based mixed effects logistic regression models predicting teacher actions. Each row group represents a predictor variable: R(t-1) x A(t-1) is the interaction between the reward at time t-1 and the action taken at time t-1, R(t-1) x A(t-2) is the interaction between the reward at t-1 and action at t-2, R(t-2) x A(t-2) is the interaction between the reward and action at time t-2, S(t) is the state at time t, and S(t) x A(t-1) is the interaction between the current state and the previous action. Columns represent different combinations of action, reward, and state, where the actions are teacher NMF components, and the rewards and states are student NMF components (but the state is never the same component as its associated reward). Cell values show the mean, standard deviation, quartiles, and proportion of teachers that show RL-like coefficients (i.e., those in the direction predicted by reinforcement learning: positive for positive rewards and negative for negative rewards)."

load("Regressions/me-standardized-coef.RData")

large_coef_threshold <- 10

filtered_results <- lapply(results, function(x) {
  if (is.null(x$State)) return(NULL)
  if (!x$convergence) return(NULL)
  temp <- data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = x$State
  ) %>%
  mutate(across(c(Action, Reward, State),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Badges",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggles",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "Activity",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Pedagogical Knowledge",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Group Instruction",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Curriculum Planning",
                            .default = .)))
  # if(nrow(
  #   temp %>%
  #   semi_join(selected_models, by = c("Action", "Reward", "State"))
  #   ) == 0) return(NULL)
  if(x$AUC < 0.5) return(NULL)
  x$recoef <- do.call(cbind, x$recoef[-c(1, length(x$recoef))]) %>%
    as.data.frame() %>%
    filter(if_all(everything(),
                  ~ . >= find_hdr(.)[1] & . <= find_hdr(.)[2])) %>%
    rownames_to_column(var = "Teacher.User.ID")
  if (any(abs(colMeans(x$recoef[,-1])) > large_coef_threshold)) return(NULL)
  
  return(list(
    Action = temp$Action,
    Reward = temp$Reward,
    State = temp$State,
    AUCmodel = x$AUC,
    BICmodel = x$bic,
    coef = data.frame(
      Teacher.User.ID = as.integer(x$recoef[,1]),
      lag1 = x$recoef[,3],
      lag2 = x$recoef[,7],
      rwd1 = x$recoef[,2],
      rwd2 = x$recoef[,6],
      rwd1_lag1 = x$recoef[,4],
      rwd1_lag2 = x$recoef[,5],
      rwd2_lag2 = x$recoef[,8],
      st = x$recoef[,9],
      st_lag1 = x$recoef[,10]
    ) %>%
      inner_join(x$perform_df %>%
                   select(Teacher.User.ID, auc_out, logLik_ind) %>%
                   filter(auc_out >= 0.5 & auc_out < 1),
                 by = "Teacher.User.ID")
  ))
  })
filtered_results <- filtered_results[!sapply(filtered_results, is.null)]

# Gather AUC and BIC and calculate thresholds
all_auc_bic <- do.call(
  rbind,
  lapply(filtered_results, function(x) {
    return(c(x$AUCmodel, x$BICmodel))
    }))
auc75 <- quantile(all_auc_bic[,1], 0.75, na.rm = TRUE)
bic25 <- quantile(all_auc_bic[,2], 0.25, na.rm = TRUE)

model_summary <- do.call(rbind, lapply(filtered_results, function(x) {
  summary <- data.frame(
    Action = x$Action,
    Reward = x$Reward,
    State = x$State,
    AUC = x$AUCmodel,
    BIC = x$BICmodel
  )
  summary$lag1 = list(x$coef$lag1)
  summary$lag2 = list(x$coef$lag2)
  summary$rwd1_lag1 = list(x$coef$rwd1_lag1)
  summary$rwd1_lag2 = list(x$coef$rwd1_lag2)
  summary$rwd2_lag2 = list(x$coef$rwd2_lag2)
  summary$st = ifelse(x$State == "None", list(0), list(x$coef$st))
  summary$st_lag1 = ifelse(x$State == "None", list(0), list(x$coef$st_lag1))
  summary$auc_ind = list(x$coef$auc_out)
  summary$logLik_ind = list(x$coef$logLik_ind)
  
  summary <- summary %>%
    # # Standardize the coefficients by dividing by the standard deviation
    # mutate(across(c(rwd1_lag1, rwd1_lag2, rwd2_lag2, st, st_lag1),
    #               ~ lapply(., function(x) x / sd(x, na.rm = T)))) %>%
    # # Fill in x$State == "None" with 0s
    # mutate(across(c(st, st_lag1), ~ if_else(is.na(.), list(0), .))) %>%
    mutate(n_teachers = lapply(summary$lag1, length)) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1,
                    auc_ind, logLik_ind),
                  ~ lapply(., mean, na.rm = T),
                  .names = "{.col}.mean")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., sd, na.rm = T),
                  .names = "{.col}.sd")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., quantile, 0.25, names = F),
                  .names = "{.col}.q1")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., median, na.rm = T),
                  .names = "{.col}.median")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., quantile, 0.75, names = F),
                  .names = "{.col}.q3")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., function(x) mean(x > 0, na.rm = T)),
                  .names = "{.col}.pos")) %>%
    select(-c(lag1, lag2,
              rwd1_lag1,
              rwd1_lag2,
              rwd2_lag2,
              st, st_lag1,
              auc_ind, logLik_ind, auc_ind.mean, logLik_ind.mean))
  
  return(summary)
}))

# Transform the model_summary dataframe to a long format for plotting
model_summary_long <- model_summary %>%
  mutate(across(!c(Action, Reward, State), as.numeric)) %>%
  mutate(across(dplyr::ends_with(".pos") & dplyr::starts_with("st"),
                ~ case_when(State == "Struggle" ~ 1 - ., .default = .)),
         across(dplyr::ends_with(".pos") & dplyr::starts_with("rwd"),
                ~ case_when(Reward == "Struggle" ~ 1 - ., .default = .))) %>%
  filter(AUC >= auc75 | BIC <= bic25) %>%
  select(!c(AUC, BIC)) %>%
  mutate(across(dplyr::starts_with("st", ignore.case = F),
                ~ if_else(State == "None", NA, .))) %>%
  pivot_longer(cols = -c(Action, Reward, State), 
               names_to = c("variable", "statistic"), values_to = "value",
               names_sep = "\\.") %>%
  arrange(Action, Reward, State) %>%
  pivot_wider(names_from = c(Action, Reward, State), values_from = c(value))
  
max <- model_summary_long %>%
  filter(statistic == "pos" & !variable %in% c("lag1","lag2")) %>%
  rowwise() %>%
  # Remove columns that do not have the max of at least 1 row
  mutate(max_val = max(c_across(is.numeric), na.rm = T)) %>%
  ungroup() %>%
  mutate(across(is.numeric, ~ if_else(. == max_val, ., NA))) %>%
  select(where(~ !all(is.na(.)))) %>%
  select(-c("max_val","statistic")) %>%
  mutate(across(is.numeric, ~ !is.na(.))) %>%
  pivot_longer(cols = -variable, names_to = "model", values_to = "keep") %>%
  filter(keep) %>% select(-keep)

model_summary_long %>%
  filter(!variable %in% c("lag1", "lag2")) %>%
  mutate(statistic = factor(statistic,
                            levels = c("mean", "sd", "q1", "median", "q3", "pos"),
                            labels = c("Mean", "SD", "Q1", "Median", "Q3", "RL-like")),
         variable = factor(variable,
                           levels = c(
                             # "lag1", "lag2",
                             "rwd1_lag1",
                             "rwd1_lag2",
                             "rwd2_lag2",
                             "st", "st_lag1",
                             "n_teachers"),
                           labels = c(
                             # "Action(t-1)",
                             # "Action(t-2)",
                             "R(t-1) x \n A(t-1)",
                             "R(t-1) x \n A(t-2)",
                             "R(t-2) x \n A(t-2)",
                             "S(t)",
                             "S(t) x \n A(t-1)",
                             "N"
                             ))) %>%
  select(order(names(model_summary_long))) %>%
  group_by(variable) %>%
  gt(rowname_col = "statistic", row_group_as_column = T) %>%
  row_group_order(groups = c(
    # "Action(t-1)",
    # "Action(t-2)",
    "R(t-1) x \n A(t-1)",
    "R(t-1) x \n A(t-2)",
    "R(t-2) x \n A(t-2)",
    "S(t)",
    "S(t) x \n A(t-1)",
    "N")) %>%
  tab_stubhead(label = "Action \n Reward \n State") %>%
  fmt_markdown(columns = "variable") %>%
  fmt_number(decimals = 2) %>%
  fmt_integer(rows = variable == "No. Teachers") %>%
  fmt_percent(rows = statistic == "RL-like") %>%
  # cols_hide(starts_with("Assessments_No. Students")) %>%
  sub_missing() %>%
  tab_spanner_delim("_") %>%
  tab_header(
    title = "Summary of Model Coefficients"
  ) %>%
  tab_options(
    table.font.size = "small"
  ) %>%
  as_latex()

```

<!-- QL Model Summary -->

<!-- AC Model Summary -->

```{r}
#| label: tbl-CBM-teachers
#| tbl-cap: "Comparison of Teacher Characteristics by Model Type. The table presents the differences in means across various characteristics between teachers classified into Logit or Actor-Critic models."

# T-tests for differences in means by model type
heterogeneity %>%
  filter(!is.na(Teacher.User.ID)) %>%
  dplyr::select(Model, `Charter School`, `Paid Account`, Poverty, 
         `No. of Weeks`, `Total Students`, `No. of Classes`,
         `Active Students`, `Avg. Badges`, `Avg. Tower Alerts`, 
         Action, Reward, State) %>%
  tbl_summary(
    by = Model,  # Grouping variable
    type = list(`No. of Classes` ~ "continuous"),
    statistic = list(all_continuous() ~ "{mean} ({sd})",
                     all_categorical() ~ "{p}%")
  ) %>%
  add_n() %>%
  add_difference() %>%
  modify_column_hide(columns = ci) %>%
  add_q()

```

```{r}
#| label: tbl-CBM-teachers-full
#| tbl-cap: "Comparison of Teacher Characteristics by Model Fit. The table presents the differences in means across various characteristics between teachers classified into Good Fit or Bad Fit groups."

# T-tests for differences in means by model type and fit group
full_data %>%
  dplyr::select(FitGroup, `Charter School`, `Paid Account`, Poverty, 
         `No. of Weeks`, `Total Students`, `No. of Classes`,
         `Active Students`, `Avg. Badges`, `Avg. Tower Alerts`, 
         Action, Reward, State) %>%
  tbl_summary(
    by = c("FitGroup"), # Grouping variables
    type = list(`No. of Classes` ~ "continuous"),
    statistic = list(all_continuous() ~ "{mean} ({sd})",
                     all_categorical() ~ "{p}%")
  ) %>%
  add_n() %>%
  add_difference() %>%
  modify_column_hide(columns = ci) %>%
  add_q()


```

```{r, results='asis'}
#| label: tbl-optimality
#| tbl-cap: "Impact of RL parameters on Weekly Badges and Tower Alerts. Six linear regression models were used to examine the correlations between a teacher's RL parameters and two measures of student engagement: average weekly Badges earned per student (Models 1-3) and average weekly Tower Alerts per student (Models 4-6). Models 2-3 and 5-6 also control for other variables including number of active students, number of classes taught, grade level, weeks, poverty level, income level, whether the school is a charter school, and whether the school has a paid account. Coefficients and standard errors are provided for each parameter in each model."

ordered_factor <- function(fact_var) {
  categories <- levels(fact_var)
  n_cat <- length(categories)
  cont <- matrix(data = 0, nrow = n_cat, ncol = (n_cat - 1))
  cont[col(cont) < row(cont)] <- 1
  rownames(cont) <- categories
  colnames(cont) <- paste(categories[2:n_cat],
                          categories[1:(n_cat - 1)],
                          sep = " vs. ")
  contrasts(fact_var) <- cont
  return(fact_var)
}
full_data <- full_data %>%
  dplyr::mutate(
    Income = factor(Income, ordered = TRUE), 
    grade = factor(grade, ordered = TRUE,
                   levels = c("Kindergarten",
                              "1st", "2nd", "3rd", "4th", "5th")),
    Poverty = factor(Poverty, ordered = TRUE)
  ) %>%
  dplyr::mutate(
    Income = ordered_factor(Income), 
    grade = ordered_factor(grade),
    Poverty = ordered_factor(Poverty)
  )

# Run regression models using the median split groups
model1 <- lm(`Avg. Badges` ~ alpha + gamma + tau + cost + ev_init,
             data = full_data)
model2 <- lm(`Avg. Badges` ~ alpha + gamma + tau + cost + ev_init +
               BIC + `No. of Weeks` + `Total Students` + `No. of Classes`,
             data = full_data)
model3 <- lm(`Avg. Badges` ~ alpha + gamma + tau + cost + ev_init +
               BIC + `No. of Weeks` + `Total Students` + `No. of Classes` +
               grade +  Poverty + `Charter School` + `Paid Account`,
             data = full_data)
model4 <- lm(`Avg. Tower Alerts` ~ alpha + gamma + tau + cost + ev_init,
             data = full_data)
model5 <- lm(`Avg. Tower Alerts` ~ alpha + gamma + tau + cost + ev_init  +
               BIC + `No. of Weeks` + `Total Students` + `No. of Classes`,
             data = full_data)
model6 <- lm(`Avg. Tower Alerts` ~ alpha + gamma + tau + cost + ev_init +
               BIC + `No. of Weeks` + `Total Students` + `No. of Classes` +
               grade +  Poverty + `Charter School` + `Paid Account`,
             data = full_data)

# # Gather tidy summaries of each model
# model_summaries <- list(
#   tidy(model1), tidy(model2), tidy(model3),
#   tidy(model4), tidy(model5), tidy(model6)
# )
# 
# # Create a function to format model summaries for gt
# format_for_gt <- function(model_summary, model_name) {
#   model_summary %>%
#     mutate(model = model_name) %>%
#     dplyr::select(model, term, estimate, std.error, statistic, p.value)
# }
# 
# # Apply the function to each model summary with an appropriate model name
# formatted_summaries <- lapply(seq_along(model_summaries), function(i) {
#   format_for_gt(model_summaries[[i]], paste("Model", i))
# }) %>%
#   bind_rows()
# 
# # Generate a gt table
# table_gt <- gt(formatted_summaries) %>%
#   cols_label(
#     model = "Model",
#     term = "Term",
#     estimate = "Estimate",
#     std.error = "Standard Error",
#     statistic = "T Value",
#     p.value = "P Value"
#   ) %>%
#   tab_spanner(
#     label = "Statistics",
#     columns = c(estimate, std.error, statistic, p.value)
#   ) %>%
#   fmt_number(
#     columns = c(estimate, std.error, statistic, p.value),
#     decimals = 3
#   ) %>%
#   data_color(
#     columns = c(p.value),
#     colors = scales::col_numeric(
#       palette = c("red", "yellow", "green"),
#       domain = c(0, 0.05)
#     )
#   )
# 
# # Print the table
# print(table_gt)

# Create table with stargazer
stargazer(model1, model2, model3,
          type = "latex",
          header=FALSE,
          align = TRUE,
          covariate.labels = c("$\\alpha$", "$\\gamma$",
                               "$\\tau$", "Cost", "Starting Q-value",
                               "No. of Weeks", "No. of Students",
                               "No. of Classes",
                               "Charter School", "Paid Zearn Account"),
          omit = c("BIC", "grade", "Poverty"),
          dep.var.labels = "Badges",
          star.cutoffs = c(.05, .01, .001), 
          star.char = c("*", "**", "***"),
          add.lines = list(c("Control for BIC",
                             "", "Yes", "Yes", "", "Yes", "Yes"),
                           c("Control for Grade Level",
                             "", "", "Yes", "", "", "Yes"),
                           c("Control for Poverty Level",
                             "", "", "Yes", "", "", "Yes")))

```

```{r, results='asis'}
#| label: tbl-optimality-2
#| tbl-cap: "Impact of RL parameters on Weekly Badges and Tower Alerts. Six linear regression models were used to examine the correlations between a teacher's RL parameters and two measures of student engagement: average weekly Badges earned per student (Models 1-3) and average weekly Tower Alerts per student (Models 4-6). Models 2-3 and 5-6 also control for other variables including number of active students, number of classes taught, grade level, weeks, poverty level, income level, whether the school is a charter school, and whether the school has a paid account. Coefficients and standard errors are provided for each parameter in each model."

# Create table with stargazer
stargazer(model4, model5, model6,
          type = "latex",
          header=FALSE,
          align = TRUE,
          covariate.labels = c("$\\alpha$", "$\\gamma$",
                               "$\\tau$", "Cost", "Starting Q-value",
                               "No. of Weeks", "No. of Students",
                               "No. of Classes",
                               "Charter School", "Paid Zearn Account"),
          omit = c("BIC", "grade", "Poverty"),
          dep.var.labels = "Tower Alerts",
          star.cutoffs = c(.05, .01, .001), 
          star.char = c("*", "**", "***"),
          add.lines = list(c("Control for BIC",
                             "", "Yes", "Yes", "", "Yes", "Yes"),
                           c("Control for Grade Level",
                             "", "", "Yes", "", "", "Yes"),
                           c("Control for Poverty Level",
                             "", "", "Yes", "", "", "Yes")))

```

```{r}
#| label: fig-heterogeneity-reg
#| fig-cap: "Predicting classroom variables with RL parameters. The data comes from a comprehensive set of classroom records, including income level, poverty level, charter school status, whether a school account is paid, and the number of classes each teaches. These classroom variables (columns) are regressed on the estimated cost parameter, the discount rate, the learning rate, and the inverse temperature. Each cell in the heatmap represents the estimate of a linear, Poisson, or logistic regression model, depending on the variable type. For ordinal classroom variables (Income Level and Poverty Level), ordered logistic regression (proportional odds model) is used, while for binary variables (Charter School, Paid School Account), logistic regression is applied. The number of classes by each teacher, being a count data, is modeled with Poisson regression. The coefficients are scaled estimates of the effect of each parameter on the respective classroom variable. The asterisks indicate the level of statistical significance based on p-values (p < .1; p < .01; p < .001). The coefficients are color-coded with a gradient from light blue (negative) to white (zero) to dark blue (positive)."

# Ensure the predictors are scaled
predictors <- c("alpha", "gamma", "tau", "ev_init", "cost")
# predictors <- c("alpha_w", "alpha_theta", "gamma", "tau",
#                 "cost", "theta_init", "w_init")
full_data[predictors] <- scale(full_data[predictors])

# Define responses for which models will be run
responses <- c("Income", "Poverty", "`Paid Account`", "`No. of Weeks`")
predictors_formula <- paste(predictors, collapse = " + ")

# Run models for each response and extract coefficients
coef_matrix <- map_dfr(responses, function(response) {
  formula <- as.formula(paste0(response, " ~ ", predictors_formula))
  if (response %in% c("Income", "Poverty")) {
    model <- polr(formula, data = full_data, Hess = TRUE)
  } else if (response == "`No. of Weeks`") {
    model <- glm(formula, data = full_data, family = poisson(link = "log"))
  } else {
    model <- glm(formula, data = full_data, family = binomial(link = "logit"))
  }
  
  tidy(model, p.values = TRUE) %>%
    filter(term %in% predictors) %>%
    dplyr::select(term, estimate, p.value) %>%
    mutate(response = response,
           sig = case_when(
             p.value < 0.001 ~ "***",
             p.value < 0.01 ~ "**",
             p.value < 0.05 ~ "*",
             TRUE ~ ""
           ))
})

# Convert to wide format for plotting
coef_matrix <- coef_matrix %>%
  pivot_wider(names_from = term, values_from = c(estimate, p.value, sig)) %>%
  mutate(response = factor(response, levels = responses)) %>%
  arrange(response)

coef_matrix_long <- coef_matrix %>%
  pivot_longer(cols = starts_with("estimate"), names_to = "term", values_to = "estimate") %>%
  pivot_longer(cols = starts_with("sig"), names_to = "term_sig", values_to = "sig")
# Clean up term names
coef_matrix_long$term <- str_replace(coef_matrix_long$term, "estimate_", "")
coef_matrix_long$term_sig <- str_replace(coef_matrix_long$term_sig, "sig_", "")
# Make sure the term columns match
coef_matrix_long <- coef_matrix_long[coef_matrix_long$term == coef_matrix_long$term_sig,]

# Plot the heatmap
ggplot(coef_matrix_long, aes(x = response, y = term, fill = as.numeric(estimate))) +
  geom_tile() +
  geom_text(aes(label = paste0(signif(as.numeric(estimate), 2), sig)), size = 3) +
  scale_fill_gradient2(low = "lightblue", high = "darkblue", mid = "white", 
                       midpoint = 0, limits = c(-1, 1), name = "Estimate") +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
#| label: fig-heterogeneity-pcor
#| fig-cap: "Partial correlations between classroom variables and RL parameters. The data comes from a comprehensive set of classroom records, including income level, poverty level, charter school status, whether a school account is paid, and the number of classes each teaches. These classroom variables (columns) are correlated with the estimated cost parameter, the discount rate, the learning rate, and the inverse temperature, while controlling for the other predictors. Each cell in the heatmap represents the partial correlation coefficient. The asterisks indicate the level of statistical significance based on p-values (p < .1; p < .01; p < .001). The coefficients are color-coded with a gradient from light blue (negative) to white (zero) to dark blue (positive)."

# Ensure the predictors are scaled
predictors <- c("alpha", "gamma", "tau", "ev_init", "cost")

# Define responses for which partial correlations will be calculated
responses <- c("Income", "Poverty",
               "Paid Account", "No. of Weeks")
pcor_list <- pcor(full_data[,c(predictors, responses)] %>%
                    mutate(across(everything(),as.numeric)) %>%
                    na.omit(), method = c("spearman"))
coef_matrix <- pcor_list$estimate[responses, predictors]
pval_matrix <- pcor_list$p.value[responses, predictors]

# Create a dataframe for plotting
coef_matrix_long <- coef_matrix %>%
  as.data.frame() %>%
  rownames_to_column("response") %>%
  pivot_longer(cols = -response, names_to = "term", values_to = "estimate")

pval_matrix_long <- pval_matrix %>%
  as.data.frame() %>%
  rownames_to_column("response") %>%
  pivot_longer(cols = -response, names_to = "term", values_to = "p.value")

coef_matrix_plot <- coef_matrix_long %>%
  left_join(pval_matrix_long, by = c("response", "term")) %>%
  mutate(
    response = factor(response, levels = responses),
    sig = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    )
  )

# Plot the heatmap
ggplot(coef_matrix_plot, aes(x = response, y = term, fill = estimate)) +
  geom_tile() +
  geom_text(aes(label = paste0(signif(estimate, 2), sig)), size = 3) +
  scale_fill_gradient2(low = "lightblue", high = "darkblue", mid = "white",
                       midpoint = 0, limits = c(-1, 1), name = "Partial Correlation") +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Supplemental Figures {#sec-supp-fig}

![Zearn Student Portal](images/student-feed.PNG){#fig-st-portal fig-align="center"}

![Professional Development Calendar](images/PD-calendar.jpg){#fig-prof-dev fig-align="center"}

```{r}
#| eval: false
#| label: fig-raw-data
#| fig-cap: "The raw data from Zearn's platform, with each row corresponding to a different timestamp for a teacher action. The columns represent the different variables, including the number of minutes spent on the platform, the number of badges earned, and the number of boosts used. The data is organized by classroom, with each classroom having a different color. The data is also organized by week, with each week having a different shape. The data is organized by teacher, with each teacher having a different line type. The data is organized by school, with each school having a different line color."

```

{{< pagebreak >}}

```{r}
#| label: fig-income-dist
#| fig-cap: "Distributions of School Socioeconomic Profiles. The first graph categorizes schools into three groups based on the percentage of students eligible for free or reduced-price lunch (FRPL): low-poverty (0-40%), mid-poverty (40-75%), and high-poverty (over 75%). The second graph presents the distribution of median incomes for a school's associated region."
#| fig-subcap: 
#|   - "School Poverty Distribution"
#|   - "Median Income Distribution"
#| layout-ncol: 2

poverty_plot
income_plot

```

```{r}
#| label: fig-teachers-map
#| fig-cap: "Geographic distribution of Zearn teachers across parishes in Louisiana. The color gradient represents the density of teachers, with darker hues indicating a higher concentration of educators using Zearn in each parish. The map also labels the top five cities where Zearn adoption is most prevalent."

map_LA

```

```{r}
#| label: fig-logins-week
#| fig-cap: "Total number of student logins over the 2019-2020 school year. The chart depicts the connection between academic schedules and platform engagement. Each bar represents a week, with peaks corresponding to active school weeks and troughs aligning with major holiday periods (e.g., Thanksgiving and Winter Break)."

logins_week
```

<!-- For histogram above: could already be changed -- the Thanksgiving and Christmas labels are faint on the histogram-->
```{r}
#| eval: false
#| label: fig-panel-nmf-methods
#| fig-cap: ""

# Wrangling the results data frame for plotting
results_df <- map_dfr(results, ~as.data.frame(t(unlist(.))))
# Convert variables to appropriate data types
results_df <- results_df %>%
  mutate(Method = as.factor(Method),
         Component = as.factor(Component),
         Lag = as.numeric(Lag),
         nloglik = as.numeric(nloglik),
         bic = as.numeric(bic)) %>%
  group_by(Method, Component, Lag) %>%
  summarise(avg_bic = mean(bic, na.rm = TRUE),
            avg_nloglik = mean(nloglik, na.rm = TRUE),
            .groups = "drop")
# Filter methods for the plots
# methods_for_plots <- c("FrobeniusNNDSVD", "KullbackLeibler", "FrobeniusNNDSVDA")
methods_for_plots <- c("FrobeniusNNDSVD")
plot_data <- filter(results_df, Method %in% methods_for_plots)



# Shared aesthetics for the plots
aes <- ggplot2::aes(x = Lag, y = value, color = Component)

# Summarize methods not included in the plots and prepare tables
methods_for_summary <- setdiff(unique(results_df$Method), methods_for_plots)
summary_data <- filter(results_df, Method %in% methods_for_summary) %>%
  group_by(Method, Component) %>%
  summarise(BIC = mean(avg_bic, na.rm = TRUE),
            NLL = mean(avg_nloglik, na.rm = TRUE),
            .groups = "drop")
# Rename method
levels(summary_data$Method)[levels(summary_data$Method)=="tch_min"] <- "Minutes"
# Convert to scientific notation
summary_data$BIC <- format(summary_data$BIC, scientific = TRUE, digits = 2)
summary_data$NLL <- format(summary_data$NLL, scientific = TRUE, digits = 2)

# Create tables and remove headers
# bic_table <- summary_data %>% select(Method, BIC) %>%
#   tableGrob(theme = ttheme_minimal(base_size = 8, 
#                                    core=list(bg_params = list(fill = "white", col=NA),
#                                              fg_params=list(fontface=3))),
#              rows = NULL)
# bic_table <- bic_table[-1, ]  # Remove header
bic_data <- filter(plot_data, !is.na(avg_bic)) %>% 
  mutate(value = avg_bic, metric = "BIC")
# BIC Plot
bic_plot <- ggplot(bic_data, aes) + 
  geom_line() + 
  labs(title = "BIC", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none")
  # annotation_custom(bic_table, xmin = max(bic_data$Lag) - 3.5,
  #                   xmax = max(bic_data$Lag) - 0.5,
  #                   ymin = 1.01*max(bic_data$value),
  #                   ymax = Inf)

# nll_table <- summary_data %>% select(Method, NLL) %>%
#   tableGrob(theme = ttheme_minimal(base_size = 8, 
#                                    core=list(bg_params = list(fill = "white", col=NA),
#                                              fg_params=list(fontface=3))), 
#              rows = NULL)
# nll_table <- nll_table[-1, ]  # Remove header
nll_data <- filter(plot_data, !is.na(avg_nloglik)) %>% 
  mutate(value = avg_nloglik, metric = "Negative Log-Likelihood")
nll_plot <- ggplot(nll_data, aes) + 
  geom_line() + 
  labs(title = "Out-of-sample NLL", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none")
  # annotation_custom(nll_table, xmin = max(nll_data$Lag) - 3.5,
  #                   xmax = max(nll_data$Lag) - 0.5,
  #                   ymin = 1.01*max(nll_data$value),
  #                   ymax = Inf)

# Combine the plots and add a legend
combined_plot <- ggarrange(bic_plot, nll_plot,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
combined_plot

```

```{r}
#| label: fig-nmf-pca-comparison
#| fig-cap: "Comparison of dimensionality reduction techniques for teacher and student data. The figures compare the performance of Principal Component Analysis (PCA) against Nonnegative Matrix Factorization (NMF) in reducing the dimensionality of teacher and student data. The NMF variants include the Frobenius norm with two different initialization strategies: Nonnegative Double Singular Value Decomposition (Frobenius NNDSVD) and NNDSVD with the average of the input matrix X filled in place of zeros (Frobenius NNDSVDA). The third NMF variant uses the Kullback-Leibler divergence as the loss function. The left column assesses reconstruction quality using R-squared, where values closer to 1 indicate that the components can better recover the original data. The right column evaluates the interpretability of the low-dimensional representation using silhouette scores. Higher silhouette scores relate to better-defined clusters, values near 0 indicate overlapping clusters, and negative values generally suggest that a sample has been assigned to the wrong cluster."
#| fig-subcap: 
#| - "Teacher Data"
#| - "Student Data"
#| layout-ncol: 1

comparison_plot
comparison_plot2

```

{{< pagebreak >}}

```{r}
#| eval: false
#| cache: true
#| label: fig-parameters-corr
#| fig-cap: "Correlations between the estimated parameters (Cost 1, Cost 2, Cost 3, Gamma, Alpha, Tau) and the outcome variables (average weekly Badges and average weekly Tower Alerts) across the six different linear regression models. Each circle represents a correlation coefficient; the size and shading of the circles indicate the magnitude and direction of the correlation, respectively. The first correlogram (top) relates to the outcome variable of average weekly Badges, while the second correlogram (bottom) relates to the outcome variable of average weekly Tower Alerts."

# Selecting relevant variables for the first model concerning badges
relevant_data_badges <- classroom_data %>%
  dplyr::select(mean_C1, mean_C2, mean_C3, gamma, alpha, tau, badges)

# Similarly, for the fourth model concerning tower alerts
relevant_data_tower <- classroom_data %>%
  dplyr::select(mean_C1, mean_C2, mean_C3, gamma, alpha, tau, tower_alers)

cor_matrix_badges <- cor(relevant_data_badges, use = "complete.obs")
cor_matrix_tower <- cor(relevant_data_tower, use = "complete.obs")

cor_matrix_badges <- cor(relevant_data_badges, use = "complete.obs")
cor_matrix_tower <- cor(relevant_data_tower, use = "complete.obs")

# For badges
corrplot(cor_matrix_badges, method = "circle")

# For tower alerts
corrplot(cor_matrix_tower, method = "circle")

```

{{< pagebreak >}}

```{r}
#| label: fig-log-evidence-histogram
#| fig-cap: "Histogram of Log Evidences for Q-learning and Actor-Critic Models (Preliminary Results). The graph illustrates the distribution of models based on their log evidence (i.e., penalized likelihood) across Actor-Critic and Q-learning models. Log evidence measures how well each model fits the data while accounting for model complexity, with higher values indicating better model fit. These results were obtained using default parameters in the Hierarchical Bayesian Inference method, possibly leading to outlier values in models that did not converge. To obtain more accurate estimates, we selected the top 4 Q-learning and top 10 Actor-Critic models from this initial estimation for further analysis. Their refined log evidence values are presented in the main text."

# Initialize vectors to hold log-evidence values for each model type
ql_log_evidences <- c()
ac_log_evidences <- c()

# Path to the folders containing the .mat files for Q-learning and Actor-Critic models
ql_folder_path <- "CBM/zearn_results/ql_subj_results"
ac_folder_path <- "CBM/zearn_results/ac_subj_results"

# Function to read and accumulate log-evidences from given folder and model type
accumulate_log_evidences <- function(folder_path) {
  log_evidences <- c()
  files <- list.files(path = folder_path, pattern = "\\.mat$", full.names = TRUE)
  
  for (file in files) {
    mat_data <- readMat(file)
    log_evidences <- c(log_evidences, sum(mat_data[["cbm"]][[5]][[2]]))
  }
  
  return(log_evidences)
}

# Accumulate log-evidences for Q-learning and Actor-Critic models
ql_log_evidences <- accumulate_log_evidences(ql_folder_path)
ac_log_evidences <- accumulate_log_evidences(ac_folder_path)

# Create a combined vector of all log-evidences and a factor indicating model type
all_log_evidences <- c(ql_log_evidences, ac_log_evidences)
model_types <- factor(c(rep("QL", length(ql_log_evidences)), rep("AC", length(ac_log_evidences))))

# Create a data frame for ggplot
data <- data.frame(
  LogEvidence = c(ql_log_evidences, ac_log_evidences),
  ModelType = factor(c(rep("Q-learning", length(ql_log_evidences)), rep("Actor-Critic", length(ac_log_evidences))))
)

# Plot using ggplot
ggplot(data, aes(x = LogEvidence, fill = ModelType)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 20) +
  scale_fill_manual(values = c("Q-learning" = brewer.pal(3, "Dark2")[1],
                               "Actor-Critic" = brewer.pal(3, "Dark2")[2])) +
  labs(x = "Log Evidence",
       y = "Frequency") +
  theme_minimal() +
  theme(legend.title = element_blank())



```

<!-- ## Education production function -->

<!-- Previous research has explored how teacher effort affects student achievement. In particular, economists have studied the "education production function," in which educational outcomes are a function of various inputs, including teacher effort, student effort, school resources, and family background [@hedges1994]. This function serves as a theoretical framework for understanding how different factors contribute to educational achievement and how interventions can be designed to improve outcomes. -->

<!-- One of the key inputs in the education production function is teacher effort, as teachers play a crucial role in shaping students' learning experiences and outcomes. Their effort, which encompasses their time, energy, dedication, and instructional strategies, can significantly influence students' academic achievement [@rivkin2005]. However, measuring teacher effort and its impact on student outcomes can be challenging due to the complex and multifaceted nature of teaching. -->

<!-- To address this challenge, researchers have employed various strategies to identify the effects of teacher effort on student achievement. One common approach is to manipulate the conditions under which teachers operate, thereby changing the levels of teacher effort. For instance, @duflo2011 conducted a randomized controlled trial in Kenya to examine the effects of tracking, a practice of grouping students based on their ability levels. Tracking has been a prominent tool in the sociology of education and assumes that teacher inputs will depend on the ability level of the students. This tailoring of teaching strategies and content potentially enhances the instructor's effectiveness. -->

<!-- However, traditional social science approaches to studying the education production function often lack the flexibility to account for changes in context and experience and individual-level differences. Reinforcement learning (RL) offers a promising alternative approach. RL models incorporate a flexibility term (i.e., a learning rate) that allows for changes in behavior with experience and an exploration versus exploitation term (e.g., inverse temperature) that captures individual differences in decision-making strategies. Furthermore, the RL framework inherently accounts for the process of learning about rewards, making it a flexible and dynamic tool for studying teacher behavior [@sutton2018]. -->

<!-- ## Markov Chain Monte Carlo (MCMC) Estimation -->

<!-- ### Reinforcement Learning Model Fit -->

<!-- The reinforcement learning models were implemented using the Stan programming language, a probabilistic programming language designed for statistical inference [@stanmod2022; @gabry2022]. We trained the models on data that included the number of weeks, choices made, and outcomes (log badges) for each classroom. The model parameters, including cost, discount rate, learning rate, and inverse temperature, were estimated from the data. The models employed a Bernoulli logit model to compute action probabilities and updated the expected values of the actions based on prediction errors. The models also generated posterior predictions and computed the log-likelihood for each subject. -->

<!-- Stan employs the Hamiltonian Monte Carlo (HMC) algorithm, a state-of-the-art Markov Chain Monte Carlo (MCMC) method that is particularly well-suited for high-dimensional and complex posterior distributions [@betancourt2017]. We specified three independent MCMC chains to check for convergence of the MCMC algorithm by comparing them. Each chain had 2,500 warmup (burn-in) iterations and 2,500 sampling iterations. During the warmup phase, the HMC algorithm adapts its parameters to the shape of the posterior distribution. The samples drawn during the warmup phase were discarded, and the models ran until they achieved convergence, as assessed by the R-hat statistic, which compares the within-chain and between-chain variance of the MCMC samples; values close to 1 indicate that the chains have converged to the same distribution [@gelman1992]. -->

<!-- #### Hierarchical RL Method -->

<!-- We extended the base reinforcement learning models by incorporating a hierarchical structure to account for individual commonalities and enhance robustness. This hierarchical framework defines individual-level parameters as random effects from a group-level distribution. We used the parameter values found by the non-hierarchical models to generate weakly informed priors for the hyper-parameters (group-level parameters). This approach was necessary to ensure the rapid convergence of the Hamiltonian Monte Carlo algorithm. We specified the priors as follows: -->

<!-- +---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+ -->

<!-- | Parameter           | Group-level Prior                                 | Individual-level Prior                                                            | -->

<!-- +=====================+===================================================+===================================================================================+ -->

<!-- | Cost                | $\mu_{\text{cost}} \sim \mathcal{N}(0.5, 1)$\     | $\text{cost}_{i,j} \sim \mathcal{N}(\mu_{\text{cost}_j}, \sigma_{\text{cost}_j})$ | -->

<!-- |                     | $\sigma_{\text{cost}} \sim \text{Cauchy}(0, 2.5)$ |                                                                                   | -->

<!-- +---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+ -->

<!-- | Discount Rate       | $\mu_{\gamma} \sim \mathcal{N}(0.7, 1)$\          | $\gamma_i \sim \mathcal{N}(\mu_{\gamma}, \sigma_{\gamma})$                        | -->

<!-- |                     | $\sigma_{\gamma} \sim \text{Cauchy}(0, 2.5)$      |                                                                                   | -->

<!-- +---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+ -->

<!-- | Step Size           | $\mu_{\alpha} \sim \mathcal{N}(0.5, 1)$\          | $\alpha_i \sim \mathcal{N}(\mu_{\alpha}, \sigma_{\alpha})$                        | -->

<!-- |                     | $\sigma_{\alpha} \sim \text{Cauchy}(0, 2.5)$      |                                                                                   | -->

<!-- +---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+ -->

<!-- | Inverse Temperature | $\mu_{\tau} \sim \mathcal{N}(1, 1)$\              | $\tau_i \sim \mathcal{N}(\mu_{\tau}, \sigma_{\tau})$                              | -->

<!-- |                     | $\sigma_{\tau} \sim \text{Cauchy}(0, 2.5)$        |                                                                                   | -->

<!-- +---------------------+---------------------------------------------------+-----------------------------------------------------------------------------------+ -->

<!-- where $\mu$ and $\sigma$ denote the group-level hyperparameters, and the subscript $i$ signifies the individual-level parameters. -->

<!-- ### Model Performance -->

<!-- To evaluate the performance of the Bayesian models, we used the Leave-One-Out Information Criterion (LOOIC), a robust measure of model quality. The LOOIC is a variant of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). However, unlike AIC and BIC, which use asymptotic approximations, LOOIC is a fully Bayesian criterion that provides a more accurate estimate of out-of-sample prediction error. We used the `loo` package in R to compute the LOOIC [@vehtari2023]. The package uses Pareto smoothed importance sampling (PSIS), a beneficial technique for models where standard cross-validation is computationally expensive or impractical [@vehtari2017]. -->

```{r Stan Results prep}
#| eval: false
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
gc(verbose = FALSE)
```

```{r}
#| eval: false

# Load the MCMC results
files <- c(
  "Bayesian/Results/Actor-Critic-hierarchical-202403251140-1-914436.csv",
  "Bayesian/Results/Actor-Critic-hierarchical-202403251140-2-914436.csv",
  "Bayesian/Results/Actor-Critic-hierarchical-202403251140-3-914436.csv",
  "Bayesian/Results/Actor-Critic-hierarchical-202403251140-4-914436.csv"
  )
csv_contents <- as_cmdstan_fit(files)
draws_df <- csv_contents$draws()
sum1 <- csv_contents$summary()

# Plotting trace plots for some parameters
color_scheme_set("brightblue")
mcmc_trace(draws_df, pars = c("lp__", "mu_cost", "mu_gamma", 
                              "mu_alpha[1]", "mu_alpha[2]", "mu_tau"),
           n_warmup = 500)

# R-hat statistics (values close to 1 indicate good convergence)
parameter_fit <- sum1 %>%
  filter(grepl("log_lik", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail)
cost_fit <- sum1 %>%
  filter(grepl("cost", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
gamma_fit <- sum1 %>%
  filter(grepl("gamma", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
alpha1_fit <- sum1 %>%
  filter(grepl("alpha\\[1", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
alpha2_fit <- sum1 %>%
  filter(grepl("alpha\\[2", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
tau_fit <- sum1 %>%
  filter(grepl("tau", variable)) %>%
  select(variable, rhat, ess_bulk, ess_tail) %>%
  slice(-c(1))
# Combine all fit data frames into a single data frame
fit_list <- list(
  log_lik_fit, 
  cost_fit, 
  gamma_fit, 
  alpha1_fit, 
  alpha2_fit, 
  tau_fit
)
nrow <- min(sapply(fit_list, nrow))
good_fit <- do.call(
  cbind, sapply(fit_list, function(x) {
    apply(x[1:nrow, -1], 2, function(y) all(y < median(y)))
  }
  )
)
good_fit <- apply(
  sapply(fit_list, function(x) {
    apply(
      apply(x[1:nrow, -1], 2, function(y) y < median(y)),
      1, function(z) any(z))
    }),
  1, all)
  

print(rhats)


mcmc_areas(draws_df, pars = c("alpha", "beta"))




stan_data$outcome
observed_data <- c(...) # You need to define this based on your data
predicted_data <- posterior_predict(csv_contents, draws = 100)
bayesplot::ppc_dens_overlay(y = observed_data, yrep = predicted_data)






# Summarizing parameter estimates
parameter_summaries <- as.data.frame(sum1) %>%
  dplyr::select(variable, mean, sd, `50%`, `2.5%`, `97.5%`) %>%
  rename(Median = `50%`, `Lower CI` = `2.5%`, `Upper CI` = `97.5%`)

kable(parameter_summaries) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))



```

<!-- ### Diagnostics -->

```{r}
#| eval: false
#| echo: false
#| cache: true
#| label: tbl-diagnostics
#| tbl-cap: "Diagnostics for the Bayesian Models used in Q-learning and Q-learning with states. The tables show diagnostics for three Non-negative Matrix Factorization (NMF) models: Frobenius (initialized with a Nonnegative Double Singular Value Decomposition), Frobenius Average (initialized with a Nonnegative Double Singular Value Decomposition with zeros filled with the average of each variable), and Kullback-Leibler. The rows correspond to 'alpha,' the learning rate, 'gamma,' the discount factor, 'tau,' the inverse temperature parameter, 'cost' for each of the components from the NMF, 'sensi,' the reward sensitivity parameter, and 'Log Posterior,' the logarithm of the posterior probability. 'Rhat' indicates the potential scale reduction factor on split chains. Values close to 1.0 imply model convergence. 'ESS Bulk' and 'ESS Tail' represent the effective sample size for the bulk of the posterior distribution and its tail, respectively. An ESS of at least a few hundred is often considered sufficient for estimation purposes."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]
results_files <- results_files[!grepl("hierarchical", results_files)]
diag_df <- data.frame()

for (result_file in results_files) {
  fit <- readRDS(result_file)
  
  # Get summary
  fit_summary <- fit$summary()
  
  # Create a data frame of the Rhat values and ESS
  result_df <- data.frame(
    Model = gsub("Bayesian/Results/||.RDS", "", result_file),
    Variable = fit_summary$variable,
    Rhat = fit_summary$rhat,
    ESS_Bulk = fit_summary$ess_bulk,
    ESS_Tail = fit_summary$ess_tail
  )
  
  # Add result to the overall diagnostics data frame
  diag_df <- rbind(diag_df, result_df)
}
# Reshape the data frame
diag_df <- diag_df %>%
  pivot_longer(cols = c(Rhat, ESS_Bulk, ESS_Tail),
               names_to = "Statistic") %>%
  pivot_wider(names_from = c(Model, Statistic),
              values_from = value) %>%
  mutate(Variable = if_else(Variable == "lp__", "Log Posterior", Variable))

# Split the data into two separate data frames
diag_df_actor_critic <- diag_df[,c(TRUE, grepl("Actor-Critic-", names(diag_df))[-1])]
diag_df_q_learning_states <- diag_df[,c(TRUE, grepl("Q-learning-states-", names(diag_df))[-1])]
diag_df_q_learning <- diag_df[,c(TRUE, (!names(diag_df) %in% names(diag_df_q_learning_states) &
                                          !names(diag_df) %in% names(diag_df_actor_critic))[-1])]

# Define the spanner labels for each model
diag_df_q_learning %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_q_learning_states %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_actor_critic %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

```

<!-- ### Estimates -->

<!-- This section presents the group parameter estimates for all Reinforcement Learning (RL) models, both non-hierarchical and hierarchical. To generate these group parameter estimates, we first extract the posterior samples from the fitted models and calculate the mean (M), standard deviation (SD), 25th percentile (Q1), and 75th percentile (Q3) for each parameter. @tbl-group-parameters summarizes the distribution of the parameter estimates across classrooms, revealing their typical values and variability. -->

```{r}
#| eval: false
#| echo: false
#| cache: true
#| label: tbl-group-parameters
#| tbl-cap: "Group parameter estimates for the Bayesian Models used in Q-learning and Q-learning with states."

# Define a function that processes one file
process_file <- function(result_file) {
  # Load the fitted model
  fit <- readRDS(result_file)
  
  # Extract the posterior samples of the parameters
  post_samples <- fit$draws()
  
  # Calculate the group parameter estimates
  group_params <- post_samples %>%
    as.data.frame() %>%
    summarise(across(everything(), list(M = mean, SD = sd, Q1 = ~quantile(., 0.25), Q3 = ~quantile(., 0.75))))
  
  # Add the model name to the data frame
  group_params$Model <- gsub("Bayesian/Results/||.RDS", "", result_file)
  
  return(group_params)
}

# Apply the function to each results file and bind the results into one data frame
group_params_df <- purrr::map_dfr(results_files, process_file)

# Continue with the rest of your code
group_params_df %>%
  gt() %>%
  tab_header(
    title = "Group Parameter Estimates",
    subtitle = "The table shows the mean (M), standard deviation (SD), 25th percentile (Q1), and 75th percentile (Q3) of the learning rate (alpha), inverse temperature (beta), weights, and cost parameters for each model."
  ) %>%
  cols_label(
    Model = "Model",
    M = "Mean",
    SD = "Standard Deviation",
    Q1 = "25th Percentile",
    Q3 = "75th Percentile"
  ) %>%
  fmt_number(
    columns = c("M", "SD", "Q1", "Q3"),
    decimals = 2
  )


```

<!-- The trade-off between model complexity and predictive accuracy justifies including four lags. Including more lags would increase the complexity of the model, potentially leading to overfitting and poorer predictive performance. Conversely, including fewer lags might result in a model that fails to capture critical temporal dependencies in the data. The selection of four lags represents an inflection point in the BIC and NLL, indicating an optimal balance between model complexity and predictive accuracy. -->

<!-- In @fig-lags, we present the estimated coefficients of the lagged variables as derived from the random effects models. The lines represent the regression coefficients of different variables and their standard errors. The grey line and shaded area correspond to the coefficients for the lagged Badges per Student variable. This graphical representation elucidates the diminishing influence of each variable as the lag increases. -->

<!-- Taking the Frobenius 1 component as an example, the coefficient for the first lag is approximately 0.30, accompanied by a standard error of 0.25. As the lag increases, there is a noticeable decrease in the coefficient, implying a waning influence of this component over time. In contrast, the Badges per Student variable demonstrates a different pattern. First, the magnitude of these coefficients is smaller and non-significant. The coefficient also fluctuates around zero as the lag increases, suggesting a relatively consistent influence over time. -->

<!-- In our pursuit to identify the most parsimonious model that optimally fits the data, we conducted a comparative analysis of the out-of-sample negative likelihood (NLL) across four distinct reinforcement learning models: State-Based Q-Learning, Kernalized Q-Learning, State-Free Q-Learning, and Actor-Critic. We evaluated these models using three different methods for non-negative matrix factorization: Frobenius (NNDSVD), Frobenius (NNDSVDA), and Kullback-Leibler. Each cell in @fig-panel-bic represents the median negative log-likelihood of a model given a specific method, with lower values signifying a superior model fit. -->

<!-- Our analysis reveals that the Kernalized Q-Learning model, when evaluated using the Frobenius (NNDSVD) method, provides the most optimal fit to the data, as evidenced by its lowest negative log-likelihood value. As a result, we select this combination of model and method for our remaining analyses. -->

```{r}
#| eval: false
#| echo: false
#| label: tbl-choose-RL-model
#| tbl-cap: "The table presents a comparison of the median negative log-likelihood values for posterior distributions across four reinforcement learning models: State-Based Q-Learning, Kernalized Q-Learning, State-Free Q-Learning, and Actor-Critic. These models are evaluated based on three methods for non-negative matrix factorization: Frobenius (NNDSVD), Frobenius (NNDSVDA), and Kullback-Leibler. Each cell in the table represents the median negative log-likelihood of a model's posterior given a method, with lower values indicating better model fit. The best performing combination of model and method (i.e., the one with the lowest negative log-likelihood value) is highlighted in light green."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]

# Function to extract median of lp__ from a model
extract_lp_median <- function(model_file) {
  fit <- readRDS(model_file)
  lp_median <- fit$summary("lp__", "median")  # Get the median of lp__
  data.frame(Model = gsub("Bayesian/Results/||.RDS", "", model_file),
             NLL_Median = -1 * as.numeric(lp_median))
}

# Apply function to all models and combine into one dataframe
lp_df <- purrr::map_df(results_files, extract_lp_median) %>%
  na.omit()
lp_df$Method <- gsub(".*-", "", lp_df$Model)
lp_df$ModelType <- stringr::str_extract(lp_df$Model, ".*(?=-)")

# Define descriptive names
model_names <- c("Q-learning-states" = "State-Based Q-Learning",
                 "Q-learning-kernel" = "Kernalized Q-Learning",
                 "Q-learning" = "State-Free Q-Learning",
                 "Actor-Critic" = "Actor-Critic")

method_names <- c("FR" = "Frobenius (NNDSVD)",
                  "FRa" = "Frobenius (NNDSVDA)",
                  "KL" = "Kullback-Leibler")

# Apply the descriptive names
lp_df$ModelType <- model_names[lp_df$ModelType]
lp_df$Method <- method_names[lp_df$Method]

# Create the table with models as rows and NMF methods as columns
table_df <- lp_df %>%
  dplyr::select(-Model) %>%
  tidyr::pivot_wider(names_from = Method, values_from = NLL_Median) %>%
  arrange(desc(ModelType)) %>%
  na.omit()

# Remove redundant 'ModelType' label
colnames(table_df)[colnames(table_df) == "ModelType"] <- "Model"

# To highlight the best value in the table
table_df %>%
  gt() %>%
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      columns = names(table_df)[colSums(table_df == min(table_df[,-1])) >= 1],
      rows = rowSums(table_df == min(table_df[,-1])) >= 1
    )
  )

```

<!-- We compared our base models, which used logistic regression, and our top-performing reinforcement learning (RL) model, which employed kernelized Q-learning, to identify which best fit the data. @tbl-RL-logit-comp presents the Leave-One-Out Information Criterion (LOOIC) estimates for these models, with lower values indicative of superior model performance. -->

<!-- Our analysis revealed that the hierarchical Q-learning model outperformed the others, as evidenced by its lowest LOOIC value. This finding suggests that reinforcement learning provides a more accurate representation of teacher behavior on Zearn when individual parameters are fitted. The hierarchical logistic regression model followed closely, demonstrating competitive performance. However, the models that did not incorporate a hierarchical structure yielded higher LOOIC values, indicating a lesser fit to the data. These findings highlight the significant heterogeneity in the data and emphasize the value of reinforcement learning models, particularly those with a hierarchical structure, in accurately capturing the dynamics of teacher behavior on Zearn. -->

```{r Bayesian LOOIC prep}
#| eval: false
#| echo: false
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py", "get_lag_value")))
gc(verbose = FALSE)
```

```{r}
#| eval: false
#| echo: false
#| cache: true
#| label: tbl-RL-logit-comp
#| tbl-cap: "Model comparison using Leave-One-Out Information Criterion (LOOIC). LOOIC values, a measure of model quality, were calculated for each model type. Lower values indicate better model performance. Q-learning models were built using a kernel-based approach. Hierarchical models incorporate a hierarchical structure to account for classroom-level variations."

# Non-hierarchical models
## Q-learning model
# post <- read_rds("Bayesian/Results/Q-learning-kernel-FR.RDS")
post <- read_rds("Bayesian/Results/Q-learning-FR.RDS")
loo_qlearn <- post$loo()$estimates["looic", ]
## Logit
models_nh <- readRDS("Bayesian/Results/logit.RDS")
loo_nh <- models_nh$loo()$estimates["looic", ]
# nll_nh <- lapply(models_nh, log_lik)
# nll_nh <- lapply(nll_nh, mean)

# Hierarchical models
## Q-learning
post_hierarchical <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
loo_qhierarchical <- post_hierarchical$loo()$estimates["looic", ]
## Logit
models_h <- readRDS("Bayesian/Results/logit-hierarchical.RDS")
loo_h <- models_h$loo()$estimates["looic", ]

# Collect LOOIC values
looic_values <- c(loo_qlearn[1],loo_nh[1],
                  loo_qhierarchical[1], loo_h[1])
df_looic <- data.frame(Model = c("Q-learning", "Logistic regression", "Hierarchical Q-learning", "Hierarchical logistic regression"),
                       LOOIC = looic_values)

# Create gt table
df_looic %>%
  gt() %>%
  cols_label(
    Model = "Model",
    LOOIC = "LOOIC Value"
  )

# Create gt table
gt(df_looic)

```

<!-- We compared the hierarchical model's predictions and the original choice data, representing a distinct action. We averaged the model predictions across teachers weekly and overlayed them with the average action from the choice data. We also calculated the standard error around these averages accounting for missing data, which provided a measure of uncertainty around these values. -->

<!-- @fig-model-fit shows the model fit for each action where the y-axis represents the probability of a=1, and the x-axis represents the time in weeks. The line plot includes the mean probabilities, highlighted by colored lines, and their respective standard errors, represented by shaded ribbons surrounding the lines. The results section of the data for 'Action 1' revealed that the model predictions initially remained relatively close to the mean probability of 0.5, demonstrating some variability but remaining within a reasonable range. The model's variability increased over time, denoted by the broadening standard error ribbons, peaking at week 36 with a significant increase in the mean predicted probability. This peak corresponded to a drastic decline in the choice data, implying a divergence between the model's predictions and the data towards the end of the observed period. -->

<!-- In the case of 'Action 2' and 'Action 3,' the model prediction started at the mean probability of 0.5 and demonstrated a declining trend over the weeks. Although the declining trend was present in both model fit and data, the model predicted a less drastic decline. The standard error for this action also increased over time, albeit not as dramatically as in the case of 'Action 1', indicating that the model's predictions became more uncertain as time progressed. Despite this variability, the model provided a reasonable fit for the choice data across the weeks for the first months of the study period. -->

```{r}
#| eval: false
#| echo: false
#| cache: true
#| label: fig-model-fit
#| fig-cap: "Model predictions for teacher actions compared with choice data over time. The y-axis denotes the probability of a particular action being taken (a=1), while the x-axis indicates time in weeks. The figure showcases three distinct actions (NMF components). The solid lines represent the weekly averaged model predictions for each action, while the shaded ribbons denote the respective standard errors."
stan_data <- read_rds("./Bayesian/Q-learn-data.RDS")
choice_data <- read_rds("./Bayesian/Results/Q-learning-FR.RDS")
choice_data <- choice_data$summary()

prediction_hierarchical <- read.csv("./Bayesian/Results/prediction_hierarchical.csv") %>%
  filter(grepl("y_pred", variable)) %>%
  dplyr::select(variable, mean)

prediction_hierarchical <- prediction_hierarchical %>%
  mutate(variable = str_extract(variable, "\\[.*\\]"),
         variable = str_replace_all(variable, "\\[|\\]", "")) %>%
  separate(variable, into = c("dim1", "dim2", "dim3"), sep = ",", convert = TRUE)

prediction_hierarchical_3d <- array(dim = c(max(prediction_hierarchical$dim1),
                                            max(prediction_hierarchical$dim2),
                                            max(prediction_hierarchical$dim3)))

for (i in 1:nrow(prediction_hierarchical)) {
  dim1 <- prediction_hierarchical$dim1[i]
  dim2 <- prediction_hierarchical$dim2[i]
  dim3 <- prediction_hierarchical$dim3[i]
  prediction_hierarchical_3d[dim1, dim2, dim3] <- prediction_hierarchical$mean[i]
}

# Loop through each subject
for (subject in seq_len(dim1)) {
  # Get the value of Tsubj for this subject
  Tsubj_value <- stan_data[["Tsubj"]][subject]
  
  # Set the values in prediction_hierarchical_3d to NA
  if (Tsubj_value != max(stan_data[["Tsubj"]])) {
    prediction_hierarchical_3d[subject, (Tsubj_value + 1):dim2, ] <- NA
    # Set the values in choice_data to NA
    choice_data[subject, (Tsubj_value + 1):dim2, ] <- NA
  }
}

# Get the number of layers
num_layers <- dim(prediction_hierarchical_3d)[3]
# Custom function to calculate standard error based on the number of non-NA elements
calc_se <- function(x) sd(x, na.rm = TRUE) / sqrt(sum(!is.na(x)))

df_compare <- data.frame()

# Generate a plot for each layer
for (k in 1:num_layers) {
  y_pred_avg <- apply(prediction_hierarchical_3d[, , k], 2, mean, na.rm = TRUE)
  y_pred_se  <- apply(prediction_hierarchical_3d[, , k], 2, calc_se)
  
  choice_data_avg <- apply(choice_data[, , k], 2, mean, na.rm = TRUE)
  choice_data_se  <- apply(choice_data[, , k], 2, calc_se)
  
  # Only consider weeks with valid SEs
  weeks <- seq_len(sum(!is.na(y_pred_se)))

  df_pred <- data.frame(weeks = weeks,
                        probability = y_pred_avg[weeks],
                        type = rep("Model Fit", max(weeks)),
                        se = y_pred_se[weeks],
                        action = rep(paste("Action", k), max(weeks)))

  df_real <- data.frame(weeks = weeks,
                        probability = choice_data_avg[weeks],
                        type = rep("Real Data", max(weeks)),
                        se = choice_data_se[weeks],
                        action = rep(paste("Action", k), max(weeks)))

  df_compare <- rbind(df_compare, df_pred, df_real)
}

p <- ggplot(df_compare, aes(x = weeks, y = probability, color = type)) +
  geom_line() +
  geom_ribbon(data = df_compare, aes(ymin = probability - se, ymax = probability + se, fill = type), alpha = 0.1) +
  labs(x = "Week", y = "Probability of a=1") +
  facet_wrap(~action, ncol = 1) +
  scale_color_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  scale_fill_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  theme_bw()

print(p)

```

```{r}
#| eval: false

logit_model <- readRDS("./Bayesian/Results/logit-hierarchical.RDS")
pred_logit_model <- lapply(logit_model, function(model) {
  fitted(model)
})
data_logit_model <- lapply(logit_model, function(model) {
  model$data[,1]
})
logit_pred_3d <- array(NA, dim = dim(prediction_hierarchical_3d))
choice_data_3d <- array(NA, dim = dim(prediction_hierarchical_3d))

# Loop through each subject
for (subject in seq_len(dim1)) {
  Tsubj_value <- stan_data[["Tsubj"]][subject]
  
  # Calculate the starting and ending indices for the current subject
  start_idx <- ifelse(subject == 1, 1,
                      sum(stan_data[["Tsubj"]][1:(subject-1)]) - subject + 2)
  end_idx <- sum(stan_data[["Tsubj"]][1:subject]) - subject
  
  # Loop through each model in pred_logit_model
  for (model_idx in seq_along(pred_logit_model)) {
    # Extract the estimated probabilities for the current model
    model_predictions <- pred_logit_model[[model_idx]][start_idx:end_idx,"Estimate"]
    real_data <- data_logit_model[[model_idx]][start_idx:end_idx]

    # Populate logit_pred_3d with the predicted values
    logit_pred_3d[subject, 2:(Tsubj_value), model_idx] <- model_predictions
    choice_data_3d[subject, 2:(Tsubj_value), model_idx] <- real_data
  }
}

# Generate a plot for each layer
df_compare <- data.frame()
for (k in 1:dim3) {
  y_pred_avg <- apply(logit_pred_3d[, , k], 2, mean, na.rm = TRUE)
  y_pred_se <- apply(logit_pred_3d[, , k], 2, calc_se)
  
  choice_data_avg <- apply(choice_data_3d[, , k], 2, mean, na.rm = TRUE)
  choice_data_se  <- apply(choice_data_3d[, , k], 2, calc_se)

  # Only consider weeks with valid SEs
  weeks <- seq_len(length(y_pred_se))[!is.na(y_pred_se)]
  idx_shift <- weeks[1] - 1

  df_pred <- data.frame(weeks = weeks,
                        probability = y_pred_avg[weeks],
                        type = rep("Model Fit", length(weeks)),
                        se = y_pred_se[weeks],
                        action = rep(paste("Action", k), length(weeks)))

  df_real <- data.frame(weeks = weeks,
                        probability = choice_data_avg[weeks],
                        type = rep("Real Data", length(weeks)),
                        se = choice_data_se[weeks],
                        action = rep(paste("Action", k), length(weeks)))

  df_compare <- rbind(df_compare, df_pred, df_real)
}

p <- ggplot(df_compare, aes(x = weeks, y = probability, color = type)) +
  geom_line() +
  geom_ribbon(data = df_compare, aes(ymin = probability - se, ymax = probability + se, fill = type), alpha = 0.1) +
  labs(x = "Week", y = "Probability of a=1") +
  facet_wrap(~action, ncol = 1) +
  scale_color_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  scale_fill_manual(values = c("Model Fit" = "blue", "Real Data" = "red")) +
  theme_bw()

print(p)

```

---
title: "Predicting Repeated Behavior in Behavioral Sciences"
subtitle: "Applying Reinforcement Learning in Teacher Decision-Making"
abstract: "This paper aims to model the decision-making process of a teacher in a math-teaching platform named Zearn with an RL algorithm. Akin to a multi-armed bandit, teachers choose how much effort to put in per week (in minutes) based on a function of the cost of teachers’ time and the number of lessons their students earned as rewards. Every teacher is attempting to both “learn” (i.e., explore) and  “optimize” (i.e., exploit) their number of minutes spent on Zearn and learn over time how they should engage with the platform. We find that teachers who prefer to explore... , whereas teachers who prefer to exploit..."
keywords: "Reinforcement Learning, education, habits"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
format:
  elsevier-pdf:
    journal:
      keep-tex: true
      formatting: preprint
      model: 3p
      layout: onecolumn
      cite-style: number
      # graphical-abstract: "![](abstract.png)"
bibliography: zearnrefs.bib

execute:
  echo: false
  warning: false
  error: false
---

# Introduction

## Predicting Repeated Behavior in the Behavioral Sciences

Predicting repeated behavior has been a goal of the behavioral sciences, including economics, psychology, and neuroscience. One prominent way of quantifying this relationship is through reinforcement learning (RL) algorithms that assign a mathematical relationship between contextual cues (states), behavior (actions), and reward (CITE RL Neuroecon). In general, reinforcement learning is widely used in both neuroscience and computer science on extremely large data sets to help model agents in specific environments. Psychology and economics generally do not create dynamic models across time (at least in this way), but computer science and neuroscience generally do not work with such practical and applied data. This presents us with a novel opportunity to use methods from one set of disciplines on data that is traditionally used in another set of disciplines. <!--# Add advance organizer -->

## A Novel Approach

ADD: goals, research questions, hypotheses, overall contribution to the field.

This paper aims to model the decision-making process of a teacher in a math-teaching platform named Zearn (CITE Zearn) with an RL algorithm. Given these instructors' context, we argue that an RL model is most appropriate. Using both real and simulated data, we model the teachers as decision makers in a reinforcement learning context, akin to a multi-armed bandit, choosing how much effort to put in per week (in minutes) based on a function of cost of teachers' time and the number of badges their students earned in previous week's rewards. We assume that every teacher has an objective function where they want to balance how much time they spend using Zearn (e.g., assigning homework, checking student progress, and reviewing content) with the potential for students to receive badges. Every teacher is attempting to both "learn" and "optimize" their number of minutes spent on Zearn and learn over time how they should engage with the platform assuming that the relationship between teacher minutes and student badges is stochastic. There is a tradeoff between learning, also known as exploring (spending varying amounts of time on the platform to see how effort affects student achievement) and optimizing, also known as exploiting (spending amounts of time on the platform that teachers know already have good outcomes without wasting their time), that our model is able to explicitly quantify (learning rate and inverse temperature for the algorithmically informed). The RL algorithm allows for this type of flexibility for learning the best strategy given certain contextual information by modeling the tradeoff between teachers exploring unknown options while exploiting the information they have about the Zearn system. Inputs can, and perhaps should be flexible to temporary needs by students. For example, on a week in which students struggle more, the teacher should adjust their effort accordingly. As such, reinforcement learning offers a flexible and robust model for our available data.

# Context and Research Questions

## The Zearn Platform

<!-- ![Teacher and Students](https://assets-global.website-files.com/60ad603a6b6b23851c3fb0d8/60f850148c03471458e4be28_hithere-poster-00001.jpg) *Image from [Zearn](https://about.zearn.org)* -->

Zearn is an online math-teaching platform.

-   Model the decision-making of teachers

-   Understand how they adapt their teaching strategies to optimize student achievement.

## Data - Zearn Platform

Personalized learning experience for students. Teachers track student progress and make informed decisions.

-   **Classroom structure:** Self-paced online lessons and small group instruction.

-   **Badge system for student achievement:** Students earn badges upon completing lessons (mastery of specific skills). Track student progress and motivate them to continue learning.

-   **Tower Alerts:** Real-time notifications sent to teachers when a student struggles with a specific concept. Teachers can provide support and address learning gaps.

-   **Teacher selection and criteria:** Consistently use the platform and work in traditional school settings.

-   **Variables of interest:** Teacher effort, student performance, lesson completion, and the time spent by both teachers and students on the platform.

## Research Questions

<!-- Get some ideas from the mega-study prereg -->

# Theory

<!-- Literature Review -->

## Teacher effort and student achievement

### Education production function

Previous research has explored how teacher effort affects student achievement. In particular, economists have studied the "education production function," in which teacher inputs are directly related to student outputs (see [@duflo2011]). One common approach to identifying this function's effects is to change the context under which teachers operate, thereby modulating the input levels. For example, [@duflo2011] test the effects of tracking: the separation and placement of students into level-appropriate classrooms. Tracking has been a prominent tool in the sociology of education and assumes that teacher inputs will depend on the classroom organization.

These common social science approaches lack the flexibility of a model that allows for changes with context and experience and individual-level differences. On the other hand, RL provides a flexible paradigm by fitting a flexibility term (i.e., a learning rate) and an exploration versus exploitation term (e.g., inverse temperature). The process of learning the reward makes the model inherently flexible.

In RL, an agent aims to learn a policy that produces the highest possible reward. A policy is a mapping from states to actions (or, most commonly, a probability distribution over actions) ([@sutton2018] Sutton and Barto 1998). Central to RL is the idea of learning a policy when its parameters are not known ahead of time.

This model has been used before in the context of teaching and education. One of the first researchers of Markov decision processes, Ronald Howard, attempted to apply his mathematical framework to instruction theory (CITE Howard 1960). In 1972, Richard Atkinson proposed a theory of instruction that requires "(1) A model of the learning process, (2) Specification of admissible instructional actions, (3) Specification of instructional objectives, (4) A measurement scale that permits costs to be assigned to each of the instructional actions and payoffs to the achievement of instructional objectives" (CITE Atkinson 1972).

Atkinson (CITE 1972) then described this model as a Markov decision process, including components well-known in modern RL theory: states, actions, transition probabilities, reward functions, and a time horizon. In this framework, actions are instructional activities (e.g., assigning problem sets) that can change a given state (e.g., student learning level). These changes of states can yield to reward minus the associated cost of the action. For example, a teacher may be rewarded by an increase in the knowledge or skill of a student, but such reward must be balanced with its associated effort (e.g., labor cost). Atkinson and colleagues continued to test many parametrizations of this idea (see CITE Doroudi et al. 2019 for a full review).

Recent work in the psychology of habit uses these Markov decision process models to explain learning and reward association (CITE). One common approach in humans is to apply the so-called "multi-armed bandit" task. In this type of experiment, participants are presented with multiple actions, each with an unknown payoff. The subject's goal is to learn the best outcome through trial and error. In the beginning, the reward-action relationships are unknown, so the participant must explore or sample from each action (CITE).

<!--# ADD: How does it fill gaps in the lit? -->

### Context and experience

## Reinforcement Learning to Capture Patterns in Repeated Behavior

### Why Reinforcement Learning?

Before presenting these models, we argue for the usefulness of RL in our setting. Unlike the traditional economics approach that maps teacher effort to student outcome through an education production function fixed in time, we aim to model teacher behavior as a flexible, context-dependent process that can change week by week. Reinforcement learning allows us to model how individual teachers may learn the action-reward (or action-state-reward) relationships, thus creating a typification of instructors. It is then possible to pinpoint how teachers differ in learning and behavior and how these characteristics relate to student outcomes. For example, teacher flexibility may be optimal in a learning environment, but without estimation of individual parameters, differentiating teachers would not be possible. Further, if a policy-maker can shift these individual-level parameters, they can affect student outcomes. These so-called "counterfactual analyses" can be powerful tools in creating innovative interventions or nudges to improve an outcome of interest.

The model carries potential beyond the goals of this study, as the model can learn the associations between actions and rewards, allowing for automation of certain instructional inputs. For example, if a teacher often assigns an activity under a certain state, the model could automate this action, freeing some of the teacher's time.

-   RL is inspired by the way animals learn from their experiences
-   An agent in RL represents a decision-maker
-   Actions: choices made by the agent
-   Environment: the context in which the agent makes decisions
-   Observations: information the agent receives about the environment
-   Rewards: feedback received by the agent based on the actions taken

In the context of predicting repeated behavior, RL algorithms can be used to model the decision-making process of individuals or groups, such as teachers, by learning from the patterns in their actions and the resulting outcomes.

RL: an **agent** learns to make decisions by interacting with an **environment**. Through [trial and error]{.underline}, agent learns the best **actions** to take in different situations to achieve its **goals**.

$$
\text{Agent} \xrightarrow[\text{Actions}]{\text{Performs}} \text{Environment} \xrightarrow[\text{Observations, Rewards}]{\text{Provides}} \text{Agent}
$$

**Suitability for modeling teacher decision-making**

-   Captures the dynamic and sequential nature of teaching

-   Example: $s_t = (TowerAlerts_t, RD\_resources\_t)$, $a_t = (RD\_small\_group\_lessons\_t, TimeSpent\_t)$

-   Allows for the exploration of optimal teaching strategies in response to students' progress and engagement

-   Example: Balancing between focusing on struggling students and challenging high-performing students

**Assumptions, objective functions, and tradeoffs**

Assumes teachers make decisions to maximize long-term rewards (e.g., student learning outcomes)

Objective: $\max_{\pi} \mathbb{E}[\sum_{t=0}^T \gamma^t r(s_t, a_t) | \pi]$

Balances the tradeoffs between exploration (trying new teaching strategies) and exploitation (using known effective strategies)

Example: $\epsilon$-greedy strategy

**Flexibility and robustness**

-   Adapts to changes in the learning environment and individual student needs

-   Example: Adapting to new curriculum or varying levels of student preparedness

-   Allows for the incorporation of various state, action, and reward variables

-   Example: Including external factors such as school policies or testing schedules

-   Can be tailored to different educational contexts and objectives

-   Example: Customizing the model for different grade levels

-   Tradeoff between learning (exploring) and optimizing (exploiting)

**Example:**

-   State: Current progress of students in the class.
-   Actions: Assigning additional practice, providing personalized feedback, adjusting lesson plans.
-   Rewards: Improved student performance, student engagement, reduced learning gaps.

$$
State (S) \xrightarrow[\text{Action (A)}]{\text{Teacher Decides}} New State (S') \xrightarrow[\text{Reward (R)}]{\text{Resulting Outcome}} \text{Feedback}
$$

### Q-Learning Model

The first class of models we apply to our data is the so-called Q-learning algorithm. This model is inspired by the so-called "multi-armed bandit" problem. In this paradigm, an agent has a finite number of choices, each associate with a given reward. The agent must simply learn to choose which action yields the highest reward. Learning in this setting occurs by adjusting expectations and minimizing "surprises" (i.e., prediction errors). Notice that this setting does not require us to define a given state: in our setting, Q-learning assumes that the best action in one week is the best action at any other week. Thus, the model only prescribes an action-reward relationship. The teacher here learns the value of logging in regardless of the history of their classroom or students.

In this Q-learning model, the reward at a given week is given as

R(a\|St)=Badges(St+1\|a)-cost(a)

where is a discount factor, cost(not work)=0, and cost(work)=c is a free parameter. However, we know, qualitatively, that teachers may expect the reward of a given input to come after a couple of weeks following the action. In particular, some Zearn teachers have mentioned that they spend the first week of each month on the platform planning activities for the rest of the month. As such, it is expected that the reward associated with a given action will be delayed.

In order to capture this property, we add a modification to this model. We define an 3-step return as the average of the first 3 rewards (i.e., at the end of each week, at the end of the following week, and at the end of the week after that). As such, the reward at a given week is

R(a\|St)=

(1-w1-w2)2Badges(St+2\|a)+w2Badges(St+1\|a)+w1Badges(St\|a)-cost(a)

where 0w1,w2,w31 are the weights associated with the rewards from the current week, next week, and the week following that, respectively. These parameters allow the flexibility to estimate the reward horizon of each teacher in the platform.

### Actor-Critic Model

1.  Full RL framework
2.  Policy learning and state value learning

On the other hand, our second model uses the full RL framework by defining states, the relationship between actions and states, and the relationship between states and rewards. In particular, we apply an Actor-Critic algorithm to fit our data. In this model, teachers do not learn the value of logging in. Instead, (1) the "critic" learns the value of each week (the states) and (2) the "actor" learns a function that determines what action to take (i.e., a parameterized policy). Importantly, the teacher may decide to login independently from the value of their actions, but the value of each week may affect the functional form of the choice function. This type of model is beneficial because it offers higher flexibility in the decision process. Because actions are a function of how the last week went for the students (the state), the Actor-Critic method can provide more powerful predictions if teachers indeed modulate their choice based on these measures. In RL terms, this model is considered fully online (updates to the actions, states, and values occur at each time step). Mathematically, we define these objects and functions as:

S={Student Minutest-1,Tower Alertst-1}

A=Prob(login)=Logit-1(S)

VS=WS

where S are the states, V are the values of each state, and A is the parameterized policy. Each teacher learns the policy and value functions by increments, taking steps determined by gradient ascent of a scalar performance measure (see appendix for detailed algorithm).

#### Eligibility traces for delayed rewards

Another difference between this model and our application of Q-learning is how we operationalize the potential delayed nature of rewards. With the 3-step algorithm, we applied a forward view, in which teachers decided how to update each action value by looking forward to future rewards.

In contrast, our Actor-Critic model is oriented backward in time. Each update depends on the prediction error, as in the previous model, but the model also assigns this error backwards to each prior state. In this paper, we estimate how much a given state in the past contributed to the current update process. This method is called eligibility traces, and in our case it is determined by trace decay rates W,\[0,1\]. These trace decay rates simply accumulate a scalar history of the previous gradient ascents of our performance measure. That is, the farther a week is in the past, the weaker the effect it will have on updating this week's actions and state values.

Eligibility traces provide an efficient, incremental way of implementing the temporal effects of rewards on our action and value functions. In particular, this technique is beneficial in settings like Zearn, where rewards are potentially delayed by many steps, and teachers have limited time to learn the optimal policy.

### Gaussian Policy Model

D. Continuous Control through a Gaussian Policy 1. Probability distribution over actions\
Given the continuous nature of some of our data (i.e., minutes teachers spent on the Zearn platform), it is possible to learn statistics of the probability distribution over actions instead of computing learned probabilities for each of many discretized actions. That is, instead of using the inverse logit function to calculate the probability of logging in, this model chooses actions (i.e., number of minutes teachers spent on the platform) from a Gaussian distribution.

Some of the most attractive features of these models are their inherent flexibility, and the typification of teachers. We can, in each of these models, extract, respectively: (1) a learning rate, an inverse temperature, and a cost; and (2) step sizes (akin to learning rates) for value weights and policy weights, and a cost. We can consequently derive a time series of averages and standard deviations of the gaussian policy.

<!-- Literature Review -->

## RL in teaching and education

1.  Markov decision processes
2.  Instructional actions, objectives, and costs

## Applying Models to Zearn Data

In simple terms, reinforcement learning may be represented as a relationship between an agent, their actions, states, and rewards, as the following image depicts (CITE Sutton Barto).

In the case of Zearn, we define this decision process as follows: (1) agents are the teachers; (2) actions are their decision to login; (3) the environment is the Zearn platform with its students; (4) the rewards are the average number of badges students attained on a given week; and (5) the states are vectors of Tower Alerts and student usage minutes from the previous week.

Mathematically mapping the agent-environment interaction, however, is a flexible endeavor, in which a large number of models may satisfy our initial assumptions. Thus, we approach this problem as a competition of models, in which the best fit for the data, given a penalty for the number of parameters, determines the winning candidate. Thus, we first choose a set of models that we believe to be applicable to our setting, fit them to the data, and compare their performances.

# Data

Our data come from the online math-teaching platform Zearn. The system is organized into classrooms led by a teacher. Each classroom contains several students who are collectively assigned certain lessons for completion. When a student finishes a lesson, they earn a "badge." Badges are the main student outcome measure used by Zearn and its teachers. The following table shows the typical profile of a Zearn classroom over the course of one month.

```{r load packages}
library(tidyverse)
library(data.table)
library(ggrepel)
library(ggpubr)
library(RColorBrewer)
library(grid)
library(gridExtra)
library(scales)
library(gt)
library(gtsummary)
library(PerformanceAnalytics)
library(doParallel)
library(foreach)
library(reticulate)
use_condaenv(condaenv = "./py-zearn")

set.seed(832399554)
random_py <- import("random")
random_py$seed(832399554)
# https://www.random.org/integers/
# Timestamp: 2023-05-17 16:18:28 UTC
```

```{r data prep}

df <- read.csv(file = "Data/df_clean.csv")

# Convert columns to appropriate data types
dt <- as.data.table(df)
# Rename variable
dt[, `:=`(
  Usage.Week = as.Date(Usage.Week),
  week = week(Usage.Week),
  poverty = factor(poverty, ordered = TRUE, exclude = c("")),
  income = factor(income, ordered = TRUE, exclude = c("")),
  charter.school = ifelse(charter.school == "Yes",
                          1, ifelse(charter.school == "No",
                                    0, NA)),
  school.account = ifelse(school.account == "Yes",
                          1, ifelse(school.account == "No",
                                    0, NA)),
  # Log Transform
  Badges.per.Active.User = log(Badges.per.Active.User + 1),
  Tower.Alerts.per.Tower.Completion = log(Tower.Alerts.per.Tower.Completion + 1),
  tch_min = log(tch_min + 1)
)]

# Create new variables using data.table syntax
dt[, min_week := week(min(Usage.Week)),
   by = Teacher.User.ID]
dt[, `:=`(
  week = ifelse(week >= min_week, week - min_week + 1, week - min_week + 53),
  mean_act_st = mean(Active.Users...Total)
), by = .(Classroom.ID)]
dt[, Tsubj := max(week), by = .(Classroom.ID)]
dt[, `:=`(
  st_login = ifelse(Minutes.per.Active.User > 0, 1, 0),
  tch_login = ifelse(tch_min > 0, 1, 0)
), by = .(Classroom.ID, Teacher.User.ID, week)]
# Update the Grade.Level values and labels
dt <- dt[!(Grade.Level %in% c(-1, 11))] # Ignore -1 and 11
dt[, Grade.Level := factor(Grade.Level,
                           ordered = TRUE,
                           exclude = c(""))]
dt[, Grade.Level := factor(Grade.Level,
                           levels = c(0:8),
                           labels = c("Kindergarten", "1st", "2nd",
                                      "3rd", "4th", "5th",
                                      "6th", "7th", "8th"))]

# Remove duplicate classroom-week pairs
dt <- dt[order(-Active.Users...Total),
         .SD[1],
         by = .(Classroom.ID, week)]

df <- as.data.frame(dt) %>%
  ungroup()

```

## Descriptive Statistics and Visualizations

The data represents various aspects of Zearn schools including identifiers, usage data, and demographic information. The dataset contains information for `r length(unique(df$Classroom.ID))` classrooms and `r length(unique(df$Teacher.User.ID))` teachers, with an average of `r mean(df$Students...Total, 1)` students per classroom. Various transformations and computations were performed on the data to prepare it for analysis, including calculating the number of distinct teachers, total students, and total weeks per school.

Descriptive statistics were computed for different measures such as the number of unique teachers, total students, and total weeks per school. Proportions were also calculated for various variables like poverty and income. The resulting statistics and proportions are displayed in @tbl-summary and @tbl-proportions, respectively. Refer to @tbl-summary-statistics for detailed information on the summary statistics for different variables by grade level.

The geographical distribution of teachers across Louisiana and the top 5 cities with the highest number of teachers are presented in @fig-teachers-map.

```{r}
#| label: tbl-summary
#| tbl-cap: "Summary statistics for the schools"

df_summary <- df %>%
  group_by(MDR.School.ID) %>%
  summarise(
    Unique_Teacher_Count = n_distinct(Teacher.User.ID)
  ) %>%
  left_join(df %>%
              group_by(Classroom.ID, MDR.School.ID) %>%
              summarise(
                Students_Total = mean(Students...Total, na.rm = TRUE),
                Weeks_Total = n_distinct(Usage.Week)
                ) %>%
              group_by(MDR.School.ID) %>%
              summarize(
                Students_Total = sum(Students_Total),
                Weeks_Total = mean(Weeks_Total)
                ),
            by = "MDR.School.ID") %>%
  ungroup() %>%
  summarise(
    Mean_Teachers = mean(Unique_Teacher_Count),
    SD_Teachers = sd(Unique_Teacher_Count),
    Min_Teachers = min(Unique_Teacher_Count),
    Max_Teachers = max(Unique_Teacher_Count),
    Mean_Students_Total = mean(Students_Total),
    SD_Students_Total = sd(Students_Total, na.rm = TRUE),
    Min_Students_Total = min(Students_Total),
    Max_Students_Total = max(Students_Total),
    Mean_Weeks_Total = mean(Weeks_Total),
    SD_Weeks_Total = sd(Weeks_Total, na.rm = TRUE),
    Min_Weeks_Total = min(Weeks_Total),
    Max_Weeks_Total = max(Weeks_Total)
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Measure", "Variable"), sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value)
  
df_proportions <- df %>%
  group_by(poverty) %>%
  summarise(n = n()) %>%
  mutate(Percentage = paste0(round(n / sum(n) * 100, digits = 2), "%")) %>%
  select(-"n") %>%
  na.omit() %>%
  rename(Variable = poverty) %>%
  mutate(Variable = as.character(Variable)) %>%
  bind_rows(df %>%
              group_by(income) %>%
              summarise(n = n()) %>%
              mutate(Percentage = paste0(round(n / sum(n) * 100, digits = 2), "%")) %>%
              select(-"n") %>%
              na.omit() %>%
              rename(Variable = income) %>%
              mutate(Variable = as.character(Variable))
            ) %>%
  bind_rows(df %>%
              ungroup() %>%
              summarise(
                Charter_Schools = mean(charter.school)*100,
                Schools_with_Paid_Account = mean(school.account)*100
                ) %>%
              mutate(
                Charter_Schools = paste0(round(Charter_Schools, digits = 2), "%"),
                Schools_with_Paid_Account = paste0(round(Schools_with_Paid_Account, digits = 2), "%")
              ) %>%
              transpose(keep.names = "Variable") %>%
              rename(Percentage = V1)) %>%
  add_row(Variable = "**Poverty Level**", Percentage = "", .before = 1) %>%
  add_row(Variable = "**Income**", Percentage = "", .before = 5) %>%
  add_row(Variable = "**Other**", Percentage = "", .before = 23)

# Summary statistics table
gt_summary <- df_summary %>%
  gt(rowname_col = "Variable") %>%
  cols_label(Mean = "Mean", SD = "Standard Deviation", Min = "Minimum", Max = "Maximum") %>%
  fmt_number(
    columns = c("Mean", "SD"),
    decimals = 2
  ) %>%
  fmt_number(
    columns = c("Min","Max"),
    decimals = 0
  )
gt_summary
```

```{r}
#| label: tbl-proportions
#| tbl-cap: "Proportions of different variables."

# Create the proportions table
gt_proportions <- df_proportions %>%
  gt(rowname_col = "Variable") %>%
  cols_label(Percentage = "Proportions") %>%
  fmt_markdown(columns = Variable)

# Print tables
gt_proportions

```

```{r}
#| cache: true
#| label: fig-teachers-map
#| fig-cap: "Geographical distribution of teachers across various parishes in Louisiana, and the top 5 cities with the highest number of teachers."

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
unique_zipcodes <- unique(dt$zipcode) %>%
  as.list()
plan(strategy = "multisession", workers = availableCores())
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
dt <- merge(dt, address_geodata,
            by.x = "zipcode",
            by.y = "postalcode",
            all.x = TRUE)

# Aggregate the data to get the number of teachers in each county
dt_map <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID)
), by = .(county)]

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID),
  lat = mean(lat),
  long = mean(long)
), by = .(city, county)]
# Get the top 5 cities with the most teachers
top_cities <- top_cities[order(-num_teachers)][1:5,]

# Get the Louisiana county map data
df_map <- tigris::counties(cb = TRUE,
                           resolution = "20m",
                           class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    as.data.frame(dt_map),
    by = c("NAMELSAD" = "county")
  ) %>%
  sf::st_as_sf()

ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers", low = "white", high = "red", na.value = "gray90") +
  labs(
    title = "Number of Teachers by Parish in Louisiana"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city), size = 3, color = "black")
```

## Unit of Analysis: Classroom-Week

We aggregate data at the weekly level. Teacher data has fine granularity with events time-stamped by the second. However, for data privacy purposes, student data is only available aggregated by classroom at the weekly level, including our variables of interest: student badge achievement, student minutes on the platform, and tower alerts.

### Zearn's Eye View

Time-series data for teacher effort and student achievement A. Weekly aggregation of data B. Privacy concerns for student data

<!-- Print head of dataset with better labels -->

### Measuring Student Achievement

```{r}
#| label: tbl-summary-statistics
#| caption: "Means (SD) of student variables by grade level."

create_summary <- function(var_name, var_label, type = "continuous") {
  summary_stat <- df %>%
    select(Grade.Level, var_name) %>%
    tbl_summary(
      by = Grade.Level,
      missing = "no",
      type = list(var_name ~ type),
      statistic = var_name ~ ifelse(type == "continuous", "{mean} ({sd})", "{n} ({p})")
    ) %>%
    add_overall() %>%
    as_tibble() 
  summary_stat[1] <- var_label
  return(summary_stat)
}

summaries_list <- list(
  create_summary("Sessions.per.Active.User", "Sessions per Student"),
  create_summary("Minutes.per.Active.User", "Minutes per Student"),
  create_summary("Badges.per.Active.User", "Badges per Student"),
  create_summary("Tower.Alerts.per.Tower.Completion", "Tower Alerts per Lesson Completion"),
  # create_summary("tch_login", "Teacher Login (0/1)", type = "dichotomous"),
  create_summary("tch_min", "Minutes per Teacher")
)

summary_table <- bind_rows(summaries_list) %>% 
  transpose(keep.names = "Characteristic", make.names = 1)
names(summary_table)[1] <- "Grade Level"

gt(summary_table)

```

### Change in Active Users and Badges per Active User Over Time

The average number of weeks of data available per classroom is shown in @fig-classroom-weeks, and the total number of student logins over time is illustrated in @fig-logins-week.

```{r}
#| label: fig-classroom-weeks
#| fig-cap: "Total number of weeks of data per classroom."

# Create the histogram
df %>%
  group_by(Classroom.ID) %>%
  summarize(Tsubj = max(Tsubj)) %>%
  mutate(Tsubj_category = if_else(Tsubj < 16, "less than 16", "16 or more")) %>%
  ggplot(aes(x = Tsubj, fill = Tsubj_category)) +
  geom_histogram(color = "black", breaks = seq(min(df$Tsubj), max(df$Tsubj) + 1, by = 2)) +
  geom_vline(xintercept = 15, color = "darkgray", linetype = "dashed", size = 0.8) +
  annotate("text", x = 9, y = 3500, label = "Excluded\nClassrooms", vjust = 1, color = "red") +
  labs(title = "Histogram of Total Number of Weeks",
       x = "Total Number of Weeks",
       y = "Frequency") +
  scale_fill_manual(values = c("less than 16" = "red", "16 or more" = "steelblue")) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(1, seq(5, max(df$Tsubj), by = 5)))

```

```{r}
#| label: fig-logins-week
#| fig-cap: "Total number of student logins over time."

# Calculate the sum of login values by Usage.Week and Teacher.User.ID
login_data <- df %>%
  group_by(Usage.Week, Teacher.User.ID) %>%
  summarize(tch_login = max(tch_login),
            st_login  = max(st_login)) %>%
  group_by(Usage.Week) %>%
  summarize(tch_logins = sum(tch_login),
            st_logins  = sum(st_login))
# Create bar plot
bar_plot <- ggplot() +
  geom_bar(data = login_data, aes(x = Usage.Week, y = st_logins), stat = "identity") +
  # geom_point(data = login_data, aes(x = Usage.Week, y = tch_logins), color = "blue") +
  labs(
    title = "Mean of Logins Across Teachers' Classrooms",
    x = "Week",
    y = "Total Logins"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12)) +
  scale_x_date(date_breaks = "3 week", date_labels = "%Y-%m-%d")

# Add labels for Christmas and Thanksgiving
bar_plot +
  geom_text(aes(x = as.Date("2019-12-25"), y = 1250, label = "Christmas"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "red") +
  geom_text(aes(x = as.Date("2019-11-25"), y = 1250, label = "Thanksgiving"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "darkorange")

```

## Exclusion criteria

Given the diverse user base of the platform, we select teachers who most likely come from traditional schools and classrooms that use the platform consistently. Specifically, we select virtual classrooms with at least five active students weekly, indicating students are enrolled by a school, and not their parents or tutors only. We also consider only classes that were active for more than 5 months during the year. That is, at least one student logged in for a period of at least 20 weeks, and we removed classes inactive for more than 7 months. This deletion most likely ensures that teachers and schools that have not used Zearn consistently will not affect the robustness of our results.

1.  Traditional schools and consistent platform usage
2.  Criteria for inclusion in the study We remove teachers with more than 4 classrooms and those who have logged in for less than 16 weeks in total. We exclude classrooms in the 6th to 8th grades, as those are a small proportion of our dataset.

```{r preprocess data}

dt[, n_weeks := .N,
   by = Classroom.ID]

dt <- dt[
  n_weeks > 15 & # At least 4 months cumulative activity
    Tsubj < 2*n_weeks & # At least activity twice a month on average
    teacher_number_classes < 5 &
    Students...Total > 5 &
    mean_act_st > 3 &
    !(Grade.Level %in% c("6th","7th","8th")) &
    !(month(Usage.Week) %in% c(6, 7, 8)) &
    !is.na(District.Rollup.ID),
]

cols_to_select <- names(dt)[sapply(dt, function(x) !is.numeric(x) ||
                                     (is.numeric(x) && is.finite(sd(x)) && sd(x) != 0))]
dt <- dt[, ..cols_to_select]

df <- as.data.frame(dt) %>%
  ungroup() %>%
  arrange(Classroom.ID, week)
# Clean environment
rm(list = setdiff(ls(), c("df","random_py")))
gc(verbose = FALSE)

```

## Variables of interest

Our Zearn data can be viewed as time-series (over the course of the school year, \~ 40 weeks). We propose to analyze the relationship between teacher effort and student achievement through temporal dynamics in the data. For the implementation of RL models, we need both an input (action) and an output (reward). For this purpose, we use teacher weekly log-ins (a binary variable, 0 = no log-ins and 1 = at least 1 log-in) or weekly time spent on Zearn's platform (in minutes) as quantifiable proxies for teacher effort. We assume that teachers on Zearn are in control of these two variables, and choosing how much time to allocate to the platform is central to this decision process. Although these are not perfect proxies for teacher effort, for example, they do not account for time spent creating assignments and performing in person instruction to students, they are the only directly quantifiable information about teacher behavior that we have in the data that even has the potential to impact student achievement. Further, we use student lesson completion (badges completed) as a measure of student effort level, given that developers at Zearn and teachers strongly emphasize the importance of lesson completion (instead of a grade, for example). The following histograms show the distribution of teacher minutes and student badges. Note the skewed nature of these distributions. As such, we transform our data prior to model fitting with log(minutes + 1) and log(badges + 1) as the inputs and outputs, respectively.

### State Variables

For this paper's second class of Reinforcement Learning models, we require variables that can determine the state space of each week. Experienced teachers suggest that they strongly focus on measures of the level of difficulty encountered during the lessons. The Zearn platform provides a measure of this kind, namely, "Tower Alerts." If a student is struggling in a given lesson, the platform automatically provides scaffolded remediation (i.e., breaking the problems step by step), and if they struggle multiple times in that same lesson, a "Tower Alert" is generated for their teacher. We use the previous week's average Tower Alerts to measure the level of difficulty faced by the students. Further, we include the previous week's average student time usage (in minutes) as a state variable. Effectively, we assume that the value of a given state (i.e., the week at hand) is a function of the previous week's "tower alerts" and "student minutes."

### Visualizing Relationships Between Variables

-   Use correlation analysis to find relationships between variables

-   Identify variables that may be strong predictors for the reinforcement learning model

@fig-corr displays the correlation matrix of selected variables.

```{r}
#| cache: true
#| label: fig-corr
#| fig-cap: "This graph represents the correlation between variables after log transformation"
#| fig-format: png

df_corr <- df  %>%
  select(Badges.per.Active.User,
         Active.Users...Total,
         Minutes.per.Active.User,
         Tower.Alerts.per.Tower.Completion,
         tch_min) %>%
  rename("Badges" = Badges.per.Active.User,
         "Minutes per Student" = Minutes.per.Active.User,
         "Tower Alerts" = Tower.Alerts.per.Tower.Completion,
         "# of Students" = Active.Users...Total,
         "Teacher Minutes" = tch_min)

chart.Correlation(df_corr, histogram = TRUE, method = "spearman",
                  pch = 20, cex = 0.5, col = rgb(0, 0, 1, 0.5))

```

#### Are Some Badges Harder than Other?

### Dimensionality Reduction

In order to capture the choices and trade-offs that teachers make, we used a Principal Component Analysis (PCA). This approach was taken to condense the multifaceted nature of our variables: Teacher Minutes, Teacher Sessions, and a series of data points including resources downloaded.

PCA was performed to mitigate the high-dimensionality of the dataset, seeking to encapsulate the maximum statistical information. We then calculated the correlations between the "Badges per Student" and the following variables: Teacher Minutes, and the three principal components (PC1, PC2, PC3). This facet of the analysis, which had a unit of analysis at the teacher level, was conducted to analyze the relationship between badges and the selected variables for each Teacher.

The Non-negative Matrix Factorization (NMF) operates as follows:

The original matrix can be seen as a detailed description of all the teachers' behaviors. Each row in the matrix represents a unique teacher, and each column represents a specific behavior or action the teacher might take. The entry in a specific row and column then corresponds to the occurrence, frequency, or intensity of that behavior for that particular teacher. After the NMF, we have two matrices:

1.  **Basis Matrix (W)**: This matrix represents underlying behavior patterns. Each column can be seen as a "meta-behavior" or a group of behaviors that tend to occur together. It is an abstraction or summary of the original behaviors.
2.  **Mixture Matrix (H)**: This matrix shows the extent to which each "meta-behavior" is present in each teacher. Each entry in this matrix represents the contribution of a "meta-behavior" to a particular teacher's behaviors.

By looking at these matrices, we can identify underlying patterns of behaviors (from the basis matrix) and see how these patterns are mixed and matched in different teachers (from the mixture matrix). This can be a powerful way of summarizing and interpreting complex behavioral data.

```{r pca nmf data-prep}
#| include: false
# Choose which Teacher.User.IDs will be train vs test
df$set <- ifelse(df$Teacher.User.ID %in%
                 sample(unique(df$Teacher.User.ID),
                        size = floor(0.8 * length(unique(df$Teacher.User.ID)))),
                 "train", "test")

# Create base data.table for models (faster than data.frame)
df_comp <- as.data.table(df %>% 
                           select(Classroom.ID, week, Badges.per.Active.User,
                                  set, tch_min,
                                  RD.elementary_schedule:RD.pd_course_guide))
# Arrange
setorder(df_comp, Classroom.ID, week)

## Prep data for PCA
df_pca <- as.data.frame(df_comp) %>%
  arrange(Classroom.ID, week) %>%
  ungroup() %>%
  mutate(across(everything(), ~ifelse(is.na(.), 0, .))) %>%
  mutate(across(RD.elementary_schedule:RD.pd_course_guide, ~log1p(.)))
# Calculate standard deviations
std_devs <- apply(df_pca %>% select(-c("Classroom.ID", "week", "set")), 2, sd)
# Identify columns with defined standard deviations (not NaN or Inf)
invalid_cols <- names(std_devs[is.na(std_devs) | is.infinite(std_devs)])
df_pca <- df_pca %>% select(-all_of(invalid_cols))

# Clean environment
rm(list = setdiff(ls(), c("df", "df_pca", "df_comp", "random_py")))
gc(verbose = FALSE)

```

```{python load data}
import numpy as np
import pandas as pd
from sklearnex import patch_sklearn
patch_sklearn()
from sklearn.decomposition import PCA, NMF
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import silhouette_score

## Basic Variables
# Import data from R
dfpca_py = pd.DataFrame(r.df_pca)
dfpca_py.sort_values(['Classroom.ID', 'week'], inplace=True)

# Initialize scaler
scaler = MinMaxScaler()

# Drop unnecessary columns
X = dfpca_py.drop(['Badges.per.Active.User', 'set', 'Classroom.ID', 'week'], axis=1)
X_cols  = X.columns
# Scale the data
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X_cols)

# Dictionaries for export
components = {}
results = {}
residuals = {}
silhouette = {}

# Number of components for NMF
n_comp = min(X.shape) // 3
```

```{python pca-nmf}

################ PCA
for n in range(2, n_comp):
  pca = PCA(n_components=n)
  X_pca = pca.fit_transform(X_scaled)
  pca_comp = pca.components_
  X_hat = pca.inverse_transform(X_pca)
  labels = np.argmax(pca_comp, axis=0)

  results.setdefault("PCA", {})[n] = X_pca
  components.setdefault("PCA", {})[n] = pca_comp
  residuals.setdefault("PCA", {})[n] =((X_scaled - X_hat)**2).sum().sum()  # RSS
  silhouette.setdefault("PCA", {})[n] = silhouette_score(pca_comp.transpose(), labels)

################ Non-negative Matrix Factorization
# Function for NMF
def nmf_method(n, method, initial, X_scaled, solv = 'mu'):
    method_name = f"{method.title()} {initial.upper()}"
    if method == 'frobenius' and initial == 'nndsvd': solv = 'cd'
    if method != 'frobenius' and initial == 'nndsvd': return
    if method != 'frobenius': method_name = f"{method.title()}"
    
    nmf = NMF(
      n_components=n,
      init=initial,
      beta_loss=method,
      solver=solv,
      max_iter=4_000
    )
    X_nmf, nmf_comp = nmf.fit_transform(X_scaled), nmf.components_
    X_hat = nmf.inverse_transform(X_nmf)
    labels = np.argmax(nmf_comp, axis=0)
    
    results.setdefault(method_name, {})[n] = X_nmf
    components.setdefault(method_name, {})[n] = nmf_comp
    residuals.setdefault(method_name, {})[n] = ((X_scaled - X_hat)**2).sum().sum()  # RSS
    silhouette.setdefault(method_name, {})[n] = silhouette_score(nmf_comp.transpose(), labels)

# Call the function for NMF
for n in range(2, n_comp):
  for method in {'frobenius', 'kullback-leibler'}:
    for initial in {'nndsvd', 'nndsvda'}:
      try:
        nmf_method(n, method, initial, X_scaled)
      except:
        continue
```

```{python train autoencoder}
#| eval: false
import os # To fix: https://github.com/tensorflow/tensorflow/issues/59779
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from keras.models import Model
from keras.layers import Input, Dense
from kerastuner.tuners import Hyperband
from kerastuner.engine.hyperparameters import HyperParameters
from keras.constraints import NonNeg
from keras.callbacks import EarlyStopping
from keras.regularizers import L1

# Create training and testing data frames
train_df = dfpca_py[dfpca_py['set'] == 'train']
test_df = dfpca_py[dfpca_py['set'] == 'test']

# Predictors
X_train = train_df.drop(['Badges.per.Active.User', 'set', 'Classroom.ID', 'week'], axis=1)
X_test = test_df.drop(['Badges.per.Active.User', 'set', 'Classroom.ID', 'week'], axis=1)
# Scale the data
X_train = scaler.transform(X_train)
X_train = pd.DataFrame(X_train, columns=X_cols)
X_test = scaler.transform(X_test)
X_test = pd.DataFrame(X_test, columns=X_cols)

# Get the target variable
Y = dfpca_py[['Badges.per.Active.User']]
Y_train = train_df[['Badges.per.Active.User']]
Y_test = test_df[['Badges.per.Active.User']]
Y = scaler.fit_transform(Y)
Y_train = scaler.transform(Y_train)
Y_test = scaler.transform(Y_test)

# Define the number of components and features
n_features = X_scaled.shape[1]
n_labels = 1  # For regression, we usually have just one output node

# Determine loss weights according to the data structure:
decoding_weight = Y.std() / (Y.std() + X_scaled.std(numeric_only=True).mean())
prediction_weight = 1 - decoding_weight
def build_model(hp):
    # Define the input layer
    input_data = Input(shape=(n_features,))
    
    # Define the encoding layer(s)
    n_layers = hp.Int('n_layers', min_value=1, max_value=4, step=1)
    n_units = [
      hp.Choice('units_' + str(i), values=[8, 16, 32, 64, 128, 256, 512])
      for i in range(n_layers)
    ]
    encoded = input_data
    for i in range(n_layers):
        encoded = Dense(
          units=n_units[i],
          activation='relu')(encoded)
          
    # Generate the latent vector
    latent_dim = hp.Int('encoding_units', min_value=2, max_value=n_comp, step=1)
    l1_value = hp.Float('l1_value', min_value=0.0001, max_value=0.001, default=0.0005, step=0.0001)
    latent = Dense(
      units=latent_dim,
      activation='linear',
      activity_regularizer= L1(l1=l1_value),
      kernel_constraint=NonNeg(),
      name='latent')(encoded)      
    
    # Decoder
    decoded = latent
    for i in range(n_layers):
        decoded = Dense(
          units=n_units[n_layers - i - 1],
          activation='relu')(decoded)
    decoded = Dense(n_features, activation='sigmoid', name='decoded')(decoded)
    
    # Define the label output layer
    label_output = Dense(n_labels, activation='linear', name='label_output')(latent)
    
    # Define the autoencoder model
    autoencoder = Model(inputs=input_data, outputs=[decoded, label_output])
    # Compile the model
    autoencoder.compile(optimizer='adadelta',
                    loss={
                      'decoded': 'mean_squared_error',
                      'label_output': 'mean_squared_error'
                    },
                    loss_weights={
                      'decoded': decoding_weight,
                      'label_output': prediction_weight
                    })
    
    return autoencoder

# Set up hyperparameter tuner
tuner = Hyperband(build_model,
                  objective='val_loss',
                  max_epochs=20,
                  directory='autoencoder_tuning',
                  project_name='autoencoder_2nd_tuning')

# Perform hyperparameter search
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)
tuner.search(x=X_train,
            y=[X_train, Y_train],
            epochs=50,
            validation_data=(X_test, [X_test, Y_test]),
            callbacks=[early_stopping_callback])
# tuner.results_summary()
# Get the optimal hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=2)[1]
model = tuner.hypermodel.build(best_hyperparameters)
history = model.fit(x=X_train, 
                    y=[X_train, Y_train],
                    epochs=2_000,
                    validation_data=(X_test, [X_test, Y_test]),
                    callbacks=[early_stopping_callback])
best_model = model


# Function to get encoded representation and components
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

# Get encoded representation and components
get_encoded_representation_and_components(best_model, X_scaled)

# save the model
model.save('./autoencoder_tuning/final_model.h5')

```

```{python load autoencoder}
#| include: false
from tensorflow.keras.models import load_model
from keras.models import Model
from keras.layers import Input

# Function to get encoded representation and components
loaded_model = load_model('./autoencoder_tuning/final_model.h5')
def get_encoded_representation_and_components(best_model, X):
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(n_features,))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X)
    X_hat = best_model.predict(X)[0]
    res = ((X - X_hat)**2).sum().sum()
    n_components = X_encoded.shape[1]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

n_features = X_scaled.shape[1]
# Get encoded representation and components
get_encoded_representation_and_components(loaded_model, X_scaled)
```

```{python clean environment}
#| include: false
# get the names of all variables in the global namespace
all_vars = list(globals().keys())

# list of variables to keep
keep_vars = ['residuals', 'silhouette', 'components', 'results', 'r']

# delete all variables that are not in keep_vars
for var in all_vars:
    if var not in keep_vars:
        del globals()[var]
del keep_vars
del all_vars
del var

# manually call the garbage collector
import gc
gc.collect()

```

```{r}
#| cache: true
#| label: fig-nmf-pca-comparison
#| fig-cap: "Comparison of residuals and silhouette scores for PCA, Frobenius, and Kullback-Leibler methods."
# Importing from Python
residuals_list <- py$residuals
silhouette_list <- py$silhouette

# Creating dataframes
df_residuals <- do.call(rbind, lapply(names(residuals_list), function(method) {
  do.call(rbind, lapply(names(residuals_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Residuals = residuals_list[[method]][[n]]
    )
  }))
}))
# Creating separate dataframes for 'Autoencoder' and the rest of the methods
df_residuals_autoencoder <- df_residuals[df_residuals$Method == "Autoencoder", ]
df_residuals_others <- df_residuals[df_residuals$Method != "Autoencoder", ]

df_silhouette <- do.call(rbind, lapply(names(silhouette_list), function(method) {
  do.call(rbind, lapply(names(silhouette_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Silhouette = silhouette_list[[method]][[n]]
    )
  }))
}))
# Add nuisance row for Autoencoder to unify legends
df_silhouette <- rbind(df_silhouette,
                       data.frame(Method = "Autoencoder",
                                  Components = 4,
                                  Silhouette = mean(df_silhouette$Silhouette)))

# Plotting residuals
p1 <- ggplot() +
  geom_line(data = df_residuals_others,
            aes(x = Components, y = Residuals, color = Method)) +
  geom_point(data = df_residuals_autoencoder,
             aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Sum of Square Residuals",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals$Components),
                                  max(df_residuals$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())
# Plotting silhouette scores
p2 <- ggplot(df_silhouette, aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette$Components),
                                  max(df_silhouette$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette$Silhouette) +
                                  2*sd(df_silhouette$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Combine the plots and place the legend at the bottom
comparison_plot <- ggarrange(p1, p2,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
comparison_plot

```

### Interpreting Components

<!-- To ensure a more intuitive interpretation, we adjust each component to ensure a positive correlation with Student Badges (multiplying them by $-1$ if necessary). -->

```{r}
#| cache: true
#| label: fig-nmf-heatmap
#| fig-cap: ""
library(pheatmap)

components_list <- py$components
df_heatmap <- components_list[["Kullback-Leibler"]][["3"]] %>%
  t() %>% as.data.frame()
row.names(df_heatmap) <- names(df_pca)[-c(1:4)]
names(df_heatmap) <- paste0("Component ", 1:3)
df_heatmap <- df_heatmap %>% arrange(-`Component 1`)

color_scheme <- colorRampPalette(
  c("#F7F7F7",brewer.pal(n = 9, name = "YlOrRd"))
  )(100)
breaks <- seq(0, 1, by = 0.01)
minmax <- function(x) ((x - min(x))/(max(x) - min(x)))
pheatmap(df_heatmap %>%
           mutate_all(minmax),
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         angle_col = 0,
         main = "Loadings of NMF components",
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE,
         show_rownames = TRUE)

```

```{r export dimension reduction}
#| eval: false
# Importing results from Python
results_list <- py$results

# List of method names
methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler", "Autoencoder")
names(results_list) <- methods
# Initialize df_components
df_components <- df

# Loop over each method, extract the results at 3 components and bind to df_components
for (method in methods) {
  result_3 <- results_list[[method]][["3"]]
  df_components <- df_components %>%
    bind_cols(result_3)
  
  # Adjust column names
  new_cols <- paste0(method, 1:3)
  names(df_components)[(ncol(df_components) - 2):ncol(df_components)] <- new_cols
}

# Write to csv
write.csv(df_components, "./Bayesian/df.csv")

```

```{r load dimension reduction}
#| include: false
df <- read.csv(file = "./Bayesian/df.csv")
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
gc(verbose = FALSE)
```

## Connecting Variables to Reinforcement Learning Model

**States**

-   Tower Alerts points to how many students are struggling with the content.

-   Minutes per Student / Badges per Student

-   Total Active Students

-   Different combinations of these variables to create unique states.

**Rewards**

-   Learning progress through objective measures such as badges, boosts.

-   Quantify the effectiveness of teacher actions in promoting student learning.

**Actions**

-   Teachers can download resources, engage in different teaching methods or activities.

-   Teachers can choose how much time they spend online.

-   RL optimizes the action selection based on the rewards observed.

<!-- ![Student View](https://help.zearn.org/hc/article_attachments/5698093595927/HC_-_StudentFeed_3.PNG) -->

<!-- *Image of Zearn's classroom structure* -->

![Badges](https://help.zearn.org/hc/article_attachments/5547000521495/HC_-_LockedLessons.PNG) *Image of the badge system for student achievement*

# Methods

## Dynamic Analysis (Lau & Glimcher, 2005)

**Introduction to dynamic analysis**

-   Uses response-by-response models to predict choice on each trial based on past reinforcers and choices

-   Based on logistic regression, it captures the linear combination of past reinforcers and choices on each trial

-   Flexible model incorporating effects of past reinforcers, choice history, and biases

**Advantages of dynamic analysis in the context of Zearn dataset**

-   Captures the temporal dependencies and complex interactions between teacher actions, student outcomes, and learning environment

-   Allows for the identification of optimal teaching strategies that evolve over time

-   Enables the evaluation of the impact of various factors (e.g., curriculum, student engagement, etc.) on the decision-making process

**Model Formulation**

$$
\begin{aligned}\log \left(\frac{p_{R, i}}{p_{L, i}}\right)= & \sum_{j=1} \alpha_{ j}( r_{R, i-j}-r_{L, i-j}) \\& +\sum_{j=1} \beta_{j} (c_{R, i-j}-c_{L, i-j})+\gamma,\end{aligned}
$$

**Preliminary insights and findings**

-   Identification of key factors that influence teacher decision-making and student outcomes

-   Evidence of adaptive teaching strategies that change in response to student progress and engagement

-   Estimation of the relative impact of different teaching actions on student learning

## Variable Selection

## Q-learning Model

-   Teacher Specific reward sensitivity
-   Reward is a linear function of badges (reward sensitivity minus cost)

## Actor-Critic Model

## Gaussian Policy Model

## Model Fit

### Base Models: Random Effects Panel Logit

### Hierarchical Bayesian Method

Our data contains 400 teacher-classroom pairs spanned across approximately 40 weeks. Estimating individual level with maximum likelihood estimation would yield noisy results as each teacher has an insufficient amount of data. A group-level estimation does yield reliable estimates (see Results) but ultimately ignores individual differences, which are important to our analysis .

Our estimation method of choice is a hierarchical Bayesian analysis, which allows us to pool information across individuals, while allowing for individual differences. Individual-level parameters are merely a function of group-level hyperparameters, and this anchoring improves our power by assuming commonalities among individuals (CITE Ahn et al., 2011; Huys et al., 2011). One particular feature of this estimation technique is that pooling is evident in the hyperparameter variance. Strong pooling means low hierarchical variance, and vice verse for weak pooling.

For Bayesian updating, we use the Stan software package (http://mc-stan.org/), which implements a Hamiltonian Monte Carlo algorithm (See <http://mc-stan.org/documentation/>). In the case of Zearn data, fast convergence of the algorithm required us to use priors that are weakly informed by parameter values found by searching the whole grid of parameter space (i.e., grid search).

## Model Comparison

## Heterogeneity

### Across Teachers

### Across Schools

### Across Demographics

# Results

## Component Selection

```{r}
bic_plm <- function(object) {
  # object is "plm", "panelmodel"
  sp = summary(object)
  if (class(object)[1] == "plm") {
    u.hat <- residuals(sp) # extract residuals
    model_data <- cbind(as.vector(u.hat), attr(u.hat, "index"))
    names(model_data)[1] <- "resid"
    c = length(unique(model_data[, "Classroom.ID"])) # extract classroom dimensions
    t = length(unique(model_data[, "week"])) # extract time dimension
    np = length(sp$coefficients[, 1]) # number of parameters
    n.N = nrow(sp$model) # number of data
    s.sq  <- log((sum(u.hat ^ 2) / (n.N))) # log sum of squares
    
    # effect = c("individual", "time", "twoways", "nested"),
    # model = c("within", "random", "ht", "between", "pooling", "fd")
    
    if (sp$args$model == "within" & sp$args$effect == "individual") {
      np = np + c + 1 # update number of parameters
    }
    
    if (sp$args$model == "within" & sp$args$effect == "time") {
      np = np + t + 1 # update number of parameters
    }
    
    if (sp$args$model == "within" & sp$args$effect == "twoways") {
      np = np + c + t # update number of parameters
    }
    
    if (sp$args$model == "random" & sp$args$effect == "twoways") {
      np = np + length(sp$ercomp$sigma2) # update number of parameters
    }
    
    bic <- round(log(n.N) * np  +  n.N * (log(2 * pi) + s.sq  + 1), 1)
    names(bic) = "BIC"
    return(bic)
  }
}

compute_nloglik <- function(residuals) {
  n <- length(residuals)
  sigma2 <- sum(residuals^2) / n
  nll <- n/2 * ( log(2 * pi) + log(sigma2) + 1 ) # Negative log likelihood
  return(as.numeric(nll))
}

```

```{r panel model comparison}
#| cache: true
library(plm)

# Helper functions
get_lag_value <- function(datatable, col, lag_period, n_comp = NULL) {
  # Add a column for week_lag
  datatable[, week_lag := c(0, diff(week)), by = Classroom.ID]

  if (is.null(n_comp)) {
    # Update the lag column with shift function
    datatable[, (paste0(col, "_", lag_period)) :=
                shift(get(col), lag_period, fill = 0, type = "lag"),
              by = Classroom.ID]
  } else {
    for (comp in 1:n_comp) {
      # Update the lag column with shift function
      datatable <- datatable[, (paste0(col, comp, "_", lag_period)) :=
                              shift(get(paste0(col, comp)), lag_period, fill = 0, type = "lag"),
                            by = Classroom.ID]
    }
  }
  
  return(datatable)
}

create_formula <- function(method, lag, comp) {
  if (lag == 1) {
    return(as.formula(
    paste0(method, comp, " ~ ",
           paste0(method, comp, "_", 1), " + ",
           "Badges.per.Active.User",
           " + ", "week_lag")))
  }
  return(as.formula(
    paste0(method, comp, " ~ ",
           paste0(method, comp, "_", 1:lag, collapse = " + "), " + ",
           "Badges.per.Active.User + ",
           paste0("Badges.per.Active.User", "_", 1:(lag - 1), collapse = " + "),
           " + ", "week_lag")))
}

model_selection <- function(wi, re) {
  if (phtest(wi, re)$p.value < 0.05) return(wi) else return(re)
}

create_model <- function(formula, data) {
  # Models
  wi <- plm(formula, data = data, effect = "twoway", model = "within")
  re <- plm(formula, data = data, effect = "twoway", model = "random")
  
  return(model_selection(wi, re))
}

# Data and Variables
df <- setDT(df)
results_df_list <- list()
lags <- c(1:8)
n_comp = 3
n_lags = max(lags)
# Use map to iterate over methods, paste0 to concatenate strings
methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler", "Autoencoder")
columns <- c("tch_min", "Badges.per.Active.User", methods)

# Create lags
for (col in columns) {
  for (lag_period in 1:n_lags) {
    if (!col %in% methods) {
      df <- get_lag_value(df, col, lag_period)
      next
    }
    df <- get_lag_value(df, col, lag_period, n_comp)
  }
}

# Panel data
train_data <- pdata.frame(df[set == "train"], index = c("Classroom.ID", "week", "Teacher.User.ID"))
test_data <- pdata.frame(df[set == "test"], index = c("Classroom.ID", "week", "Teacher.User.ID"))

# Estimation
cl <- makeCluster(detectCores())
registerDoParallel(cl)
params <- expand.grid(col = c("tch_min", methods),
                      lag = lags,
                      comp = 1:n_comp) %>%
  mutate(comp = as.character(comp),
         comp = case_when(col == "tch_min" ~ "",
                          .default = comp)) %>%
  unique()
results <- foreach(i = 1:nrow(params),
                   .multicombine = TRUE,
                   .noexport = c("formula", "model",
                                 "residuals", "predictions"),
                   .export = ls(),
                   .packages = "plm") %dopar% {
  col <- as.character(params$col[i])
  lag <- params$lag[i]
  comp <- params$comp[i]
  formula <- create_formula(col, lag, comp)
  model <- create_model(formula, train_data)

  # Out of Sample Log Likelihood
  predictions <- predict(model, newdata = test_data, na.fill = TRUE)
  residuals <- test_data[,paste0(col, comp)] - predictions

  # Return the results as a list
  list(Method = col,
       Component = comp,
       Lag = lag,
       nloglik = compute_nloglik(residuals),
       bic = as.numeric(bic_plm(model)),
       coef = summary(model)$coefficients)
}
# Stop the cluster
stopCluster(cl)
```

```{r}
#| label: fig-panel-bic
#| fig-cap: ""
# Wrangling the results data frame for plotting
results_df <- map_dfr(results, ~as.data.frame(t(unlist(.))))
# Convert variables to appropriate data types
results_df <- results_df %>%
  mutate(Method = as.factor(Method),
         Component = as.numeric(Component),
         Lag = as.numeric(Lag),
         nloglik = as.numeric(nloglik),
         bic = as.numeric(bic)) %>%
  group_by(Method, Lag) %>%
  summarise(avg_bic = mean(bic, na.rm = TRUE),
            avg_nloglik = mean(nloglik, na.rm = TRUE),
            .groups = "drop")
# Filter methods for the plots
methods_for_plots <- c("FrobeniusNNDSVD", "KullbackLeibler", "FrobeniusNNDSVDA")
plot_data <- filter(results_df, Method %in% methods_for_plots)
# Shared aesthetics for the plots
aes <- ggplot2::aes(x = Lag, y = value, color = Method)

# Summarize methods not included in the plots and prepare tables
methods_for_summary <- setdiff(unique(results_df$Method), methods_for_plots)
summary_data <- filter(results_df, Method %in% methods_for_summary) %>%
  group_by(Method) %>%
  summarise(BIC = mean(avg_bic, na.rm = TRUE),
            NLL = mean(avg_nloglik, na.rm = TRUE),
            .groups = "drop")
# Rename method
levels(summary_data$Method)[levels(summary_data$Method)=="tch_min"] <- "Minutes"
# Convert to scientific notation
summary_data$BIC <- format(summary_data$BIC, scientific = TRUE, digits = 2)
summary_data$NLL <- format(summary_data$NLL, scientific = TRUE, digits = 2)

# Create tables and remove headers
bic_table <- summary_data %>% select(Method, BIC) %>%
  tableGrob(theme = ttheme_minimal(base_size = 8, 
                                   core=list(bg_params = list(fill = "white", col=NA),
                                             fg_params=list(fontface=3))),
             rows = NULL)
bic_table <- bic_table[-1, ]  # Remove header
bic_data <- filter(plot_data, !is.na(avg_bic)) %>% 
  mutate(value = avg_bic, metric = "BIC")
# BIC Plot
bic_plot <- ggplot(bic_data, aes) + 
  geom_line() + 
  labs(title = "BIC", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none") +
  annotation_custom(bic_table, xmin = max(bic_data$Lag) - 3.5,
                    xmax = max(bic_data$Lag) - 0.5,
                    ymin = 1.01*max(bic_data$value),
                    ymax = Inf)

nll_table <- summary_data %>% select(Method, NLL) %>%
  tableGrob(theme = ttheme_minimal(base_size = 8, 
                                   core=list(bg_params = list(fill = "white", col=NA),
                                             fg_params=list(fontface=3))), 
             rows = NULL)
nll_table <- nll_table[-1, ]  # Remove header
nll_data <- filter(plot_data, !is.na(avg_nloglik)) %>% 
  mutate(value = avg_nloglik, metric = "Negative Log-Likelihood")
nll_plot <- ggplot(nll_data, aes) + 
  geom_line() + 
  labs(title = "Out-of-sample NLL", x = "Number of Lags") +
  scale_y_continuous(labels = scientific) +
  theme_minimal() +
  theme(axis.title.y = element_blank(), legend.position = "none") +
  annotation_custom(nll_table, xmin = max(nll_data$Lag) - 3.5,
                    xmax = max(nll_data$Lag) - 0.5,
                    ymin = 1.01*max(nll_data$value),
                    ymax = Inf)

# Combine the plots and add a legend
combined_plot <- ggarrange(bic_plot, nll_plot,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
combined_plot

```

## Base Models

In order to get a baseline understanding of the influence of our key variables on the number of badges per active user, we employed a series of panel data models, with control variables: 1) the number of classes each teacher is responsible for, 2) the grade level of the classes, and 3) the total number of students. The 'Minutes Model' considers the number of minutes each teacher spends on the platform. The 'PCA Models' incorporate the three principal components we derived earlier.

Both of these models use a random effects approach, which is suitable for our panel data structure and accounts for unobserved heterogeneity.

## Models with Lags

Subsequently, we accounted for the temporal dynamics of our dataset by applying the Lau & Glimcher (2005) method. We introduced lagged variables into the models, thereby allowing us to account for temporal autocorrelation and potential delayed effects. We included lagged versions of the variables 'Teacher Minutes', 'PC1', 'PC2', 'PC3', and 'Badges per Students', with eight lags for each.

We then ran models with these lagged variables using the same random effects approach as in the base models.

In order to better understand and interpret the output of our models, we created a plot of the coefficients associated with each of the lagged variables. This plot allows us to see how the influence of each variable changes as the lag increases, and to compare these dynamics across variables.

```{r}
#| eval: false
#| label: fig-lags
#| fig-cap: "The estimated coefficients of the lagged variables in the random effects models. The lines represent different variables, and the shaded areas indicate the standard errors of the coefficients. The grey line and shaded area represent the coefficients for the lagged Badges per Student."

# Extract coefficients from models
lag_period = 4
col = "FrobeniusNNDSVD"
select_model <- function(model) {
  model[["Method"]] == col & model[["Lag"]] == lag_period
}
model_coeffs <- Filter(select_model, results)

extract_coeffs <- function(model) {
  coef_df <- data.frame(coeff_value_estimate = model$coef[,1],
                        coeff_value_se = model$coef[,2])
  coef_df$Coefficient_Name <- rownames(model$coef)
  # Add other model properties
  coef_df$Method <- model$Method
  coef_df$Component <- model$Component
  return(coef_df)
}
df_estimates <- do.call(rbind, lapply(model_coeffs, extract_coeffs)) %>%
  filter(Coefficient_Name != "(Intercept)" &
           Coefficient_Name != "week_lag") %>%
  mutate(Lag = as.numeric(str_extract(Coefficient_Name, "\\d+$")),
         Coefficient_Name = case_when(
           grepl("FrobeniusNNDSVD1", Coefficient_Name) ~ "Frobenius 1",
           grepl("FrobeniusNNDSVD2", Coefficient_Name) ~ "Frobenius 2",
           grepl("FrobeniusNNDSVD3", Coefficient_Name) ~ "Frobenius 3",
           grepl("Badges.per.Active.User", Coefficient_Name) ~ "Badges",
           TRUE ~ "Other"),
         Lag = case_when(is.na(Lag) ~ 1,
                         Coefficient_Name == "Badges" ~ Lag + 1,
                         .default = Lag),
         Coefficient_Name = case_when(Coefficient_Name == "Badges" ~ 
                                 paste0(Coefficient_Name, " ", Component),
                               .default = Coefficient_Name))

plot_comp <- ggplot(df_estimates %>% filter(!grepl("Badges", Coefficient_Name)),
       aes(x = Lag,
           y = coeff_value_estimate,
           color = Component,
           group = Component)) +
  geom_line() +
  geom_ribbon(aes(ymin = coeff_value_estimate - coeff_value_se,
                  ymax = coeff_value_estimate + coeff_value_se),
              alpha = 0.1) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 0.2) +
  facet_wrap(~Coefficient_Name, scales = "free_y") +
  labs(x = "Lag", y = "Coefficient", linetype = "") +
  theme_light() +
  theme(legend.position = "none")
plot_badge <- ggplot(data = df_estimates %>% filter(grepl("Badges", Coefficient_Name)),
            aes(x = Lag,
                y = coeff_value_estimate,
                group = Component,
                linetype = "Badges")) +
  geom_line(color = "black") +
  geom_ribbon(aes(ymin = coeff_value_estimate - coeff_value_se,
                  ymax = coeff_value_estimate + coeff_value_se),
              alpha = 0.1,
              fill = "grey",
              color = "grey") +  # set color to gray
  geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 0.2) +
  facet_wrap(~Coefficient_Name, scales = "free_y") +
  labs(x = "Lag", y = "Coefficient", linetype = "") +
  theme_light() +
  theme(legend.position = "none")

grid.arrange(plot_comp, plot_badge)

```

## Variable selection

To ensure that our models are parsimonious and to help determine which set of variables provides the best fit for the data, we compared the Bayesian Information Criterion (BIC) of the different models: the Minutes Model, PCA Model, and Lag Models. @tbl-choose-RL-model displays the BICs, with each row corresponding to a different model.

BIC penalizes models based on their complexity (number of parameters used) and the number of observations, favoring simpler models and models that fit the data better. In @fig-panel-bic, the 8-lag PC1 Model has the lowest BIC value, suggesting that it provides the best fit to the data when considering both complexity and fit.

## Q-Learning Analysis

```{r}
#| label: tbl-choose-RL-model
#| tbl-cap: "Comparison of Negative Log likelihood values of posteriors across different models. Lower values indicate better model fit."
results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]

# Function to extract median of lp__ from a model
extract_lp_median <- function(model_file) {
  fit <- readRDS(model_file)
  lp_median <- fit$summary("lp__", "median")  # Get the median of lp__
  data.frame(Model = gsub("Bayesian/Results/||.RDS", "", model_file),
             NLL_Median = -1 * as.numeric(lp_median))
}

# Apply function to all models and combine into one dataframe
lp_df <- map_df(results_files, extract_lp_median) %>%
  na.omit()
lp_df$Method <- gsub(".*-", "", lp_df$Model)
lp_df$ModelType <- str_extract(lp_df$Model, ".*(?=-)")

# Create the table with models as rows and NMF methods as columns
table_df <- lp_df %>%
  dplyr::select(-Model) %>%
  pivot_wider(names_from = Method, values_from = NLL_Median) %>%
  arrange(desc(ModelType)) %>%
  na.omit()
# To highlight the best value in the table
table_df %>%
  gt() %>%
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(
      columns = names(table_df)[colSums(table_df == min(table_df[,-1])) >= 1],
      rows = rowSums(table_df == min(table_df[,-1])) >= 1
    )
  )


```

<!-- Find a graph that displays results better (rather than the one user to fit) -->

We first present the following tables with the 25th, 50th, and 75th percentile of the learning rate (), inverse temperature (), weights, and cost parameters of the Q-learning models (non-hierarchical and hierarchical).

These are the group-level parameters that reflect the distribution of the subject-level parameters.

## Actor-Critic Analysis

### **(non-hierarchical)**

Table. Fitted Parameters for Non-Hierarchical Actor-Critic learning model with Eligibility

### **(hierarchical)**

Table. Average of Fitted Parameters for Actor-Critic learning model with Eligibility from

## Model Comparison

We compare these four models by calculating the associated Bayesian Information Criterion (BIC) of each. Models with lags will be penalized for including two extra parameters.

```{r Bayesian LOOIC prep}
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py", "get_lag_value")))
gc(verbose = FALSE)
```

```{r}
#| cache: true
#| label: tbl-RL-logit-comp
#| tbl-cap: "LOOIC (Leave-One-Out Information Criterion) values were calculated for four different models: Q-learning, Logit, Q-learning Hierarchical, and Logit Hierarchical. The LOOIC provides a measure of model quality, with lower values indicating better model performance. Q-learning models were built using a kernel-based approach. Logit models were fit using logistic regression, with the Hierarchical versions incorporating a hierarchical structure to account for classroom-level variations. The LOOIC was calculated using the 'loo' function from the 'loo' package in R, which estimates the expected log predictive density for a held-out data point, based on the rest of the data."
library(brms)
library(loo)

# Non-hierarchical models
## Q-learning model
post <- read_rds("Bayesian/Results/Q-learning-kernel-FR.RDS")
loo_qlearn <- post$loo()$estimates["looic", ]
## Logit
models_nh <- readRDS("Bayesian/Results/logit.RDS")
loo_nh <- sapply(models_nh, loo)
looic_nh <- sum(unlist(loo_nh["looic",]))

# Hierarchical models
## Q-learning
post <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
loo_qhierarchical <- post$loo()$estimates["looic", ]
## Logit
models_h <- readRDS("Bayesian/Results/logit-hierarchical.RDS")
loo_h <- sapply(models_h, loo)
looic_h <- sum(unlist(loo_h["looic",]))

# Collect LOOIC values
looic_values <- c(loo_qlearn[1],looic_nh, 
                  loo_qhierarchical[1], looic_h)
df_looic <- data.frame(Model = c("Q-learning", "Logit", "Q-learning Hierarchical", "Logit Hierarchical"),
                       LOOIC = looic_values)

# Create gt table
gt(df_looic)

```

```{r}
#| label: fig-model-fit

# Here I'd like to have a visual representation of the model fit.
# I'm thinking of Having the number of weeks in the x-axis
# and the probability of each y=1 on the y-axis.
# Then graph two lines: one for the model fit (y_pred from the bayesian models)
#                   and one for the real data.
# Probably need to do this for the average across a number of teachers.

```

## **Optimality**

If the goal of a teacher is to maximize lesson completion, analyzing the performance of teachers across parameter levels is interesting. The following correlation plot shows the relationship between parameters and average weekly badges per teacher.

## Heterogeneity

```{r Heterogeneity analysis}

hierarchical_model <- read_rds("Bayesian/Results/Q-kernel-hierarchical-FR.RDS")
hierarchical_model <- hierarchical_model$summary()

# Here I would like to correlate individual parameters from the hierarchical
# model with specific classroom level variables.
# Including:
## Student Variables (mean across classroom):
# "Active.Users...Total", "Minutes.per.Active.User",
# "Badges.per.Active.User", "Boosts.per.Tower.Completion",
# "Tower.Alerts.per.Tower.Completion",
## Classroom and Teacher Variables
# "teacher_number_classes", "Grade.Level",
# "Students...Total", "n_weeks",
## School Variables
# "poverty", "income", "charter.school",
# "school.account

# That is, do the parameters from the Bayesian model change with these variables?
# If so, how?


```

# Discussion

6.  Comparing the performance of the models
7.  Advantages and limitations of each model
8.  Insights from each model

## Implications for Teachers and Schools

3.  Decision-making patterns
4.  Optimal strategies
5.  Implications for the education field
6.  Potential impact on teaching practices
7.  Policy recommendations

## Limitations

3.  Data limitations and biases
4.  Model assumptions and simplifications
5.  Generalizability of results

## Challenges

## Future research

6.  Application to other educational contexts
7.  Integration with other models and approaches
8.  Expanding the scope of variables and data sources

# References

::: {#refs}
:::

# Supplemental Information {.appendix}

## Meta-Analysis of Correlations between Components and Student Outcomes

Subsequently, we performed a meta-analysis of these correlations to reveal the pooled effect of our variables. In this case, we conducted a multivariate meta-analysis, offering the advantage of modeling multiple, potentially correlated, outcomes. We transformed the correlations using Fisher's z-transformation (to ensure a normal distribution of the correlations) and ran a random effects model with each unique combination of "Teacher" and "School".

The resulting multivariate meta-analysis provides a comprehensive estimate of the correlations for each outcome, considering the hierarchical structure of the data. Thus, we can understand the overarching relationships between the different outcomes and the Badges across diverse schools and teachers. This robust conclusion, therefore, provides a more resilient analysis than a simple correlation analysis.

@fig-meta-analysis presents the results of a meta-analysis on the correlation.

```{r Meta-correlation prep}
#| include: false
# Clean environment
rm(list = setdiff(ls(), c("df", "random_py")))
# Importing results from Python
results_list <- py$results
gc(verbose = FALSE)
```

```{r}
#| cache: true
#| label: fig-meta-analysis
#| fig-cap: "Results of the correlation meta-analysis."
library(metafor)
library(ppcor)

n_comp <- 3
method <- "FrobeniusNNDSVD"
selected_cols <- c("Classroom.ID", "Teacher.User.ID",
                   "MDR.School.ID", "District.Rollup.ID",
                   "week", "Usage.Week",
                   # Main loadings of Components:
                   "tch_min", "tch_min_1",
                   paste0("FrobeniusNNDSVD", seq_len(n_comp)),
                   paste0("FrobeniusNNDSVD", seq_len(n_comp), "_1"),
                   "RD.optional_problem_sets",
                   "Guided.Practice.Completed", "Tower.Completed",
                   # Student Variables
                   "Active.Users...Total", "Minutes.per.Active.User",
                   "Badges.per.Active.User", "Boosts.per.Tower.Completion",
                   "Tower.Alerts.per.Tower.Completion",
                   # Classroom and Teacher Variables
                   "teacher_number_classes", "Grade.Level",
                   "Students...Total", "n_weeks",
                   # School Variables
                   "poverty", "income", "charter.school",
                   "school.account", "zipcode")  # Column to select

df_corr <- df %>%
  ungroup() %>%
  arrange(Classroom.ID, week) %>%
  dplyr::select(all_of(selected_cols)) %>%
  rename_with(~paste0("Frobenius", seq_len(n_comp)),
              paste0("FrobeniusNNDSVD", seq_len(n_comp))) %>%
  rename_with(~paste0("Frobenius", seq_len(n_comp), "_1"),
              paste0("FrobeniusNNDSVD", seq_len(n_comp), "_1"))

# Define a safe version of pcor.test that returns NA when there's an error
safe_pcor <- possibly(~pcor.test(..1, ..2, ..3, method = "spearman")$estimate,
                      otherwise = NA)
df_corr <- df_corr %>%
  group_by(Classroom.ID,Teacher.User.ID,MDR.School.ID,District.Rollup.ID) %>%
  summarise(
    n = n(),
    Frobenius1 = safe_pcor(Frobenius1, Badges.per.Active.User, Frobenius1_1),
    Frobenius2 = safe_pcor(Frobenius2, Badges.per.Active.User, Frobenius2_1),
    Frobenius3 = safe_pcor(Frobenius3, Badges.per.Active.User, Frobenius3_1),
    Minutes    = safe_pcor(tch_min, Badges.per.Active.User, tch_min_1),
    n_weeks = mean(n_weeks),
    teacher_number_classes = mean(teacher_number_classes),
    poverty = first(poverty),
    income = first(income),
    school.account = mean(school.account)
  ) %>%
  filter(!is.na(Frobenius1) &
           !is.na(Frobenius2) &
           !is.na(Frobenius3))

df_corr_sub <- df_corr %>%
  as_tibble() %>%
  slice_sample(prop = 0.1) %>%
  mutate_at(vars(paste0("Frobenius", seq_len(n_comp)), "Minutes"),
            list(~atanh(.))) %>%  # Fisher's z transformation
  mutate_at(vars(paste0("Frobenius", seq_len(n_comp)), "Minutes"),
            list(se = ~sqrt(1/(n - 2 - 2)))) %>%  # standard error sqrt(1/N−2−g)
  gather(key = "outcome", value = "correlation",
         c(paste0("Frobenius", seq_len(n_comp)), "Minutes")) %>%
  gather(key = "outcome_se", value = "se",
         c(paste0("Frobenius", seq_len(n_comp), "_se"), "Minutes_se")) %>%
  filter(str_replace(outcome, "_se", "") == str_replace(outcome_se, "_se", "")) %>%
  dplyr::select(-"outcome_se") %>%
  filter(!is.na(se))

# Run multivariate meta-analysis
res <- rma.mv(yi = correlation,
              V = se^2,
              random = ~ 1 | Classroom.ID/Teacher.User.ID/MDR.School.ID/District.Rollup.ID,
              mods = ~ -1 + outcome,
              data = df_corr_sub)

# Add columns for back-transformed effect sizes and their standard errors
res_df <- data.frame(
  estimate = coef(res),
  outcome = str_replace(names(coef(res)), "outcome", ""),
  # se = sqrt(diag(vcov(res)))
  ci.lb = res$ci.lb,
  ci.ub = res$ci.ub
  ) %>%
  mutate(
    estimate_r = tanh(estimate),
    ci.lb = tanh(ci.lb),
    ci.ub = tanh(ci.ub)
    # se_r = se / (1 - estimate^2)  # delta method for SE
  )
# Reverse the Fisher's Z transformation
df_corr_sub <- df_corr_sub %>%
  mutate(correlation = tanh(correlation))
# Create the plot
ggplot(df_corr_sub, aes(x = correlation, y = after_stat(density))) +
  geom_histogram(binwidth = 0.1, fill = "lightblue") +
  geom_density(aes(weight = weights(res)),
               alpha = 0.5) +
  geom_vline(data = res_df,
             aes(xintercept = estimate_r,
                 color = outcome),
             linetype = "dashed") +
  geom_segment(data = res_df,
               aes(x = ci.lb,
                   y = 0,
                   xend = ci.ub,
                   yend = 0,
                   color = outcome),
               linewidth = 1.5) +
  geom_text(data = res_df,
            aes(x = estimate_r,
                y = 0.15,
                label = sub('0\\.', '.', round(estimate_r, 2))),
            color = "black",
            size = 3.5,
            check_overlap = TRUE) +
  facet_wrap(~outcome, scales = "free",  ncol = 2) +
  labs(x = "Correlation", y = "Density") +
  theme_light() +
  scale_color_discrete(name = "Pooled effects \n (with 95% C.I.)")
  theme(legend.position = c(0.85, 0.24),
        legend.direction = "vertical")  # Set legend position and direction

```

```{r meta-analysis summary}
#| eval: false
summary(res)
```

## **Policy Gradient Methods**

R +VS'-VS

WW+W

## **Binary Q-Learning Model**

### **Notation and System Constraints**

Let 𝑡 ∈ {1,\...,𝒯} denote the index time (in weeks), aj ∈ {Zearn, Not Zearn} the binary (j ∈ {1, 2}) action to use the platform or not, and Qt(aj) the value of choosing action aj at week 𝑡. Moreover, for each teacher i, let ci(a2)ℝ be the fixed cost of working on the Zearn platform (with ci(a1)=0), rit the reward (average number of badges from teacher i's students) obtained at week t, 0i1 a learning rate parameter, and i a free inverse temperature parameter.

### **Learning and Decision Problem**

The primary decision problem for a teacher i on week 𝑡 is to choose aj, that is, whether to exert effort on the Zearn platform or not. In this two-armed bandit model, the value of each decision is associated with its reward in student badges minus the cost (rt-ci(ajt)). Note that this structure implies that the ci is in the same units as rit (i.e., number of badges). The model is initialized with Qt(aj)=0 for both actions. After executing a decision, the teacher updates the action value as such:

Qt+1(ajt)=Qt(ajt)+i(rt-ci(ajt)-Qt(ajt)).

At any given week t, the teacher uses the current learned Q-values to make a probabilistic decision. Thus, the teacher's probability of choosing aj is given by a softmax equation:

P(aj)=eQ(aj)k=12eQ(ak).

## **Binary Actor-Critic Model**

## **Two-Step Decision Process**

### **First-Stage Notation and System Constraints**

Let 𝑡 ∈ {1,\...,𝒯} denote the index time (in weeks), at0 the number of minutes spent on the platform, rit the reward (average number of badges from teacher i's students) obtained, and xit the average number of tower alerts from teacher i's students. Let Sℝd be a vector of state variables that define the state of a given week. In our case, where d=3:

St+1=\[xt , at , rt\].

Moreover, denote wℝdto be a vector of state-value weights and v(s,w) a state-value function parameterization, such that

v(St+1,w)=St+1w=w1xt+w2at +w3rt.

### **First-Stage Learning and Decision Problem**

The first-stage decision problem for a teacher i on week 𝑡 is to choose aj, that is, how many minutes to spend on the Zearn platform. In this two-stage model, the value of aj=0 is learned through Q-learning, such that

Qt+1(ajt=0)=Qt(ajt=0)+(rt-Qt(ajt=0)).

However, the value of any aj\>0 is given by v(s,w). As such, the first decision stage is the binary decision between aj=0 and aj\>0 in a probabilistic fashion, as such:

P(aj=0)=eQ(aj=0)eQ(aj=0)+ev(s,w) and P(aj\>0)=ev(s,w)eQ(aj=0)+ev(s,w).

Therefore, if aj=0 is chosen at week t, Q(ajt=0) is updated and the decision process repeated for the following week. If aj\>0 is chosen, however, the teacher must decide the exact number of minute aj to spend on the platform.

### **Second-Stage Notation and System Constraints**

To determine a number of minutes aj\>0, we must define a differentiable policy parameterization (a\|s,), where ℝd' is a vector of policy parameters that determine a teacher's policy. In our case, we define it as the normal probability density over minutes spent on the platform, with mean and standard deviation given by parametric function approximators that depend on state characteristics. Formally, we have

where (s,) and (s,) are two parameterized function approximators. In our model, we approximate the mean as a linear function and the standard deviation as the exponential of a linear function of our state variables. As such, we define =(,), where ,ℝd and d'=2d, and the approximations as follows:

(St+1,)=St+1=1xt+2at +3rt

(St+1,)=(St+1)=(1xt+2at +3rt).

### **Second-Stage Learning and Decision Problem**

The second-stage decision problem for a teacher i on week 𝑡 is to choose aj\~ (\|s,). The value of each state-action is associated with the consequent reward in student badges minus the cost, rt-ci(ajt), where ci=f(a) is the cost function of working on the Zearn platform, with f(0)=0, f'(a)\>0, f"(a)\<0. The model is initialized with ,w=0, therefore v(S1,w), (S1,)=0 and (S1,)=1.

After sampling from the distribution (\|s,), the teacher observes the new state St+1 and the associated reward Rt=rt-ci(ajt). She then learns through a one-step Actor-Critic, as follows (CITE Sutton Barto):

1.  Calculate a prediction error R+v(St+1,w)-v(St,w), where:

    1.  01 is a discount factor.

2.  Update ww+wv(St,w), where:

    1.  w\>0 is a step size parameter that determines the learning rate of w

    2.  v(St,w)=\[xt-1 ,at-1 ,rt-1\]

3.  Update +t-1(aj\|St,), where

    1.  \>0 is a step size parameter that determines the learning rate of

    2.  (aj\|St,)=1(St,)2(aj-(St,))\[xt-1 ,at-1 ,rt-1\]

4.  Update +t-1(aj\|St,), where

    1.  (aj\|St,)=((aj-(St,))2(St,)2-1)\[xt-1 ,at-1 ,rt-1\]

## **Model Estimation**

The three free parameters (, , and cost for Q-learning; w, , and cost for Gaussian policy) were estimated separately for each subject, but jointly (in a hierarchical random effects model) with group-level mean and variance parameters reflecting the distribution, over the population, of each subject-level parameter.

The parameters were estimated using Hamiltonian Monte Carlo, as implemented in the Stan programming language (CITE Carpenter et al., 2017). We ran the model with 4 chains of 1000 iterations for each (of which the first 250 were discarded for burn-in). We verified convergence by visual inspection and by verifying that the potential scale reduction statistic R-hat (CITE Gelman and Rubin, 1992) was close to 1.0 (\<0.003 for all parameters) (PASTE Table 1). Note that for the Gaussian model, we limited parameter estimation of w, to very small values \[0, 0.0001\] as simulation demonstrated that values outside this range creates policy parameters and state value weights of extreme magnitudes, pushing the model to diverge with large standard deviations of the policy function.

For the Q-learning model, we used the sampled parameters to compute per-trial Q values for each action, week, and teacher. We also calculated the probability of each choice through the softmax function. For the Gaussian model, we used the sampled parameters to compute per-trial mean and standard deviations, along with state values for each action, week, and teacher.

## Bayesian Model Diagnostics

```{r}
#| eval: false
#| cache: true
#| label: tbl-diagnostics
#| tbl-cap: "Diagnostics for the Bayesian Models used in Q-learning and Q-learning with states. The tables show diagnostics for three Non-negative Matrix Factorization (NMF) models: Frobenius (initialized with a Nonnegative Double Singular Value Decomposition), Frobenius Average (initialized with a Nonnegative Double Singular Value Decomposition with zeros filled with the average of each variable), and Kullback-Leibler. The rows correspond to 'alpha,' the learning rate, 'gamma,' the discount factor, 'tau,' the temperature parameter, 'cost' for each of the components from the NMF, 'sensi,' the reward sensitivity parameter, and 'Log Posterior,' the logarithm of the posterior probability. 'Rhat' indicates the potential scale reduction factor on split chains. Values close to 1.0 imply model convergence. 'ESS Bulk' and 'ESS Tail' represent the effective sample size for the bulk of the posterior distribution and its tail, respectively. An ESS of at least a few hundred is often considered sufficient for estimation purposes."

results_files <- list.files(path = "Bayesian/Results", pattern = "*.RDS", full.names = TRUE)
results_files <- results_files[!grepl("logit", results_files)]
results_files <- results_files[!grepl("hierarchical", results_files)]
diag_df <- data.frame()

for (result_file in results_files) {
  fit <- readRDS(result_file)
  
  # Get summary
  fit_summary <- fit$summary()
  
  # Create a data frame of the Rhat values and ESS
  result_df <- data.frame(
    Model = gsub("Bayesian/Results/||.RDS", "", result_file),
    Variable = fit_summary$variable,
    Rhat = fit_summary$rhat,
    ESS_Bulk = fit_summary$ess_bulk,
    ESS_Tail = fit_summary$ess_tail
  )
  
  # Add result to the overall diagnostics data frame
  diag_df <- rbind(diag_df, result_df)
}
# Reshape the data frame
diag_df <- diag_df %>%
  pivot_longer(cols = c(Rhat, ESS_Bulk, ESS_Tail),
               names_to = "Statistic") %>%
  pivot_wider(names_from = c(Model, Statistic),
              values_from = value) %>%
  mutate(Variable = if_else(Variable == "lp__", "Log Posterior", Variable))

# Split the data into two separate data frames
diag_df_actor_critic <- diag_df[,c(TRUE, grepl("Actor-Critic-", names(diag_df))[-1])]
diag_df_q_learning_states <- diag_df[,c(TRUE, grepl("Q-learning-states-", names(diag_df))[-1])]
diag_df_q_learning <- diag_df[,c(TRUE, (!names(diag_df) %in% names(diag_df_q_learning_states) &
                                          !names(diag_df) %in% names(diag_df_actor_critic))[-1])]

# Define the spanner labels for each model
diag_df_q_learning %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_q_learning_states %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

diag_df_actor_critic %>%
  gt() %>%
  fmt_number(columns = c(-Variable), decimals = 2) %>%
  tab_spanner(
    label = "Frobenius",
    columns = c(ends_with("FR_Rhat"), ends_with("FR_ESS_Bulk"), ends_with("FR_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Frobenius Average",
    columns = c(ends_with("FRa_Rhat"), ends_with("FRa_ESS_Bulk"), ends_with("FRa_ESS_Tail"))
  ) %>%
  tab_spanner(
    label = "Kullback-Leibler",
    columns = c(ends_with("KL_Rhat"), ends_with("KL_ESS_Bulk"), ends_with("KL_ESS_Tail"))
  ) %>%
  cols_label(
    ends_with("Rhat") ~ "Rhat",
    ends_with("ESS_Bulk") ~ "ESS Bulk",
    ends_with("ESS_Tail") ~ "ESS Tail"
  )

```

# Alternative models {.appendix}

```{python LSTM autoencoder}
#| eval: false
import os # To fix: https://github.com/tensorflow/tensorflow/issues/59779
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from keras.models import Model, load_model
from keras.layers import Input, Dense, LSTM, GRU, RepeatVector, TimeDistributed, Reshape, Flatten
from keras.constraints import NonNeg
from keras.callbacks import EarlyStopping
from keras.regularizers import L1
from keras_tuner import Hyperband
from keras_tuner.engine.hyperparameters import HyperParameters

# Function to reshape a 2D DataFrame into a 3D array
def reshape_data(df, num_samples, num_features):
    num_timesteps = dfpca_py['week'].nunique()
    X_reshaped = np.zeros((num_samples, num_timesteps, num_features))
    grouped = df.groupby('Classroom.ID')
    for i, (classroom_id, group) in enumerate(grouped):
        group = group.sort_values('week')
        group_features = group.drop(['Classroom.ID', 'week'], axis=1)
        X_reshaped[i, :len(group), :] = group_features.values
    X_reshaped = np.nan_to_num(X_reshaped)
    return X_reshaped
# Function to prepare the data
def prepare_data(df):
    df_id_week = df[['Classroom.ID', 'week']]
    df = df.drop(['Classroom.ID', 'week'], axis=1)
    df = scaler.transform(df)
    df = pd.DataFrame(df, columns=X_cols)
    df = pd.concat([df_id_week, df], axis=1)
    df_reshaped = reshape_data(
      df,
      df['Classroom.ID'].nunique(),
      len(df.columns) - 2
    )
    return df_reshaped

# Separate predictors and target variable for both training and testing data
X_train = train_df.drop(['Badges.per.Active.User', 'set'], axis=1)
Y_train = train_df[['Badges.per.Active.User', 'Classroom.ID', 'week']]
X_test = test_df.drop(['Badges.per.Active.User', 'set'], axis=1)
Y_test = test_df[['Badges.per.Active.User', 'Classroom.ID', 'week']]

# Prepare the data
X_train_reshaped = prepare_data(X_train)
X_test_reshaped = prepare_data(X_test)
Y_train_reshaped = reshape_data(
  Y_train,
  Y_train['Classroom.ID'].nunique(), 1)
Y_test_reshaped = reshape_data(
  Y_test,
  Y_test['Classroom.ID'].nunique(), 1)

################ Neural Net
# Determine loss weights according to the data structure:
decoding_weight = Y_train_reshaped.std() / (Y_train_reshaped.std() + X_train_reshaped.std())
prediction_weight = 1 - decoding_weight
# Function to build model
def build_model(hp):
    n_timesteps = X_train_reshaped.shape[1]
    n_features = X_train_reshaped.shape[2]
    n_labels = 1 # Regression: just one output node
    
    input_data = Input(shape=(n_timesteps, n_features))
    x = input_data
    
    # Add a variable number of hidden LSTM layers for encoder
    num_layers = hp.Int('num_layers', 2, 4)
    lstm_units = [
      hp.Choice('units_' + str(i), values=[2, 4, 8, 16, 32, 64, 128])
      for i in range(num_layers)
    ]
    for i in range(num_layers):
        x = LSTM(
          units=lstm_units[i],
          activation='tanh',
          return_sequences=True)(x)
    
    # Generate the latent vector
    latent_dim = hp.Int('encoding_units', min_value=2, max_value=n_comp, step=1)
    l1_value = hp.Float('l1_value', min_value=0.005, max_value=0.05, default=0.01, step=0.001)
    latent = TimeDistributed(Dense(
      units=latent_dim,
      activation='linear',
      activity_regularizer= L1(l1=l1_value),
      kernel_constraint=NonNeg(),
      name='encoded-vector'),
      name='latent')(x)
    
    # Decoder
    x = latent
    for i in range(num_layers):
        x = LSTM(
          units=lstm_units[i],
          activation='tanh',
          return_sequences=True)(x)
          
    decoded = TimeDistributed(
      Dense(units=n_features,
            activation='linear'),
      name='decoding')(x)
    
    label_output = TimeDistributed(
      Dense(1, activation='linear'),
      name='prediction')(latent)
    
    # Instantiate Autoencoder Model using Input and Output
    autoencoder = Model(inputs=input_data, outputs=[decoded, label_output])
    autoencoder.compile(optimizer='adadelta',
                        loss={
                          'decoding': 'mean_squared_error',
                          'prediction': 'mean_squared_error'
                        },
                        loss_weights={
                          'decoding': decoding_weight,
                          'prediction': prediction_weight
                        })
    return autoencoder

# Set up hyperparameter tuner
tuner = Hyperband(build_model,
                  objective='val_loss',
                  max_epochs=50,
                  factor=3,
                  directory='autoencoder_tuning',
                  project_name='LSTM_2nd_tuning')

# Perform hyperparameter search
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)
tuner.search(x=X_train_reshaped, 
            y=[X_train_reshaped, Y_train_reshaped],
            epochs=500,
            validation_data=(X_test_reshaped, [X_test_reshaped, Y_test_reshaped]),
            callbacks=[early_stopping_callback])
# tuner.results_summary()
# Get the optimal hyperparameters
best_hyperparameters = tuner.get_best_hyperparameters()[0]
model = tuner.hypermodel.build(best_hyperparameters)
history = model.fit(x=X_train_reshaped, 
                    y=[X_train_reshaped, Y_train_reshaped],
                    epochs=5_000,
                    validation_data=(X_test_reshaped, [X_test_reshaped, Y_test_reshaped]))
best_model = history

# Function to get encoded representation and components
def get_encoded_representation_and_components(best_model, X):
    X_reshaped = reshape_data(
      X,
      X['Classroom.ID'].nunique(),
      len(X.columns) - 2
    )
    # Get index of encoding layer
    encoding_layer_index = next(
      i for i,
      layer in enumerate(best_model.layers) if layer.name == 'latent'
    )
    encoder_layers = [layer for layer in best_model.layers[:encoding_layer_index + 1]]
    input_data = Input(shape=(X_reshaped.shape[1], X_reshaped.shape[2]))
    x = input_data
    for layer in encoder_layers[1:]:
      x = layer(x)
    encoder = Model(input_data, x)
    
    # Copy weights for each layer from the best model
    for i, layer in enumerate(encoder.layers):
      layer.set_weights(best_model.layers[i].get_weights())
    
    X_encoded = encoder.predict(X_reshaped)
    X_hat = best_model.predict(X_reshaped)[0]
    res = ((X_reshaped - X_hat)**2).sum(axis=(1,2)).sum()
    n_components = X_encoded.shape[2]
    
    results.setdefault("Autoencoder", {})[n_components] = X_encoded
    residuals.setdefault("Autoencoder", {})[n_components] = res

# Get encoded representation and components
X = pd.concat([dfpca_py[['Classroom.ID', 'week']], X_scaled], axis=1)
get_encoded_representation_and_components(best_model, X)

```

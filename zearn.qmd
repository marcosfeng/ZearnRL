---
title: "Unveiling Adaptive Pedagogy: Reinforcement Learning Models Illuminate Teacher Decision-Making in an Online Math-Teaching Platform"
abstract: "This study introduces a new use of reinforcement learning (RL) models to gain insight into teachers' decision-making processes on the Zearn online math-teaching platform. Analyzing data from 3029 classrooms, our approach treats pedagogical decision-making as a complex adaptive system, taking into account educators' reward history and the unique contexts of their instructional environments. Using Q-learning and Actor-Critic models, we identify distinct patterns of teaching behavior. Our comparison of RL models with a traditional logistic regression baseline demonstrates the superiority of the Actor-Critic model in capturing teacher behavior across most classrooms. Importantly, our findings reveal a strong positive correlation between the learning rates of state value weights and improved student achievement within the preferred Actor-Critic model. This finding highlights educators' adaptability in meeting their students' evolving needs in the digital learning landscape, ultimately leading to better learning outcomes. By demonstrating the relevance of RL models in the educational domain, our study introduces a new approach to exploring teacher decision-making and showcases their potential to inform the development of more effective, adaptive teaching strategies."
keywords: "Reinforcement Learning, Pedagogical Decision-Making, Digital Education Platforms, Instructional Adaptation, Q-learning, Actor-Critic"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
format:
  elsevier-pdf:
    keep-tex: true
    include-in-header:
      - text: |
          \usepackage{dcolumn}
          \usepackage{typearea}
          \usepackage{longtable}
    journal:
      formatting: preprint
      model: 3p
      layout: onecolumn
      cite-style: number
      # graphical-abstract: "![](abstract.png)"
bibliography: zearnrefs.bib

execute:
  echo: false
  warning: false
  error: false

knitr:
  opts_chunk:
    cache.extra: set.seed(832399554)
---

```{r load packages}
# Figures
library(ggforce)
library(pheatmap)
library(scales)
library(RColorBrewer)
library(ggrepel)
library(ggpubr)

# Tables
library(stargazer)
library(gtsummary)
library(gtExtras)
library(kableExtra)

# Other platforms
library(R.matlab)
library(reticulate)

# Statistics
library(broom)
library(PerformanceAnalytics)
library(ppcor)
library(fixest)
library(pROC)
library(glmmTMB)
library(broom.mixed)

# Basic packages
library(doParallel)
library(data.table)
library(tidyverse)

# library(cmdstanr)
# library(brms)
# library(bayesplot)
```

```{r}
set.seed(832399554)
random_py <- reticulate::import("random")
random_py$seed(832399554)
# https://www.random.org/integers/
# Timestamp: 2023-05-17 16:18:28 UTC
```

# Introduction

Predicting repeated behavior has been a long-standing goal of the behavioral sciences, including economics, psychology, and neuroscience [@verplanken2022; @venkatesh2023; @hagger2023]. Reinforcement learning (RL) algorithms have emerged as a prominent way of quantifying these relationships, assigning a mathematical relationship between contextual cues (states), behavior (actions), and reward [@sutton2018; @kaelbling1996]. These algorithms have found wide application in neuroscience and cognitive psychology, where they are used in data sets to model agents in specific environments [@zhang2020].

@niv2022, for example, proposed a perspective on cognitive behavioral therapy (CBT) by framing it within reinforcement learning (RL) theory. Highlighting the similarities between these two domains offers a promising avenue for advancing our understanding of CBT and refining its clinical applications. Specifically, the researchers propose that CBT's cognitive aspect may correspond to the model-based learning system, while its behavioral component relates to the model-free system. There are two examples of RL applications. First, prolonged exposure therapy (i.e., vividly recounting a traumatic experience in a safe environment) is akin to updating the value of a traumatic state. One novel insight from RL is that moderate prediction errors (i.e., gradual extinction) are more effective than high prediction errors, which can lead to new values being assigned to an entirely different state. Second, RL theory can explain how CBT treatments for obsessive-compulsive disorder work. Patients are asked to imagine and write down the worst-case outcomes of not performing their obsessions. In RL terms, this activity forces patients to create a model of the world with low transition probabilities for worst-case scenarios, effectively reducing the perceived probability of feared outcomes. While this theoretical framework is compelling, future research must substantiate these novel insights with concrete empirical evidence.

@collins2020, however, discussed the complexities of reinforcement learning, moving beyond the traditional model-based (MB) and model-free (MF) dichotomies. These dichotomies, while beneficial in understanding certain aspects of human learning and decision-making, also impose limitations by oversimplifying the diverse mechanisms at play in RL. Such dichotomies have advanced understanding of neural and social factors influencing behavior; however, Collins and Cockburn pointed out the challenge of uniquely and reliably classifying behaviors under any one controller due to the diversity and complexity of learning mechanisms. Instead, MB and MF learning should be seen as high-level processes emerging from coordinating various sub-computations; however, it is important to consider independent underlying computations in MB and MF algorithms and the potential to recombine these subcomponents meaningfully, thereby questioning the strict separation between MB and MF RL. From this theoretical discussion, further research is needed to explore the complexities of reinforcement learning by refocusing on the diverse computational components that constitute learning and decision-making, particularly in empirical studies. The nuanced understanding of learning processes can inform the development of more sophisticated models to capture the diverse decision-making strategies of teachers. This approach aligns with our goal of transcending simple binary models to embrace the multifaceted nature of individual teacher behavior in digital learning environments.

More pragmatically, @park2019 explored the benefits of using a personalized social robot learning companion using a model-free Q-learning approach to improve engagement and learning outcomes among young English language learners. Their approach employed affective reinforcement learning to train a customized storytelling approach for each child, optimized for their skill progression. The experiment included 67 children between the ages of 4-6 divided into one of three groups: personalized robot, non-personalization robot, and a no-robot baseline. Throughout 6-8 sessions, the children participated in storytelling activities with the robots; the personalized robot used a model-free Q-learning approach to predict the complexity levels of the activities to maximize a child's future engagement and learning gains. As such, a) the reward was a combination of engagement and learning progress (assessed through the child's use of new words and syntax structures), b) states were the users' engagement (measured by verbal and nonverbal cues) and affective arousal levels (measured by facial expressions), and c) actions were the complexity level of the activities. Results indicated that the personalized robot effectively adapted to each child's needs, leading to better engagement and learning outcomes than non-personalized and no-robot conditions. The positive results suggest that a Q-learning strategy can offer tailored and improved teaching methods. However, Park et al. chose the model and multiple parameters arbitrarily; therefore, future models could enhance the robot's outcomes by, for example, defining other state spaces or learning rates. Additionally, the generalizability of these findings to other educational settings and populations is needed. The affective reinforcement learning approach effectively operationalizes states, actions, and rewards in an educational space, resulting in adaptive, engaging learning experiences for students.

Researchers have also attempted to bridge computational neuroscience and clinical applications. @brown2021 investigated whether depression symptoms are related to features of reinforcement learning, fitting a state-free q-learning algorithm to participants' behavior in a reward and loss learning task (i.e., learning a stimulus's expected monetary gains or losses, respectively). The researchers then regressed depression symptoms on the model-derived parameters and found that depression symptoms may selectively disrupt specific components of the reinforcement learning process. Notably, the Brown et al. study bridges computational neuroscience and clinical applications, offering a novel way of characterizing depression, typifying patients, and opening new avenues for personalized treatment. The study resonates with using estimated parameters as markers of individual differences, hinting at the potential for personalized interventions based on these models. Likewise, our study examines the variation in estimated learning rate parameters and their association with overall student outcomes.

In another investigation of reinforcement learning in neuroscience, @eckstein2021 reviewed the interpretability and generalizability of reinforcement learning (RL) models in neuroscience and cognitive science, highlighting the widely adopted assumption that estimated RL parameters explain specific (neuro)cognitive functions and that these explanations translate across contexts. Methodological differences among RL studies yield a considerable variation in interpretation; for instance, learning rates have been linked to incremental updating, reward sensitivity, and approximate inference. Consequently, generalizing findings across tasks, populations, and model parameterizations is not often appropriate. Eckstein et al. emphasized that future research should contextualize findings within specific experimental setups and consider alternative models that better capture the complexities of human cognition. We take this advice to address the degrees of freedom in our research design while contextualizing our results accordingly.

The focus primarily on neuroscience and cognitive psychology applications, however, presents a novel opportunity to use methods from one set of disciplines on data traditionally used in another. In this paper, we aim to further this integration by applying RL algorithms to model the decision-making process of teachers in the math-teaching platform Zearn. RL provides a system of rewards and punishments where the agent (in this case, the teacher) learns to make optimal decisions by maximizing the rewards and minimizing the punishments [@sutton2018; @kaelbling1996]. By assuming every teacher has an objective function to balance with their potential rewards, we model the sequential behavior of a teacher throughout a school year. For instance, the teacher chooses which pedagogical actions to employ, such as assigning homework, checking student progress, or reviewing content, in anticipation of enhancing student achievement. Applying RL algorithms allows for flexibility in learning the best strategy given certain contextual information. We provide a model of how teachers adapt their strategies in response to student performance and other contextual factors. This approach offers a flexible model for our available data and opens new avenues for understanding and enhancing human behavior in complex, real-world settings.

<!-- Discuss existing models and where they fall short, justifying your approach. -->

## The Zearn Platform

Zearn is a digital platform for mathematics education designed to facilitate the teaching and learning of mathematics. About 25% of elementary school students and over 1 million middle school students across the United States use Zearn [@zearn2024v]. Its unique blend of hands-on teaching and immersive digital learning, paired with its widespread adoption, provides the perfect setting for understanding how teachers adapt their strategies to optimize student achievement.

Zearn's pedagogical approach includes interactive digital lessons using visual aids (see @fig-zearn-poster) and real-time student feedback. The platform's approach to mathematical concepts, such as fractions, is particularly noteworthy. Students go through a series of representations — concrete, pictorial, and abstract — each designed to scaffold their understanding (i.e., "breaking down" problems, see [@jumaat2014; @reiser2014]) and prepare them for subsequent levels.

![Screenshot of student lesson on Zearn. The image is an example of teaching with visual models on the platform.](images/zearn-poster.jpg){#fig-zearn-poster fig-align="center"}

The platform's structure provides students with a personalized learning experience (see Appendix for a screenshot of the student portal) and teachers with resources to track student progress and make informed decisions (see @fig-class-report for a sample class report). Zearn follows a rotation model of learning — that is, a blend of traditional face-to-face learning (i.e., small group instruction) with online learning (i.e., self-paced online lessons). With this approach, students can learn new grade-level content in two distinct ways: independently, by engaging in digital lessons, and in small groups with their teacher and peers.

![Screenshot of sample classroom report. The image displays a summary dashboard where teachers can follow their students' progress as indicated by the number of lessons completed.](images/class-report.png){#fig-class-report fig-align="center"}

A key feature of Zearn is its badge system, which tracks student progress and motivates continued learning (see @fig-badges-screen). Students earn badges upon mastery of specific skills, providing a tangible representation of their achievement. This system motivates students and provides teachers with valuable data on student performance, informing their decision-making process [@knudsen2020]. Zearn also incorporates notifications, known as Tower Alerts, sent to teachers when a student struggles with a specific concept. This feature allows teachers to provide timely support and address learning gaps, enhancing the platform's capacity for personalized learning.

![Screenshot of student badges page. The image displays a summary dashboard where students can see their total badges earned (i.e., lessons completed) for a given mission (i.e., course module). Faded badges on the image signify open lessons to be completed, unfaded badges represent earned badges, and the locked icons correspond to future digital lessons that will open once all activities in the current lesson are completed.](images/badges.PNG){#fig-badges-screen fig-align="center"}

@morrison2019, for example, evaluated the effectiveness of Zearn Math, a digital math curriculum, in a large urban school district for improving student outcomes. The study employed a mixed-methods approach, combining quantitative and qualitative data from various sources, including student achievement data, student and teacher usage data (i.e., time spent on the platform), student and teacher surveys, classroom observations, teacher focus groups, and administrator interviews. The study revealed mixed results. Administrators, students, and teachers generally viewed Zearn Math positively, citing increased student engagement and improved instruction differentiation. However, challenges included initial discomfort with the shift from whole class to half-class instruction and uneven teacher preparedness, primarily due to limited professional development opportunities. While Zearn Math seemed to improve student engagement and higher order thinking skills, its impact on student achievement was unclear. Achievement gains on standardized tests were not significantly different overall, though there were positive correlations between Zearn Math usage and achievement gains. The program's success may be attributed to its engaging, gamified components and alignment with the Common Core State Standards. Additionally, it is important to provide adequate professional development and support for teachers to effectively implement the program. The findings of Morrison et al. highlight the potential and challenges of integrating digital platforms like Zearn Math into the curriculum, which is relevant for understanding how such tools can support teacher strategies and student engagement in mathematics.

Another noteworthy feature is the platform's comprehensive professional development component, which is accessible to schools with a paid account (see @fig-prof-dev for a sample training schedule). In this program, teachers within a school collaborate to explore each unit or mission through word problems, fluencies, and small group lessons. They also analyze student work and problem-solving strategies. This professional development prioritizes (1) each mission's primary mathematical concept, (2) visual representations to scaffold learning, and (3) strategies to address unfinished learning from prior grades while preparing for future learning [@morrison2019].

Researchers have also examined the Zearn approach for teaching teachers professional development knowledge. @knudsen2020a focused on the effectiveness of the Curriculum Study Professional Development (CS PD) program developed by Zearn to enhance elementary school teachers' Pedagogical Content Knowledge (PCK) in teaching mathematics. The researchers used a case study approach, examining eight teachers across various schools and districts who had undergone the CS PD program. Data collection methods included think-aloud interviews, classroom and CS PD session observations, and interviews with teachers and administrators; the researchers found that 75% of the teachers experienced growth in their PCK. Key strengths of the CS PD program included its relevance to practice, encouragement of collaboration among teachers, and its effectiveness in developing big ideas. However, challenges in responding to students' in-the-moment problem-solving and varied teacher engagement were noted. The findings underscore the importance of enhancing teachers' pedagogical content knowledge, especially in mathematics. One of the measures we developed in our study specifically captures pedagogical content knowledge. These insights can inform the development of similar professional development programs in the current project, ensuring they are effectively tailored to teachers' instructional needs and students’ diverse backgrounds. Further, we are able to model teacher behavior at the individual level.

Zearn's integrated framework provides a rich repository of data for our analysis. The variables delineated for investigation by Zearn encompass: (1) teacher engagement, quantified through a diverse set of actions; (2) student achievement, denoted by variables such as lesson completion (i.e., "badges" earned after each lesson is finished with full proficiency); and (3) student struggles, monitored through variables such as "tower alerts" (see @tbl-teacher-variables for a full glossary of available variables).

## Research Questions

We propose the following research questions:

1\. Characterizing Teacher Behavior: How can we best explain teachers' action choices? How does the explanatory power of reinforcement learning compare to simpler baseline models? Which specific reinforcement learning model best captures the empirical data on teacher behavior?

2\. Impact of Estimated RL Parameters: How do individual differences in teachers' decision-making patterns, as inferred from the parameters of the best-fitting reinforcement learning model, relate to heterogeneity in student achievement gains? Can we identify specific teacher behavioral profiles that predict better student learning outcomes?

3\. Influence of Teacher and School Background: To what extent do school contextual factors (e.g., socioeconomic status) account for variation in teachers' instructional choices, as quantified by the parameters of the reinforcement learning model?

# Theory

## Reinforcement Learning to Capture Patterns in Repeated Behavior

In RL, an agent learns to make decisions over time. Formally, an RL task is a tuple $\langle S,A,R,P,\gamma \rangle$, where $S$ is a set of states, $A$ is a set of actions, $R = \mathbb{E}[R_{t+1}|S_t = s, A_t = a]$ is a reward function, $P= \Pr[S_{t+1} = s′ |S_t = s,A_t = a]$ is a state transition probability, and $\gamma \in [0,1]$ is a discount factor. We define the agent's decisions as a probability distribution over actions, namely, the policy $\pi(a|s) = \Pr[A_t = a|S_t = s]$ [@sutton2018].

RL models have been used in the psychology of habit to explain learning and reward association [@thorndike1931; @Rescorla1972ATO]. One common approach in human studies is to apply the "multi-armed bandit" task [@daw2006; @dennison2022]. In this type of experiment, participants are presented with multiple actions, each with an unknown payoff. The subject's goal is to learn the best outcome through trial and error. In the beginning, the reward-action relationships are unknown, so the participant must explore or sample each action [@sutton2018] (p. 3). This exploration-exploitation trade-off is a central theme in RL and has the potential to provide valuable insights into how individuals learn and make decisions over time.

In the context of education and teaching, RL has been present as early as 1960, with Ronald Howard applying this mathematical framework to instruction theory [@howard1960]. Later, in 1972, Richard Atkinson proposed a theory of instruction that encapsulates the key components of a Markov decision process, including states, actions, transition probabilities, reward functions, and a time horizon [@atkinson1972]. In Atkinson's framework, actions are instructional activities (e.g., assigning problem sets) that can change a given state (e.g., student learning level). These changes in states can yield rewards minus the associated cost of the action. For example, a teacher may be rewarded with an increase in the knowledge or skill of a student, but such reward must be balanced with its associated effort (e.g., labor cost). Atkinson and colleagues continued to test many parametrizations of this idea, contributing significantly to the development of RL theory in the context of education (see @doroudi2019 for a full review).

In the Zearn context, we define the decision process as follows:

1.  Agents are the teachers.

2.  Actions include the teachers' choices of specific pedagogical strategies.

3.  The reward is a function of the average student performance or activity within a classroom.

4.  The state is a function of other classroom variables not used in the reward function.

Note that we use $S$ in only one of our models. Further, we only consider model-free RL algorithms, which are so called because they do not require the agent to learn $P$ (i.e., state transition probabilities) to approximate expected rewards [@watkins1992]. In the following sections, we describe the two models we use to capture teacher behavior in the Zearn platform: Q-learning and Actor-Critic.

## Q-Learning Model

Consider a teacher using the Zearn platform. Each week, they must decide between two actions: assigning additional homework (action 1) or spending more time reviewing the material in class (action 2). At first, the teacher is uncertain about the best action to take. They start with initial beliefs about each action's long-term value (Q-value). However, they know that these beliefs may not be accurate and that they need to learn from experience.

Each week, the teacher chooses an action based on the current Q-value estimates. For example, in week 1, the teacher believes that assigning homework (action 1) has a slightly higher Q-value than reviewing in class (action 2). So, they assign homework and observe the outcome.

Then, the teacher receives a reward signal (e.g., the students' performance after the homework assignment). They use this reward to update their estimate of the Q-value for assigning homework, following the Q-learning update rule. This rule adjusts the Q-value estimate based on the difference between the observed reward and the previous estimate multiplied by a learning rate parameter.

Over the following weeks, the teacher continues to make decisions and update their estimates based on the outcomes observed. Sometimes, they explore new actions to gather more information, even if these actions do not seem optimal based on the current estimates. Other times, the teacher exploits their experience by choosing the action with the highest estimated Q-value.

As the teacher learns from experience, the Q-value estimates gradually converge toward the true values for each action.

| Week | Q-value Difference | Policy               | Choice | Reward | Prediction Error              | Updated Q-value                 |
|------|--------------------|----------------------|--------|--------|-------------------------------|---------------------------------|
| $t$  | $Q_t$              | $\text{Pr}_t(a = 1)$ | $a$    | $R_t$  | $\delta_t = \gamma R_t-c-Q_t$ | $Q_{t+1}=Q_{t}+\alpha \delta_t$ |
| 1    | 1.533              | 1                    | 1      | 0.052  | -2.891                        | 0.815                           |
| 2    | 0.815              | 1                    | 1      | 0.163  | -2.122                        | 0.288                           |
| 3    | 0.288              | 0.955                | 1      | 0.047  | -1.649                        | -0.121                          |
| 4    | -0.121             | 0.218                | 2      | 0.039  | -0.018                        | -0.125                          |
| 5    | -0.125             | 0.210                | 2      | 0.064  | -0.030                        | -0.133                          |
| ...  |                    |                      |        |        |                               |                                 |

: Example of a Q-learning algorithm for a teacher on the Zearn platform. Each week ($t$), the teacher decides between action 1 and action 2 based on the difference in Q-values ($Q_t$) for each action. The teacher's choice ($a$) is determined by the policy ($\text{Pr}_t(a = 1)=1/(1+e^{-\tau Q_t})$). After observing the reward ($R_t$) associated with the chosen action, the teacher computes the prediction error ($\delta_t$) using the discount factor ($\gamma$) and the cost ($c$) of the action. The Q-value is then updated using the learning rate ($\alpha$) and the prediction error. As the teacher learns from experience, the Q-value converges toward the value that yields the highest reward. The values used here were drawn from an actual Zearn account ($\alpha = 0.25$, $\tau = 10.57$, $\gamma = 0.46$, $\text{cost} = 1.38$). {#tbl-qvalue-example}

This model frames decision-making as a result of accumulated experience and the anticipation of future rewards. In other words, Q-learning involves the iterative refinement of Q-value functions, which map an agent's actions to evolving expectations of future rewards (analogous to subjective value or utility). This methodological approach is closely related to the classic "multi-armed bandit" problem, wherein the agent faces a finite set of choices (e.g., slot machines), each linked to a specific reward schedule, and aims to learn the action that yields the highest returns. Learning in this model depends on adjusting expectations to reduce the impact of prediction errors (the "surprise level," or the difference between expected and realized outcomes), using the Bellman equation to update Q-values iteratively [@rummery].

In this study, we opt for a state-independent version of Q-learning. That is, the Q-values do not vary with a contextual variable but are learned for each action only. This assumption is useful in scenarios where the state exerts minimal influence on the outcome of the action or when the state is difficult to define or observe [@sutton2018]. As such, the Q-function represents the expected return or future reward for taking action $a \in A = \{a_1,a_2,...\}$ following a certain policy $\pi = \Pr(a)$. The updating rule uses the Bellman equation as follows:

$$
Q_{t}(a) = Q_{t-1}(a) + \alpha \delta_t
$$ {#eq-q-learn}

where $\alpha$ is the learning rate, which determines how much the Q-value is updated based on $\delta$, the reward prediction error. The reward prediction error is the difference between the estimated Q-value and the observed reward. This error is used to update the Q-value of the chosen action in the direction of the observed reward, scaled by the learning rate $\alpha$, as follows:

$$
\delta_t = \gamma R_t - \text{cost}(a) - Q_{t-1}(a)
$$ {#eq-RPE}

where:

-   $a$ is the chosen action,

-   $R_t$ is the immediate reward received after taking action $a$,

-   $\text{cost}(a)$ is the perceived effort or inconvenience associated with action $a$,

-   $\gamma$ is the discount factor[^1],

-   $Q_{t-1}(a)$ is the estimate of the Q-value for action $a$ in the previous period.

[^1]: Commonly, this parameter captures the degree to which future rewards are discounted compared to immediate rewards. In this example, it could also act as a scaling factor of net reward.

In other words, $\alpha$ is the extent to which the newly acquired information will override the old information. A value of 0 means the agent does not learn anything. The agent starts with an initial Q-value (which can be arbitrary) and then updates the Q-values based on the experiences it gathers from interactions with the environment. The update rule is applied every time the agent takes action $a$ and receives a reward $R$. The agent selects actions based on a policy function of the Q-values. A common choice is the softmax action selection method, which chooses actions probabilistically based on their Q-values, as follows:

$$
\text{Pr}_t(a) = \frac{e^{\tau Q_t(a)}}{\sum_{a'} e^{\tau Q_t(a')}}
$$ {#eq-softmax}

where:

-   $\Pr_t(a)$ is the probability of choosing action $a$ at time $t$,

-   $Q_t(a)$ is the Q-value of action $a$ at time $t$,

-   $\tau$ is a parameter known as the inverse temperature, or the degree of randomness in the choice behavior,[^2]

-   The denominator is the sum over all possible actions $a' \in A$ of the exponential of their Q-values multiplied by the inverse temperature, and it functions as a normalizing value.

[^2]: One possible interpretation of the inverse temperature parameter $\tau$ is the agent's confidence in its Q-values, which controls the trade-off between exploration and exploitation. When $\tau$ is high, the agent explores more because the action probabilities are more uniform. When $\tau$ is low, the agent exploits more because the action with the highest Q-value is more likely to be chosen than the others. Some models may also allow for agents to start with a high inverse temperature to encourage exploration and then gradually decrease it to favor the exploitation of the learned policy.

### Binary Actions

For cases in which actions are binary (i.e., only two options $a_1$ and $a_2$), we set one of the actions as an outside option with a Q-value and cost of zero (i.e., base value $Q(a_2)=0$ and $\text{cost}(a_2)=0$). In this case, we only update the Q-value of the other variable ($Q(a_1)$), using the same Bellman equation but a modified prediction error:

$$
\delta_t =
\begin{cases}
\gamma R_t - \text{cost}(a_1) - Q_{t-1}(a) & \text{if } a_1 \text{ is chosen} \\
-\gamma R_t & \text{if } a_2 \text{ is chosen}
\end{cases}
$$ {#eq-state-free}

where:

-   $Q_{t}(a)=Q_{t}(a_1)=Q_{t}(a_1)-Q_{t}(a_2)$ is the estimate of the difference in Q-values for the two actions,

-   $\alpha$ is the learning rate,

-   $R_t$ is the immediate reward received after taking the action.

Thus, the probability of choosing a particular action is determined by the logistic function as follows:

$$
\text{Pr}_t(a_1) = \frac{1}{1+e^{-\tau Q_{t}(a)}}
\\
\text{Pr}_t(a_2) = 1 - \text{Pr}_t(a_1)
$$

## The Actor-Critic Model

In the previous example, we considered a state-independent model in which the teacher's actions were based solely on their Q-values. However, in many real-world scenarios, the best action may depend on the current state of the environment. For example, if the teacher uses the Zearn platform, the effectiveness of assigning homework or reviewing material may vary depending on the students' current level of understanding. To model this type of behavior, we use the Actor-Critic model, which divides the decision-making process into two components: the "actor" and the "critic" [@sutton2018b].

Consider a teacher who, each week, observes how much the students understand and chooses an action based on this observation. For example, in week 4, the teacher sees that students struggled (e.g., more "tower alerts") with the week 3 material and believes that reviewing in class (action 2) helps students progress more than assigning extra homework (action 1). So, they review in class and observe the outcome. After taking this action, the teacher checks the students' performance and updates the action selection rule (the actor) and the state value estimate (the critic) based on the observed outcome.

Over time, as the teacher continues to interact with the environment and receive feedback, the actor learns to select actions that lead to higher-than-expected returns, while the critic provides increasingly accurate estimates of state values.

| Week | State               | Value         | Policy                               | Action | Reward | TD Error   | Updated Policy Weights      | Updated Value Weights       |
|------|---------------------|---------------|--------------------------------------|--------|--------|------------|-----------------------------|-----------------------------|
| $t$  | $S_t$               | $V_t=w_t S_t$ | $\pi_t=1/(1+e^{-\tau \theta_t S_t})$ | $a$    | $R_t$  | $\delta_t$ | $\theta_{t+1}$              | $w_{t+1}$                   |
| 1    | \[1, 0, 0\]         | 0.7106        | 0.3582                               | 1      | 0.0000 | -0.2968    | \[-1.1045,0.0725, 0.6201\]  | \[0.4761, 1.1903, 0.3878\]  |
| 2    | \[1, 0.066, 0.091\] | 0.5901        | 0.3555                               | 2      | 0.0116 | -0.5393    | \[-1.1447, 0.0669, 0.6205\] | \[0.0500, 1.1621, 0.3490\]  |
| 3    | \[1, 0.080, 0.089\] | 0.1738        | 0.3502                               | 2      | 0.1354 | -0.4077    | \[-1.1530, 0.0650, 0.6207\] | \[-0.2722, 1.1364, 0.3202\] |
| 4    | \[1, 0.041, 0.090\] | -0.1969       | 0.3489                               | 2      | 0.1040 | -0.1231    | \[-1.1536, 0.0648, 0.6207\] | \[-0.3694, 1.1325, 0.3114\] |
| 5    | \[1, 0.129, 0.090\] | -0.1957       | 0.3495                               | 1      | 0.0953 | -0.2499    | \[-1.1540, 0.0646, 0.6207\] | \[-0.5669, 1.1071, 0.2935\] |
| ...  |                     |               |                                      |        |        |            |                             |                             |

: Example of an Actor-Critic model for a teacher on the Zearn platform. Each week (\$t\$), the teacher observes the state (\$S_t\$), which is a vector representing the students' struggles. The critic estimates the value (\$V_t\$) of the current state using the value weights (\$w_t\$). The actor selects an action (\$a\$) based on the policy (\$\\pi_t\$), which is determined by the policy weights (\$\\theta_t\$) and the state. After observing the reward (\$R_t\$) associated with the chosen action, the teacher computes the temporal difference (TD) error (\$\\delta_t\$). The policy weights and value weights are then updated using the TD error. As the teacher learns from experience, the actor learns to select actions that lead to higher-than-expected returns, while the critic provides increasingly accurate estimates of state values. The values used here were drawn from an actual Zearn account ($\alpha = 0.25$, $\tau = 10.57$, $\gamma = 0.46$, $\text{cost} = 1.38$). {#tbl-ac-example}

More formally, the actor selects action $a \in A$ based on a policy function, denoted as $\pi_t(a|S_t)$, which maps states to actions, determining the probability of taking each action in each state.

In our setting, we set the policy as a softmax function:

$$
\pi_t(a|S_t,\theta^a_t) = \frac{e^{\tau \theta^a_t S_t}}{\sum_{a'} e^{\tau \theta^{a'}_t S_t}}
$$

where $S_{t}$ is a vector that characterizes the current state, with the first element, $1$, allowing for a bias term, followed by the remaining state variables. $\theta^a_t S_t$ is a linear combination of the state variables as determined by the policy weights $\theta^a_t$ for action $a$ at time $t$.

The "critic," on the other hand, evaluates the actions taken by the actor by learning a value function, denoted as $V_t$. Given the actor's current policy, the critic estimates the expected reward for each state, and the critic's feedback (i.e., value function) guides the actor's learning. In our case:

$$
V_t = w_t S_t
$$

where $w_t$ is the vector of value weights at time $t$.

We update the value function based on the difference between the estimated and actual return, analogous to the Temporal Difference (TD) error:

$$
\delta_t = (R_{t} - \text{cost}(a)) + \gamma V_t - V_{t-1}
$$

where $a$ is the chosen action at $t-1$.

The agent aims to learn a policy that maximizes the expected reward by taking the gradient ascent with the partial derivatives of the value and policy functions with respect to the parameters $w_t$ and $\theta^a_t$. The update rules for the critic and actor are as follows:

$$
w_{t+1} = w_{t} + \alpha^w \delta_t \nabla V_t
$$

$$
\theta_{t+1} = \theta_{t} + \alpha^{\theta} \gamma^{t-1} \delta_t \nabla \ln \pi_t
$$

where $\alpha^w$ and $\alpha^{\theta}$ are the learning rates for the critic and actor, respectively.

### Binary Actions

For cases in which actions are binary (i.e., only two options $a_1$ and $a_2$), we set one of the actions as an outside option with $\pi_t(a_2) = 1 - \pi_t(a_1|S_t,\theta^{a_1}_t)$. In this case, we have the policy function:

$$
\pi_t(a_1|S_t,\theta_t) = \frac{1}{1 + e^{-\tau \theta_t S_t}}
$$

where we call $\theta_t = \theta^{a_1}_t - \theta^{a_2}_t$ for simplicity. Therefore, we update the value and policy weights as follows:

$$
\delta_t =
\begin{cases}
R_{t} - \text{cost}(a_1) + \gamma V_t - V_{t-1} & \text{if } a_1 \text{ is chosen} \\
R_{t} + \gamma V_t - V_{t-1} & \text{if } a_2 \text{ is chosen}
\end{cases}
$$

$$
w_{t+1} = w_{t} + \alpha^w \delta_t S_t
$$

$$
\theta_{t+1} = \theta_{t} + \alpha^{\theta} \gamma^{t-1} \delta_t \frac{\tau S_t}{1 + e^{\tau \theta_t S_t}}
$$

## Why Reinforcement Learning?

Reinforcement Learning (RL) presents a few advantages over models that employ a static approach to link teacher efforts with student outcomes. It embodies the flexibility to adapt and evolve strategies over time. This dynamic framework reflects the continuous learning process seen in biological systems and naturally aligns with the evolving nature of teacher-student interactions in the classroom. Beyond our immediate study goals, RL models hold the potential for automating instructional decisions based on identified patterns, potentially alleviating the workload on teachers and optimizing the educational process.

In this study, we position teachers as agents who navigate their environment (i.e., the classroom) by taking actions based on their observations and the feedback they receive. RL algorithms can characterize individual profiles for teachers, providing insights into how they adapt and respond to various states and rewards within the educational setting. By estimating individual teacher parameters, RL provides insights into valuable aspects for policymakers to design targeted interventions aimed at enhancing educational outcomes.

Further, the flexibility of RL makes it an ideal tool to model how teachers address changing classroom needs. By incorporating a wide range of variables (i.e., states, actions, and rewards), RL models are customizable to diverse educational contexts and objectives. Given this flexibility in mathematically mapping the agent-environment interaction (i.e., many models potentially satisfy our initial assumptions), our first step is a competition of models, selecting a set of models applicable to our setting, fitting them to the data, and comparing their performances.

# Results

## Selecting a model specification

To analyze Zearn data spanning an entire academic year, we first establish a framework for actions, rewards, and, optionally, state variables for state-based RL models. In this framework, teacher activities drive the educational process, student achievements result from these efforts, and the constantly changing educational environment represents the states. Instead of relying solely on one analytical approach, our strategy involves a large set of candidate models, progressively escalating in complexity, as shown in @tbl-methods. Our overarching goal was to strengthen the reliability of our findings and offer a detailed understanding of the underlying behavioral patterns.

```{r data prep}

dt <- read.csv(file = "Data/df_clean.csv")

# Convert columns to appropriate data types
setDT(dt)
# Rename variable
dt[, `:=`(
  poverty = factor(poverty, ordered = TRUE, exclude = c("", NA)),
  income = factor(income, ordered = TRUE, exclude = c("", NA)),
  st_login = fifelse(Minutes.per.Active.User > 0, 1, 0, na=0),
  tch_login = fifelse(User.Session > 0, 1, 0, na=0)
  # Log Transform
  # Badges.per.Active.User = log(Badges.per.Active.User + 1),
  # Tower.Alerts.per.Tower.Completion = log(Tower.Alerts.per.Tower.Completion + 1),
  # tch_min = log(tch_min + 1)
)]

# Code datetime variables and compute additional metrics
setorder(dt, Teacher.User.ID, year, week, Classroom.ID)
dt[, isoweek := week]
dt[, week := week + 52*(year - 2019)]
# Fixing week == 1 (last week of 2019 counts as week 1 of 2020)
dt[week == 1, week := week + 52]
dt[, first_week := min(week), by = .(Teacher.User.ID)]
dt[, week := week - first_week + 1]
dt[, Tsubj := max(week), by = .(Classroom.ID)]

# Update the Grade.Level values and labels
dt <- dt[!(Grade.Level %in% c(-1, 11))] # Ignore -1 and 11
dt[, Grade.Level := factor(Grade.Level,
                           ordered = TRUE,
                           exclude = c(""))]
dt[, Grade.Level := factor(Grade.Level,
                           levels = c(0:8),
                           labels = c("Kindergarten", "1st", "2nd",
                                      "3rd", "4th", "5th",
                                      "6th", "7th", "8th"))]

df <- as.data.frame(dt) %>%
  ungroup()

# Convert year and isoweek to a date (Monday of that week)
df <- df %>%
  mutate(date = as.Date(paste(year, isoweek, 1, sep="-"), format="%Y-%U-%u"))

```

```{r table-summary-code}

df_summary <- df %>%
  group_by(MDR.School.ID) %>%
  summarise(
    Unique_Teacher_Count = n_distinct(Teacher.User.ID)
  ) %>%
  na.omit() %>%
  left_join(df %>%
              group_by(Classroom.ID, MDR.School.ID) %>%
              summarise(
                Students_Total = mean(Students...Total, na.rm = TRUE),
                Weeks_Total = max(week)
                ) %>%
              group_by(MDR.School.ID) %>%
              summarize(
                Students_Total = sum(Students_Total, na.rm = TRUE),
                Weeks_Total = mean(Weeks_Total)
                ),
            by = "MDR.School.ID") %>%
  ungroup()

# Summary statistics table
gt_school_sum <- df_summary %>%
  summarise(
    Mean_Teachers = mean(Unique_Teacher_Count),
    Median_Teachers = median(Unique_Teacher_Count),
    SD_Teachers = sd(Unique_Teacher_Count),
    Min_Teachers = min(Unique_Teacher_Count),
    Max_Teachers = max(Unique_Teacher_Count),
    Mean_Students_Total = mean(Students_Total),
    Median_Students_Total = median(Students_Total),
    SD_Students_Total = sd(Students_Total, na.rm = TRUE),
    Min_Students_Total = min(Students_Total),
    Max_Students_Total = max(Students_Total),
    Mean_Weeks_Total = mean(Weeks_Total),
    Median_Weeks_Total = median(Weeks_Total),
    SD_Weeks_Total = sd(Weeks_Total, na.rm = TRUE),
    Min_Weeks_Total = min(Weeks_Total),
    Max_Weeks_Total = max(Weeks_Total)
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Measure", "Variable"),
           sep = "_", extra = "merge") %>%
  pivot_wider(names_from = Measure, values_from = Value) %>%
  mutate(Variable = gsub("_Total", "", Variable)) %>%
  gt(rowname_col = "Variable") %>%
  # cols_label(Mean = "Mean", SD = "Standard Deviation",
  #            Min = "Minimum", Max = "Maximum") %>%
  fmt_number(
    columns = c("Mean", "SD"),
    decimals = 2
  ) %>%
  fmt_number(
    columns = c("Median", "Min","Max"),
    decimals = 0
  )

# Pre-exclusion variables
N_class_pre = length(unique(df$Classroom.ID))
N_teachers_pre = length(unique(df$Teacher.User.ID))
avg_std_pre = round(mean(df$Students...Total, na.rm = T), 1)

```

```{r draw-classroom-weeks}

# Create the histogram
fig_classroom_weeks <- df %>%
  group_by(Classroom.ID) %>%
  summarize(Tsubj = n()) %>%
  mutate(Tsubj_category = if_else(Tsubj < 18, "less than 18", "18 or more")) %>%
  ggplot(aes(x = Tsubj, fill = Tsubj_category)) +
  geom_histogram(color = "black", breaks = seq(min(df$Tsubj, na.rm = T),
                                               max(df$Tsubj, na.rm = T) + 1,
                                               by = 2)) +
  geom_vline(xintercept = 17, color = "darkgray",
             linetype = "dashed", linewidth = 0.8) +
  annotate("text", x = 10, y = 4000, label = "Excluded\nClassrooms",
           vjust = 1, color = "red") +
  labs(x = "Total Number of Weeks",
       y = "Frequency (Classrooms)") +
  scale_fill_manual(values = c("less than 18" = "red",
                               "18 or more" = "steelblue")) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = c(1, seq(5, max(df$Tsubj, na.rm = T), by = 5)))

```

```{r draw-logins-week}

# Calculate the sum of login values by date and Teacher.User.ID
login_data <- df %>%
  group_by(date, Teacher.User.ID) %>%
  summarize(tch_login = max(tch_login),
            st_login  = max(st_login)) %>%
  group_by(date) %>%
  summarize(tch_logins = sum(tch_login),
            st_logins  = sum(st_login))
# Create bar plot
bar_plot <- ggplot() +
  geom_bar(data = login_data, aes(x = date, y = st_logins), stat = "identity") +
  # geom_point(data = login_data, aes(x = date, y = tch_logins), color = "blue") +
  labs(
    x = "Week",
    y = "Total Logins"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12)) +
  scale_x_date(date_breaks = "3 week", date_labels = "%Y-%m-%d")

# Add labels for Christmas and Thanksgiving
logins_week <- bar_plot +
  geom_text(aes(x = as.Date("2019-12-25"), y = 1250, label = "Christmas"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "red") +
  geom_text(aes(x = as.Date("2019-11-25"), y = 1250, label = "Thanksgiving"),
            size = 4, angle = 90, hjust = 0.5, vjust = 0.5,
            color = "darkorange")

```

```{r draw-map}
#| cache: true

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
# Set the library path to where proj.db is located
# Sys.setenv(PROJ_LIB = "")
unique_zipcodes <- unique(dt$zipcode) %>%
  na.omit() %>%
  as.list()
# plan(strategy = "multisession", workers = availableCores())
plan(strategy = "multisession", workers = 1)
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes,
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
dt <- merge(dt, address_geodata,
            by.x = "zipcode",
            by.y = "postalcode",
            all.x = TRUE)
dt <- dt[!is.na(zipcode)]

# Aggregate the data to get the number of teachers in each county
dt_map <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID)
), by = .(county)]

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- dt[, .(
  num_teachers = n_distinct(Teacher.User.ID),
  lat = mean(lat),
  long = mean(long)
), by = .(city, county)]
# Get the top 5 cities with the most teachers
top_cities <- top_cities[order(-num_teachers)][1:5,]

# Get the Louisiana county map data
df_map <- tigris::counties(cb = TRUE,
                           resolution = "20m",
                           class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    as.data.frame(dt_map),
    by = c("NAMELSAD" = "county")
  ) %>%
  sf::st_as_sf() %>%
  mutate(num_teachers = ifelse(is.na(num_teachers), 0, num_teachers))

map_LA <- ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers", low = "white", high = "red", na.value = "gray90") +
  theme_minimal() +
  theme(
    legend.position = "right",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city), size = 3, color = "black")
```

```{r draw-income-dist}

df_proportions <- df %>%
  filter(!is.na(poverty)) %>%
  group_by(poverty) %>%
  summarise(n = n()) %>%
  mutate(Percentage = paste0(
    round(n / sum(n) * 100, digits = 2), "%"
    )) %>%
  select(-"n") %>%
  na.omit() %>%
  rename(Variable = poverty) %>%
  mutate(Variable = as.character(Variable)) %>%
  bind_rows(df %>%
              filter(!is.na(income)) %>%
              group_by(income) %>%
              summarise(n = n()) %>%
              mutate(Percentage = paste0(
                round(n / sum(n) * 100, digits = 2), "%"
                )) %>%
              select(-"n") %>%
              na.omit() %>%
              rename(Variable = income) %>%
              mutate(Variable = as.character(Variable))
            ) %>%
  bind_rows(df %>%
              ungroup() %>%
              summarise(
                Charter_Schools = mean(charter.school, na.rm = T)*100,
                Schools_with_Paid_Account = mean(school.account, na.rm = T)*100
                ) %>%
              mutate(
                Charter_Schools = paste0(round(Charter_Schools, digits = 2), "%"),
                Schools_with_Paid_Account = paste0(round(Schools_with_Paid_Account,
                                                         digits = 2), "%")) %>%
              t() %>% as.data.frame() %>%
              rename(Percentage = V1) %>%
              mutate(Variable = row.names(.))) %>%
  add_row(Variable = "Poverty Level", Percentage = "", .before = 1) %>%
  add_row(Variable = "Income", Percentage = "", .before = 5) %>%
  add_row(Variable = "Other", Percentage = "", .before = 23)

# Splitting df_proportions into different categories for pie charts
df_poverty <- df_proportions[2:4,]
df_income <- df_proportions[6:22,]

# Convert Percentage to numeric
df_poverty$Percentage <- as.numeric(gsub("%", "", df_poverty$Percentage))/100

# Create Bar Graph for Poverty Level
poverty_plot <- ggplot(df_poverty, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Poverty Level",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Convert Percentage to numeric
df_income$Percentage <- as.numeric(gsub("%", "", df_income$Percentage))/100

# Create Bar Graph for Income Distribution
income_plot <- ggplot(df_income, aes(x = Variable, y = Percentage)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "Income Range",
       y = "Percentage") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r preprocess data}
#| include: false

dt <- setDT(df)
dt[, `:=`(
  n_weeks = .N,
  mean_act_st = mean(Active.Users...Total, na.rm = TRUE)
  ), by = Classroom.ID]

dt <- dt[
  n_weeks > 18 & # At least 4.5 months cumulative activity
    Tsubj < 2*n_weeks & # At least activity twice a month on average
    teacher_number_classes < 5 &
    Students...Total > 5 &
    mean_act_st > 3 &
    !(Grade.Level %in% c("6th","7th","8th")) &
    !(month(date) %in% c(6, 7, 8)) &
    !is.na(District.Rollup.ID),
]

df <- as.data.frame(dt) %>%
  select(-X) %>%
  mutate(Minutes.per.Active.User = as.numeric(Minutes.per.Active.User)) %>%
  mutate(across(Active.Users...Total:Tower.Alerts.per.Tower.Completion,
         ~ ifelse(is.na(.), 0, .))) %>%
  arrange(Classroom.ID, week)

```

```{r table-classroom-summary-code}

create_summary <- function(var_name, var_label, type = "continuous") {
  summary_stat <- df %>%
    select(Grade.Level, var_name) %>%
    tbl_summary(
      by = Grade.Level,
      missing = "no",
      type = list(var_name ~ type),
      statistic = var_name ~ ifelse(type == "continuous",
                                    "{mean} ({sd})",
                                    "{n} ({p})")
    ) %>%
    add_overall() %>%
    as_tibble()
  summary_stat[1] <- var_label
  return(summary_stat)
}

summaries_list <- list(
  # create_summary("Sessions.per.Active.User", "Sessions per Student"),
  create_summary("Minutes.per.Active.User", "Minutes per Student"),
  create_summary("Badges.per.Active.User", "Badges per Student"),
  create_summary("Tower.Alerts.per.Tower.Completion",
                 "Tower Alerts per Lesson Completion"),
  # create_summary("tch_login", "Teacher Login (0/1)", type = "dichotomous"),
  create_summary("Minutes.on.Zearn...Total", "Minutes per Teacher")
)

school_summary <- bind_rows(summaries_list) %>%
  select(-c(9:11)) %>% t() %>% as.data.frame()
colnames(school_summary) <- school_summary[1,]
# Remove the first row and set row names
school_summary <- school_summary[-1, ]
school_summary$`Grade Level` <- rownames(school_summary)

# Create a gt table
gt_classroom_sum <- school_summary %>%
  mutate(`Grade Level` = gsub("\\*\\*", "", `Grade Level`)) %>%
  gt(rowname_col = "Grade Level") %>%
  cols_label(
    `Minutes per Student` = "Minutes",
    `Badges per Student` = "Badges",
    `Tower Alerts per Lesson Completion` = "Tower Alerts",
    `Minutes per Teacher` = "Teacher Minutes"
  )


```

## Dimensionality Reduction

We first conducted a dimensionality reduction with Non-negative Matrix Factorization (NMF) and four components (see @fig-nmf-pca-comparison for a comparison of different methods by balancing reconstruction accuracy, i.e., R-squared, with clustering clarity, i.e., Silhouette Scores). While our dataset offers many potential action and reward variables, the direct use of these variables presents significant challenges:

1.  **Complexity**: The sheer number of available variables complicates the identification of meaningful patterns and relationships.
2.  **Dimensionality**: The high-dimensional nature of the data risks diluting important signals due to the "curse of dimensionality."[^3]
3.  **Interpretability**: Directly interpreting the impact of specific actions or behaviors on outcomes can be obscured by the intertwined nature of the data.

[^3]: Richard Bellman coined this phrase to describe the challenge of optimizing a control process by searching over a discrete multidimensional grid, where the number of grid points increases exponentially with the number of dimensions. He wrote: “In view of all that we have said in the foregoing sections, the many obstacles we appear to have surmounted, what casts the pall over our victory celebration? It is the curse of dimensionality, a malediction that has plagued the scientist from the earliest days” [@bellman2015adaptive].

By reducing the data to a manageable number of components, we can more readily identify underlying patterns of behavior and interaction. To achieve this, we applied Nonnegative Matrix Factorization (NMF) and used the results to define action, reward, and state variables rather than using individual metrics. One desirable feature of NMF is that it produces sparse components, providing a distilled representation of the data, where each one reflects a combination of behaviors or activities with a potential thematic linkage. Given that our chosen RL models require discreet action variables, we choose to split teacher actions into binary variables, following a median split. In our case, a median split is equivalent to giving a value of 1 to any positive value.

```{r pca nmf data-prep}
#| include: false

df <- df %>%
  filter(!is.na(Minutes.on.Zearn...Total)) %>%
  group_by(Classroom.ID) %>%
  # train/test split
  mutate(set = sample(c("train", "test"), size = n(),
                      prob = c(0.8, 0.2), replace = TRUE)) %>%
  ungroup() %>% arrange(Classroom.ID, week)

columns <- names(
  df %>% select(RD.elementary_schedule:Minutes.on.Zearn...Total,
                Active.Users...Total:Tower.Alerts.per.Tower.Completion)
  )

# Create base data.table for models (faster than data.frame)
df_pca <- as.data.table(df)
# Convert columns to double to prevent precision loss
df_pca[, (columns) := lapply(.SD, as.numeric),
        .SDcols = columns]

# Apply the scaling operation
df_pca[, (columns) := lapply(.SD, function(x) {
  x[is.na(x)] <- 0
  sd_x <- sd(x, na.rm = TRUE)
  if(sd_x == 0 | is.na(sd_x)) return(rep(0, .N))
  x / sd_x
}), by = MDR.School.ID, .SDcols = columns]
setorder(df_pca, Classroom.ID, week)

# Calculate standard deviations
std_devs <- apply(df_pca %>% select(all_of(columns)), 2, sd, na.rm = T)
# Identify columns with defined standard deviations (not NaN or Inf)
invalid_cols <- names(std_devs[is.na(std_devs) |
                                 is.infinite(std_devs) |
                                 std_devs == 0])
df_pca <- df_pca %>% select(-all_of(invalid_cols))

```

```{python load data}
#| include: false
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA, NMF
from sklearn.metrics import silhouette_score

## Basic Variables
# Import data from R
dfpca_py = pd.DataFrame(r.df_pca)
dfpca_py.sort_values(['Classroom.ID', 'week'], inplace=True)

# Split the data into teacher and student subsets
teacher_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "RD.elementary_schedule"
    ):dfpca_py.columns.get_loc(
      "RD.grade_level_teacher_materials"
      # "Minutes.on.Zearn...Total"
      )+1
  ]
student_variables = dfpca_py.columns[
  dfpca_py.columns.get_loc(
    "Active.Users...Total"
    ):dfpca_py.columns.get_loc(
      "Tower.Alerts.per.Tower.Completion"
      )+1
  ]

X_teachers = dfpca_py[teacher_variables]
X_students = dfpca_py[student_variables]

# Dictionaries for export
components = {}
results = {}
residuals = {}
silhouette = {}

```

```{python pca-nmf}
#| cache: true
#| include: false

# Function for NMF
def nmf_method(n, method, initial, X_scaled, data_label, solv = 'mu', nomin = False):
  method_name = f"{method.title()} {initial.upper()}"

  if method == 'frobenius' and initial == 'nndsvd': solv = 'cd'
  if method != 'frobenius' and initial == 'nndsvd': return
  if method != 'frobenius': method_name = f"{method.title()}"
  if nomin: method_name = method_name + "_nomin"

  nmf = NMF(
    n_components=n,
    init=initial,
    beta_loss=method,
    solver=solv,
    max_iter=4_000
  )
  X_nmf, nmf_comp = nmf.fit_transform(X_scaled), nmf.components_
  X_hat = nmf.inverse_transform(X_nmf)
  labels = np.argmax(nmf_comp, axis=0)

  results.setdefault(f"{method_name}_{data_label}", {})[n] = X_nmf
  components.setdefault(f"{method_name}_{data_label}", {})[n] = nmf_comp
  residuals.setdefault(f"{method_name}_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
  silhouette.setdefault(f"{method_name}_{data_label}", {})[n] = silhouette_score(nmf_comp.transpose(), labels)

def perform_pca_nmf(X_scaled, data_label, n_comp):
  for n in range(2, n_comp):
    ## PCA
    pca = PCA(n_components=n)
    X_pca = pca.fit_transform(X_scaled)
    pca_comp = pca.components_
    X_hat = pca.inverse_transform(X_pca)
    labels = np.argmax(pca_comp, axis=0)
    results.setdefault(f"PCA_{data_label}", {})[n] = X_pca
    components.setdefault(f"PCA_{data_label}", {})[n] = pca_comp
    residuals.setdefault(f"PCA_{data_label}", {})[n] = ((X_scaled - X_hat)**2).sum().sum()
    silhouette.setdefault(f"PCA_{data_label}", {})[n] = silhouette_score(pca_comp.transpose(), labels)

    ## Non-negative Matrix Factorization
    for method in {'frobenius', 'kullback-leibler'}:
      for initial in {'nndsvd', 'nndsvda'}:
        nmf_method(
          n, method, initial, X_scaled,
          data_label=data_label,
          nomin = True    # No teacher minutes included
          )

# Run PCA and NMF for teachers
perform_pca_nmf(X_teachers, "teachers", min(X_teachers.shape) // 3)
# Run PCA and NMF for students
perform_pca_nmf(X_students, "students", min(X_students.shape))

```

```{python clean environment}

# get the names of all variables in the global namespace
all_vars = list(globals().keys())

# list of variables to keep
keep_vars = ['residuals', 'silhouette', 'components', 'results', 'r']

# delete all variables that are not in keep_vars
for var in all_vars:
    if var not in keep_vars:
        del globals()[var]
del keep_vars
del all_vars
del var

# manually call the garbage collector
import gc
gc.collect()

```

```{r nmf-pca-comparison-code}
#| cache: true

# Importing from Python
residuals_list <- py$residuals
silhouette_list <- py$silhouette

teacher_variables <- names(
  df_pca[,RD.elementary_schedule:RD.grade_level_teacher_materials]
)
student_variables <- names(
  df_pca[,Active.Users...Total:Tower.Alerts.per.Tower.Completion]
)
TSS_teacher <- df_pca %>%
  select(all_of(teacher_variables)) %>%
  mutate(across(all_of(teacher_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(teacher_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()
TSS_student <- df_pca %>%
  select(all_of(student_variables)) %>%
  mutate(across(all_of(student_variables), ~ (. - mean(., na.rm = TRUE))^2)) %>%
  summarize(across(all_of(student_variables), sum, na.rm = TRUE)) %>%
  unlist() %>% sum()

# Creating dataframes
df_residuals <- do.call(rbind, lapply(names(residuals_list), function(method) {
  do.call(rbind, lapply(names(residuals_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Residuals = residuals_list[[method]][[n]]
    )
  }))
}))
df_residuals_teachers <- df_residuals[
  grepl("_teachers", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_teacher)
df_residuals_students <- df_residuals[
  grepl("_students", df_residuals$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method)) %>%
  mutate(Rsq = 1 - Residuals/TSS_student)

df_silhouette <- do.call(rbind, lapply(names(silhouette_list), function(method) {
  do.call(rbind, lapply(names(silhouette_list[[method]]), function(n) {
    data.frame(
      Method = method,
      Components = as.integer(n),
      Silhouette = silhouette_list[[method]][[n]]
    )
  }))
}))
df_silhouette_teachers <- df_silhouette[
  grepl("_teachers", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_teachers|_nomin", "", Method))
df_silhouette_students <- df_silhouette[
  grepl("_students", df_silhouette$Method),
  ] %>%
  mutate(Method = gsub("_students|_nomin", "", Method))

# Plotting residuals
p1 <- ggplot() +
  geom_line(data = df_residuals_teachers,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R-squared",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_teachers$Components),
                                  max(df_residuals_teachers$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p2 <- ggplot(df_silhouette_teachers,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_teachers$Components),
                                  max(df_silhouette_teachers$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_teachers$Silhouette) +
                                  3*sd(df_silhouette_teachers$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting residuals
p3 <- ggplot() +
  geom_line(data = df_residuals_students,
            aes(x = Components, y = Rsq, color = Method)) +
  # geom_point(data = df_residuals_autoencoder,
  #            aes(x = Components, y = Residuals, color = Method)) +
  labs(title = "Reconstruction R-squared",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_residuals_students$Components),
                                  max(df_residuals_students$Components),
                                  by = 1)) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Plotting silhouette scores
p4 <- ggplot(df_silhouette_students,
             aes(x = Components, y = Silhouette, color = Method)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Silhouette Score",
       x = "Number of Components") +
  scale_x_continuous(breaks = seq(min(df_silhouette_students$Components),
                                  max(df_silhouette_students$Components),
                                  by = 1)) +
  coord_cartesian(ylim = c(NA, (mean(df_silhouette_students$Silhouette) +
                                  3*sd(df_silhouette_students$Silhouette)))) +
  theme_minimal() +
  theme(axis.title.y = element_blank())

# Combine the plots and place the legend at the bottom
comparison_plot <- ggarrange(p1, p2,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")
comparison_plot2 <- ggarrange(p3, p4,
                           ncol = 2,
                           common.legend = TRUE,
                           legend = "bottom")

```

#### Interpreting Components

After analyzing the NMF data, we identified four significant components for teachers and students. @fig-nmf-heatmap displays these components as heatmaps, offering insight into the underlying behavioral structures. Given the loadings, we interpret the components as follows:

##### Teachers Components

**Component 1 (Assessments)**: This component has substantial weights on supplemental assessment materials, such as "Optional Problem Sets Download," "Optional Homework Download," and "Student Notes and Exit Tickets Download," indicating a proactive approach to evaluating and supporting student learning progress. It could also reflect a proactive approach to monitoring student understanding and providing feedback.

**Component 2 (Pedagogical Knowledge)**: The high weights on "Guided Practice Completed," "Tower Completed," "Tower Stage Failed," and "Fluency Completed" suggest that this component reflects when teachers are engaged in acquiring subject-matter-specific pedagogy, learning to scaffold and explain concepts in various ways.

**Component 3 (Group Instruction)**: This component, with prominent weights on "Small Group Lesson Download," "Whole Group Word Problems Download," and "Whole Group Fluency Download," suggests a pedagogical approach focused on fostering interactive and comprehensive classroom instruction. It implies engagement in activities that promote group learning dynamics and collective problem-solving skills.

**Component 4 (Curriculum Planning)**: The dominance of "Mission Overview Download" and "Grade Level Overview Download" in this component suggests that teachers are highly involved in strategic planning and curriculum mapping. It involves organizing the curriculum content and structuring lesson plans to align with grade-level objectives and mission overviews.

##### Student Components

**Component 1 (Badges)**: This component emphasizes "On-grade Badges" and "Badges," indicating that it measures students' overall engagement and advancement through the curriculum.

**Component 2 (Struggles)**: This component, which heavily weights "Boosts" and "Tower Alerts," seems to capture the frequency of occasions when students require additional scaffolding and assistance.

**Component 3 (Number of Students)**: This component mainly consists of "Active Students," which provides insight into what proportion of students regularly log in to complete Digital Lessons.

**Component 4 (Activity)**: Dominated by "Student Minutes" and "Student Logins," this component highlights the amount of time students invest in and the frequency of their interactions with Zearn.

```{r}
#| cache: true
#| label: fig-nmf-heatmap
#| fig-cap: "Heatmap of Non-negative Matrix Factorization (NMF) components for teacher and student data. The rows represent the original variables, and the columns correspond to the components. The color gradient indicates the relative importance of each variable within a component based on the proportion of the component's total weight attributed to that variable. These proportions were calculated by normalizing each variable weight within a component so that they all sum to 1. To provide context, the heatmaps label examples of low, moderate, and high proportion values."
#| fig-subcap:
#| - "Teacher Data. Component 1 (Assessments) focuses on using supplemental materials for student evaluation; Component 2 (Pedagogical Knowledge) emphasizes developing subject-specific teaching strategies; Component 3 (Group Instruction) centers on collaborative and whole-class teaching methods; Component 4 (Curriculum Planning) highlights planning and lesson preparation."
#| - "Student Data. Component 1 (Badges) measures curriculum engagement and progression; Component 2 (Struggles) indicates the need for additional academic support; Component 3 (Number of Students) tracks student participation within the platform; Component 4 (Activity) reflects the overall time spent and frequency of platform usage."
#| layout-ncol: 1

components_list <- py$components
df_heatmap_teachers <-
  components_list[["Frobenius NNDSVD_nomin_teachers"]][["4"]] %>%
  t() %>% as.data.frame()
df_heatmap_students <-
  components_list[["Frobenius NNDSVD_nomin_students"]][["4"]] %>%
  t() %>% as.data.frame()

# Define a named vector to map old variable names to new ones
variable_names <- c(
  "Minutes.on.Zearn...Total" = "Teacher Minutes",
  "RD.optional_problem_sets" = "Optional Problem Sets Download",
  "RD.student_notes_and_exit_tickets" = "Student Notes and Exit Tickets Download",
  "RD.mission_overview" = "Mission Overview Download",
  "RD.pd_course_notes" = "Course Notes Download",
  "RD.elementary_schedule" = "Elementary Schedule Download",
  "RD.whole_group_fluency" = "Whole Group Fluency Download",
  "Guided.Practice.Completed" = "Guided Practice Completed",
  "RD.small_group_lessons" = "Small Group Lesson Download",
  "Tower.Completed" = "Tower Completed",
  "Fluency.Completed" = "Fluency Completed",
  "Number.Gym.Activity.Completed" = "Number Gym Activity Completed",
  "RD.grade_level_overview" = "Grade Level Overview Download",
  "Tower.Stage.Failed" = "Tower Stage Failed",
  "Kindergarten.Activity.Completed" = "Kindergarten Activity Completed",
  "Tower.Struggled" = "Tower Struggled",
  "RD.k_mission" = "Kindergarten Mission Download",
  "RD.whole_group_word_problems" = "Whole Group Word Problems Download",
  "RD.assessments" = "Assessments Download",
  "RD.teaching_and_learning_approach" = "Teaching and Learning Approach Download",
  "RD.optional_homework" = "Optional Homework Download",
  "RD.k_schedule" = "Kindergarten Schedule Download",
  "RD.curriculum_map" = "Curriculum Map Download",
  "RD.assessments_answer_key" = "Assessments Answer Key Download",
  "RD.pd_course_guide" = "Course Guide Download",
  "RD.grade_level_teacher_materials" = "Teacher Materials Download",
  "Active.Users...Total" = "Active Students",
  "Sessions.per.Active.User" = "Student Logins",
  "Badges.per.Active.User" = "Badges",
  "Badges..on.grade..per.Active.Student" = "On-grade Badges",
  "Minutes.per.Active.User" = "Student Minutes",
  "Tower.Alerts.per.Tower.Completion" = "Tower Alerts",
  "Boosts.per.Tower.Completion" = "Boosts"
)
# Rename the rows of the dataframe
row.names(df_heatmap_teachers) <- variable_names[teacher_variables]
row.names(df_heatmap_students) <- variable_names[student_variables]

names(df_heatmap_teachers) <- paste0("Comp ", 1:4)
names(df_heatmap_students) <- paste0("Comp ", 1:4)
df_heatmap_teachers <- df_heatmap_teachers %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)
df_heatmap_students <- df_heatmap_students %>%
  arrange(-`Comp 1`, -`Comp 2`, -`Comp 3`, -`Comp 4`)

# Normalize by column sums to show proportion of contribution from each variable
df_heatmap_teachers <- df_heatmap_teachers %>%
  mutate(across(everything(), ~ round(.x / sum(.x), 3)))
df_heatmap_students <- df_heatmap_students %>%
  mutate(across(everything(), ~ round(.x / sum(.x), 3)))

# Define color scheme and breaks
breaks <- seq(0, max(df_heatmap_students,df_heatmap_teachers),
              by = round(max(df_heatmap_students,df_heatmap_teachers)/50,2))
color_scheme <- colorRampPalette(
  c("#F7F7F7", brewer.pal(n = 11, name = "RdYlBu")[6:1])
  )(length(breaks))

# Calculate annotations for both dataframes
calculate_annotations <- function(df) {
  data <- unlist(df)
  data <- data[data > breaks[4]]
  high <- max(data)
  low <- min(data)
  median <- ifelse(any(data == median(data)), median(data),
                   data[which.min(abs(data - median(data)))])
  return(df %>%
           mutate(across(everything(),
                         ~ if_else(. == low | . == high | . == median,
                                   ., 0))) %>%
           mutate(across(everything(), ~ as.character(.))) %>%
           mutate(across(everything(), ~ if_else(. == "0", "", .))) %>%
           as.matrix())
}

pheatmap(df_heatmap_teachers,
         cluster_rows = FALSE, cluster_cols = FALSE,
         angle_col = 0,
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE, show_rownames = TRUE,
         display_numbers = calculate_annotations(df_heatmap_teachers),
         number_color = RColorBrewer::brewer.pal(n = 11, name = "RdYlBu")[11])

pheatmap(df_heatmap_students,
         cluster_rows = FALSE, cluster_cols = FALSE,
         angle_col = 0,
         color = color_scheme,  # Use the color scheme defined above
         breaks = breaks,  # Use breaks defined above
         border_color = NA,
         show_colnames = TRUE, show_rownames = TRUE,
         display_numbers = calculate_annotations(df_heatmap_students),
         number_color = RColorBrewer::brewer.pal(n = 11, name = "RdYlBu")[11],
         fontsize = 15)

```

```{r export dimension reduction}
#| eval: false
# Importing results from Python
results_list <- py$results

# List of method names
# methods <- c("PCA", "FrobeniusNNDSVD", "FrobeniusNNDSVDA", "KullbackLeibler")
methods <- c("Frobenius NNDSVD")
# Initialize df_components
df_components <- df_pca

# Loop over each method, extract the results at 3 components and bind to df_components
for (method in methods) {
  method_teacher <- paste0(method, "_nomin_teachers")
  method_student <- paste0(method, "_nomin_students")
  result_teacher <- results_list[[method_teacher]][["4"]]
  result_student <- results_list[[method_student]][["4"]]
  df_components <- df_components %>%
    bind_cols(result_teacher, result_student, .name_repair = )

  # Adjust column names
  new_cols <- paste0("Frobenius NNDSVD",
                     rep(c("_teacher", "_student"), each = 4),
                     rep(1:4, 2))
  names(df_components)[
    (ncol(df_components) -
       ncol(result_teacher) -
       ncol(result_student) + 1):ncol(df_components)
    ] <- new_cols
}

# Write to csv
write.csv(df_components, "./Data/df.csv")

```

### Feature Selection

```{r load dimension reduction}
#| include: false

minimum_weeks <- 12

df <- read.csv(file = "./Data/df.csv") %>%
  mutate(across(where(is.numeric),
                ~ ifelse(. < .Machine$double.eps, 0, .))) %>%
  arrange(Classroom.ID, week) %>%
  group_by(Classroom.ID) %>%
  # Filter out classrooms with low action rate
  filter(if_any(dplyr::starts_with("Frobenius.NNDSVD_teacher"),
                ~ sum(.>0) >= minimum_weeks/2)) %>%
  # Filter out classrooms with little data
  filter(n() >= minimum_weeks)

FR_cols <- grep("Frobenius.NNDSVD_teacher", names(df), value = TRUE)
df <- df %>%
  filter(sd(!!sym(FR_cols[1])) != 0) %>%
  filter(sd(!!sym(FR_cols[2])) != 0) %>%
  filter(sd(!!sym(FR_cols[3])) != 0) %>%
  filter(sd(!!sym(FR_cols[4])) != 0) %>%
  mutate(row_n = row_number()) %>%
  ungroup()

```

In order to pre-select the most appropriate action, reward, and state variables, we used a panel logistic regression model inspired by dynamic analysis [@lau2005]. This approach acted as a filter to capture the action-reward (or action-reward-state) configurations displaying characteristics reminiscent of reinforcement learning (RL). We applied four criteria: a) the influence of consistent rewards on the propensity of actions being repeated, b) the immediate impact of states on action selection, c) the strategic role of actions in navigating towards desirable states, and d) the identification of action auto-correlation as an indicator of incremental learning processes.

Herein, the interaction between lagged rewards and actions aimed to capture the reinforcement aspect (a), where prior rewards enhance the likelihood of repeating specific actions. Including current state variables addressed (b) and examining how present educational contexts inform action choices. The interaction between current states and lagged actions encapsulated (c) that actions are deliberately chosen to navigate towards or sustain preferable educational states. Lastly, considering lagged rewards alone, we sought to elucidate (d) the phenomenon where past successes influence future endeavors, indicative of a learning trajectory.

We first explore reinforcement learning (RL)-like characteristics within the teacher and classroom usage data. We aimed to uncover patterns indicative of RL, where actors (teachers) select actions (teaching strategies) that historically yield higher rewards (improved student outcomes) and use states (classroom contexts) as signals for action selection. Further, we sought to understand how actions contribute to achieving or maintaining desired states and the extent to which actions exhibit auto-correlation due to incremental learning processes.

In order to capture the temporal dynamics of actions influenced by lagged rewards and states, we employed panel logistic regression models across different combinations of variables and lags. We incorporate lagged variables (ranging from one to six weeks) into the models using the Dynamic Analysis approach proposed to account for temporal autocorrelation and potential delayed effects. We applied reward and state structures extracted from classroom data via non-negative matrix factorization (NMF) with the Frobenius Non-negative Double Singular Value Decomposition (NNDSVD) and actions derived similarly from teacher data. We evaluate these models using the Bayesian Information Criterion (BIC) for model complexity and fit and the Area Under the Receiver Operating Characteristic curve (AUC) for predictive accuracy.

More specifically, we select one teacher component as the action and one student component as the reward. When constructing state-based models, we incorporate an additional student component as the state variable. This choice yields 16 state-free models (4 possible actions and 4 possible rewards) and 48 state-based models (4 possible actions, 4 possible rewards, and 3 possible state variables).

## Behavioral Signatures

```{r}

remove_holiday_weeks <- function(data) {
  # Define holiday weeks (Thanksgiving and two weeks of Christmas)
  holiday_weeks <- c("2019-11-25",
                     "2019-12-23", "2019-12-30")

  # Filter out holiday weeks
  data %>%
    filter(!date %in% holiday_weeks)
}

# Function to delete months with no teacher activity
remove_inactive_periods <- function(data) {
  active_months <- data %>%
    mutate(month = floor_date(as.Date(date), "month")) %>%
    group_by(Classroom.ID, month) %>%
    summarise(monthly_activity = sum(rowSums(
      across(starts_with("Frobenius.NNDSVD_teacher"), ~ . > 0)
      )) > 0, .groups = "drop") %>%
    filter(monthly_activity)

  data %>%
    mutate(month = floor_date(as.Date(date), "month")) %>%
    semi_join(active_months, by = c("Classroom.ID", "month")) %>%
    select(-month)
}
df_cleaned <- df %>%
  mutate(date=(as.Date(date,format = "%Y-%m-%d") - 7)) %>%
  remove_inactive_periods() %>%
  remove_holiday_weeks()

calculate_ev_uncertainty <- function(data, action_col, reward_col) {
  data %>%
    arrange(Classroom.ID, week) %>%
    group_by(Classroom.ID) %>%
    mutate(
      v_0 = if_else(!!sym(action_col) == 0,
                    !!sym(reward_col), NA),
      v_1 = if_else(!!sym(action_col) > 0,
                    !!sym(reward_col), NA),
      ev_0 = cummean_ignore_na(v_0),
      ev_1 = cummean_ignore_na(v_1),
      ev = ev_1 - ev_0,  # EV is the difference of the means
      uncertainty_0 = cumsd(v_0),
      uncertainty_1 = cumsd(v_1),
      uncertainty = uncertainty_1 - uncertainty_0,
      most_uncertain = case_when(
        is.na(uncertainty_0) & is.na(uncertainty_1) ~ NA_real_,
        is.na(uncertainty_0) ~ 0,
        is.na(uncertainty_1) ~ 1,
        uncertainty_1 > uncertainty_0 ~ 1,
        TRUE ~ 0
      )
    ) %>%
    ungroup()
}

# Function to calculate cumulative mean ignoring NAs
cummean_ignore_na <- function(x) {
  cumsum(!is.na(x)) -> n
  ifelse(n == 0, NA, cumsum(replace(x, is.na(x), 0)) / n)
}

# Function to calculate cumulative standard deviation
cumsd <- function(x) {
  # Create a vector to store results, initially all NA
  result <- rep(NA_real_, length(x))
  # Identify non-NA positions
  valid_pos <- which(!is.na(x))
  if (length(valid_pos) > 1) {
    # Calculate cumsd for non-NA values
    valid_x <- x[valid_pos]
    n <- seq_along(valid_x)
    cumsd_values <-
      sqrt((cumsum(valid_x^2) - (cumsum(valid_x)^2) / n) / (n - 1))
    # Assign calculated values to their original positions
    result[valid_pos] <- cumsd_values
    # Forward fill NA positions
    for (i in seq_along(result)) {
      if (is.na(result[i]) && i > 1) {
        result[i] <- result[i-1]
      }
    }
  }
  return(result)
}

df_ev_uncertainty <- df_cleaned %>%
  select(Classroom.ID, Teacher.User.ID, MDR.School.ID, week, date,
         starts_with("Frobenius.NNDSVD_teacher"),
         starts_with("Frobenius.NNDSVD_student"))

for (i in 1:4) {
  for (j in 1:4) {
    action_col <- paste0("Frobenius.NNDSVD_teacher", i)
    reward_col <- paste0("Frobenius.NNDSVD_student", j)

    df_ev_uncertainty <- calculate_ev_uncertainty(df_ev_uncertainty,
                                                  action_col, reward_col)

    new_cols <- c(paste0("ev_", i, j),
                  paste0("sd_", i, j),
                  paste0("uncertain_", i, j))
    names(df_ev_uncertainty)[names(df_ev_uncertainty) %in% c(
      "ev", "uncertainty", "most_uncertain"
    )] <- new_cols
  }
}

```

```{r}

plot_reward_seeking <- function(data, ev_col, action_col) {
  # Calculate 5th and 95th percentiles
  lower_bound <- quantile(data[[ev_col]], 0.05, na.rm = TRUE)
  upper_bound <- quantile(data[[ev_col]], 0.95, na.rm = TRUE)

  # Filter data for geom_smooth
  smooth_data <- data %>%
    filter(!!sym(ev_col) >= lower_bound, !!sym(ev_col) <= upper_bound)

  # Create quantile-based bins
  data <- data %>%
    mutate(ev_bin = cut(!!sym(ev_col),
                        breaks = quantile(!!sym(ev_col),
                                          probs = seq(0.05, 0.95, 0.15),
                                          na.rm = TRUE),
                        labels = FALSE, include.lowest = TRUE))

  plot_data <- data %>%
    group_by(ev_bin) %>%
    summarise(
      mean_choice = mean(!!sym(action_col) > 0, na.rm = TRUE),
      se_choice = sd(!!sym(action_col) > 0, na.rm = TRUE) / sqrt(n()),
      mean_ev = mean(!!sym(ev_col), na.rm = TRUE)
    ) %>%
    filter(!is.na(ev_bin))

  # Calculate x-axis breaks and labels
  breaks <- plot_data$mean_ev
  labels <- c("5-20%", "20-35%", "35-50%", "50-65%", "65-80%", "80-95%")

  ggplot(data, aes(x = !!sym(ev_col),
                   y = as.numeric(!!sym(action_col) > 0))) +
    geom_hline(yintercept = 0.5, linetype = "dotted") +
    geom_smooth(data = smooth_data, method = "glm",
                method.args = list(family = "binomial"),
                se = FALSE, color = "gray") +
    geom_point(data = plot_data, aes(x = mean_ev,
                                     y = mean_choice), size = 1) +
    geom_errorbar(data = plot_data,
                  aes(x = mean_ev, y = mean_choice,
                      ymin = mean_choice - se_choice,
                      ymax = mean_choice + se_choice),
                  width = 0) +
    labs(x = "Expected value (percentile)", y = "% Choice",
         title = paste("Action", substr(action_col,
                                        nchar(action_col),
                                        nchar(action_col)),
                       "Reward", substr(ev_col,
                                        nchar(ev_col),
                                        nchar(ev_col)))) +
    scale_x_continuous(breaks = breaks, labels = labels) +
    coord_cartesian(ylim = c(0.2, 0.6),
                    xlim = c(min(plot_data$mean_ev),
                             max(plot_data$mean_ev))) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                       limits = c(0, 1)) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 10),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Create a list to store all plots
all_plots <- list()

# Generate all 16 plots
for (i in 1:4) {
  for (j in 1:4) {
    ev_col <- paste0("ev_", i, j)
    action_col <- paste0("Frobenius.NNDSVD_teacher", i)

    plot <- plot_reward_seeking(df_ev_uncertainty, ev_col, action_col)
    all_plots[[paste(i, j, sep = "_")]] <- plot
  }
}

```

```{r}

plot_uncertainty_aversion <- function(data, ev_col, uncertain_col, action_col) {
  # Calculate delta EV and choice_uncertain
  data <- data %>%
    mutate(delta_ev = if_else(!!sym(uncertain_col) == 1,
                              !!sym(ev_col), -!!sym(ev_col)),
           choice_uncertain = xor(!!sym(action_col) == 0,
                                  !!sym(uncertain_col) == 1))

  # Calculate 5th and 95th percentiles
  lower_bound <- quantile(data$delta_ev, 0.05, na.rm = TRUE)
  upper_bound <- quantile(data$delta_ev, 0.95, na.rm = TRUE)

  # Filter data for geom_smooth
  smooth_data <- data %>%
    filter(delta_ev >= lower_bound, delta_ev <= upper_bound)

  # Create quantile-based bins
  data <- data %>%
    mutate(ev_bin = cut(delta_ev,
                        breaks = quantile(delta_ev,
                                          probs = seq(0.05, 0.95, 0.15),
                                          na.rm = TRUE),
                        labels = FALSE, include.lowest = TRUE))

  plot_data <- data %>%
    group_by(ev_bin) %>%
    summarise(
      mean_choice_uncertain = mean(choice_uncertain, na.rm = TRUE),
      se_choice_uncertain = sd(choice_uncertain, na.rm = TRUE) / sqrt(n()),
      mean_delta_ev = mean(delta_ev, na.rm = TRUE)
    ) %>%
    filter(!is.na(ev_bin))

  # Calculate x-axis breaks and labels
  breaks <- plot_data$mean_delta_ev
  labels <- c("5-20%", "20-35%", "35-50%", "50-65%", "65-80%", "80-95%")

  ggplot(data, aes(x = delta_ev, y = as.numeric(choice_uncertain))) +
    geom_hline(yintercept = 0.5, linetype = "dotted") +
    geom_smooth(data = smooth_data, method = "glm",
                method.args = list(family = "binomial"),
                se = FALSE, color = "gray") +
    geom_point(data = plot_data,
               aes(x = mean_delta_ev, y = mean_choice_uncertain), size = 1) +
    geom_errorbar(data = plot_data,
                  aes(x = mean_delta_ev,
                      y = mean_choice_uncertain,
                      ymin = mean_choice_uncertain - se_choice_uncertain,
                      ymax = mean_choice_uncertain + se_choice_uncertain),
                  width = 0) +
    labs(x = "ΔEV (Uncertain - Certain) (percentile)",
         y = "% Choice Uncertain",
         title = paste("Uncertainty Aversion:", substr(ev_col, 4, 5))) +
    scale_x_continuous(breaks = breaks, labels = labels) +
    coord_cartesian(ylim = c(0.25, 0.75),
                    xlim = c(min(plot_data$mean_delta_ev),
                             max(plot_data$mean_delta_ev))) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                       limits = c(0, 1)) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 10),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Create a list to store all plots
all_uncertainty_plots <- list()
for (i in 1:4) {
  for (j in 1:4) {
    ev_col <- paste0("ev_", i, j)
    uncertain_col <- paste0("uncertain_", i, j)
    action_col <- paste0("Frobenius.NNDSVD_teacher", i)

    plot <- plot_uncertainty_aversion(df_ev_uncertainty, ev_col, uncertain_col, action_col)
    all_uncertainty_plots[[paste(i, j, sep = "_")]] <- plot
  }
}

# # Display the plots (you might want to arrange them in a grid)
# library(gridExtra)
# do.call(grid.arrange, c(all_uncertainty_plots, ncol = 4))
```

### Adaptation to Task Horizon

```{r}

plot_adaptation_to_task_horizon <- function(data, i, j, firstweek = 10) {
  ev_col <- paste0("ev_", i, j)
  sd_col <- paste0("sd_", i, j)
  action_col <- paste0("Frobenius.NNDSVD_teacher", i)

  # Filter for teachers who start on 2019-09-02 and adjust weeks
  filtered_data <- data %>%
    filter(week == firstweek & date == "2019-09-02") %>%
    pull(Classroom.ID) %>%
    {filter(data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - firstweek + 1,
           month = floor_date(as.Date(date), "month")) %>%
    filter(adjusted_week >= 1)

  plot_data <- filtered_data %>%
    ungroup() %>%
    arrange(Teacher.User.ID,adjusted_week) %>%
    mutate(
      higher_ev = !!sym(ev_col) > 0,
      higher_uncertainty = !!sym(sd_col) > 0,
      choice = !!sym(action_col) > 0
    ) %>%
    group_by(adjusted_week, month) %>%
    summarise(
      prop_ev = mean(choice == higher_ev, na.rm = TRUE),
      prop_u = mean(choice == higher_uncertainty, na.rm = TRUE),
      se_ev = sd(choice == higher_ev, na.rm = TRUE) / sqrt(n()),
      se_u = sd(choice == higher_uncertainty, na.rm = TRUE) / sqrt(n())
    ) %>%
    na.omit()

  month_breaks <- plot_data %>%
    group_by(month) %>%
    summarise(week = min(adjusted_week)) %>%
    mutate(month_label = format(month, "%b"))

  ggplot(plot_data, aes(x = adjusted_week)) +
    geom_hline(yintercept = 0.5, linetype = "dashed",
               size=0.25, color = "black") +
    geom_line(aes(y = prop_ev, color = "EV"), size = 0.25) +
    geom_errorbar(aes(ymin = prop_ev - se_ev,
                      ymax = prop_ev + se_ev,
                      color = "EV"),
                  width = 0, size = 0.25) +
    geom_point(aes(y = prop_ev, color = "EV", shape = "EV"),
               size = 1.5, fill = "white") +
    geom_line(aes(y = prop_u, color = "U"), size = 0.25) +
    geom_errorbar(aes(ymin = prop_u - se_u,
                      ymax = prop_u + se_u,
                      color = "U"),
                  width = 0, size = 0.25) +
    geom_point(aes(y = prop_u, color = "U", shape = "U"),
               size = 1.5, fill = "white") +
    scale_color_manual(values = c("EV" = "red", "U" = "blue"),
                       labels = c("EV" = "Expected Value",
                                  "U" = "Uncertainty")) +
    scale_shape_manual(values = c("EV" = 21, "U" = 22),
                       labels = c("EV" = "Expected Value",
                                  "U" = "Uncertainty")) +
    labs(x = "Week", y = "% Chosen",
         title = paste("Adaptation to task horizon -", i, j)) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      legend.position = "bottom",
      legend.title = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1),
      axis.ticks.x = element_line(color = "black", linewidth = 0.25),
      axis.minor.ticks.x = element_line(color = "black", linewidth = 0.25)
    ) +
    scale_y_continuous(limits = c(0.25, 0.75),
                       labels = scales::percent_format(accuracy = 1)) +
    scale_x_continuous(breaks = month_breaks$week,
                       labels = month_breaks$month_label,
                       minor_breaks = seq(min(plot_data$adjusted_week),
                                          max(plot_data$adjusted_week),
                                          by = 1)) +
    guides(color = guide_legend(override.aes = list(shape = c(21, 22),
                                                    fill = "white",
                                                    size = 2)),
           shape = "none")  # remove the duplicate shape legend
}

# Create a list to store all plots
adaptation_plots <- list()

# Generate all 16 plots
for (i in 1:4) {
  for (j in 1:4) {
    plot <- plot_adaptation_to_task_horizon(df_ev_uncertainty, i, j)
    adaptation_plots[[paste(i, j, sep = "_")]] <- plot
  }
}

```

<!-- Check code -->
```{r}

# Assuming df_ev_uncertainty is your main dataframe
plot_ev_u_over_time <- function(data, i, j) {
  ev_col <- paste0("ev_", i, j)
  sd_col <- paste0("sd_", i, j)


  # Filter for teachers who start on 2019-09-02 and adjust weeks
  plot_data <- data %>%
    filter(week == firstweek & date == "2019-09-02") %>%
    pull(Classroom.ID) %>%
    {filter(data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - firstweek + 1,
           month = floor_date(as.Date(date), "month")) %>%
    filter(adjusted_week >= 1) %>%
    group_by(week, date) %>%
    summarise(
      mean_ev = mean(!!sym(ev_col), na.rm = TRUE),
      se_ev = sd(!!sym(ev_col), na.rm = TRUE) / sqrt(n()),
      mean_u = mean(!!sym(sd_col), na.rm = TRUE),
      se_u = sd(!!sym(sd_col), na.rm = TRUE) / sqrt(n())
    ) %>%
    ungroup()

  # Create month breaks for x-axis
  month_breaks <- plot_data %>%
    mutate(month = floor_date(as.Date(date), "month")) %>%
    group_by(month) %>%
    slice(1) %>%
    ungroup()

  ggplot(plot_data, aes(x = date)) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    geom_line(aes(y = mean_ev, color = "EV"), size = 0.5) +
    geom_ribbon(aes(ymin = mean_ev - se_ev,
                    ymax = mean_ev + se_ev,
                    fill = "EV"), alpha = 0.2) +
    geom_line(aes(y = mean_u, color = "U"), size = 0.5) +
    geom_ribbon(aes(ymin = mean_u - se_u,
                    ymax = mean_u + se_u,
                    fill = "U"), alpha = 0.2) +
    scale_color_manual(values = c("EV" = "blue", "U" = "red"),
                       labels = c("EV" = "Expected Value", "U" = "Uncertainty")) +
    scale_fill_manual(values = c("EV" = "blue", "U" = "red"),
                      labels = c("EV" = "Expected Value", "U" = "Uncertainty")) +
    labs(x = "Date", y = "Value",
         title = paste("EV and U over time -", i, j),
         color = "Measure", fill = "Measure") +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      legend.position = "bottom",
      axis.text.x = element_text(angle = 45, hjust = 1),
      axis.ticks.x = element_line(color = "black")
    ) +
    scale_x_date(date_breaks = "1 month", date_labels = "%b",
                 limits = c(min(plot_data$date), max(plot_data$date)))
}

# Create a list to store all plots
ev_u_plots <- list()

# Generate plots for all combinations
for (i in 1:4) {
  for (j in 1:4) {
    plot <- plot_ev_u_over_time(df_ev_uncertainty, i, j)
    ev_u_plots[[paste(i, j, sep = "_")]] <- plot
  }
}

```


## Model Performance

Drawing on the resulting metrics, we strategically narrowed our focus to the action-reward-state configurations that most closely align with Reinforcement Learning (RL) principles. @fig-RL-exploration delineates the top models based on their high fixed-effects Area Under the Curve (AUC) and mean teacher-specific AUC, as well as their Fixed Effects Bayesian Information Criterion (BIC) and higher mean teacher-specific log-likelihood. Panels A and B of the figure show the state-free and state-dependent models, respectively. Each point on the plot represents a different model, identified by its BIC and average teacher-specific AUC. The actions "Pedagogical Knowledge" and "Group Instruction" stand out for both models. However, the choice of associated rewards and states is unclear, suggesting the need to fit RL models using all configurations featuring these two actions.

### Estimating RL Models

The next step is to identify and evaluate reinforcement learning (RL) models that best represent the teaching strategies derived from our data. The performance of each model is quantified by computing the Bayesian Information Criterion (BIC). Our computational analysis identified a distinct group of models that excelled in their low BIC scores. @tbl-top-CBM provides a breakdown of scores for different combinations of rewards and states under this action, offering insight into the comparative performance of various model setups. Notably, all top-performing models featured "Pedagogical Knowledge" as their primary action. Models with "Badges" and "Activity" as their rewards outperform others, as do state-free models, likely due to their higher simplicity.

```{r}
#| label: tbl-top-CBM
#| tbl-cap: "Best-fit models. The table presents the best-fit models, organized by their reward and state components, based on Bayesian Information Criterion (BIC) scores obtained from Laplace approximations. All models feature the 'Pedagogical Knowledge' action component. The BIC scores serve as a metric for model fit, penalizing model complexity, with lower scores indicating superior performance. Missing values in the State column denote Q-learning models, while non-empty State values indicate Actor-Critic models. Note that these estimates are not hierarchical; they represent the individual estimates of each teacher."

# Define paths and models
model_folder_path <- "CBM/zearn_results/aggr_results"
models <- c("q_model", "logit_model", "cockburn_model","baseline_model")
model_names <- c("Q-learning", "Lau & Glimcher", "Cockburn et al.","Baseline")

# Create a mapping for action and reward
action_reward_mapping <- list(
  "1" = list(action = "Assessments", reward = "Badges"),
  "2" = list(action = "Pedagogical Knowledge", reward = "Badges"),
  "3" = list(action = "Group Instruction", reward = "Badges"),
  "4" = list(action = "Curriculum Planning", reward = "Badges"),
  "5" = list(action = "Assessments", reward = "Struggles"),
  "6" = list(action = "Pedagogical Knowledge", reward = "Struggles"),
  "7" = list(action = "Group Instruction", reward = "Struggles"),
  "8" = list(action = "Curriculum Planning", reward = "Struggles"),
  "9" = list(action = "Assessments", reward = "No. Students"),
  "10" = list(action = "Pedagogical Knowledge", reward = "No. Students"),
  "11" = list(action = "Group Instruction", reward = "No. Students"),
  "12" = list(action = "Curriculum Planning", reward = "No. Students"),
  "13" = list(action = "Assessments", reward = "Activity"),
  "14" = list(action = "Pedagogical Knowledge", reward = "Activity"),
  "15" = list(action = "Group Instruction", reward = "Activity"),
  "16" = list(action = "Curriculum Planning", reward = "Activity")
)

# Create a data frame for model comparison
model_comparison_df <- data.frame(
  Action = character(),
  Reward = character()
)

# Process models
for (wrapper in 1:16) {
  wrapper_str <- sprintf("%d", wrapper)

  row_data <- data.frame(
    Action = action_reward_mapping[[wrapper_str]]$action,
    Reward = action_reward_mapping[[wrapper_str]]$reward
  )

  for (model in models) {
    file_path <- file.path(model_folder_path, sprintf("lap_aggr_%s_%d.mat", model, wrapper))
    if (file.exists(file_path)) {
      mat_data <- readMat(file_path)
      log_lik <- sum(mat_data[["cbm"]][[5]][[2]])
      row_data[[model]] <- log_lik
    } else {
      row_data[[model]] <- NA
    }
  }

  model_comparison_df <- rbind(model_comparison_df, row_data)
}

# Rename columns
colnames(model_comparison_df)[3:6] <- model_names

# Display the table
model_comparison_df %>%
  gt(groupname_col = "Action", row_group_as_column = TRUE) %>%
  tab_header(title = "Model Comparison - Log Likelihood") %>%
  fmt_number(columns = model_names, decimals = 2)

```

### Hierarchical Bayesian Inference (HBI) Results and Model Comparison

To further refine our selection, we employ Hierarchical Bayesian Inference (HBI) to perform a more detailed comparison of the selected models. This statistical approach allows us to compare models not just by their individual fits but also by their ability to explain data across different subjects. In interpreting the results from HBI, we observed the differential performances of Q-learning (QL) and Actor-Critic (AC) models. @tbl-CBM-HBI provides quantifiable insights into each model's fit (BIC) and predictive power (AUC, the area under the receiver operating characteristic curve) across teachers. Generally, Q-learning models fit the data better than Actor-Critic models (i.e., lower BIC scores), and AC models outperform in terms of predictive power (higher AUC scores).

<!-- tbl-CBM-HBI -->

```{r}
#| label: tbl-CBM-HBI
#| tbl-cap: "Top Q-learning and Actor-Critic models under Hierarchical Bayesian Inference (HBI). The table presents the best-performing models, all featuring the 'Pedagogical Knowledge' action component, ranked by their Bayesian Information Criterion (BIC) scores. BIC quantifies the models' fit to the data, penalizing for model complexity, with lower values indicating better performance. The model frequency represents the proportion of teachers best described by each model. Q-learning models (denoted by missing values in the State column) assume teachers learn directly from rewards, while Actor-Critic models incorporate state information to guide decision-making."

# Define paths
model_folder_path <- "CBM/zearn_results/hbi_results"

# Create a mapping for action and reward
action_reward_mapping <- list(
  "14" = list(action = "NNDSVD_teacher2", reward = "NNDSVD_student4"),
  "10" = list(action = "NNDSVD_teacher2", reward = "NNDSVD_student3"),
  "6" = list(action = "NNDSVD_teacher2", reward = "NNDSVD_student2"),
  "2" = list(action = "NNDSVD_teacher2", reward = "NNDSVD_student1")
)

# Create a data frame for model comparison
model_comparison_df <- data.frame(
  Action = character(),
  Reward = character(),
  Q_learning = numeric(),
  Lau_Glimcher = numeric(),
  Cockburn = numeric()
)

# Process models
for (wrapper in c("2", "6", "10", "14")) {
  q_file <- list.files(path = model_folder_path,
                       pattern = paste0("hbi_compare_wrapper_",
                                        wrapper, "_model_1.mat"),
                       full.names = TRUE)
  lg_file <- list.files(path = model_folder_path,
                        pattern = paste0("hbi_compare_wrapper_",
                                         wrapper, "_model_2.mat"),
                        full.names = TRUE)
  cb_file <- list.files(path = model_folder_path,
                        pattern = paste0("hbi_compare_wrapper_",
                                         wrapper, "_model_3.mat"),
                        full.names = TRUE)
  bl_file <- list.files(path = model_folder_path,
                        pattern = "hbi_compare_wrapper_2_model_4.mat",
                        full.names = TRUE)

  q_data <- readMat(q_file)
  lg_data <- readMat(lg_file)
  cb_data <- readMat(cb_file)
  bl_data <- readMat(bl_file)

  model_comparison_df <- rbind(model_comparison_df, data.frame(
    Action = action_reward_mapping[[wrapper]]$action,
    Reward = action_reward_mapping[[wrapper]]$reward,
    Q_learning = mean(q_data[["cbm"]][[5]][[9]], na.rm = TRUE),
    Lau_Glimcher = mean(lg_data[["cbm"]][[5]][[9]], na.rm = TRUE),
    Cockburn = mean(cb_data[["cbm"]][[5]][[9]], na.rm = TRUE),
    Baseline = mean(bl_data[["cbm"]][[5]][[9]], na.rm = TRUE)
  ))
}

# Display the table
model_comparison_df %>%
  mutate(across(c(Action, Reward), ~ case_when(
    . == "NNDSVD_student1" ~ "Badges",
    . == "NNDSVD_student2" ~ "Struggles",
    . == "NNDSVD_student3" ~ "No. Students",
    . == "NNDSVD_student4" ~ "Activity",
    . == "NNDSVD_teacher1" ~ "Assessments",
    . == "NNDSVD_teacher2" ~ "Pedagogical Knowledge",
    . == "NNDSVD_teacher3" ~ "Group Instruction",
    . == "NNDSVD_teacher4" ~ "Curriculum Planning",
    . == "None" ~ "NA",
    .default = .
    ))) %>%
  gt() %>%
  tab_header(title = "Model Comparison - Mean BIC Scores") %>%
  fmt_number(columns = c(Q_learning, Lau_Glimcher, Cockburn), decimals = 3) %>%
  cols_label(
    Q_learning = "Q-learning",
    Lau_Glimcher = "Lau & Glimcher",
    Cockburn = "Cockburn et al."
  )

```

## Posterior Predictive Checks

```{r}

load_predictions <- function(file_path) {
  mat_data <- readMat(file_path)
  if(grepl("_2_", file_path)) student_var = 1
  if(grepl("_6_", file_path)) student_var = 2
  if(grepl("_10_", file_path)) student_var = 3
  if(grepl("_14_", file_path)) student_var = 4
  raw_data <- mat_data[["cbm"]][[5]][[11]]
  predictions <- mat_data[["cbm"]][[5]][[10]]
  if(grepl("_1.mat", file_path)) q_values <- mat_data[["cbm"]][[5]][[12]]

  # Create a data frame
  data <- list_rbind(
    map(1:length(predictions), function(i) {
      ID = raw_data[[i]][[1]][[9]][[3]]
      week_number <- raw_data[[i]][[1]][[9]][[1]]
      action <- raw_data[[i]][[1]][[2]]
      reward <- raw_data[[i]][[1]][[4 + student_var]]
      pred <- predictions[[i]][[1]]
      if(exists("q_values")) {
        q_val <- q_values[[i]][[1]][1,]
      } else {
        q_val <- NA
      }

      tibble(
        Classroom.ID = rep(ID, each = length(action)),
        week = as.vector(week_number),
        action = as.vector(action),
        reward = as.vector(reward),
        pred_1 = pred[,1],
        pred_2 = pred[,2],
        q_val = q_val
        )
      })
    )

  return(data)
}

# Load predictions for all wrappers and models
load_all_predictions <- function() {
  model_folder_path <- "CBM/zearn_results/hbi_results/"
  prediction_files <- list.files(path = model_folder_path,
                                 pattern = "hbi_compare_wrapper_[0-9]+_model_[0-9]+\\.mat$",
                                 full.names = TRUE)

  predictions <- map(prediction_files, load_predictions)
  names(predictions) <- basename(prediction_files)

  return(predictions)
}

# Load all predictions
all_predictions <- load_all_predictions()

```

```{r}

# Function to calculate cumulative mean ignoring NAs
cummean_ignore_na <- function(x) {
  n <- cumsum(!is.na(x))
  result <- cumsum(replace(x, is.na(x), 0)) / n
  result[n == 0] <- 0
  return(result)
}

# Function to calculate cumulative standard deviation
cumsd <- function(x) {
  n <- cumsum(!is.na(x))
  cumsum_x <- cumsum(replace(x, is.na(x), 0))
  cumsum_x2 <- cumsum(replace(x^2, is.na(x), 0))

  variance <- (cumsum_x2 - (cumsum_x^2) / n) / (n - 1)
  variance[is.infinite(variance)] <- NA
  result <- sqrt(variance)

  # Forward fill NA values
  last_valid <- NA
  for (i in seq_along(result)) {
    if (!is.na(result[i])) {
      last_valid <- result[i]
    } else if (!is.na(last_valid)) {
      result[i] <- last_valid
    }
  }
  return(result)
}

# Function to calculate EV and Uncertainty
calculate_ev_uncertainty <- function(action, outcome) {
  v_0 <- outcome
  v_0[action > 0] <- NA
  if (is.na(v_0[1])) v_0[1] <- 0

  v_1 <- outcome
  v_1[action == 0] <- NA
  if (is.na(v_1[1])) v_1[1] <- 0

  ev_0 <- cummean_ignore_na(v_0)
  ev_1 <- cummean_ignore_na(v_1)
  ev <- ev_1 - ev_0

  uncertainty_0 <- cumsd(v_0)
  uncertainty_1 <- cumsd(v_1)
  uncertainty <- uncertainty_1 - uncertainty_0

  return(list(ev = ev, uncertainty = uncertainty))
}

plot_reward_seeking <- function(sim_data, wrapper) {
  # Calculate EV and Uncertainty
  sim_data <- sim_data %>%
    group_by(Classroom.ID) %>%
    mutate(ev = calculate_ev_uncertainty(action, reward)[[1]],
           sd = calculate_ev_uncertainty(action, reward)[[2]]) %>%
    mutate(
      # Create lagged predictors
      lag_ev = c(0, ev[-n()]),
      lag_sd = c(0, replace_na(sd[-n()], 0))
    ) %>%
    mutate(
      q_val_1_percentile = percent_rank(q_val_1),
      lag_ev_percentile = percent_rank(lag_ev)
    ) %>%
    ungroup()

  lower_bound = 0.05
  upper_bound = 0.95
  # Filter data for geom_smooth
  smooth_data <- sim_data %>%
    filter(lag_ev_percentile >= lower_bound,
           lag_ev_percentile <= upper_bound,
           q_val_1_percentile >= lower_bound,
           q_val_1_percentile <= upper_bound)

  # Create quantile-based bins for EV
  sim_data <- smooth_data %>%
    mutate(
      ev_bin = cut(
        lag_ev_percentile,
        breaks = seq(0.05, 0.95, 0.15),
        labels = FALSE,
        include.lowest = TRUE),
      q_bin = cut(
        q_val_1_percentile,
        breaks = seq(0.05, 0.95, 0.15),
        labels = FALSE,
        include.lowest = TRUE
        ))

  # Prepare plot data
  plot_data_ev <- sim_data %>%
    group_by(ev_bin) %>%
    summarise(
      mean_choice = mean(action, na.rm = TRUE),
      se_choice = sd(action, na.rm = TRUE) / sqrt(n()),
      mean_ev = mean(lag_ev, na.rm = TRUE),
      mean_ev_perc = mean(lag_ev_percentile, na.rm = TRUE),
      mean_uncertainty = mean(lag_sd, na.rm = TRUE),
      mean_pred_1 = mean(pred_2_1, na.rm = TRUE),
      se_pred_1 = sd(pred_2_1, na.rm = TRUE) / sqrt(n()),
      mean_pred_2 = mean(pred_2_2, na.rm = TRUE),
      se_pred_2 = sd(pred_2_2, na.rm = TRUE) / sqrt(n()),
      mean_pred_3 = mean(pred_2, na.rm = TRUE),
      se_pred_3 = sd(pred_2, na.rm = TRUE) / sqrt(n())
    ) %>%
    filter(!is.na(ev_bin))
  plot_data_qv <- sim_data %>%
    group_by(q_bin) %>%
    summarise(
      mean_choice = mean(action, na.rm = TRUE),
      se_choice = sd(action, na.rm = TRUE) / sqrt(n()),
      mean_q_diff = mean(q_val_1, na.rm = TRUE),
      mean_q_diff_perc = mean(q_val_1_percentile, na.rm = TRUE),
      mean_uncertainty = mean(lag_sd, na.rm = TRUE),
      mean_pred_1 = mean(pred_2_1, na.rm = TRUE),
      se_pred_1 = sd(pred_2_1, na.rm = TRUE) / sqrt(n()),
      mean_pred_2 = mean(pred_2_2, na.rm = TRUE),
      se_pred_2 = sd(pred_2_2, na.rm = TRUE) / sqrt(n()),
      mean_pred_3 = mean(pred_2, na.rm = TRUE),
      se_pred_3 = sd(pred_2, na.rm = TRUE) / sqrt(n())
    ) %>%
    filter(!is.na(q_bin))

  # Calculate x-axis breaks and labels
  breaks_ev <- plot_data_ev$mean_ev_perc
  breaks_q <- plot_data_qv$mean_q_diff_perc
  labels <- c("5-20%", "20-35%", "35-50%", "50-65%", "65-80%", "80-95%")

  ggplot(smooth_data) +
    geom_hline(yintercept = 0.5, linetype = "dotted") +
    # Lau & Glimcher model (pred_2_2)
    geom_smooth(aes(x = lag_ev_percentile, y = pred_2_2), method = "glm",
                method.args = list(family = "quasibinomial"),
                color = "red", fill = "red", alpha = 0.2) +
    # Model 3 (pred_3)
    geom_smooth(aes(x = lag_ev_percentile, y = pred_2), method = "glm",
                method.args = list(family = "quasibinomial"),
                color = "green", fill = "green", alpha = 0.2) +
    # Q-learning model (pred_2_1)
    geom_smooth(aes(x = q_val_1_percentile, y = pred_2_1), method = "glm",
                method.args = list(family = "quasibinomial"),
                color = "blue", fill = "blue", alpha = 0.2) +
    # Smooth line for real data
    geom_smooth(aes(x = lag_ev_percentile, y = action),
                method = "glm", method.args = list(family = "binomial"),
                se = FALSE, color = "black", linetype = "dashed") +
    # Points and error bars for binned data
    geom_point(data = plot_data_ev,
               aes(x = mean_ev_perc, y = mean_choice), size = 1) +
    geom_errorbar(data = plot_data_ev,
                  aes(x = mean_ev_perc, y = mean_choice,
                      ymin = mean_choice - se_choice,
                      ymax = mean_choice + se_choice),
                  width = 0) +
    # Q-diff plot
    geom_smooth(data = smooth_data,
                aes(x = q_val_1_percentile, y = action),
                method = "glm", method.args = list(family = "binomial"),
                se = FALSE, color = "gray", linetype = "dashed") +
    geom_point(data = plot_data_qv,
               aes(x = mean_q_diff_perc, y = mean_choice),
               size = 1, color = "gray") +
    geom_errorbar(data = plot_data_qv,
                  aes(x = mean_q_diff_perc, y = mean_choice,
                      ymin = mean_choice - se_choice,
                      ymax = mean_choice + se_choice),
                  width = 0, color = "gray") +
    labs(x = "Expected value / Q-value difference (percentile)",
         y = "% Choice",
         title = paste("Reward Seeking", wrapper)) +
    scale_x_continuous(
      # sec.axis = sec_axis(~ ., breaks = breaks_q, labels = labels),
      breaks = breaks_ev,
      labels = labels
    ) +
    coord_cartesian(ylim = c(0.2, 0.6)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                       limits = c(0, 1)) +
    scale_color_manual(values = c("Data (EV)" = "black", 
                                  "Data (Q-value)" = "gray", 
                                  "Q-learning" = "blue", 
                                  "Lau & Glimcher" = "red", 
                                  "Cockburn" = "green")) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 10),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "bottom"
    )
    
}

# Create a list to store all plots
all_reward_seeking_plots <- list()

# Generate all 4 plots
for (wrapper in c(2, 6, 10, 14)) {
  full_data <- all_predictions[[
    paste0("hbi_compare_wrapper_", wrapper, "_model_", 1, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 2, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_1", "_2")
    ) %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 3, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"))
  full_data <- full_data %>%
    # Filter data for teachers with week 9
    group_by(Classroom.ID) %>%
    filter(week == 9) %>%
    pull(Classroom.ID) %>%
    {filter(full_data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - 9 + 1) %>%
    filter(adjusted_week >= 1)

  plot <- plot_reward_seeking(full_data, wrapper)
  all_reward_seeking_plots[[paste(wrapper, sep = "_")]] <- plot
}

```

```{r}

plot_uncertainty_aversion <- function(sim_data, wrapper) {
  # Calculate delta EV and choice_uncertain
  sim_data <- sim_data %>%
    group_by(Classroom.ID) %>%
    mutate(
      ev = calculate_ev_uncertainty(action, reward)[[1]],
      sd = calculate_ev_uncertainty(action, reward)[[2]],
      lag_ev = c(0, ev[-n()]),
      lag_sd = c(0, replace_na(sd[-n()], 0)),
      delta_ev = if_else(lag_sd > 0, lag_ev, -lag_ev),
      choice_uncertain = as.numeric(xor(action == 0, lag_sd > 0)),
      # Calculate choice_uncertain for each model
      choice_uncertain_pred_1 = as.numeric(xor(pred_1_1 > 0.5, lag_sd > 0)),
      choice_uncertain_pred_2 = as.numeric(xor(pred_1_2 > 0.5, lag_sd > 0)),
      choice_uncertain_pred_3 = as.numeric(xor(pred_1 > 0.5, lag_sd > 0))
    ) %>%
    ungroup()

  # Calculate 5th and 95th percentiles
  lower_bound <- quantile(sim_data$delta_ev, 0.05, na.rm = TRUE)
  upper_bound <- quantile(sim_data$delta_ev, 0.95, na.rm = TRUE)

  # Filter data for geom_smooth
  smooth_data <- sim_data %>%
    filter(delta_ev >= lower_bound, delta_ev <= upper_bound)

  # Create quantile-based bins
  sim_data <- sim_data %>%
    mutate(ev_bin = cut(delta_ev,
                        breaks = quantile(delta_ev,
                                          probs = seq(0.05, 0.95, 0.15),
                                          na.rm = TRUE),
                        labels = FALSE, include.lowest = TRUE))

  # Prepare plot data
  plot_data <- sim_data %>%
    group_by(ev_bin) %>%
    summarise(
      mean_choice_uncertain = mean(choice_uncertain, na.rm = TRUE),
      se_choice_uncertain = sd(choice_uncertain, na.rm = TRUE) / sqrt(n()),
      mean_delta_ev = mean(delta_ev, na.rm = TRUE),
      mean_pred_1 = mean(choice_uncertain_pred_1, na.rm = TRUE),
      se_pred_1 = sd(choice_uncertain_pred_1, na.rm = TRUE) / sqrt(n()),
      mean_pred_2 = mean(choice_uncertain_pred_2, na.rm = TRUE),
      se_pred_2 = sd(choice_uncertain_pred_2, na.rm = TRUE) / sqrt(n()),
      mean_pred_3 = mean(choice_uncertain_pred_3, na.rm = TRUE),
      se_pred_3 = sd(choice_uncertain_pred_3, na.rm = TRUE) / sqrt(n())
    ) %>%
    filter(!is.na(ev_bin))

  # Calculate x-axis breaks and labels
  breaks <- plot_data$mean_delta_ev
  labels <- c("5-20%", "20-35%", "35-50%", "50-65%", "65-80%", "80-95%")

  ggplot(smooth_data, aes(x = delta_ev, y = as.numeric(choice_uncertain))) +
    geom_hline(yintercept = 0.5, linetype = "dotted") +
    # Lau & Glimcher model (pred_2)
    geom_smooth(aes(y = choice_uncertain_pred_2), method = "glm",
                method.args = list(family = "binomial"),
                color = "red", fill = "red", alpha = 0.2) +
    # Model 3 (pred_3)
    geom_smooth(aes(y = choice_uncertain_pred_3), method = "glm",
                method.args = list(family = "binomial"),
                color = "green", fill = "green", alpha = 0.2) +
    # Q-learning model (pred_1)
    geom_smooth(aes(y = choice_uncertain_pred_1), method = "glm",
                method.args = list(family = "binomial"),
                color = "blue", fill = "blue", alpha = 0.2) +
    # Smooth line for real data
    geom_smooth(method = "glm", method.args = list(family = "binomial"),
                se = FALSE, color = "black", linetype = "dashed") +
    # Points and error bars for binned data
    geom_point(data = plot_data,
               aes(x = mean_delta_ev, y = mean_choice_uncertain), size = 1) +
    geom_errorbar(data = plot_data,
                  aes(x = mean_delta_ev,
                      y = mean_choice_uncertain,
                      ymin = mean_choice_uncertain - se_choice_uncertain,
                      ymax = mean_choice_uncertain + se_choice_uncertain),
                  width = 0) +
    labs(x = "ΔEV (Uncertain - Certain) (percentile)",
         y = "% Choice Uncertain",
         title = paste("Uncertainty Aversion", wrapper)) +
    scale_x_continuous(breaks = breaks, labels = labels) +
    coord_cartesian(ylim = c(0, 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                       limits = c(0, 1)) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 10),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
}

# Create a list to store all plots
all_uncertainty_plots <- list()

# Generate all 4 plots
for (wrapper in c(2, 10, 14)) {
  full_data <- all_predictions[[
    paste0("hbi_compare_wrapper_", wrapper, "_model_", 1, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 2, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_1", "_2")
    ) %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 3, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"))
  full_data <- full_data %>%
    # Filter data for teachers with week 9
    group_by(Classroom.ID) %>%
    filter(week == 9) %>%
    pull(Classroom.ID) %>%
    {filter(full_data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - 9 + 1) %>%
    filter(adjusted_week >= 1)

  plot <- plot_uncertainty_aversion(full_data, wrapper)
  all_uncertainty_plots[[paste(wrapper, sep = "_")]] <- plot
}

```

```{r}

load_parameters <- function(file_path) {
  mat_data <- readMat(file_path)
  if(grepl("_2_", file_path)) student_var = 1
  if(grepl("_6_", file_path)) student_var = 2
  if(grepl("_10_", file_path)) student_var = 3
  if(grepl("_14_", file_path)) student_var = 4
  
  parameters <- mat_data[["cbm"]][[5]][[1]][[1]][[1]]
  raw_data <- mat_data[["cbm"]][[5]][[11]]

  # Create a data frame
  data <- list_rbind(
    map(1:nrow(parameters), function(i) {
      tibble(Classroom.ID = raw_data[[i]][[1]][[9]][[3]][,1])
      })
    ) %>%
    bind_cols(
      tibble(
        alpha = 1 / (1 + exp(-parameters[, 1])),
        gamma = 1 / (1 + exp(-parameters[, 2])),
        tau = exp(parameters[, 3]),
        cost = exp(parameters[, 4])
        )
    )

  return(data)
}

plot_prediction_errors <- function(sim_data, wrapper, matlab_file) {
  parameters <- load_parameters(matlab_file)
  
  # Calculate prediction errors
  sim_data <- sim_data %>%
    inner_join(parameters, by = "Classroom.ID") %>%
    group_by(Classroom.ID) %>%
    mutate(
      # Q-learning PE
      q_pe = gamma * reward - q_val_1,
      
      # Cockburn PE: reward - EV
      ev = calculate_ev_uncertainty(action, reward)[[1]],
      cockburn_pe = reward - ev,
      
      # Shift actions to align with previous week's PE
      next_action = lead(action)
    ) %>%
    ungroup()

  # Calculate 5th and 95th percentiles for both PEs
  lower_bound_q <- quantile(sim_data$q_pe, 0.05, na.rm = TRUE)
  upper_bound_q <- quantile(sim_data$q_pe, 0.95, na.rm = TRUE)
  lower_bound_c <- quantile(sim_data$cockburn_pe, 0.05, na.rm = TRUE)
  upper_bound_c <- quantile(sim_data$cockburn_pe, 0.95, na.rm = TRUE)
  
  # Filter data for geom_smooth
  smooth_data <- sim_data %>%
    filter(q_pe >= lower_bound_q, q_pe <= upper_bound_q,
           cockburn_pe >= lower_bound_c, cockburn_pe <= upper_bound_c)

  # Create quantile-based bins for both PEs
  sim_data <- sim_data %>%
    mutate(
      q_pe_bin = cut(
        q_pe, 
        breaks = quantile(q_pe, probs = seq(0.05, 0.95, 0.15), na.rm = TRUE),
        labels = FALSE,
        include.lowest = TRUE
      ),
      cockburn_pe_bin = cut(
        cockburn_pe, 
        breaks = quantile(cockburn_pe, probs = seq(0.05, 0.95, 0.15), na.rm = TRUE),
        labels = FALSE,
        include.lowest = TRUE
      )
    )

  # Prepare plot data
  plot_data_q <- sim_data %>%
    group_by(q_pe_bin) %>%
    summarise(
      mean_choice_q = mean(next_action, na.rm = TRUE),
      se_choice_q = sd(next_action, na.rm = TRUE) / sqrt(n()),
      mean_q_pe = mean(q_pe, na.rm = TRUE)
    ) %>%
    filter(!is.na(q_pe_bin))
  plot_data_cb <- sim_data %>%
    group_by(cockburn_pe_bin) %>%
    summarise(
      mean_choice_cb = mean(next_action, na.rm = TRUE),
      se_choice_cb = sd(next_action, na.rm = TRUE) / sqrt(n()),
      mean_cockburn_cb = mean(cockburn_pe, na.rm = TRUE)
    ) %>%
    filter(!is.na(cockburn_pe_bin))

  # Calculate x-axis breaks and labels
  breaks_q <- plot_data_q$mean_q_pe
  breaks_c <- plot_data_cb$mean_cockburn_cb
  labels <- c("5-20%", "20-35%", "35-50%", "50-65%", "65-80%", "80-95%")

  # Create the plot
  p <- ggplot() +
    geom_hline(yintercept = 0.5, linetype = "dotted") +
    # Q-learning PE plot
    geom_smooth(data = smooth_data,
                aes(x = q_pe, y = next_action), 
                method = "glm", method.args = list(family = "binomial"),
                se = FALSE, color = "blue", linetype = "dashed") +
    geom_point(data = plot_data_q,
               aes(x = mean_q_pe, y = mean_choice_q), 
               size = 1, color = "blue") +
    geom_errorbar(data = plot_data_q, 
                  aes(x = mean_q_pe, y = mean_choice_q,
                      ymin = mean_choice_q - se_choice_q,
                      ymax = mean_choice_q + se_choice_q), 
                  width = 0, color = "blue") +
    # Cockburn PE plot
    geom_smooth(data = smooth_data,
                aes(x = cockburn_pe, y = next_action), 
                method = "glm", method.args = list(family = "binomial"),
                se = FALSE, color = "red", linetype = "dashed") +
    geom_point(data = plot_data_cb,
               aes(x = mean_cockburn_cb, y = mean_choice_cb), 
               size = 1, color = "red") +
    geom_errorbar(data = plot_data_cb, 
                  aes(x = mean_cockburn_cb, y = mean_choice_cb,
                      ymin = mean_choice_cb - se_choice_cb,
                      ymax = mean_choice_cb + se_choice_cb), 
                  width = 0, color = "red") +
    labs(x = "Previous Week's Prediction Error (percentile)", 
         y = "% Choice This Week", 
         title = paste("Next Week's Choice vs. This Week's Prediction Errors", wrapper)) +
    scale_x_continuous(
      sec.axis = sec_axis(~ ., breaks = breaks_c, labels = labels),
      breaks = breaks_q, 
      labels = labels
    ) +
    coord_cartesian(ylim = c(0.1, 0.7)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                       limits = c(0, 1)) +
    theme_classic() +
    theme(
      plot.title = element_text(size = 10),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1)
    )

  return(p)
}

# Create a list to store all plots
all_pe_plots <- list()

# Generate all 4 plots
for (wrapper in c(2, 6, 10, 14)) {
  full_data <- all_predictions[[
    paste0("hbi_compare_wrapper_", wrapper, "_model_", 1, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 2, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_1", "_2")
    ) %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 3, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"))
  full_data <- full_data %>%
    # Filter data for teachers with week 9
    group_by(Classroom.ID) %>%
    filter(week == 9) %>%
    pull(Classroom.ID) %>%
    {filter(full_data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - 9 + 1) %>%
    filter(adjusted_week >= 1)

  plot <- plot_prediction_errors(
    full_data, wrapper,
    paste0("CBM/zearn_results/model_results/hbi_compare_wrapper_",
           wrapper, "_model_", 1, ".mat")
    )
  all_pe_plots[[paste(wrapper, sep = "_")]] <- plot
}

```

```{r}

plot_learning_curves <- function(sim_data, wrapper, matlab_file) {
  parameters <- load_parameters(matlab_file)
  
  # Calculate prediction errors
  sim_data <- sim_data %>%
    inner_join(parameters, by = "Classroom.ID") %>%
    remove_holiday_weeks() %>%
    group_by(Classroom.ID) %>%
    mutate(
      # Q-learning PE
      q_pe = gamma * reward - q_val_1,
      
      # Cockburn PE: reward - EV
      ev = calculate_ev_uncertainty(action, reward)[[1]],
      cockburn_pe = reward - ev,
      
      # Shift actions to align with previous week's PE
      next_action = lead(action)
    ) %>%
    ungroup()
  
  plot_data <- sim_data %>%
    # Prepare plot data
    group_by(adjusted_week) %>%
    summarise(
      mean_q_pe = mean(q_pe, na.rm = TRUE),
      se_q_pe = sd(q_pe, na.rm = TRUE) / sqrt(n()),
      mean_cb_pe = mean(cockburn_pe, na.rm = TRUE),
      se_cb_pe = sd(cockburn_pe, na.rm = TRUE) / sqrt(n()),
      mean_q_value = mean(q_val_1, na.rm = TRUE),
      se_q_value = sd(q_val_1, na.rm = TRUE) / sqrt(n()),
      mean_reward = mean(reward, na.rm = TRUE),
      se_reward = sd(reward, na.rm = TRUE) / sqrt(n())
    )

  # Prepare plot data
  month_breaks <- sim_data %>%
    mutate(month = floor_date(as.Date(date), "month")) %>%
    group_by(month) %>%
    summarise(week = min(adjusted_week)) %>%
    mutate(month_label = format(month, "%b")) %>%
    # Remove August label
    mutate(month_label = if_else(month_label == "Aug", "", month_label))

  # Create the plot
  ggplot(plot_data, aes(x = adjusted_week, y = mean_q_pe)) +
    geom_line(color = "blue", size = 0.5) +
    geom_ribbon(aes(ymin = mean_q_pe - se_q_pe, ymax = mean_q_pe + se_q_pe),
                fill = "blue", alpha = 0.2) +
    geom_line(aes(y = mean_reward), color = "black", linetype = "dashed") +
    geom_ribbon(aes(ymin = mean_reward - se_reward,
                    ymax = mean_reward + se_reward),
                fill = "black", alpha = 0.2) +
    geom_line(aes(y = mean_cb_pe), color = "red", size = 0.5) +
    geom_ribbon(aes(ymin = mean_cb_pe - se_cb_pe, ymax = mean_cb_pe + se_cb_pe),
                fill = "red", alpha = 0.2) +
    labs(x = "Week", y = "Mean Absolute Prediction Error",
         title = paste("Learning Curve -", wrapper)) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1),
      axis.ticks.x = element_line(color = "black", linewidth = 0.25),
      axis.minor.ticks.x = element_line(color = "black", linewidth = 0.25)
    ) +
    scale_x_continuous(breaks = month_breaks$week,
                       labels = month_breaks$month_label,
                       minor_breaks = seq(min(plot_data$adjusted_week),
                                          max(plot_data$adjusted_week),
                                          by = 1))
}

plot_learning_curves <- function(sim_data, wrapper, matlab_file) {
  parameters <- load_parameters(matlab_file)
  
  # Calculate prediction errors
  sim_data <- sim_data %>%
    inner_join(parameters, by = "Classroom.ID") %>%
    remove_holiday_weeks() %>%
    group_by(Classroom.ID) %>%
    mutate(
      # Q-learning PE
      q_pe = gamma * reward - q_val_1,
      
      # Cockburn PE: reward - EV
      ev = calculate_ev_uncertainty(action, reward)[[1]],
      cockburn_pe = reward - ev,
      
      # Shift actions to align with previous week's PE
      next_action = lead(action),
      
      # Create a biweekly grouping
      biweek = ceiling(adjusted_week / 2)
    ) %>%
    ungroup()
  
  plot_data <- sim_data %>%
    # Prepare plot data, now grouped by biweek
    group_by(biweek) %>%
    summarise(
      mean_q_pe = mean(q_pe, na.rm = TRUE),
      se_q_pe = sd(q_pe, na.rm = TRUE) / sqrt(n()),
      mean_cb_pe = mean(cockburn_pe, na.rm = TRUE),
      se_cb_pe = sd(cockburn_pe, na.rm = TRUE) / sqrt(n()),
      mean_q_value = mean(q_val_1, na.rm = TRUE),
      se_q_value = sd(q_val_1, na.rm = TRUE) / sqrt(n()),
      mean_reward = mean(reward, na.rm = TRUE),
      se_reward = sd(reward, na.rm = TRUE) / sqrt(n()),
      date = mean(date)  # Take the average date for each biweek
    )

  # Prepare plot data
  month_breaks <- plot_data %>%
    mutate(month = floor_date(date, "month")) %>%
    group_by(month) %>%
    summarise(biweek = first(biweek)) %>%
    mutate(month_label = format(month, "%b")) %>%
    # Remove August label
    mutate(month_label = if_else(month_label == "Aug", "", month_label))

  # Create the plot
  ggplot(plot_data, aes(x = biweek)) +
    geom_line(aes(y = mean_q_pe, color = "Q-learning PE"), size = 0.5) +
    geom_ribbon(aes(ymin = mean_q_pe - se_q_pe, ymax = mean_q_pe + se_q_pe, fill = "Q-learning PE"), alpha = 0.2) +
    geom_line(aes(y = mean_reward, color = "Reward"), linetype = "dashed") +
    geom_ribbon(aes(ymin = mean_reward - se_reward, ymax = mean_reward + se_reward, fill = "Reward"), alpha = 0.2) +
    geom_line(aes(y = mean_cb_pe, color = "Cockburn PE"), size = 0.5) +
    geom_ribbon(aes(ymin = mean_cb_pe - se_cb_pe, ymax = mean_cb_pe + se_cb_pe, fill = "Cockburn PE"), alpha = 0.2) +
    labs(x = "Biweekly Period", y = "Mean Prediction Error",
         title = paste("Learning Curve -", wrapper),
         color = "Measure", fill = "Measure") +
    scale_color_manual(values = c("Q-learning PE" = "blue", "Reward" = "black", "Cockburn PE" = "red")) +
    scale_fill_manual(values = c("Q-learning PE" = "blue", "Reward" = "black", "Cockburn PE" = "red")) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1),
      axis.ticks.x = element_line(color = "black", linewidth = 0.25),
      axis.minor.ticks.x = element_line(color = "black", linewidth = 0.25),
      legend.position = "bottom"
    ) +
    scale_x_continuous(breaks = month_breaks$biweek,
                       labels = month_breaks$month_label,
                       minor_breaks = seq(min(plot_data$biweek),
                                          max(plot_data$biweek),
                                          by = 1))
}

# Create a list to store all plots
learning_curve_plots <- list()

# Generate all 4 plots
for (wrapper in c(2, 10, 14)) {
  full_data <- all_predictions[[
    paste0("hbi_compare_wrapper_", wrapper, "_model_", 1, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 2, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_1", "_2")
    ) %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 3, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"))
  full_data <- full_data %>%
    # Filter data for teachers with week 10
    group_by(Classroom.ID) %>%
    filter(week == 10) %>%
    pull(Classroom.ID) %>%
    {filter(full_data, Classroom.ID %in% .)} %>%
    # Keep week 9
    mutate(adjusted_week = week - 8) %>%
    filter(adjusted_week >= 1) %>%
    mutate(date = as_date("2019-08-26") + weeks(adjusted_week - 1))

  plot <- plot_learning_curves(
    full_data, wrapper,
    paste0("CBM/zearn_results/model_results/hbi_compare_wrapper_",
           wrapper, "_model_", 1, ".mat")
    )
  learning_curve_plots[[paste(wrapper, sep = "_")]] <- plot
}

```

```{r}

plot_action_percentages <- function(sim_data, wrapper, matlab_file) {
  parameters <- load_parameters(matlab_file)
  
  # Calculate action percentages
  sim_data <- sim_data %>%
    inner_join(parameters, by = "Classroom.ID") %>%
    remove_holiday_weeks() %>%
    group_by(Classroom.ID) %>%
    mutate(
      biweek = ceiling(adjusted_week / 2),
      actual_action = action,
      q_action = ifelse(pred_2_1 > 0.5, 1, 0),  # Q-learning prediction
      cockburn_action = ifelse(pred_2 > 0.5, 1, 0)  # Cockburn model prediction
    ) %>%
    ungroup()
  
  plot_data <- sim_data %>%
    group_by(biweek) %>%
    summarise(
      actual_percent = mean(actual_action, na.rm = TRUE),
      se_actual = sd(actual_action, na.rm = TRUE) / sqrt(n()),
      q_percent = mean(q_action, na.rm = TRUE),
      se_q = sd(q_action, na.rm = TRUE) / sqrt(n()),
      cockburn_percent = mean(cockburn_action, na.rm = TRUE),
      se_cockburn = sd(cockburn_action, na.rm = TRUE) / sqrt(n()),
      date = mean(date)
    )

  # Prepare plot data
  month_breaks <- plot_data %>%
    mutate(month = floor_date(date, "month")) %>%
    group_by(month) %>%
    summarise(biweek = first(biweek)) %>%
    mutate(month_label = format(month, "%b")) %>%
    mutate(month_label = if_else(month_label == "Aug", "", month_label))

  # Create the plot
  ggplot(plot_data, aes(x = biweek)) +
    geom_line(aes(y = actual_percent, color = "Actual"), size = 0.5) +
    geom_ribbon(aes(ymin = actual_percent - se_actual, 
                    ymax = actual_percent + se_actual, 
                    fill = "Actual"), alpha = 0.2) +
    geom_line(aes(y = q_percent, color = "Q-learning"), size = 0.5) +
    geom_ribbon(aes(ymin = q_percent - se_q, 
                    ymax = q_percent + se_q, 
                    fill = "Q-learning"), alpha = 0.2) +
    geom_line(aes(y = cockburn_percent, color = "Cockburn"), size = 0.5) +
    geom_ribbon(aes(ymin = cockburn_percent - se_cockburn, 
                    ymax = cockburn_percent + se_cockburn, 
                    fill = "Cockburn"), alpha = 0.2) +
    labs(x = "Biweekly Period", y = "Action Percentage",
         title = paste("Action Percentages -", wrapper),
         color = "Model", fill = "Model") +
    scale_color_manual(values = c("Actual" = "black", "Q-learning" = "blue", "Cockburn" = "red")) +
    scale_fill_manual(values = c("Actual" = "black", "Q-learning" = "blue", "Cockburn" = "red")) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 12, face = "bold"),
      axis.line = element_line(colour = "black"),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text.x = element_text(angle = 45, hjust = 1),
      axis.ticks.x = element_line(color = "black", linewidth = 0.25),
      axis.minor.ticks.x = element_line(color = "black", linewidth = 0.25),
      legend.position = "bottom"
    ) +
    scale_x_continuous(breaks = month_breaks$biweek,
                       labels = month_breaks$month_label,
                       minor_breaks = seq(min(plot_data$biweek),
                                          max(plot_data$biweek),
                                          by = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                       limits = c(0, 1))
}

# Create a list to store all plots
action_percentage_plots <- list()

# Generate all plots
for (wrapper in c(2, 10, 14)) {
  full_data <- all_predictions[[
    paste0("hbi_compare_wrapper_", wrapper, "_model_", 1, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 2, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_1", "_2")
    ) %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 3, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"))
  full_data <- full_data %>%
    group_by(Classroom.ID) %>%
    filter(week == 10) %>%
    pull(Classroom.ID) %>%
    {filter(full_data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - 8) %>%
    filter(adjusted_week >= 1) %>%
    mutate(date = as_date("2019-08-26") + weeks(adjusted_week - 1))

  plot <- plot_action_percentages(
    full_data, wrapper,
    paste0("CBM/zearn_results/model_results/hbi_compare_wrapper_",
           wrapper, "_model_", 1, ".mat")
    )
  action_percentage_plots[[paste(wrapper, sep = "_")]] <- plot
}

```

```{r}

library(changepoint)

analyze_performance_split <- function(sim_data, wrapper, matlab_file, weeks_to_show = 4) {
  parameters <- load_parameters(matlab_file)

  # Calculate running average of teacher actions
  sim_data <- sim_data %>%
    inner_join(parameters, by = "Classroom.ID") %>%
    remove_holiday_weeks() %>%
    group_by(MDR.School.ID, adjusted_week) %>%
    summarize(
      action_avg = mean(action),
      q_prediction = mean(pred_2_1),
      cockburn_prediction = mean(pred_2)
    ) %>%
    filter(!is.na(action_avg)) %>%
    filter(n() > weeks_to_show) %>%
    group_by(MDR.School.ID) %>%
    mutate(
      change_point = cpt.meanvar(action_avg)@cpts[1],
      weeks_from_change = adjusted_week - change_point,
      pre_change_mean = mean(
        action_avg[weeks_from_change < 0 & weeks_from_change >= -weeks_to_show],
        na.rm = TRUE),
      post_change_mean = mean(
        action_avg[weeks_from_change >= 0 & weeks_from_change <= weeks_to_show],
        na.rm = TRUE),
      change_direction = ifelse(post_change_mean > pre_change_mean,
                                "Increase", "Decrease")
    ) %>%
    ungroup()
  
# Prepare data for plotting
  plot_data <- sim_data %>%
    filter(abs(weeks_from_change) <= weeks_to_show) %>%
    mutate(period = ifelse(weeks_from_change <= 0, "Before", "After"))
  
  # Calculate averages across teachers after realignment
  avg_data <- plot_data %>%
    group_by(change_direction, weeks_from_change) %>%
    summarize(
      avg_action = mean(action_avg, na.rm = TRUE),
      se_action = sd(action_avg, na.rm = TRUE) / sqrt(n()),
      avg_q_prediction = mean(q_prediction, na.rm = TRUE),
      se_q_prediction = sd(q_prediction, na.rm = TRUE) / sqrt(n()),
      avg_cockburn_prediction = mean(cockburn_prediction, na.rm = TRUE),
      se_cockburn_prediction = sd(cockburn_prediction, na.rm = TRUE) / sqrt(n())
    )
  
  # Create plots for increase and decrease
  create_plot <- function(data, direction) {
    ggplot(data = data, aes(x = weeks_from_change)) +
      geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
      geom_line(aes(y = avg_action, color = "Actual Avg"), size = 1) +
      geom_ribbon(aes(ymin = avg_action - se_action, 
                      ymax = avg_action + se_action, 
                      fill = "Actual Avg"), alpha = 0.2) +
      geom_line(aes(y = avg_q_prediction, color = "Q-learning"), size = 1) +
      geom_ribbon(aes(ymin = avg_q_prediction - se_q_prediction, 
                      ymax = avg_q_prediction + se_q_prediction, 
                      fill = "Q-learning"), alpha = 0.2) +
      geom_line(aes(y = avg_cockburn_prediction, color = "Cockburn"), size = 1) +
      geom_ribbon(aes(ymin = avg_cockburn_prediction - se_cockburn_prediction, 
                      ymax = avg_cockburn_prediction + se_cockburn_prediction, 
                      fill = "Cockburn"), alpha = 0.2) +
      scale_color_manual(values = c("Actual Avg" = "blue",
                                    "Q-learning" = "red",
                                    "Cockburn" = "green")) +
      scale_fill_manual(values = c("Actual Avg" = "blue",
                                   "Q-learning" = "red",
                                   "Cockburn" = "green")) +
      labs(x = "Weeks from Change Point", y = "% Action",
           title = paste("Performance Split Analysis -", wrapper, "-", direction),
           subtitle = "Average across teachers, aligned at individual change points",
           color = "Measure", fill = "Measure") +
      theme_minimal() +
      theme(
        plot.title = element_text(size = 12, face = "bold"),
        axis.line = element_line(colour = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "bottom"
      ) +
      scale_y_continuous(labels = scales::percent_format(accuracy = 1))
  }
  
  increase_plot <- create_plot(filter(avg_data, change_direction == "Increase"), "Increase")
  decrease_plot <- create_plot(filter(avg_data, change_direction == "Decrease"), "Decrease")
  
  return(list(increase = increase_plot, decrease = decrease_plot))
}

# Create a list to store all plots
performance_split_plots <- list()

# Generate all plots
for (wrapper in c(2, 10, 14)) {
  full_data <- all_predictions[[
    paste0("hbi_compare_wrapper_", wrapper, "_model_", 1, ".mat")
  ]] %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 2, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward"),
    suffix = c("_1", "_2")
    ) %>%
    full_join(all_predictions[[
      paste0("hbi_compare_wrapper_", wrapper, "_model_", 3, ".mat")
    ]], by = c("Classroom.ID", "week", "action", "reward")) %>%
    right_join(df %>%
                 select(Classroom.ID, MDR.School.ID) %>%
                 unique(),
               by = c("Classroom.ID"))
  full_data <- full_data %>%
    group_by(Classroom.ID) %>%
    filter(week == 10) %>%
    pull(Classroom.ID) %>%
    {filter(full_data, Classroom.ID %in% .)} %>%
    mutate(adjusted_week = week - 8) %>%
    filter(adjusted_week >= 1) %>%
    mutate(date = as_date("2019-08-26") + weeks(adjusted_week - 1))

  plot <- analyze_performance_split(
    full_data, wrapper,
    paste0("CBM/zearn_results/model_results/hbi_compare_wrapper_",
           wrapper, "_model_", 1, ".mat")
    )
  performance_split_plots[[paste(wrapper, sep = "_")]] <- plot
}

```


```{r}
#| label: tbl-CBM-second
#| tbl-cap: "Comparative fit of hybrid models. The columns Logit, RL, Logit-RL Hybrid, and QL-AC Hybrid present the proportion of data best explained by each model configuration when the four models are compared. The top model (highest percentage) from each row is then included in a final comparison, with the results displayed in the 'Top Model Frequency' column. All models are based on the action 'Pedagogical Knowledge.' The QL-AC Hybrid model combines predictions from an Actor-Critic model with a Q-learning model that shares the same action and reward components."

# Load the MATLAB files for the comparative models
comp_models <- list(
  readMat('CBM/zearn_results/comp_results/hbi/hbi_compare_3.mat'),
  readMat('CBM/zearn_results/comp_results/hbi/hbi_compare_7.mat'),
  readMat('CBM/zearn_results/comp_results/hbi/hbi_compare_51s.mat'),
  readMat('CBM/zearn_results/comp_results/hbi/hbi_compare_13s.mat')
)
BIC_comp <- sapply(comp_models, FUN = function(x) {
  apply(x[["cbm"]][[5]][[11]], 2, mean, na.rm = TRUE)
}) %>% t()
AUC_comp <- sapply(comp_models, FUN = function(x) {
  apply(x[["cbm"]][[5]][[8]], 2, mean, na.rm = TRUE)
}) %>% t()
model_freq <- data.frame(cbind(BIC_comp,AUC_comp))
names(model_freq) <- c(paste0("BIC_",c("Logit", "RL", "Hybrid")),
                       paste0("AUC_",c("Logit", "RL", "Hybrid")))

model_freq$Reward <- c(
  "Struggles", # QL 3
  "Activity", # QL 7
  "Activity", # AC 51
  "Badges" # AC 13
)
model_freq$State <- c(
  NA, # QL
  NA,
  "Struggles, No. Students", # AC 51
  "Struggles, No. Students, Activity" # AC 13
)

# Find the maximum value in each row (excluding the "Top Model Frequency" column)
min_bics <- apply(model_freq[, 1:3], 1, min)
max_aucs <- apply(model_freq[, 4:6], 1, max)

model_freq %>%
  gt(groupname_col = "Reward",
     row_group_as_column = T) %>%
  tab_spanner_delim(delim = "_") %>%
  tab_stubhead(label = "Reward") %>%
  cols_move_to_start(State) %>%
  sub_missing() %>%
  fmt_number(columns = gt::starts_with("BIC"),
              decimals = 1) %>%
  fmt_number(columns = gt::starts_with("AUC"),
              decimals = 3) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = BIC_Logit,
                                   rows = BIC_Logit == min_bics)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = BIC_RL,
                                   rows = BIC_RL == min_bics)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = BIC_Hybrid,
                                   rows = BIC_Hybrid == min_bics)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = AUC_Logit,
                                   rows = AUC_Logit == max_aucs)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = AUC_RL,
                                   rows = AUC_RL == max_aucs)) %>%
  tab_style(style = list(cell_fill(color = brewer.pal(8, "Set2")[6])),
            locations = cells_body(columns = AUC_Hybrid,
                                   rows = AUC_Hybrid == max_aucs)) %>%
  tab_footnote(
    footnote = "Q-learning model in state-free case and Actor-Critic in state-dependent case.",
    locations = cells_column_labels(columns = gt::ends_with("RL"))
  ) %>%
  tab_footnote(
    footnote = "Combines predictions of Logit and RL models through a linear combination.",
    locations = cells_column_labels(columns = gt::ends_with("Hybrid"))
  )
```

### Top Model Selection

@tbl-CBM-second also shows that Q-learning consistently outperforms logistic regression and actor-critic models in BIC scores. As such, we further analyze the fit from the top models from this table to the entire dataset. That is, we compare both Q-learning models and the top actor-critic model with their corresponding logistic regression. The histogram of teacher-specific log-likelihoods in @fig-loglik-histogram reveals that the Q-learning model with "Struggles" as the reward has the lowest BIC scores.

```{r}
#| label: fig-loglik-histogram
#| fig-cap: "Distribution of teacher-specific Bayesian Information Criteria (BIC). The histogram displays the spread of individual teachers' log-likelihoods, with higher values indicating better fit. The dashed line represents the median log-likelihood across all teachers."

# Load the full model data
full_model <- list(
  readMat("CBM/zearn_results/top_results/hbi_compare_3.mat"),
  readMat("CBM/zearn_results/top_results/hbi_compare_7.mat"),
  readMat("CBM/zearn_results/top_results/hbi_compare_51.mat")
)
BIC_full <- sapply(full_model, FUN = function(x) {
  apply(x[["cbm"]][[5]][[11]], 2, mean, na.rm = TRUE)
}) %>% t()
AUC_full <- sapply(full_model, FUN = function(x) {
  apply(x[["cbm"]][[5]][[8]], 2, mean, na.rm = TRUE)
})
# # Extract log likelihoods
# model_freq_full <- full_model[["cbm"]][[5]][[5]]
# responsibility_full <- full_model[["cbm"]][[5]][[2]]
# model_classifier <- responsibility_full > 1/3

loglik_df <- data.frame(
  cbind(full_model[[1]][["cbm"]][[5]][[11]],
        full_model[[2]][["cbm"]][[5]][[11]],
        full_model[[3]][["cbm"]][[5]][[11]])
)
Model = c("Logit", "QL",
          "Logit", "QL",
          "Logit", "AC")
Action = c(rep("Pedagogical Knowledge", 6))
Reward = c("Struggles", "Struggles",
           "Activity", "Activity",
           "Activity", "Activity")
State = c(NA, NA, NA, NA,
          "Struggles, No. Students", "Struggles, No. Students")
names(loglik_df) <- paste0(Model,"_",Reward,"_",State)

loglik_df <- loglik_df %>%
  pivot_longer(cols = everything(),
               names_to = "Model_Reward_State",
               values_to = "BIC") %>%
  separate(Model_Reward_State, c("Model", "Reward", "State"), "_")

# Create a histogram of log likelihoods
ggplot(data = loglik_df, aes(x = BIC, fill = interaction(Model, Reward),
                             color = interaction(Model, Reward))) +
  geom_histogram(alpha = 0.8) +
  scale_fill_manual(values = c("Logit.Struggles" = brewer.pal(8, "Set2")[1],
                               "QL.Struggles" = brewer.pal(8, "Set2")[2],
                               "Logit.Activity" = brewer.pal(8, "Set2")[3],
                               "QL.Activity" = brewer.pal(8, "Set2")[4],
                               "AC.Activity" = brewer.pal(8, "Set2")[5]),
                    labels = c("AC, Activity",
                               "Logit, Activity",
                               "QL, Activity",
                               "Logit, Struggles",
                               "QL, Struggles")) +
  scale_color_manual(values = c("Logit.Struggles" = brewer.pal(8, "Dark2")[1],
                                "QL.Struggles" = brewer.pal(8, "Dark2")[2],
                                "Logit.Activity" = brewer.pal(8, "Dark2")[3],
                                "QL.Activity" = brewer.pal(8, "Dark2")[4],
                                "AC.Activity" = brewer.pal(8, "Dark2")[5]),
                    labels = c("AC, Activity",
                               "Logit, Activity",
                               "QL, Activity",
                               "Logit, Struggles",
                               "QL, Struggles")) +
  theme_minimal() +
  labs(x = "Individual BIC",
       y = "Frequency",
       fill = "Model, Reward",
       color = "Model, Reward") +
  guides(fill = guide_legend(title = "Model, Reward", nrows = 2),
         color = guide_legend(title = "Model, Reward", nrows = 2))

```

## Heterogeneity and Optimality

The spread of BIC scores in @fig-loglik-histogram also implies a diversity in model fit, which warrants further investigation into heterogeneity. We analyzed teacher-specific parameters to capture these individual differences. @fig-params-histogram provides a set of visualizations depicting relationships between teacher-specific parameters and various contextual factors. We observed differences in the cost parameter across poverty levels, with high-poverty schools showing lower cost estimates (a). The learning rate parameter (alpha) also varied across poverty levels (b), with high-poverty schools exhibiting lower learning rates. The number of weeks a teacher used the platform showed a small negative correlation with the estimated cost (r = -0.11, p \< .001) (d), while it positively correlated with the inverse temperature parameter (tau) (r = 0.14, p = 1.5e-09) (e). When examining the relationship between the learning rate, inverse temperature, and student outcomes (c, f), we found a positive correlation between inverse temperature and both average badges (r = 0.17, p \< .001) and average tower alerts (r = 0.1, p \< .001). However, the learning rate did not significantly correlate with average badges earned (r = 0.03, p = 0.23) or average tower alerts (r = 0.0, p = .85).

```{r}
#| label: fig-params-histogram
#| fig-cap: "Histogram of Teacher-Specific Parameters"
#| fig-subcap:
#| - "Poverty x Cost"
#| - "Poverty x Alpha"
#| - "Student Outcomes x Gamma"
#| - "No. of Weeks x Cost"
#| - "No. of Weeks x Tau"
#| - "Student Outcomes x Tau"
#| layout-ncol: 3

# Load the full model data
validsubj <- as.logical(
  unlist(readMat("CBM/data/full_data.mat"))
  )
# Extract log likelihoods
BIC_ind_full <- full_model[[1]][["cbm"]][[5]][[11]]

# Classify each teacher according to the best-fit model
model_classifier <- BIC_ind_full[,1] - BIC_ind_full[,2] >
  median(BIC_ind_full[,1] - BIC_ind_full[,2])
teacher_classification <- ifelse(model_classifier, 2, 1)
logit_params <- full_model[[1]][["cbm"]][[5]][[1]][[1]][[1]]
ql_params <- full_model[[1]][["cbm"]][[5]][[1]][[2]][[1]]
# Make min(teacher_classification) the baseline
load("CBM/data/classrooms_full.RData")
heterogeneity <- classrooms %>%
  cbind(
    data.frame(
      ModelType = factor(teacher_classification - min(teacher_classification),
                         labels = c("Logit", "Q-learning"))
      )) %>%
  cbind(logit_params) %>%
  cbind(ql_params)
names(heterogeneity) <- c(
  "Classroom.ID", "Model",
  paste0("beta", 1:8),
  "alpha", "gamma", "tau", "ev_init", "cost"
  # "alpha_w", "alpha_theta", "gamma", "tau", "theta_init", "w_init", "cost"
)
heterogeneity <- heterogeneity %>%
  # mutate(across(c(alpha, gamma), ~ 1/(1+exp(-.)))) %>%
  # mutate(across(c(alpha_w, alpha_theta, gamma), ~ 1/(1+exp(-.)))) %>%
  # mutate(across(c(tau, cost), ~ exp(.))) %>%
  left_join(
    df %>%
      group_by(Classroom.ID, Teacher.User.ID) %>%
      summarize(
        `Charter School` = mean(charter.school, na.rm = TRUE),
        `Paid Account` = mean(school.account, na.rm = TRUE),
        `Students...Total` = mean(Students...Total, na.rm = TRUE),
        Action = mean(Frobenius.NNDSVD_teacher2>0, na.rm = TRUE),
        Reward = sum(Frobenius.NNDSVD_student1, na.rm = TRUE),
        State = sum(Frobenius.NNDSVD_student3, na.rm = TRUE),
        `Active Students` = sum(`Active.Users...Total`, na.rm = TRUE),
        `Avg. Badges` = sum(`Badges.per.Active.User`, na.rm = TRUE),
        `Avg. Tower Alerts` = sum(`Tower.Alerts.per.Tower.Completion`, na.rm = TRUE),
        Income = unique(income),
        Poverty = unique(poverty),
        `No. of Weeks` = n(),
        n_classes_by_teacher = median(teacher_number_classes),
        grade = first(Grade.Level)
        ), by = "Classroom.ID"
    ) %>%
  mutate(across(c(Income, Poverty), as.ordered)) %>%
  rename(`Total Students` = `Students...Total`,
         `No. of Classes` = n_classes_by_teacher)

# Extract BIC and classify teachers based on median split
full_data <- heterogeneity %>%
  mutate(
    BIC = BIC_ind_full[,2]
  ) %>%
  filter(!is.na(Teacher.User.ID)) %>%
  mutate(
    FitGroup = factor(BIC > median(BIC),
                      levels = c(FALSE, TRUE),
                      labels = c("Bad Fit", "Good Fit"))
  ) %>%
  mutate(
    `Paid Account` = factor(`Paid Account`, levels = c(0, 1),
                            labels = c("No", "Yes"))
  )

# Update color palette
color_palette <- brewer.pal(4, "Set2")
color_palette_dark <- brewer.pal(4, "Dark2")

# Create a transformation for the inverse logit function
transform_inv_logit <- new_transform(
  name = "inv_logit",
  transform = function(x) 1/(1+exp(-x)),
  inverse = function(x) log(x/(1-x))
)


# Create a data frame for correlation tests
cor_tests <- data.frame(
  Variable = c("Avg. Badges", "Avg. Tower Alerts"),
  Alpha_cor = c(
    cor.test(full_data$alpha, full_data$`Avg. Badges`)$estimate,
    cor.test(full_data$alpha, full_data$`Avg. Tower Alerts`)$estimate
  ),
  Alpha_p = c(
    cor.test(full_data$alpha, full_data$`Avg. Badges`)$p.value,
    cor.test(full_data$alpha, full_data$`Avg. Tower Alerts`)$p.value
  ),
  Tau_cor = c(
    cor.test(full_data$tau, full_data$`Avg. Badges`)$estimate,
    cor.test(full_data$tau, full_data$`Avg. Tower Alerts`)$estimate
  ),
  Tau_p = c(
    cor.test(full_data$tau, full_data$`Avg. Badges`)$p.value,
    cor.test(full_data$tau, full_data$`Avg. Tower Alerts`)$p.value
  ),
  Gamma_cor = c(
    cor.test(full_data$gamma, full_data$`Avg. Badges`)$estimate,
    cor.test(full_data$gamma, full_data$`Avg. Tower Alerts`)$estimate
  ),
  Gamma_p = c(
    cor.test(full_data$gamma, full_data$`Avg. Badges`)$p.value,
    cor.test(full_data$gamma, full_data$`Avg. Tower Alerts`)$p.value
  )
)

## Grid

## [1,1:2]

ggplot(data = full_data %>%
         filter(!is.na(Poverty)) %>%
         mutate(cost = exp(cost)),
       aes(x = Poverty, y = cost)) +
  geom_violin(alpha = 0.6, scale = "count", fill = color_palette[3]) +
  geom_boxplot(width = 0.025, outlier.shape = NA) +
  theme_minimal() +
  labs(x = "Poverty", y = "Cost") +
  stat_compare_means(label = "p.signif",
                     comparisons = list(
                       c("0-40% (Low)", "40-75% (Mid-High)"),
                       c("0-40% (Low)", "75%+ (High)"),
                       c("40-75% (Mid-High)", "75%+ (High)")))

ggplot(data = full_data %>%
         filter(!is.na(Poverty)) %>%
         mutate(alpha = 1/(1+exp(-alpha))),
       aes(x = Poverty, y = alpha)) +
  geom_violin(alpha = 0.6, scale = "count", fill = color_palette[4]) +
  geom_boxplot(width = 0.025, outlier.shape = NA) +
  # coord_trans(y = "log") +
  theme_minimal() +
  labs(x = "Poverty", y = "Alpha") +
  stat_compare_means(label = "p.signif",
                     comparisons = list(
                       c("0-40% (Low)", "40-75% (Mid-High)"),
                       c("0-40% (Low)", "75%+ (High)"),
                       c("40-75% (Mid-High)", "75%+ (High)")))

## [1,3]

# ggplot(data = full_data, aes(x = 1/(1+exp(-alpha)))) +
#   geom_point(aes(y = `Avg. Badges`),
#              color = color_palette[2], alpha = 0.1) +
#   geom_smooth(aes(y = `Avg. Badges`), method = "lm",
#               color = color_palette[2], fill = color_palette[2]) +
#   geom_point(aes(y = `Avg. Tower Alerts`),
#              color = color_palette[1], alpha = 0.1) +
#   geom_smooth(aes(y = `Avg. Tower Alerts`), method = "lm",
#               color = color_palette[1], fill = color_palette[1]) +
#   scale_x_continuous(name = "Alpha", transform = "logit") +
#   scale_y_continuous(name = "Avg. Badges / Tower Alerts") +
#   theme_minimal() +
#   labs(color = "Variable") +
#   geom_text(data = cor_tests[1,],
#             aes(x = min(1/(1+exp(-full_data$alpha))),
#                 y = mean(full_data$`Avg. Badges`),
#                 label = paste0("r = ", round(Alpha_cor, 2),
#                                ", p = ", signif(Alpha_p, 2))),
#             color = color_palette[2], vjust = -1.5, hjust = 0) +
#   geom_text(data = cor_tests[2,],
#             aes(x = min(1/(1+exp(-full_data$alpha))),
#                 y = mean(full_data$`Avg. Tower Alerts`),
#                 label = paste0("r = ", round(Alpha_cor, 2),
#                                ", p = ", signif(Alpha_p, 2))),
#             color = color_palette[1], vjust = -1.5, hjust = 0)

ggplot(data = full_data, aes(x = exp(gamma))) +
  geom_point(aes(y = `Avg. Badges`), size = 0.4,
             color = color_palette[2], alpha = 0.19,
             position = position_jitter(width = 0.1, height = 0)) +
  geom_smooth(aes(y = `Avg. Badges`), method = "lm",
              color = color_palette[2], fill = color_palette[2]) +
  geom_point(aes(y = `Avg. Tower Alerts`), size = 0.4,
             color = color_palette[1], alpha = 0.19,
             position = position_jitter(width = 0.1, height = 0)) +
  geom_smooth(aes(y = `Avg. Tower Alerts`), method = "lm",
              color = color_palette[1], fill = color_palette[1]) +
  scale_x_continuous(name = "Gamma", transform = "log") +
  scale_y_continuous(name = "Avg. Badges / Tower Alerts") +
  theme_minimal() +
  labs(color = "Variable") +
  geom_text(data = cor_tests[1,],
            aes(x = min(exp(full_data$gamma)),
                y = mean(full_data$`Avg. Badges`),
                label = paste0("r = ", round(Gamma_cor, 2),
                               ", p ", ifelse(Gamma_p > 0.001,
                                                paste0("= ", signif(Gamma_p, 2)),
                                                " < 0.001"))),
            color = color_palette_dark[2], vjust = 1.5, hjust = 0) +
  geom_text(data = cor_tests[2,],
            aes(x = min(exp(full_data$gamma)),
                y = mean(full_data$`Avg. Tower Alerts`),
                label = paste0("r = ", round(Gamma_cor, 2),
                               ", p ", ifelse(Gamma_p > 0.001,
                                                paste0("= ", signif(Gamma_p, 2)),
                                                " < 0.001"))),
            color = color_palette_dark[1], vjust = 1.5, hjust = 0)


# [2,1:2]

ggplot(data = full_data, aes(x = `No. of Weeks`, y = exp(cost))) +
  geom_point(alpha = 0.2, position = position_jitter(width = 0, height = 0.01)) +
  geom_smooth(method = "lm", color = color_palette[3], fill = color_palette[3]) +
  theme_minimal() +
  labs(x = "No. of Weeks", y = "Cost") +
  geom_text(aes(x = max(`No. of Weeks`), y = min(exp(cost)),
                label = paste0("r = ",
                               round(cor.test(full_data$`No. of Weeks`,
                                              full_data$cost)$estimate, 2),
                               ", p ",
                               ifelse(cor.test(full_data$`No. of Weeks`,
                                               full_data$cost)$p.value > 0.001,
                                                paste0("= ", signif(
                                                  cor.test(full_data$`No. of Weeks`,
                                                           full_data$cost)$p.value, 2)),
                                                " < 0.001"))),
                color = color_palette_dark[3],
            vjust = -1.5, hjust = 1)

ggplot(data = full_data, aes(x = `No. of Weeks`, y = exp(tau))) +
  geom_point(alpha = 0.2, position = position_jitter(width = 0, height = 0.1)) +
  geom_smooth(method = "lm", color = color_palette[4], fill = color_palette[4]) +
  scale_y_continuous(trans = "log", breaks = c(1, 2, 4, 8, 16, 32)) +
  theme_minimal() +
  labs(x = "No. of Weeks", y = "Tau") +
  geom_text(aes(x = max(`No. of Weeks`), y = min(exp(tau)),
                label = paste0("r = ", round(cor.test(full_data$`No. of Weeks`,
                                                      full_data$tau)$estimate, 2),
                               ", p ",
                               ifelse(cor.test(full_data$`No. of Weeks`,
                                               full_data$tau)$p.value > 0.001,
                                                paste0("= ", signif(
                                                  cor.test(full_data$`No. of Weeks`,
                                                           full_data$tau)$p.value, 2)),
                                                " < 0.001"))),
                color = color_palette_dark[4],
            vjust = -1.5, hjust = 1)

# ggplot(data = full_data %>% mutate(gamma = 1/(1+exp(-gamma))),
#        aes(x = `Paid Account`, y = gamma)) +
#   geom_violin(alpha = 0.6, scale = "count", fill = color_palette[3]) +
#   geom_boxplot(width = 0.05, outlier.shape = NA) +
#   theme_minimal() +
#   labs(x = "Paid Account", y = "Gamma") +
#   stat_compare_means(comparisons = list(c("No", "Yes")), label = "p.signif")
#
# ggplot(data = full_data %>% mutate(cost = exp(cost)),
#        aes(x = `Paid Account`, y = cost)) +
#   geom_violin(alpha = 0.6, scale = "count", fill = color_palette[3]) +
#   geom_boxplot(width = 0.05, outlier.shape = NA) +
#   theme_minimal() +
#   labs(x = "Paid Account", y = "Cost") +
#   stat_compare_means(comparisons = list(c("No", "Yes")), label = "p.signif")

# [2,3]

ggplot(data = full_data, aes(x = exp(tau))) +
  geom_point(aes(y = `Avg. Badges`), size = 0.4,
             color = color_palette[2], alpha = 0.19,
             position = position_jitter(width = 0.1, height = 0)) +
  geom_smooth(aes(y = `Avg. Badges`), method = "lm",
              color = color_palette[2], fill = color_palette[2]) +
    geom_point(aes(y = `Avg. Tower Alerts`), size = 0.4,
             color = color_palette[1], alpha = 0.19,
             position = position_jitter(width = 0.1, height = 0)) +
  geom_smooth(aes(y = `Avg. Tower Alerts`), method = "lm",
              color = color_palette[1], fill = color_palette[1]) +
  scale_x_continuous(name = "Tau", transform = "log",
                     breaks = c(1, 2, 4, 8, 16, 32)) +
  scale_y_continuous(name = "Avg. Badges / Tower Alerts") +
  theme_minimal() +
  labs(color = "Variable") +
  geom_text(data = cor_tests[1,],
            aes(x = min(exp(full_data$tau)),
                y = mean(full_data$`Avg. Badges`),
                label = paste0("r = ", round(Tau_cor, 2),
                               ", p ", ifelse(Tau_p > 0.001,
                                                paste0("= ", signif(Tau_p, 2)),
                                                " < 0.001"))),
            color = color_palette_dark[2], vjust = 0.5, hjust = 0) +
  geom_text(data = cor_tests[2,],
            aes(x = min(exp(full_data$tau)),
                y = mean(full_data$`Avg. Tower Alerts`),
                label = paste0("r = ", round(Tau_cor, 2),
                               ", p ", ifelse(Tau_p > 0.001,
                                                paste0("= ", signif(Tau_p, 2)),
                                                " < 0.001"))),
            color = color_palette_dark[1], vjust = 0.5, hjust = 0)

```

We further investigated the heterogeneity in teacher behavior by comparing classrooms based on the relative fit of the logit and Q-learning models. Classrooms were classified into two groups using a median split of the distribution of the differences in Bayesian Information Criterion (BIC) scores between the two models, i.e., BIC(logit) - BIC(Q-learning). Q-learning demonstrated a greater advantage over the logistic regression model in classrooms with a higher number of weeks and average weekly student activity (see @tbl-CBM-teachers). Additionally, classrooms were divided into 'Good Fit' and 'Bad Fit' groups based on a median split of their Q-learning BIC scores. Classrooms in the 'Good Fit' group exhibited higher student activity and overall weekly usage by teachers (see @tbl-CBM-teachers-full).

To understand what makes a teacher effective in helping students complete their lessons, we examined teachers' performance across the parameters from the previous hierarchical model. We were especially interested in the learning rate ("Alpha") and the inverse temperature ("Tau") because of their potential impact on behavior adaptation and decision consistency. We found that tau, but not alpha, correlated positively to the number of weekly Badges earned by the average student, i.e., average lesson completion, and Tower Alerts, i.e., students struggling with the material. Further, the starting Q-values negatively correlated with Badges, and the discount factor ("Gamma") was positively correlated with Tower Alerts (see @tbl-optimality and @tbl-optimality-2 for the models controlling the number of active students, the number of classes taught by the teacher, the grade level, the number of weeks, the poverty level, the income level, whether the school is a charter school, and whether the school has a paid account).

We then regressed classroom characteristics on the estimated model parameters and found several notable relationships (see @fig-heterogeneity-reg). The cost parameter was negatively associated with income level (b = -0.13, p \< .05) and the number of weeks (b = -0.021, p \< .001), and positively associated with poverty level (b = 0.18, p \< .01). The learning rate (alpha) was positively correlated with income level (b = 0.094, p \< .05) and negatively correlated with poverty level (b = -0.1, p \< .05). The inverse temperature (tau) was negatively related to income level (b = -0.18, p \< .001) and positively related to having a paid account (b = 0.24, p \< .001) and the number of weeks (b = 0.016, p \< .01). The discount factor (gamma) was negatively associated with the number of weeks (b = -0.016, p \< .01). Lastly, the initial Q-value was positively associated with income level (b = 0.17, p \< .001) and negatively associated with having a paid account (b = -0.16, p \< .05).

<!-- ## Posterior Predictive Checks -->

```{r}
#| eval: false
#| label: fig-timelines
#| fig-cap: ""

# Load the data
hbi_ac_data <- readMat('CBM/zearn_results/top_results/hbi_ac_data.mat')
model_fit <- readMat('CBM/zearn_results/top_results/hbi_ac.mat')
teacher_specific_params <- model_fit[["cbm"]][[5]][[1]][[1]][[1]]

simulate_predictions <- function(parameters, subj) {
  alpha_w <- 1 / (1 + exp(-parameters[1]))  # Learning rate for w (critic)
  alpha_theta <- 1 / (1 + exp(-parameters[2]))  # Learning rate for theta (actor)
  gamma <- 1 / (1 + exp(-parameters[3]))  # Discount factor
  tau <- exp(parameters[4])
  theta_init <- parameters[5]
  w_init <- parameters[6]
  cost <- exp(parameters[7])

  # Unpack data
  Tsubj <- length(subj[[1]][[2]])
  choice <- subj[[1]][[2]]
  outcome <- subj[[1]][[5]]
  state <- cbind(1, as.matrix(subj[[1]][[7]]))
  week <- subj[[1]][[9]][[1]]

  # Initialize
  w <- w_init
  theta <- theta_init
  log_probabilities <- rep(0, Tsubj)

  # Loop through trials
  for (t in 1:Tsubj) {
    if (week[t] == 0) break  # End loop if week is zero
    s <- state[t, ]
    a <- choice[t]
    o <- outcome[t]

    # Actor: Compute policy (log probability of taking the action)
    product <- sum(s * theta * tau)
    # Handle numerical issues for large values of theta
    if (product < -8) {
      log_probabilities[t] <- log_probabilities[t] - product * a
    } else if (product > 8) {
      log_probabilities[t] <- log_probabilities[t] + (1 - product) * (1 - a)
    } else {
      log_probabilities[t] <- log_probabilities[t] - log(1 + exp(-product)) * a - log(1 + exp(product)) * (1 - a)
    }

    # Critic: Compute TD error (delta)
    delta <- (o - cost)  # Basic TD error, without considering future states

    # Update weights
    theta <- theta + alpha_theta * gamma ^ (week[t] - week[1]) * (tau * s) / (1 + exp(product)) * delta
    w <- w + alpha_w * s * delta
  }

  # Sum log probabilities to get log-likelihood
  log_likelihood <- sum(log_probabilities)

  return(list(predictions = choice, log_likelihood = log_likelihood))
}

# Prepare the data for plotting (replace with actual variables and data)
long_df <- as.data.frame(hbi_ac_data$Variable) %>%
  mutate(Week = rep(1:nrow(.), each = ncol(.)),
         Value = as.vector(t(.)))

# Simulate predictions using group mean parameters
predictions <- simulate_predictions(group_mean_transformed, long_df)

# Variables to plot
variables_to_plot <- names(hbi_ac_data$Variable)

# Create individual plots
plots <- lapply(variables_to_plot, plot_variable, data = long_df)

# Reshape the data to long format
long_df <- df %>%
  select(MDR.School.ID, week, Active.Users...Total, Sessions.per.Active.User,
         Minutes.per.Active.User, Badges.per.Active.User,
         Boosts.per.Tower.Completion, Tower.Alerts.per.Tower.Completion,
         FrobeniusNNDSVD1, FrobeniusNNDSVD2, FrobeniusNNDSVD3,
         poverty, school.account) %>%
  group_by(MDR.School.ID, week, poverty, school.account) %>%
  summarize(across(where(is.numeric), ~ mean(., na.rm = TRUE))) %>%
  gather(key = "Variable", value = "Value",
         -MDR.School.ID, -week, -poverty, -school.account)

# Function to create a plot for each variable with error ribbons
plot_variable <- function(variable_name, var) {
  # Calculate overall average and standard error
  avg_data <- long_df %>%
    filter(Variable == variable_name) %>%
    filter(!is.na({{var}})) %>%
    group_by({{var}}, week) %>%
    summarize(
      Avg = mean(Value, na.rm = TRUE),
      SE = sd(Value, na.rm = TRUE)/sqrt(n()),
      CI_low = Avg - SE,
      CI_high = Avg + SE
    )

  # Calculate 2 standard deviations from the overall data
  y_high <- quantile(avg_data$Value[avg_data$Variable == variable_name],
                     0.85, na.rm = T)
  y_low <- quantile(avg_data$Value[avg_data$Variable == variable_name],
                    0.15, na.rm = T)

  ggplot(avg_data, aes(x = week, y = Avg, group = {{var}})) +
    geom_line(aes(color = {{var}})) +
    geom_ribbon(data = avg_data,
                aes(x = week, y = Avg, group = {{var}}, alpha = 0.2,
                    fill = {{var}}, ymin = CI_low, ymax = CI_high)) +
    theme_minimal() +
    labs(title = variable_name, x = "Time", y = "Value") +
    theme(legend.position = "none") +
    coord_cartesian(ylim = c(y_low, y_high))  # Adjust y-axis range
}

# Variables to plot
variables_to_plot <- c(
  "Active.Users...Total", "Sessions.per.Active.User",
  "Minutes.per.Active.User", "Badges.per.Active.User",
  "Boosts.per.Tower.Completion", "Tower.Alerts.per.Tower.Completion",
  "FrobeniusNNDSVD1", "FrobeniusNNDSVD2", "FrobeniusNNDSVD3"
  )

# Create individual plots
plots <- lapply(variables_to_plot, plot_variable, var = school.account)

# Combine the plots into a panel
do.call(grid.arrange, c(plots, ncol = 3))

```

# Discussion

Our study aimed to unravel the complex and adaptive nature of teacher behavior within the Zearn Math online platform. By leveraging the power of reinforcement learning (RL) models, particularly the hierarchical Q-learning approach, we uncover compelling evidence that teachers are not merely adhering to a fixed set of pedagogical strategies but are actively learning and adapting based on feedback from student performance.

## Characterizing Teacher Behavior

The superior fit of the hierarchical Q-learning model underscores the importance of accounting for individual teacher differences in learning rates and decision-making processes. The model's parameters serve as a window into the diverse ways teachers navigate the digital learning environment, offering a more nuanced understanding than traditional, static educational practice models.

## Impact of Estimated RL Parameters

Crucially, we find that teachers with higher inverse temperature values had students with superior outcomes, indicating more consistent decision-making. This trade-off suggests a teacher's ability to balance using new teaching strategies (exploration) and sticking with known effective strategies (exploitation). @morrison2019 noted that teachers exhibited varied levels of preparedness for implementing the Zearn Math curriculum, with just under half of the teachers reporting feeling adequately prepared for implementation. This lack of preparation could influence how effectively teachers navigate the exploration-exploitation trade-off, impacting student outcomes. In contrast, the starting Q-value is negatively associated with Badges earned, implying that teachers who initially overestimate the value of certain actions may struggle to adapt their strategies to optimize student outcomes.

Interestingly, while the discount factor shows a significant negative association with Badges, this effect disappears when controlling for additional variables. This result highlights the importance of considering the broader context in which teachers operate, such as school characteristics and student demographics. When examining the impact of RL parameters on student struggles, as measured by average weekly Tower Alerts per student, we find that the discount factor is consistently negatively associated with Tower Alerts. This association suggests that teachers who place greater value on future rewards are more effective in preventing student struggles.

These findings align with qualitative evidence from @knudsen2020, where teachers expressed the value of Zearn Math's multifaceted instructional approaches. A 3rd-grade Zearn teacher stated, "I like that Zearn provides several strategies to get to the answer...you see the problems; you see what you need to hit on and stress the first time around." The ability to adapt teaching strategies based on feedback, akin to higher tau values in our RL models, is a valuable attribute in promoting student achievement. However, the relationship between teacher characteristics and student outcomes is complex, with veteran teachers relying more heavily on traditional methods while novice teachers blend innovative and traditional practices [@knudsen2020].

## Influence of Teacher and School Background

Our investigation also reveals the profound influence of socioeconomic factors on teachers' interactions with Zearn Math. Teachers in lower-income and high-poverty schools may perceive higher costs associated with implementing new teaching strategies, possibly due to resource constraints or lack of adequate training, underscoring the challenges in adopting new educational technologies in disadvantaged contexts.

In contrast, teachers in more affluent regions exhibit greater adaptability (learning rate) in adjusting their pedagogical approaches based on student feedback. This finding suggests that access to resources and support plays a crucial role in teachers' ability to effectively use online learning platforms and adapt their teaching methods to meet students' needs.

We also observe that teachers with paid Zearn Math accounts demonstrate more consistent decision-making patterns (higher inverse temperatures). This may be attributed to the structured support and training provided by the platform, which helps teachers navigate the challenges of implementing new technologies and pedagogies.

These findings underscore the complex interplay between socioeconomic factors, school characteristics, and teachers' decision-making processes in the context of online learning platforms. They highlight the need for targeted interventions and support to ensure that all teachers, regardless of their school's socioeconomic status, have the resources and training necessary to effectively adapt their teaching strategies and promote student success.

## Implications for Teachers and Schools

Our Reinforcement Learning model highlights the dynamic, adaptive nature of teaching and the need for ongoing professional development and supportive learning environments. The heterogeneity in optimal teaching strategies across educators underscores the importance of personalized training that builds on individual strengths. Policymakers and educational leaders should prioritize allocating resources and providing professional development opportunities to teachers, especially in disadvantaged communities.

Moreover, our results shed light on the systemic educational disparities across different school contexts and emphasize the importance of developing targeted policies and interventions to bridge these gaps. By modeling the factors that influence teachers' decision-making and adaptability, RL can inform the design of interventions to enhance student achievement. For instance, interventions could help teachers improve their learning rates or better balance exploration and exploitation in their teaching strategies.

## Future Directions

While our findings contribute significantly to educational technology and teacher behavior analysis, they are not without limitations. While the focus on Zearn Math provides a rich dataset for analysis, it limits the generalizability of our conclusions. Future research should explore the applicability of our findings across different populations, educational platforms, and learning environments. Additionally, our study opens new avenues for integrating RL models with other analytical approaches, such as machine learning and large language models, to further enhance our understanding of effective teaching strategies in digital contexts.

Future research holds immense potential to explore the applicability of advanced RL models across diverse educational contexts and populations. They should also integrate a broader spectrum of variables, including teacher background and training, to enrich the understanding of teaching and learning's multifaceted nature. Additionally, exploring novel RL models and methodologies promises to uncover more profound insights into the dynamics of educational technologies, guiding the development of more effective and equitable learning environments.

In sum, our study marks a significant stride in applying RL to elucidate the complex, dynamic behaviors of teachers in online learning environments. The insights gleaned from this work pave the way for more personalized, equitable, and evidence-based approaches to digital education, ultimately fostering better outcomes for all students.

# Materials and Methods

| **Step**                 | **Method**                                                                  | **Software/Tools**                                               |
|--------------------------|-----------------------------------------------------------------------------|------------------------------------------------------------------|
| Data Preprocessing       | Cleaning, normalization                                                     | R [@rcoreteam2024]                                               |
| Dimensionality Reduction | Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF) | Python (`scikit-learn`) [@pedregosa2011]                         |
| Feature Selection        | Regression analysis                                                         | R (`fixest` package) [@berge2018]                                |
| Analytical Methods       | Q-learning, Actor-Critic Model Estimation                                   | R, Matlab (`CBM` package for Laplace approximation) [@piray2019] |
| Statistical Analysis     | Hierarchical Bayesian Inference                                             | Matlab (`CBM` package for expectation-maximization algorithm)    |
| Model Evaluation         | Heterogeneity analyses of model performance across teachers                 | R                                                                |

: Analytical steps employed in the study. {#tbl-methods}

## Data

Zearn provided administrative data for teachers and students, spanning across the 2020-2021 academic year. Teacher activity is time-stamped to the second and includes the time spent on the platform and specific actions taken. On the other hand, student data is aggregated at the classroom-week level due to data privacy considerations. As such, we aggregated the teacher data to the classroom-week unit of analysis. This level of granularity still enables us to capture the temporal dynamics of teacher-student interactions and their subsequent influence on student achievement.

```{r}
#| label: tbl-summary
#| tbl-cap: "Summary statistics by school. The table presents the mean, median, standard deviation (SD), minimum, and maximum values for the number of teachers, total students, and average weeks of active engagement (across all classrooms within a school)."

gt_school_sum

```

The dataset includes `r N_class_pre` classrooms and `r N_teachers_pre` educators, with an average of `r avg_std_pre` students per classroom. Classrooms and teachers are also linked to a school, and @tbl-summary provides a summary of the number of students, teachers, and weeks per school (see also @fig-income-dist for the distributions of school median poverty and income levels).

### Preprocessing and Exclusion criteria

We focus our analysis on the teachers who most likely take advantage of a wide range of resources on the platform. Thus, we selected teachers who consistently use the platform and work in traditional school settings. First, we selected virtual classrooms with at least five active students weekly, filtering out parents or tutors who may use Zearn outside the classroom setting. We also removed teachers with more than four classrooms and those who logged in for less than 16 weeks (@fig-classroom-weeks reveals that a non-negligible number of classrooms has less than 3 to 4 months of data). Finally, we excluded classrooms in the 6th to 8th grades, as they represent only a small proportion of the data. @tbl-classroom-summary summarizes the refined dataset, providing a snapshot of the key variables of interest. Their means and standard deviations (SD) are computed for each grade level and overall (across all grades).

```{r}
#| label: fig-classroom-weeks
#| fig-cap: "Histogram of number of weeks of data per classroom. Most classrooms include a full year (52 weeks) of data. A smaller but significant subset of classrooms has less than 18 weeks of data. The dashed line acts as a threshold that excludes a notable segment of classrooms from further analysis. The lack of data between these two peaks suggests distinct patterns of usage. Some classrooms consistently use the platform throughout the academic year, while others show sporadic engagement, possibly reflecting trial periods or intermittent usage."

fig_classroom_weeks

```

```{r, results='asis'}
#| label: tbl-classroom-summary
#| tbl-cap: "Classroom engagement metrics by grade level. The table presents the means and standard deviations (in parentheses) of the following averages: minutes spent on the platform per student per week, badges earned per student per week (indicating lesson completion), Tower Alerts per lesson completion (indicating student struggle), and minutes spent on the platform per teacher per week."

gt_classroom_sum

```

## Operationalizing Actions and Rewards

### Teacher Actions

Teacher actions encompass a broad spectrum, from platform log-ins to resource downloads and specific instructional activities. @tbl-teacher-variables provides a list of the actions available in the data.

### Rewards (Student Actions)

In reinforcement learning (RL) models, reward and state variables capture the dynamics of the environment in which learning and decision-making occur. The Zearn platform provides a rich set of student activity and performance data that can be used to define these variables, offering a quantifiable snapshot of classroom engagement and learning challenges.

State variables represent the current condition or situation of the learning environment, providing the context for the agent's decision-making. Reward variables, on the other hand, quantify the desirability of the outcomes resulting from the agent's actions, serving as feedback signals that guide the learning process.

In this study, we take an agnostic approach, allowing the following student variables to be treated as either reward or state variables depending on our RL model specification:

1.  "Active Students": This variable represents the number of students actively logging in to complete digital lessons within a given week [@zearn2022].

-   As a state variable, "Active Students" provides information about the current level of student participation, which can influence a teacher's decision.
-   As a reward variable, a high number of active students could be considered a positive outcome, indicating successful student engagement.

2.  "Student Logins": This variable tallies the frequency of students entering the platform, potentially serving as an engagement metric [@zearn2024].

-   As a state variable, "Student Logins" offers insights into the consistency of student participation, another aspect of the learning environment state.
-   As a reward variable, a high frequency of student logins could be viewed as a positive outcome, reflecting consistent student engagement.

3.  "Badges (on grade)" and "Badges": These metrics reflect the number of new lessons completed weekly at students' grade level and in general [@zearn2024a].

-   As state variables, these metrics provide information about the current level of student progress through the curriculum.
-   As reward variables, accumulating badges can serve as a positive signal, indicating students' mastery of the curriculum.

4.  "Minutes per active student": This variable measures students' time on the platform, potentially correlating with their focus and learning progress [@zearn2022].

-   As a state variable, "Minutes per active student" offers insights into the current level of student engagement and time spent on learning activities.
-   As a reward variable, achieving or exceeding the expected minutes can be considered a positive outcome, while falling short may be viewed as a negative outcome.

5.  "Tower Alerts": These alerts signal instances when students repeatedly encounter difficulties within the same lesson [@zearn2024b].

-   As state variables, "Tower Alerts" provide information about the current level of student struggle and areas where additional support may be needed.
-   As reward variables, "Tower Alerts" could be viewed as a negative signal, indicating that the current teaching strategies may not be effective in addressing student difficulties.

By allowing models that use these variables as either reward or state variables, we can investigate which configuration of action, reward, and states best capture teacher decision-making on Zearn.

### Dimensionality Reduction

First, we standardized the dataset by z-scoring the variables of interest at the school level (using school-wide means and standard deviations). We performed NMF and evaluated the data's reconstruction accuracy and cluster separation using, respectively, the sum of squared residuals (a measure of the difference between the original data and the reconstructed data) and silhouette scores (a measure of how similar an object is to its cluster compared to other clusters [@rousseeuw1987]).

We calculate the silhouette score with the formula $(b - a) / \max(a, b)$, where $a$ is the average distance within a cluster and $b$ is the average distance to the nearest neighboring cluster. This score ranges from -1 to 1, with higher values indicating a data point is well-matched to its cluster and poorly matched to neighboring clusters.

### Nonnegative Matrix Factorization (NMF) Methodology

Let the original matrix ($\mathbf{X}$) be a detailed description of all the teachers' (or students') behaviors. Each row in the matrix represents a unique teacher-week (or classroom-week), and each column represents a specific behavior or action. The entry in a specific row and column corresponds to the frequency of that behavior for that particular teacher-week (or classroom-week). We then estimate $\mathbf{X} \simeq \mathbf{W}\mathbf{H}$, such that we minimize the following:

$$
\left\| \mathbf{X} - \mathbf{W}\mathbf{H} \right\| , \mathbf{W} \geq 0, \mathbf{H} \geq 0.
$$

We used two different loss functions (Frobenius norm and Kullback-Leibler divergence) and two different initialization methods (nonnegative double singular value decomposition (NNDSVD) and NNDSVD with zeros filled with the average of the input matrix (NNDSVDA)). The resulting matrices are:

1.  Basis Matrix ($\mathbf{W}$): This matrix represents underlying behavior patterns. Each column is a "meta-behavior" or a group of behaviors occurring together.
2.  Mixture Matrix ($\mathbf{H}$): This matrix shows the extent to which each "meta-behavior" is present in each teacher-week (or classroom-week). Each entry in this matrix represents the contribution of a "meta-behavior" to a particular behavior present in the data.

These matrices can reveal underlying patterns of behaviors (from the basis matrix) and how these patterns are mixed and matched in different teachers (from the mixture matrix). It allows us to assess the method's performance under varying configurations, with the sum of squared residuals and silhouette scores for comparison.

### Feature Selection

The general model formulation for state-free and state-based scenarios is as follows:

#### State-Free Model

```{=tex}
\begin{align*}
\text{Action}_t =& \ \sum_{i=1}^{L} \left( \beta_{i} R_{t-i} + \gamma_i \text{Action}_{t-i} + \sum_{j=i}^{L} \delta_{ij} (R_{t-i} \times \text{Action}_{t-j}) \right) \\
& + \mu_{\text{Teacher}} + \lambda_{\text{Week}} + \epsilon_t
\end{align*}
```

## Reinforcement Learning Model Estimation

For the initial model selection, we fit the data from approximately 10 percent (190) classrooms due to computational constraints. After selecting the optimal model, we scale up the analysis to include all classrooms.

We adopt the Hierarchical Bayesian Inference (HBI) framework, as described by @piray2019a, to assess the fitness of our RL models and estimate their respective parameters across subjects. This approach uses Laplace approximations for efficient computation of posteriors by approximating the integrals involved in Bayesian inference. Subsequently, it leverages population-level distributions to refine individual parameter variation. Within this framework, we assume that for any given model $m$ and subject $n$, the individual parameters ($h_{m,n}$) are normally distributed across the population with $h_{m,n} \sim N(\mu_m, \Sigma_m)$, where $\mu_m$ and $\Sigma_m$ represent the vector of means and the variance-covariance matrix of the distribution over $h_{m,n}$, respectively.

We use an expectation-maximization algorithm, iteratively performing the following two steps:

1.  Expectation Step: The algorithm calculates a posteriori estimates of the individual parameters ($h_{m,n}$) based on the existing group-level distributions.
2.  Maximization Step: The algorithm refines the group-level parameters ($\mu_m$ and $\Sigma_m$) using current individual parameter estimates. The updated mean group parameter $\mu_m$ is computed as the average of subject-level mean estimates across all subjects, $\mu_m = \frac{1}{N}\sum_{n}h_{m,n}$, where $N$ is the total number of subjects.

With this approach, we can estimate the log-likelihood for each subject's data, given the proposed models and parameter estimates. Recognizing the constraint of normality, we transform the initial estimates to generate constrained model parameters (e.g., the learning rate and discount factor in the Q-learning model). For parameters within a $(0,1)$ interval, we use the inverse logit function transform, $\text{Logit}^{-1}(x)=1/(1+e^{-x})$, and for intrinsically non-negative parameters, we use an exponential transformation. Consequently, HBI estimates the following unconstrained parameters:

1.  Q-learning:
    -   Learning Rate: $\text{Logit}(\alpha)=\log(\frac{\alpha}{1-\alpha})$
    -   Discount Rate: $\text{Logit}(\gamma)=\log(\frac{\gamma}{1-\gamma})$
    -   Inverse Temperature: $\log(\tau)$
    -   Cost: $\log(\text{cost})$
2.  Actor-Critic:
    -   Weights $w$ and $\theta$: $\log(w)$, $\log(\theta)$
    -   Learning rates ($\alpha_w$, $\alpha_\theta$): $\text{Logit}(\alpha_w)=\log(\frac{\alpha_w}{1-\alpha_w})$, $\text{Logit}(\alpha_\theta)=\log(\frac{\alpha_\theta}{1-\alpha_\theta})$
    -   Discount Factor: $\text{Logit}(\gamma)=\log(\frac{\gamma}{1-\gamma})$
    -   Inverse Temperature: $\log(\tau)$
    -   Initial Values: $\theta_{\text{init}}$, $w_{\text{init}}$
    -   Costs: $\log(\text{cost})$
3.  Logistic Regression Model:
    -   Parameters: $\beta$

### Top Model Selection

We determined the best-fit model from our set of candidates by considering each model's Bayesian Information Criterion (BIC). To do so, we first computed the log-likelihood of the data given the estimated model parameters obtained from the Hierarchical Bayesian Inference (HBI) procedure. We then computed the average of the BICs across subjects for each model using the formula:

$$
\overline{\text{BIC}} = \frac{1}{N} \sum_{i=1}^{N} \left( -2 \cdot \text{log-likelihood}_i + p \cdot \log(T_i) \right)
$$

where $N$ is the number of subjects, $p$ is the number of model parameters, $T_i$ is the number of data points (trials) for subject $i$, and $\text{log-likelihood}_i$ is the log-likelihood of the data for subject $i$ given the estimated model parameters.

### Heterogeneity Analysis

After selecting the top-performing model, we re-estimated the parameters using the entire dataset of 3029 classrooms. We explore the heterogeneity across schools and teachers by analyzing individual and group-level parameters as follows:

-   Classification of Individual Responses: Using the hierarchical Bayesian framework, we assign each teacher to the model that best captures their behavior, recognizing the individual differences that emerge from our population-level analysis.

-   Parameter Estimation Across Models: We estimate individual-specific parameters for each teacher and use them as a behavioral profile.

-   Analysis of Group-Level Trends: By aggregating the teacher data at the school level, we identify patterns and trends beyond individual variation. This aggregation allows us to investigate the influence of collective attributes (e.g., school income levels and classroom size) on educational outcomes and teacher performance.

-   Investigation of Influential Variables: We are particularly interested in how socioeconomic factors, such as zipcode median income, may impact the models' ability to describe teacher behavior. These variables provide insights into the heterogeneity in teacher classification and parameter estimates, offering a deeper understanding of the complex interplay between educational resources and pedagogical success.

# References

::: {#refs}
:::

{{< pagebreak >}}

# Supplemental Information {.appendix}

## Supplemental Methods {.appendix}

### PCA vs. NMF

Principal Component Analysis (PCA) was our first methodological choice. It is widely utilized but assumes data normality [@jolliffe2016] and maximizes variance explained, potentially overlooking subtle relationships between variables. Consequently, we also employed NMF, which, by contrast, imposes a non-negativity constraint and is more closely related to clustering algorithms, creating a more interpretable, sparse representation of behaviors [@ding2005; @lee1999]. This technique is particularly advantageous for data representing counts or frequencies. By trying different techniques, we can explore the reduced-dimension representation best suited to our specific dataset and research questions.

### Temporal Dynamics

Our investigation into temporal dynamics confirmed the impact of lagged rewards and actions on decision-making: shaping future decisions by past experiences. @fig-panel-bic illustrates this relationship, showcasing the predictive accuracy and model fit across fixed-effects models with different lags, with BIC and AUC scores for the models with one-week lags as the baseline. The results suggest a preference for a lag of two periods as optimal, based on the "elbow" in the AUC curves and the minima in the BIC curves.

```{r}
#| label: fig-panel-bic
#| fig-cap: "BIC and AUC variations across lags for fixed-effects panel logistic regression models. The plots show the percent change in model prediction accuracy (AUC) and fit (BIC) for different lag periods compared to the one-week lag baseline. The thin lines represent the percent change for each combination of reward functions and methods, while the dashed gray lines represent their average. The shaded bands around the average lines indicate the standard error. The optimal lag period can be determined based on the 'elbow' in the AUC curves (where increasing lags yields diminishing improvements in AUC) and the minima in the BIC curves (lower BIC indicates better model fit when penalizing for complexity)."
#| fig-subcap:
#|   - "BIC state-free"
#|   - "AUC state-free"
#|   - "BIC state-dependent"
#|   - "AUC state-dependent"
#| layout-ncol: 2

load("Regressions/fe-results.RData")

fe_results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = "None",
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
}))

teachers <- Reduce(intersect, sapply(results, function(x) {
  names(x$recoef$Teacher.User.ID)
}))

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    # State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         # bic = bic - bic_base,
         bic = bic/bic_base - 1,
         auc_base = auc[which(Lag == 1)],
         # auc = auc - auc_base,
         auc = auc/auc_base - 1) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "Percent Change in BIC",
                    "auc" = "Percent Change in AUC")

  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(Reward, Method),
                               # group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    scale_color_brewer(palette = "Set2") +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = brewer.pal(n = 8, name = "Set2")[8], alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, linetype = "dashed",
              color = brewer.pal(n = 8, name = "Dark2")[8]) +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    scale_y_continuous(labels = percent_format()) +
    theme(axis.text.x = element_text(hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#------------------

load("Regressions/fe-state-results.RData")

fe_results_df <- fe_results_df %>%
  rbind(
    do.call(rbind, lapply(results, function(x) {
      data.frame(
        Method = x$Method,
        Lag = x$Lag,
        State = x$State,
        Reward = x$Reward,
        auc = x$AUC,
        bic = x$bic,
        stringsAsFactors = FALSE
        )
      }))
    )

teachers <- intersect(
  teachers, Reduce(intersect, sapply(results, function(x) {
    names(x$recoef$Teacher.User.ID)
    }))
  )

results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(
    Method = x$Method,
    Lag = x$Lag,
    State = x$State,
    Reward = x$Reward,
    auc = x$AUC,
    bic = x$bic,
    stringsAsFactors = FALSE
  )
})) %>%
  group_by(Method, Reward, State) %>%
  mutate(bic_base = bic[which(Lag == 1)],
         # bic = bic - bic_base,
         bic = bic/bic_base - 1,
         auc_base = auc[which(Lag == 1)],
         # auc = auc - auc_base,
         auc = auc/auc_base - 1) %>%
  ungroup()

results_df_se <- results_df %>%
  group_by(Lag) %>%
  summarise(se_bic = sd(bic, na.rm = TRUE) / sqrt(n()),
            bic = mean(bic, na.rm = TRUE),
            se_auc = sd(auc, na.rm = TRUE) / sqrt(n()),
            auc = mean(auc, na.rm = TRUE))

generate_plots_with_se <- function(data, data_se, metric_name, metric_se_name) {
  y_label <- switch(metric_name,
                    "bic" = "Percent Change in BIC",
                    "auc" = "Percent Change in AUC")

  plot <- ggplot() +
    geom_line(data = data, aes(x = Lag, y = !!sym(metric_name),
                               group = interaction(State, Reward, Method),
                               color = Method),
              linewidth = 0.3) +
    scale_color_brewer(palette = "Set2") +
    geom_ribbon(data = data_se,
                aes(x = Lag, ymin = !!sym(metric_name) - !!sym(metric_se_name),
                    ymax = !!sym(metric_name) + !!sym(metric_se_name)),
                fill = brewer.pal(n = 8, name = "Set2")[8], alpha = 0.3) +
    geom_line(data = data_se,
              aes(x = Lag, y = !!sym(metric_name)),
              linewidth = 1, linetype = "dashed",
              color = brewer.pal(n = 8, name = "Dark2")[8]) +
    theme_minimal() +
    labs(y = y_label, x = "Lag") +
    scale_y_continuous(labels = percent_format()) +
    theme(axis.text.x = element_text(hjust = 1),
          legend.title = element_blank(),
          legend.position = "none")
  return(plot)
}

# Example usage for plotting Average BIC with Standard Error
bic_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "bic", "se_bic")
auc_plot_se_st <- generate_plots_with_se(results_df, results_df_se, "auc", "se_auc")

#----------------

bic_plot_se
auc_plot_se
bic_plot_se_st
auc_plot_se_st

```



### Predictors of Choice

```{r}
## Cockburn et al. (2022) logit model:

# Function to run model and extract results
run_model <- function(data, i, j) {
  action_col <- paste0("Frobenius.NNDSVD_teacher", i)
  ev_col <- paste0("ev_", i, j)
  uncertain_col <- paste0("sd_", i, j)

  # Create unit-scaled trial number t and lagged predictors
  data <- data %>%
    group_by(Teacher.User.ID) %>%
    mutate(t = (week - min(week)) / (max(week) - min(week))) %>%
    group_by(Classroom.ID) %>%
    mutate(lag_ev = lag(!!sym(ev_col)),
           lag_sd = lag(!!sym(uncertain_col))) %>%
    mutate(
      complete_obs = sum(!is.na(lag_ev) & !is.na(lag_sd) & !is.na(t))
      ) %>%
  filter(complete_obs >= 6) %>%
    ungroup()

  formula <- as.formula(paste0(action_col, " > 0 ~ ",
                               "1 + (lag_ev + lag_sd) * t + ",
                               "(1 + (lag_ev + lag_sd) * t | Classroom.ID)"))

  model <- glmmTMB(formula, data = data, family = binomial)

  # Extract fixed effects with standard errors
  fixed_effects_se <- tidy(model, effects = "fixed", conf.int = TRUE)

  # Extract random effects
  random_effects <- tidy(model, effects = "ran_vals")

  list(model = model, fixed = fixed_effects_se, random = random_effects)
}

# Determine the number of cores to use
num_cores <- detectCores() - 1  # Use all but one core

# Set up the parallel backend
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run all 16 models in parallel
all_models <- foreach(i = 1:4, .combine = c,
                      .packages = c("glmmTMB", "broom.mixed", "dplyr")) %:%
  foreach(j = 1:4, .combine = c,
          .packages = c("glmmTMB", "broom.mixed", "dplyr")) %dopar% {
    result <- run_model(df_ev_uncertainty, i, j)
    setNames(list(result), paste0(i, "_", j))
  }

# Stop the parallel backend
stopCluster(cl)

# Combine all results with model identifiers
fixed_effects <- map2_dfr(all_models,
                          names(all_models),
                          ~mutate(.x$fixed, model = .y))
random_effects <- map2_dfr(all_models,
                           names(all_models),
                           ~mutate(.x$random, model = .y))

```

```{r}

# Prepare data for plotting
plot_data <- fixed_effects %>%
  mutate(factor = case_when(
    term == "lag_ev" ~ "EV",
    term == "lag_sd" ~ "U",
    term == "lag_ev:t" ~ "EV x t",
    term == "lag_sd:t" ~ "U x t",
    TRUE ~ NA_character_
  )) %>%
  filter(!is.na(factor)) %>%
  mutate(significance = case_when(
    p.value < 0.001 ~ "***",
    p.value < 0.01 ~ "**",
    p.value < 0.05 ~ "*",
    TRUE ~ ""
  ))

random_data <- random_effects %>%
  mutate(factor = case_when(
    term == "lag_ev" ~ "EV",
    term == "lag_sd" ~ "U",
    term == "lag_ev:t" ~ "EV x t",
    term == "lag_sd:t" ~ "U x t",
    TRUE ~ NA_character_
  )) %>%
  filter(!is.na(factor))

# Create a separate plot for each model
plot_list <- map(unique(plot_data$model),
                 function(model_name) {
  model_random_data <- filter(random_data, model == model_name)
  model_plot_data <- filter(plot_data, model == model_name)

  y_max <- max(c(model_plot_data$estimate + model_plot_data$estimate,
                 quantile(model_random_data$estimate, 1, na.rm = TRUE)),
               na.rm = TRUE)
  y_min <- min(c(model_plot_data$estimate - model_plot_data$conf.low,
                 quantile(model_random_data$estimate, 0, na.rm = TRUE)),
               na.rm = TRUE)

  ggplot() +
    geom_violin(data = model_random_data, aes(x = factor, y = estimate),
                alpha = 0.3, bounds = c(y_min,y_max),
                scale = "width") +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_pointrange(data = model_plot_data,
                    aes(x = factor, y = estimate,
                        ymin = conf.low, ymax = conf.high),
                    color = "red", size = 0.2) +
    geom_text(data = model_plot_data,
              aes(x = factor, y = y_max + 0.05 * (y_max - y_min),
                  label = significance),
              vjust = 0) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          panel.grid = element_blank(),
          axis.line = element_line(color = "black"),
          axis.ticks = element_line(color = "black")) +
   labs(x = "Factor", y = "Estimate",
         title = paste("Predictors of Choice -", model_name)) +
    scale_x_discrete(limits = c("EV", "EV x t", "U", "U x t"))
})


```


### Regression Models Coefficient Analyses

Prior to fitting the reinforcement learning (RL) models, we analyzed the regression coefficients in our best-fitting logit models to identify patterns that align with RL principles. Our primary focus was on the relationship between rewards and actions. Their interaction should positively influence future actions for desired outcomes (e.g., lesson completion) and negatively for undesired outcomes (e.g., struggles). Additionally, we examined the effect of current states on strategic action selection and the interaction between states and lagged actions to indicate how actions attain and maintain desired states.

We re-estimated the models with scaled independent variables, allowing for a direct coefficient comparison. In this context, a unit increase in these variables equates to a one standard deviation increase. @tbl-re-estimation-statefree and @tbl-re-estimation summarize these results, presenting a coherent overview of the standardized coefficients and highlighting the significance of interactions between rewards, states, and lagged actions.

The results convey that no singular approach fits all educational contexts. Instead, a spectrum of pedagogical strategies exists, with specific teaching methods aligning more closely with the principles of RL. By focusing on coefficients that display RL-like effects, particularly the interaction term R(t-1) x A(t-2), we identified dominant strategies that include 1) Action: Group Instruction with Reward: Struggles, and 2) Action: Group Instruction with Reward: Activity and State: Badges.

In contrast, @fig-RL-exploration favors models that occupy the upper-left quadrant, indicative of an optimized balance between model complexity and predictive accuracy. Noteworthy configurations include 1) Action: Pedagogical Knowledge with Reward: Activity, and 2) Action: Pedagogical Knowledge with Reward: Activity, State: Number of Students.

These configurations highlight teacher individual differences and support the idea that using a hybrid modeling approach could improve the process of fitting RL.

### Correlations Between Variables

We begin to unveil the intricate relationships among the variables under consideration through a comprehensive correlation analysis, as depicted in @fig-corr. This correlation matrix elucidates the magnitude and direction of associations among variables such as badges earned, minutes spent per student, tower alerts, the number of students, and teacher minutes. These interconnections inform the construction of our reinforcement learning models by suggesting the influence of teacher effort on student achievement. In this correlation matrix, each cell represents the Spearman correlation coefficient between a pair of variables. The color and size of the circles in each cell reflect the strength and direction of the correlation, with blue indicating positive correlations and red indicating negative correlations. The histograms along the diagonal provide a visual representation of the distribution of each variable.

```{r}
#| cache: true
#| label: fig-corr
#| fig-cap: "Correlation coefficients between variables after stardardization"
#| fig-format: png

df_corr <- df  %>%
  select(Badges.per.Active.User,
         Active.Users...Total,
         Minutes.per.Active.User,
         Tower.Alerts.per.Tower.Completion,
         Minutes.on.Zearn...Total) %>%
  rename("Badges" = Badges.per.Active.User,
         "Minutes per Student" = Minutes.per.Active.User,
         "Tower Alerts" = Tower.Alerts.per.Tower.Completion,
         "# of Students" = Active.Users...Total,
         "Teacher Minutes" = Minutes.on.Zearn...Total)

chart.Correlation(df_corr, histogram = TRUE, method = "spearman",
                  pch = 20, cex = 0.5, col = rgb(0, 0, 1, 0.5))

```

## Supplemental Tables {.appendix}

### Teacher Variables

| **Variable**                                                        | **Description**                                                                                                                                                                                                                 |
|---------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| PD Course Guide Download [@zearn2023; @zearn2024c]                  | Detailed agenda for Professional Development (PD) courses focusing on classroom implementation, leadership, supporting diverse learners, using data to inform teaching practices, and accelerating student learning.            |
| PD Course Notes Download [@zearn2023; @zearn2024c]                  | Professional development session notes offering insights into effectively using Zearn's curriculum.                                                                                                                             |
| Curriculum Map Download [@zearn2024d]                               | Detailed outline of learning objectives and content. Presents a sequence of interconnected math concepts across grades, aligning with states' instructional requirements.                                                       |
| Assessments Download [@zearn2024e]                                  | Assessments to evaluate student understanding of the material, including ongoing formative assessments, digital daily checks, and paper-based unit assessments.                                                                 |
| Assessments Answer Key Download [@zearn2024f]                       | Solutions for assessments to aid in grading and feedback. Provides detailed rubrics for mission-level assessments.                                                                                                              |
| Elementary Schedule Download [@zearn2024g]                          | A recommended schedule for elementary school-level Zearn curriculum activities to guide daily and weekly instructional planning, ensuring comprehensive coverage of curriculum content.                                         |
| Grade Level Overview Download [@zearn2024h]                         | Provides a summary of learning objectives, pacing guidance, key grade-level terminology, a list of required materials, and details on the standards covered by each lesson.                                                     |
| Kindergarten Schedule Download [@zearn2024i]                        | Recommended schedules for Kindergarten, supporting structured instruction planning.                                                                                                                                             |
| Kindergarten Mission Download [@zearn2024j]                         | Details interactive activities focused on kindergarten-level concepts and their learning objectives.                                                                                                                            |
| Mission Overview Download [@zearn2024h]                             | Outlines a mission's (i.e., learning module) flow of topics, lessons, and assessments; highlights foundational concepts introduced earlier; lists recently introduced terms and required materials for teacher-led instruction. |
| Optional Homework Download [@zearn2024k]                            | Assignments for additional practice, enhancing student learning outside of class.                                                                                                                                               |
| Optional Problem Sets Download [@zearn2024l]                        | Exercises for extra practice, tailored to reinforce lesson concepts.                                                                                                                                                            |
| Small Group Lesson Download [@zearn2024m]                           | Lessons designed for small-group engagement.                                                                                                                                                                                    |
| Student Notes and Exit Tickets Download [@zearn2024n; \@zearn2024o] | Student notes supplement digital lessons with paper-and-pencil activities. Exit tickets are lesson-level assessments for teachers to monitor daily learning.                                                                    |
| Teaching and Learning Approach Download [@zearn2024p]               | Resources outlining Zearn's pedagogical methods.                                                                                                                                                                                |
| Whole Group Fluency Download [@zearn2024q]                          | Lesson-aligned practice activities to build math fluency through whole-class engagement.                                                                                                                                        |
| Whole Group Word Problems Download [@zearn2024m]                    | Word problem-solving activities intended for collaborative, whole-class engagement.                                                                                                                                             |
| Fluency Completed [@zearn2024q]                                     | Indicates teacher completed a fluency activity, typically given to students before their daily digital lessons.                                                                                                                 |
| Guided Practice Completed [@zearn2024r]                             | Indicates teacher completed a guided practice segment, where students learn new concepts. These include videos with on-screen teachers, interactive activities, and paper-and-pencil Student Notes.                             |
| Kindergarten Activity Completed [@zearn2024s]                       | Indicates teacher completed an activity within the Kindergarten curriculum.                                                                                                                                                     |
| Number Gym Activity Completed [@zearn2024t]                         | Indicates teacher completed a Number Gym, an individually adaptive activity that builds number sense, reinforces previously learned skills, and addresses areas of unfinished learning.                                         |
| Tower Completed [@zearn2024b]                                       | Indicates teacher completed a Tower of Power, an activity that requires full mastery of lesson objectives and that students must complete independently.                                                                        |
| Tower Struggled [@zearn2024u]                                       | Indicates teacher committed a mistake when engaging with the Tower of Power activity in a student role, triggering a "boost" (scaffolding remediation).                                                                         |
| Tower Stage Failed [@zearn2024b]                                    | Indicates teacher received three consecutive "boosts" due to repeated errors when engaging with the Tower of Power in a student role.                                                                                           |

: Catalog of Teacher Activities. This table presents teachers' actions, including curriculum engagement, downloads of pedagogical materials, and completion of various interactive components within the Zearn educational platform. {#tbl-teacher-variables}

{{< pagebreak >}}

<!-- FE Logit Model Summary -->

<!-- FE Logit State-free -->

```{r, results='asis'}
#| label: tbl-fe-results-statefree-full
#| tbl-cap: "State-Free Panel Logistic Regression Results"

# Load the results from both subset and restricted analyses
load("Regressions/fe-subset-results.RData")
original_results <- results
load("Regressions/fe-restricted-results.RData")
restricted_results <- results

# Combine the model stats from original and restricted results for comparison
original_models <- do.call(rbind, lapply(original_results, function(x) {
  if (x$Lag != 2) return(NULL)
  data.frame(
    Action = x$Method,
    Reward = x$Reward,
    AUC = x$AUC,
    BIC = x$bic,
    Type = "Original"
  )
}))

# Select top 2 AUC models and top 2 BIC models for both original and restricted
top_models <- original_models %>%
  arrange(desc(AUC)) %>%
  slice_head(n = 2)
top_models <- original_models %>%
  arrange(BIC) %>%
  slice_head(n = 2) %>%
  bind_rows(top_models)

# Prepare the results from selected top models for the table
tidy_fe_results <- lapply(original_results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (!(paste(x$Method, x$Reward) %in%
        paste(top_models$Action, top_models$Reward))) return(NULL)
  temp <- as.data.frame(x$fecoef) %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  as.numeric)) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")

  return(list(
    Model = paste(x$Method, x$Reward,"Full"),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results <- tidy_fe_results[!sapply(tidy_fe_results, is.null)]
tidy_fe_results_restrict <- lapply(restricted_results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (!(paste(x$Method, x$Reward) %in%
        paste(top_models$Action, top_models$Reward))) return(NULL)
  temp <- as.data.frame(x$fecoef) %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  as.numeric)) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")

  return(list(
    Model = paste(x$Method, x$Reward,"Restricted"),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results_restrict <-
  tidy_fe_results_restrict[!sapply(tidy_fe_results_restrict, is.null)]
tidy_fe_results <- c(tidy_fe_results, tidy_fe_results_restrict)

# Combine all model summaries into one dataframe
model_summary <- do.call(rbind, lapply(tidy_fe_results, function(x) {
  x_name <- unlist(strsplit(x[[1]], " "))
  x_data <- x$Data %>%
    rename(Coefficient = Estimate,
           Std_Error = `Std. Error`) %>%
    mutate(
      Significance = case_when(
        `Pr(>|z|)` < .001 ~ "***",
        `Pr(>|z|)` < .01 ~ "**",
        `Pr(>|z|)` < .05 ~ "*",
        TRUE ~ ""),
      Term = gsub(x_name[1], "lag", Term),
      Term = gsub(x_name[2], "rwd", Term),
      Term = gsub("_", "",  Term),
      Term = gsub(":", "_",  Term)) %>%
    add_row(Term = "BIC", Coefficient = x[[3]]) %>%
    add_row(Term = "N", Coefficient = x[[4]])
  x_data$Model <- x[[1]]

  x_data
})) %>%
  dplyr::select(Model, Term, Coefficient, Std_Error, Significance) %>%
  mutate(Model = gsub(" ", "&", Model),
         Model = gsub("Frobenius.NNDSVD_student1", "Badges", Model),
         Model = gsub("Frobenius.NNDSVD_student2", "Struggles", Model),
         Model = gsub("Frobenius.NNDSVD_student3", "No. Students", Model),
         Model = gsub("Frobenius.NNDSVD_student4", "Activity", Model),
         Model = gsub("Frobenius.NNDSVD_teacher1", "Assessments", Model),
         Model = gsub("Frobenius.NNDSVD_teacher2", "Pedagogical Knowledge", Model),
         Model = gsub("Frobenius.NNDSVD_teacher3", "Group Instruction", Model),
         Model = gsub("Frobenius.NNDSVD_teacher4", "Curriculum Planning", Model))

# Create the table using the 'gt' package
gt_table <- model_summary %>%
  filter(!Term %in% c("rwd1","rwd2","lag1","lag2")) %>%
  mutate(Estimate = ifelse(Term %in% c("BIC", "N"),
                           sprintf("%d", as.integer(Coefficient)),
                           sprintf("%1.3f%s\n(%1.3f)",
                                   Coefficient, Significance, Std_Error)),
         Term = factor(Term, levels = c("rwd1_lag1", "rwd1_lag2", "lag2_rwd2",
                                        # "lag1", "lag2",
                                        "st", "lag1_st",
                                        "BIC", "N"),
                       labels = c("R(t-1) x \n A(t-1)",
                                  "R(t-1) x \n A(t-2)",
                                  "R(t-2) x \n A(t-2)",
                                  # "Action(t-1)", "Action(t-2)",
                                  "S(t)", "S(t) x \n A(t-1)",
                                  "BIC", "N"))) %>%
  dplyr::select(-Coefficient, -Std_Error, -Significance) %>%
  # Pivot wider to have one column per model
  pivot_wider(names_from = Model,
              values_from = Estimate,
              names_sep = "_") %>%
  dplyr::select(order(names(.))) %>%
  arrange(Term) %>%
  gt(rowname_col = "Term") %>%
  sub_missing() %>%
  tab_spanner_delim("&") %>%
  tab_header(
    title = "Fixed Effects Logistic Regression Results"
  )

# Print the table
gt_table %>% as_latex()
```

<!-- FE Logit State-based -->

```{r}
#| label: tbl-fe-results-full
#| tbl-cap: "State-Based Panel Logistic Regression Results"

load("Regressions/fe-state-subset-results.RData")

# Combine the model stats from original and restricted results for comparison
results_df <- do.call(rbind, lapply(results, function(x) {
  if (x$Lag != 2) return(NULL)
  data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = x$State,
    AUC = x$AUC,
    BIC = x$bic,
    Type = "Original"
  )
}))
# Select top 2 AUC models and top 2 BIC models for both original and restricted
top_models <- results_df %>%
  arrange(desc(AUC)) %>%
  slice_head(n = 2)
top_models <- results_df %>%
  arrange(BIC) %>%
  slice_head(n = 2) %>%
  bind_rows(top_models)

# Filter and tidy the results
tidy_fe_results <- lapply(results, function(x) {
  if (x$Lag != 2) return(NULL)
  if (top_models %>%
      filter(Action == x$Method &
             Reward == x$Reward &
             State  == x$State) %>%
      nrow() == 0) return(NULL)
  temp <- x$fecoef %>%
    as.data.frame() %>%
    rownames_to_column(var = "Term") %>%
    mutate(across(c(Estimate, `Std. Error`, `z value`, `Pr(>|z|)`),
                  ~ as.numeric(.))) %>%
    filter(Term != "(Intercept)" & Term != "week_lag")

  return(list(
    Model = paste(x$Method, x$Reward, x$State),
    Data  = temp,
    BIC   = x$bic,
    N     = length(x$recoef$Teacher.User.ID)))
})
tidy_fe_results <- tidy_fe_results[!sapply(tidy_fe_results, is.null)]

# Combine all model summaries into one dataframe
model_summary <- do.call(rbind, lapply(tidy_fe_results, function(x) {
  x_name <- unlist(strsplit(x$Model, " "))
  x_data <- x$Data %>%
    rename(Coefficient = Estimate,
           Std_Error = `Std. Error`) %>%
    mutate(
      Significance = case_when(
        `Pr(>|z|)` < .001 ~ "***",
        `Pr(>|z|)` < .01 ~ "**",
        `Pr(>|z|)` < .05 ~ "*",
        TRUE ~ ""),
      Term = gsub(x_name[1], "lag", Term),
      Term = gsub(x_name[2], "rwd", Term),
      Term = gsub(x_name[3], "st",  Term),
      Term = gsub("_", "",  Term),
      Term = gsub(":", "_",  Term)) %>%
    add_row(Term = "BIC", Coefficient = x[[3]]) %>%
    add_row(Term = "N", Coefficient = x[[4]])
  x_data$Model <- x$Model

  x_data
})) %>% dplyr::select(Model, Term, Coefficient, Std_Error, Significance) %>%
  mutate(Model = gsub(" ", "&", Model),
         Model = gsub("Frobenius.NNDSVD_student1", "Badges", Model),
         Model = gsub("Frobenius.NNDSVD_student2", "Struggles", Model),
         Model = gsub("Frobenius.NNDSVD_student3", "No. Students", Model),
         Model = gsub("Frobenius.NNDSVD_student4", "Activity", Model),
         Model = gsub("Frobenius.NNDSVD_teacher1", "Assessments", Model),
         Model = gsub("Frobenius.NNDSVD_teacher2", "Pedagogical Knowledge", Model),
         Model = gsub("Frobenius.NNDSVD_teacher3", "Group Instruction", Model),
         Model = gsub("Frobenius.NNDSVD_teacher4", "Curriculum Planning", Model))

# Create the table using the 'gt' package
gt_table <- model_summary %>%
  filter(!Term %in% c("rwd1","rwd2","lag1","lag2")) %>%
  mutate(Estimate = ifelse(Term %in% c("BIC", "N"),
                           sprintf("%d", as.integer(Coefficient)),
                           sprintf("%1.3f%s\n(%1.3f)",
                                   Coefficient, Significance, Std_Error)),
         Term = factor(Term, levels = c("rwd1_lag1", "rwd1_lag2", "lag2_rwd2",
                                        # "lag1", "lag2",
                                        "st1", "lag2_st1",
                                         "BIC", "N"),
                       labels = c("R(t-1) x \n A(t-1)",
                                  "R(t-1) x \n A(t-2)",
                                  "R(t-2) x \n A(t-2)",
                                  # "Action(t-1)", "Action(t-2)",
                                  "S(t-1)", "S(t-1) x \n A(t-2)",
                                  "BIC", "N"))) %>%
  dplyr::select(-Coefficient, -Std_Error, -Significance) %>%
  # Pivot wider to have one column per model
  pivot_wider(names_from = Model,
              values_from = Estimate,
              names_sep = "_") %>%
  dplyr::select(order(names(.))) %>%
  arrange(Term) %>%
  gt(rowname_col = "Term") %>%
  sub_missing() %>%
  tab_spanner_delim("&") %>%
  tab_header(
    title = "Fixed Effects Logistic Regression Results"
    )

# Print the table
gt_table

```

<!-- VC Logit Model Summary -->

```{r, results='asis'}
#| label: tbl-re-estimation-statefree
#| tbl-cap: "Summary statistics of coefficients from state-free mixed effects logistic regression models predicting teacher actions. Each row group represents a predictor variable: R(t-1) x A(t-1) is the interaction between the reward at time t-1 and the action taken at time t-1, R(t-1) x A(t-2) is the interaction between the reward at t-1 and action at t-2, and R(t-2) x A(t-2) is the interaction between the reward and action at time t-2. Columns represent different combinations of action, reward, and state, where the actions are teacher NMF components, and the rewards are student NMF components. Cell values show the mean, standard deviation, quartiles, and proportion of teachers that show RL-like coefficients (i.e., those in the direction predicted by reinforcement learning: positive for positive rewards and negative for negative rewards)."

load("Regressions/me-standardized-coef.RData")

large_coef_threshold <- 10

filtered_results <- lapply(results, function(x) {
  if (!is.null(x$State)) return(NULL)
  if (!x$convergence) return(NULL)
  temp <- data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = "None"
  ) %>%
  mutate(across(c(Action, Reward),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Badges",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggles",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "Activity",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Pedagogical Knowledge",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Group Instruction",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Curriculum Planning",
                            .default = .)))
  # if(nrow(
  #   temp %>%
  #   semi_join(selected_models, by = c("Action", "Reward", "State"))
  #   ) == 0) return(NULL)
  if(x$AUC < 0.5) return(NULL)
  x$recoef <- do.call(cbind, x$recoef[-c(1, length(x$recoef))]) %>%
    as.data.frame() %>%
    filter(if_all(everything(),
                  ~ . >= find_hdr(.)[1] &
                    . <= find_hdr(.)[2])) %>%
    rownames_to_column(var = "Teacher.User.ID")
  if (any(abs(colMeans(x$recoef[,-1])) > large_coef_threshold)) return(NULL)

  return(list(
    Action = temp$Action,
    Reward = temp$Reward,
    AUCmodel = x$AUC,
    BICmodel = x$bic,
    coef = data.frame(
      Teacher.User.ID = as.integer(x$recoef[,1]),
      lag1 = x$recoef[,3],
      lag2 = x$recoef[,7],
      rwd1 = x$recoef[,2],
      rwd2 = x$recoef[,6],
      rwd1_lag1 = x$recoef[,4],
      rwd1_lag2 = x$recoef[,5],
      rwd2_lag2 = x$recoef[,8]
    ) %>%
      inner_join(x$perform_df %>%
                   select(Teacher.User.ID, auc_out, logLik_ind) %>%
                   filter(if_all(everything(), ~ . >= find_hdr(.)[1] &
                                                 . <= find_hdr(.)[2])) %>%
                   filter(0.5 < auc_out & auc_out < 1),
                 by = "Teacher.User.ID")
  ))
})
filtered_results <- filtered_results[!sapply(filtered_results, is.null)]

# Gather AUC and BIC and calculate thresholds
all_auc_bic <- do.call(
  rbind,
  lapply(filtered_results, function(x) {
    return(c(x$AUCmodel, x$BICmodel))
    }))
auc75 <- quantile(all_auc_bic[,1], 0.75, na.rm = TRUE)
bic25 <- quantile(all_auc_bic[,2], 0.25, na.rm = TRUE)

model_summary <- do.call(rbind, lapply(filtered_results, function(x) {
  summary <- data.frame(
    Action = x$Action,
    Reward = x$Reward,
    AUC = x$AUCmodel,
    BIC = x$BICmodel
  )
  summary$lag1 = list(x$coef$lag1)
  summary$lag2 = list(x$coef$lag2)
  summary$rwd1_lag1 = list(x$coef$rwd1_lag1)
  summary$rwd1_lag2 = list(x$coef$rwd1_lag2)
  summary$rwd2_lag2 = list(x$coef$rwd2_lag2)
  summary$auc_ind = list(x$coef$auc_out)
  summary$logLik_ind = list(x$coef$logLik_ind)

  summary <- summary %>%
    # # Standardize the coefficients by dividing by the standard deviation
    # mutate(across(c(rwd1_lag1, rwd1_lag2, rwd2_lag2, st, st_lag1),
    #               ~ lapply(., function(x) x / sd(x, na.rm = T)))) %>%
    # # Fill in x$State == "None" with 0s
    # mutate(across(c(st, st_lag1), ~ if_else(is.na(.), list(0), .))) %>%
    mutate(n_teachers = lapply(summary$lag1, length)) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    auc_ind, logLik_ind),
                  ~ lapply(., mean, na.rm = T),
                  .names = "{.col}.mean")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., sd, na.rm = T),
                  .names = "{.col}.sd")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., quantile, 0.25, names = F),
                  .names = "{.col}.q1")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., median, na.rm = T),
                  .names = "{.col}.median")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., quantile, 0.75, names = F),
                  .names = "{.col}.q3")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2
                    ),
                  ~ lapply(., function(x) mean(x > 0, na.rm = T)),
                  .names = "{.col}.pos")) %>%
    select(-c(lag1, lag2,
              rwd1_lag1,
              rwd1_lag2,
              rwd2_lag2,
              auc_ind, logLik_ind, auc_ind.mean, logLik_ind.mean))

  return(summary)
}))

# Transform the model_summary dataframe to a long format for plotting
model_summary_long <- model_summary %>%
  mutate(across(!c(Action, Reward), as.numeric)) %>%
  mutate(across(dplyr::ends_with(".pos") & dplyr::starts_with("rwd"),
                ~ case_when(Reward == "Struggle" ~ 1 - ., .default = .))) %>%
  filter(AUC >= auc75 | BIC <= bic25) %>%
  select(!c(AUC, BIC)) %>%
  pivot_longer(cols = -c(Action, Reward),
               names_to = c("variable", "statistic"), values_to = "value",
               names_sep = "\\.") %>%
  arrange(Action, Reward) %>%
  pivot_wider(names_from = c(Action, Reward), values_from = c(value))

max <- model_summary_long %>%
  filter(statistic == "pos" & !variable %in% c("lag1","lag2")) %>%
  rowwise() %>%
  # Remove columns that do not have the max of at least 1 row
  mutate(max_val = max(c_across(is.numeric), na.rm = T)) %>%
  ungroup() %>%
  mutate(across(is.numeric, ~ if_else(. == max_val, ., NA))) %>%
  select(where(~ !all(is.na(.)))) %>%
  select(-c("max_val","statistic")) %>%
  mutate(across(is.numeric, ~ !is.na(.))) %>%
  pivot_longer(cols = -variable, names_to = "model", values_to = "keep") %>%
  filter(keep) %>% select(-keep)

model_summary_long %>%
  filter(!variable %in% c("lag1", "lag2")) %>%
  mutate(statistic = factor(statistic,
                            levels = c("mean", "sd", "q1", "median", "q3", "pos"),
                            labels = c("Mean", "SD", "Q1", "Median", "Q3", "RL-like")),
         variable = factor(variable,
                           levels = c(
                             # "lag1", "lag2",
                             "rwd1_lag1",
                             "rwd1_lag2",
                             "rwd2_lag2",
                             "n_teachers"),
                           labels = c(
                             # "Action(t-1)",
                             # "Action(t-2)",
                             "R(t-1) x \n A(t-1)",
                             "R(t-1) x \n A(t-2)",
                             "R(t-2) x \n A(t-2)",
                             "N"
                             ))) %>%
  select(order(names(model_summary_long))) %>%
  group_by(variable) %>%
  gt(rowname_col = "statistic", row_group_as_column = T) %>%
  row_group_order(groups = c(
    # "Action(t-1)",
    # "Action(t-2)",
    "R(t-1) x \n A(t-1)",
    "R(t-1) x \n A(t-2)",
    "R(t-2) x \n A(t-2)",
    "N")) %>%
  tab_stubhead(label = "Action \n Reward") %>%
  fmt_markdown(columns = "variable") %>%
  fmt_number(decimals = 2) %>%
  fmt_integer(rows = variable == "N") %>%
  fmt_percent(rows = statistic == "RL-like") %>%
  # cols_hide(starts_with("Assessments_No. Students")) %>%
  sub_missing() %>%
  tab_spanner_delim("_") %>%
  tab_header(
    title = "Summary of Model Coefficients"
  ) %>%
  tab_options(
    table.font.size = "small"
  ) %>%
  as_latex()

```

{{< pagebreak >}}

```{r, results='asis'}
#| label: tbl-re-estimation
#| tbl-cap: "Summary statistics of coefficients from state-based mixed effects logistic regression models predicting teacher actions. Each row group represents a predictor variable: R(t-1) x A(t-1) is the interaction between the reward at time t-1 and the action taken at time t-1, R(t-1) x A(t-2) is the interaction between the reward at t-1 and action at t-2, R(t-2) x A(t-2) is the interaction between the reward and action at time t-2, S(t) is the state at time t, and S(t) x A(t-1) is the interaction between the current state and the previous action. Columns represent different combinations of action, reward, and state, where the actions are teacher NMF components, and the rewards and states are student NMF components (but the state is never the same component as its associated reward). Cell values show the mean, standard deviation, quartiles, and proportion of teachers that show RL-like coefficients (i.e., those in the direction predicted by reinforcement learning: positive for positive rewards and negative for negative rewards)."

load("Regressions/me-standardized-coef.RData")

large_coef_threshold <- 10

filtered_results <- lapply(results, function(x) {
  if (is.null(x$State)) return(NULL)
  if (!x$convergence) return(NULL)
  temp <- data.frame(
    Action = x$Method,
    Reward = x$Reward,
    State = x$State
  ) %>%
  mutate(across(c(Action, Reward, State),
                ~ case_when(. == "Frobenius.NNDSVD_student1" ~ "Badges",
                            . == "Frobenius.NNDSVD_student2" ~ "Struggles",
                            . == "Frobenius.NNDSVD_student3" ~ "No. Students",
                            . == "Frobenius.NNDSVD_student4" ~ "Activity",
                            . == "Frobenius.NNDSVD_teacher1" ~ "Assessments",
                            . == "Frobenius.NNDSVD_teacher2" ~ "Pedagogical Knowledge",
                            . == "Frobenius.NNDSVD_teacher3" ~ "Group Instruction",
                            . == "Frobenius.NNDSVD_teacher4" ~ "Curriculum Planning",
                            .default = .)))
  # if(nrow(
  #   temp %>%
  #   semi_join(selected_models, by = c("Action", "Reward", "State"))
  #   ) == 0) return(NULL)
  if(x$AUC < 0.5) return(NULL)
  x$recoef <- do.call(cbind, x$recoef[-c(1, length(x$recoef))]) %>%
    as.data.frame() %>%
    filter(if_all(everything(),
                  ~ . >= find_hdr(.)[1] & . <= find_hdr(.)[2])) %>%
    rownames_to_column(var = "Teacher.User.ID")
  if (any(abs(colMeans(x$recoef[,-1])) > large_coef_threshold)) return(NULL)

  return(list(
    Action = temp$Action,
    Reward = temp$Reward,
    State = temp$State,
    AUCmodel = x$AUC,
    BICmodel = x$bic,
    coef = data.frame(
      Teacher.User.ID = as.integer(x$recoef[,1]),
      lag1 = x$recoef[,3],
      lag2 = x$recoef[,7],
      rwd1 = x$recoef[,2],
      rwd2 = x$recoef[,6],
      rwd1_lag1 = x$recoef[,4],
      rwd1_lag2 = x$recoef[,5],
      rwd2_lag2 = x$recoef[,8],
      st = x$recoef[,9],
      st_lag1 = x$recoef[,10]
    ) %>%
      inner_join(x$perform_df %>%
                   select(Teacher.User.ID, auc_out, logLik_ind) %>%
                   filter(auc_out >= 0.5 & auc_out < 1),
                 by = "Teacher.User.ID")
  ))
  })
filtered_results <- filtered_results[!sapply(filtered_results, is.null)]

# Gather AUC and BIC and calculate thresholds
all_auc_bic <- do.call(
  rbind,
  lapply(filtered_results, function(x) {
    return(c(x$AUCmodel, x$BICmodel))
    }))
auc75 <- quantile(all_auc_bic[,1], 0.75, na.rm = TRUE)
bic25 <- quantile(all_auc_bic[,2], 0.25, na.rm = TRUE)

model_summary <- do.call(rbind, lapply(filtered_results, function(x) {
  summary <- data.frame(
    Action = x$Action,
    Reward = x$Reward,
    State = x$State,
    AUC = x$AUCmodel,
    BIC = x$BICmodel
  )
  summary$lag1 = list(x$coef$lag1)
  summary$lag2 = list(x$coef$lag2)
  summary$rwd1_lag1 = list(x$coef$rwd1_lag1)
  summary$rwd1_lag2 = list(x$coef$rwd1_lag2)
  summary$rwd2_lag2 = list(x$coef$rwd2_lag2)
  summary$st = ifelse(x$State == "None", list(0), list(x$coef$st))
  summary$st_lag1 = ifelse(x$State == "None", list(0), list(x$coef$st_lag1))
  summary$auc_ind = list(x$coef$auc_out)
  summary$logLik_ind = list(x$coef$logLik_ind)

  summary <- summary %>%
    # # Standardize the coefficients by dividing by the standard deviation
    # mutate(across(c(rwd1_lag1, rwd1_lag2, rwd2_lag2, st, st_lag1),
    #               ~ lapply(., function(x) x / sd(x, na.rm = T)))) %>%
    # # Fill in x$State == "None" with 0s
    # mutate(across(c(st, st_lag1), ~ if_else(is.na(.), list(0), .))) %>%
    mutate(n_teachers = lapply(summary$lag1, length)) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1,
                    auc_ind, logLik_ind),
                  ~ lapply(., mean, na.rm = T),
                  .names = "{.col}.mean")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., sd, na.rm = T),
                  .names = "{.col}.sd")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., quantile, 0.25, names = F),
                  .names = "{.col}.q1")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., median, na.rm = T),
                  .names = "{.col}.median")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., quantile, 0.75, names = F),
                  .names = "{.col}.q3")) %>%
    mutate(across(c(lag1, lag2,
                    rwd1_lag1,
                    rwd1_lag2,
                    rwd2_lag2,
                    st, st_lag1),
                  ~ lapply(., function(x) mean(x > 0, na.rm = T)),
                  .names = "{.col}.pos")) %>%
    select(-c(lag1, lag2,
              rwd1_lag1,
              rwd1_lag2,
              rwd2_lag2,
              st, st_lag1,
              auc_ind, logLik_ind, auc_ind.mean, logLik_ind.mean))

  return(summary)
}))

# Transform the model_summary dataframe to a long format for plotting
model_summary_long <- model_summary %>%
  mutate(across(!c(Action, Reward, State), as.numeric)) %>%
  mutate(across(dplyr::ends_with(".pos") & dplyr::starts_with("st"),
                ~ case_when(State == "Struggle" ~ 1 - ., .default = .)),
         across(dplyr::ends_with(".pos") & dplyr::starts_with("rwd"),
                ~ case_when(Reward == "Struggle" ~ 1 - ., .default = .))) %>%
  filter(AUC >= auc75 | BIC <= bic25) %>%
  select(!c(AUC, BIC)) %>%
  mutate(across(dplyr::starts_with("st", ignore.case = F),
                ~ if_else(State == "None", NA, .))) %>%
  pivot_longer(cols = -c(Action, Reward, State),
               names_to = c("variable", "statistic"), values_to = "value",
               names_sep = "\\.") %>%
  arrange(Action, Reward, State) %>%
  pivot_wider(names_from = c(Action, Reward, State), values_from = c(value))

max <- model_summary_long %>%
  filter(statistic == "pos" & !variable %in% c("lag1","lag2")) %>%
  rowwise() %>%
  # Remove columns that do not have the max of at least 1 row
  mutate(max_val = max(c_across(is.numeric), na.rm = T)) %>%
  ungroup() %>%
  mutate(across(is.numeric, ~ if_else(. == max_val, ., NA))) %>%
  select(where(~ !all(is.na(.)))) %>%
  select(-c("max_val","statistic")) %>%
  mutate(across(is.numeric, ~ !is.na(.))) %>%
  pivot_longer(cols = -variable, names_to = "model", values_to = "keep") %>%
  filter(keep) %>% select(-keep)

model_summary_long %>%
  filter(!variable %in% c("lag1", "lag2")) %>%
  mutate(statistic = factor(statistic,
                            levels = c("mean", "sd", "q1", "median", "q3", "pos"),
                            labels = c("Mean", "SD", "Q1", "Median", "Q3", "RL-like")),
         variable = factor(variable,
                           levels = c(
                             # "lag1", "lag2",
                             "rwd1_lag1",
                             "rwd1_lag2",
                             "rwd2_lag2",
                             "st", "st_lag1",
                             "n_teachers"),
                           labels = c(
                             # "Action(t-1)",
                             # "Action(t-2)",
                             "R(t-1) x \n A(t-1)",
                             "R(t-1) x \n A(t-2)",
                             "R(t-2) x \n A(t-2)",
                             "S(t)",
                             "S(t) x \n A(t-1)",
                             "N"
                             ))) %>%
  select(order(names(model_summary_long))) %>%
  group_by(variable) %>%
  gt(rowname_col = "statistic", row_group_as_column = T) %>%
  row_group_order(groups = c(
    # "Action(t-1)",
    # "Action(t-2)",
    "R(t-1) x \n A(t-1)",
    "R(t-1) x \n A(t-2)",
    "R(t-2) x \n A(t-2)",
    "S(t)",
    "S(t) x \n A(t-1)",
    "N")) %>%
  tab_stubhead(label = "Action \n Reward \n State") %>%
  fmt_markdown(columns = "variable") %>%
  fmt_number(decimals = 2) %>%
  fmt_integer(rows = variable == "No. Teachers") %>%
  fmt_percent(rows = statistic == "RL-like") %>%
  # cols_hide(starts_with("Assessments_No. Students")) %>%
  sub_missing() %>%
  tab_spanner_delim("_") %>%
  tab_header(
    title = "Summary of Model Coefficients"
  ) %>%
  tab_options(
    table.font.size = "small"
  ) %>%
  as_latex()

```

<!-- QL Model Summary -->

<!-- AC Model Summary -->

```{r}
#| label: tbl-CBM-teachers
#| tbl-cap: "Comparison of Teacher Characteristics by Model Type. The table presents the differences in means across various characteristics between teachers classified into Logit or Actor-Critic models."

# T-tests for differences in means by model type
heterogeneity %>%
  filter(!is.na(Teacher.User.ID)) %>%
  dplyr::select(Model, `Charter School`, `Paid Account`, Poverty,
         `No. of Weeks`, `Total Students`, `No. of Classes`,
         `Active Students`, `Avg. Badges`, `Avg. Tower Alerts`,
         Action, Reward, State) %>%
  tbl_summary(
    by = Model,  # Grouping variable
    type = list(`No. of Classes` ~ "continuous"),
    statistic = list(all_continuous() ~ "{mean} ({sd})",
                     all_categorical() ~ "{p}%")
  ) %>%
  add_n() %>%
  add_difference() %>%
  modify_column_hide(columns = ci) %>%
  add_q()

```

```{r}
#| label: tbl-CBM-teachers-full
#| tbl-cap: "Comparison of Teacher Characteristics by Model Fit. The table presents the differences in means across various characteristics between teachers classified into Good Fit or Bad Fit groups."

# T-tests for differences in means by model type and fit group
full_data %>%
  dplyr::select(FitGroup, `Charter School`, `Paid Account`, Poverty,
         `No. of Weeks`, `Total Students`, `No. of Classes`,
         `Active Students`, `Avg. Badges`, `Avg. Tower Alerts`,
         Action, Reward, State) %>%
  tbl_summary(
    by = c("FitGroup"), # Grouping variables
    type = list(`No. of Classes` ~ "continuous"),
    statistic = list(all_continuous() ~ "{mean} ({sd})",
                     all_categorical() ~ "{p}%")
  ) %>%
  add_n() %>%
  add_difference() %>%
  modify_column_hide(columns = ci) %>%
  add_q()


```

```{r, results='asis'}
#| label: tbl-optimality
#| tbl-cap: "Impact of RL parameters on Weekly Badges and Tower Alerts. Six linear regression models were used to examine the correlations between a teacher's RL parameters and two measures of student engagement: average weekly Badges earned per student (Models 1-3) and average weekly Tower Alerts per student (Models 4-6). Models 2-3 and 5-6 also control for other variables including number of active students, number of classes taught, grade level, weeks, poverty level, income level, whether the school is a charter school, and whether the school has a paid account. Coefficients and standard errors are provided for each parameter in each model."

ordered_factor <- function(fact_var) {
  categories <- levels(fact_var)
  n_cat <- length(categories)
  cont <- matrix(data = 0, nrow = n_cat, ncol = (n_cat - 1))
  cont[col(cont) < row(cont)] <- 1
  rownames(cont) <- categories
  colnames(cont) <- paste(categories[2:n_cat],
                          categories[1:(n_cat - 1)],
                          sep = " vs. ")
  contrasts(fact_var) <- cont
  return(fact_var)
}
full_data <- full_data %>%
  dplyr::mutate(
    Income = factor(Income, ordered = TRUE),
    grade = factor(grade, ordered = TRUE,
                   levels = c("Kindergarten",
                              "1st", "2nd", "3rd", "4th", "5th")),
    Poverty = factor(Poverty, ordered = TRUE)
  ) %>%
  dplyr::mutate(
    Income = ordered_factor(Income),
    grade = ordered_factor(grade),
    Poverty = ordered_factor(Poverty)
  )

# Run regression models using the median split groups
model1 <- lm(`Avg. Badges` ~ alpha + gamma + tau + cost + ev_init,
             data = full_data)
model2 <- lm(`Avg. Badges` ~ alpha + gamma + tau + cost + ev_init +
               BIC + `No. of Weeks` + `Total Students` + `No. of Classes`,
             data = full_data)
model3 <- lm(`Avg. Badges` ~ alpha + gamma + tau + cost + ev_init +
               BIC + `No. of Weeks` + `Total Students` + `No. of Classes` +
               grade +  Poverty + `Charter School` + `Paid Account`,
             data = full_data)
model4 <- lm(`Avg. Tower Alerts` ~ alpha + gamma + tau + cost + ev_init,
             data = full_data)
model5 <- lm(`Avg. Tower Alerts` ~ alpha + gamma + tau + cost + ev_init  +
               BIC + `No. of Weeks` + `Total Students` + `No. of Classes`,
             data = full_data)
model6 <- lm(`Avg. Tower Alerts` ~ alpha + gamma + tau + cost + ev_init +
               BIC + `No. of Weeks` + `Total Students` + `No. of Classes` +
               grade +  Poverty + `Charter School` + `Paid Account`,
             data = full_data)

# # Gather tidy summaries of each model
# model_summaries <- list(
#   tidy(model1), tidy(model2), tidy(model3),
#   tidy(model4), tidy(model5), tidy(model6)
# )
#
# # Create a function to format model summaries for gt
# format_for_gt <- function(model_summary, model_name) {
#   model_summary %>%
#     mutate(model = model_name) %>%
#     dplyr::select(model, term, estimate, std.error, statistic, p.value)
# }
#
# # Apply the function to each model summary with an appropriate model name
# formatted_summaries <- lapply(seq_along(model_summaries), function(i) {
#   format_for_gt(model_summaries[[i]], paste("Model", i))
# }) %>%
#   bind_rows()
#
# # Generate a gt table
# table_gt <- gt(formatted_summaries) %>%
#   cols_label(
#     model = "Model",
#     term = "Term",
#     estimate = "Estimate",
#     std.error = "Standard Error",
#     statistic = "T Value",
#     p.value = "P Value"
#   ) %>%
#   tab_spanner(
#     label = "Statistics",
#     columns = c(estimate, std.error, statistic, p.value)
#   ) %>%
#   fmt_number(
#     columns = c(estimate, std.error, statistic, p.value),
#     decimals = 3
#   ) %>%
#   data_color(
#     columns = c(p.value),
#     colors = scales::col_numeric(
#       palette = c("red", "yellow", "green"),
#       domain = c(0, 0.05)
#     )
#   )
#
# # Print the table
# print(table_gt)

# Create table with stargazer
stargazer(model1, model2, model3,
          type = "latex",
          header=FALSE,
          align = TRUE,
          covariate.labels = c("$\\alpha$", "$\\gamma$",
                               "$\\tau$", "Cost", "Starting Q-value",
                               "No. of Weeks", "No. of Students",
                               "No. of Classes",
                               "Charter School", "Paid Zearn Account"),
          omit = c("BIC", "grade", "Poverty"),
          dep.var.labels = "Badges",
          star.cutoffs = c(.05, .01, .001),
          star.char = c("*", "**", "***"),
          add.lines = list(c("Control for BIC",
                             "", "Yes", "Yes", "", "Yes", "Yes"),
                           c("Control for Grade Level",
                             "", "", "Yes", "", "", "Yes"),
                           c("Control for Poverty Level",
                             "", "", "Yes", "", "", "Yes")))

```

```{r, results='asis'}
#| label: tbl-optimality-2
#| tbl-cap: "Impact of RL parameters on Weekly Badges and Tower Alerts. Six linear regression models were used to examine the correlations between a teacher's RL parameters and two measures of student engagement: average weekly Badges earned per student (Models 1-3) and average weekly Tower Alerts per student (Models 4-6). Models 2-3 and 5-6 also control for other variables including number of active students, number of classes taught, grade level, weeks, poverty level, income level, whether the school is a charter school, and whether the school has a paid account. Coefficients and standard errors are provided for each parameter in each model."

# Create table with stargazer
stargazer(model4, model5, model6,
          type = "latex",
          header=FALSE,
          align = TRUE,
          covariate.labels = c("$\\alpha$", "$\\gamma$",
                               "$\\tau$", "Cost", "Starting Q-value",
                               "No. of Weeks", "No. of Students",
                               "No. of Classes",
                               "Charter School", "Paid Zearn Account"),
          omit = c("BIC", "grade", "Poverty"),
          dep.var.labels = "Tower Alerts",
          star.cutoffs = c(.05, .01, .001),
          star.char = c("*", "**", "***"),
          add.lines = list(c("Control for BIC",
                             "", "Yes", "Yes", "", "Yes", "Yes"),
                           c("Control for Grade Level",
                             "", "", "Yes", "", "", "Yes"),
                           c("Control for Poverty Level",
                             "", "", "Yes", "", "", "Yes")))

```

```{r}
#| label: fig-heterogeneity-reg
#| fig-cap: "Predicting classroom variables with RL parameters. The data comes from a comprehensive set of classroom records, including income level, poverty level, charter school status, whether a school account is paid, and the number of classes each teaches. These classroom variables (columns) are regressed on the estimated cost parameter, the discount rate, the learning rate, and the inverse temperature. Each cell in the heatmap represents the estimate of a linear, Poisson, or logistic regression model, depending on the variable type. For ordinal classroom variables (Income Level and Poverty Level), ordered logistic regression (proportional odds model) is used, while for binary variables (Charter School, Paid School Account), logistic regression is applied. The number of classes by each teacher, being a count data, is modeled with Poisson regression. The coefficients are scaled estimates of the effect of each parameter on the respective classroom variable. The asterisks indicate the level of statistical significance based on p-values (p < .1; p < .01; p < .001). The coefficients are color-coded with a gradient from light blue (negative) to white (zero) to dark blue (positive)."

# Ensure the predictors are scaled
predictors <- c("alpha", "gamma", "tau", "ev_init", "cost")
# predictors <- c("alpha_w", "alpha_theta", "gamma", "tau",
#                 "cost", "theta_init", "w_init")
full_data[predictors] <- scale(full_data[predictors])

# Define responses for which models will be run
responses <- c("Income", "Poverty", "`Paid Account`", "`No. of Weeks`")
predictors_formula <- paste(predictors, collapse = " + ")

# Run models for each response and extract coefficients
coef_matrix <- map_dfr(responses, function(response) {
  formula <- as.formula(paste0(response, " ~ ", predictors_formula))
  if (response %in% c("Income", "Poverty")) {
    model <- polr(formula, data = full_data, Hess = TRUE)
  } else if (response == "`No. of Weeks`") {
    model <- glm(formula, data = full_data, family = poisson(link = "log"))
  } else {
    model <- glm(formula, data = full_data, family = binomial(link = "logit"))
  }

  tidy(model, p.values = TRUE) %>%
    filter(term %in% predictors) %>%
    dplyr::select(term, estimate, p.value) %>%
    mutate(response = response,
           sig = case_when(
             p.value < 0.001 ~ "***",
             p.value < 0.01 ~ "**",
             p.value < 0.05 ~ "*",
             TRUE ~ ""
           ))
})

# Convert to wide format for plotting
coef_matrix <- coef_matrix %>%
  pivot_wider(names_from = term, values_from = c(estimate, p.value, sig)) %>%
  mutate(response = factor(response, levels = responses)) %>%
  arrange(response)

coef_matrix_long <- coef_matrix %>%
  pivot_longer(cols = starts_with("estimate"), names_to = "term", values_to = "estimate") %>%
  pivot_longer(cols = starts_with("sig"), names_to = "term_sig", values_to = "sig")
# Clean up term names
coef_matrix_long$term <- str_replace(coef_matrix_long$term, "estimate_", "")
coef_matrix_long$term_sig <- str_replace(coef_matrix_long$term_sig, "sig_", "")
# Make sure the term columns match
coef_matrix_long <- coef_matrix_long[coef_matrix_long$term == coef_matrix_long$term_sig,]

# Plot the heatmap
ggplot(coef_matrix_long, aes(x = response, y = term, fill = as.numeric(estimate))) +
  geom_tile() +
  geom_text(aes(label = paste0(signif(as.numeric(estimate), 2), sig)), size = 3) +
  scale_fill_gradientn(colours = RColorBrewer::brewer.pal(11, "RdYlBu"),
                       limits = c(-1, 1),
                       name = "Estimate") +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
#| label: fig-heterogeneity-pcor
#| fig-cap: "Partial correlations between classroom variables and RL parameters. The data comes from a comprehensive set of classroom records, including income level, poverty level, charter school status, whether a school account is paid, and the number of classes each teaches. These classroom variables (columns) are correlated with the estimated cost parameter, the discount rate, the learning rate, and the inverse temperature, while controlling for the other predictors. Each cell in the heatmap represents the partial correlation coefficient. The asterisks indicate the level of statistical significance based on p-values (p < .1; p < .01; p < .001). The coefficients are color-coded with a gradient from light blue (negative) to white (zero) to dark blue (positive)."

# Ensure the predictors are scaled
predictors <- c("alpha", "gamma", "tau", "ev_init", "cost")

# Define responses for which partial correlations will be calculated
responses <- c("Income", "Poverty",
               "Paid Account", "No. of Weeks")
pcor_list <- pcor(full_data[,c(predictors, responses)] %>%
                    mutate(across(everything(),as.numeric)) %>%
                    na.omit(), method = c("spearman"))
coef_matrix <- pcor_list$estimate[responses, predictors]
pval_matrix <- pcor_list$p.value[responses, predictors]

# Create a dataframe for plotting
coef_matrix_long <- coef_matrix %>%
  as.data.frame() %>%
  rownames_to_column("response") %>%
  pivot_longer(cols = -response, names_to = "term", values_to = "estimate")

pval_matrix_long <- pval_matrix %>%
  as.data.frame() %>%
  rownames_to_column("response") %>%
  pivot_longer(cols = -response, names_to = "term", values_to = "p.value")

coef_matrix_plot <- coef_matrix_long %>%
  left_join(pval_matrix_long, by = c("response", "term")) %>%
  mutate(
    response = factor(response, levels = responses),
    sig = case_when(
      p.value < 0.001 ~ "***",
      p.value < 0.01 ~ "**",
      p.value < 0.05 ~ "*",
      TRUE ~ ""
    )
  )

# Plot the heatmap
ggplot(coef_matrix_plot, aes(x = response, y = term, fill = estimate)) +
  geom_tile() +
  geom_text(aes(label = paste0(signif(estimate, 2), sig)), size = 3) +
  scale_fill_gradientn(colours = RColorBrewer::brewer.pal(11, "RdYlBu"),
                       limits = c(-1, 1),
                       name = "Partial Correlation") +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Supplemental Figures {#sec-supp-fig}

![Zearn Student Portal](images/student-feed.PNG){#fig-st-portal fig-align="center"}

![Professional Development Calendar](images/PD-calendar.jpg){#fig-prof-dev fig-align="center"}

```{r}
#| eval: false
#| echo: false
#| label: fig-raw-data
#| fig-cap: "The raw data from Zearn's platform, with each row corresponding to a different timestamp for a teacher action. The columns represent the different variables, including the number of minutes spent on the platform, the number of badges earned, and the number of boosts used. The data is organized by classroom, with each classroom having a different color. The data is also organized by week, with each week having a different shape. The data is organized by teacher, with each teacher having a different line type. The data is organized by school, with each school having a different line color."

```

{{< pagebreak >}}

```{r}
#| label: fig-income-dist
#| fig-cap: "Distributions of School Socioeconomic Profiles. The first graph categorizes schools into three groups based on the percentage of students eligible for free or reduced-price lunch (FRPL): low-poverty (0-40%), mid-poverty (40-75%), and high-poverty (over 75%). The second graph presents the distribution of median incomes for a school's associated region."
#| fig-subcap:
#|   - "School Poverty Distribution"
#|   - "Median Income Distribution"
#| layout-ncol: 2

poverty_plot
income_plot

```

```{r}
#| label: fig-teachers-map
#| fig-cap: "Geographic distribution of Zearn teachers across parishes in Louisiana. The color gradient represents the density of teachers, with darker hues indicating a higher concentration of educators using Zearn in each parish. The map also labels the top five cities where Zearn adoption is most prevalent."

map_LA

```

```{r}
#| label: fig-logins-week
#| fig-cap: "Total number of student logins over the 2019-2020 school year. The chart depicts the connection between academic schedules and platform engagement. Each bar represents a week, with peaks corresponding to active school weeks and troughs aligning with major holiday periods (e.g., Thanksgiving and Winter Break)."

logins_week
```
